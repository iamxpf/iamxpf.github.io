<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon32x32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon16x16.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <link rel="alternate" href="/atom.xml" title="八戒大强攻" type="application/atom+xml">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">
  <meta name="google-site-verification" content="Tj66HjG9CPxGLh7uYasY8iP95HLgLgbff61lHXIZMJQ">
  <meta name="baidu-site-verification" content="oKzS1ngP4YpcIQWZ">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.5.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":"default"},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":true},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: true,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="RNN原理（0）标记符号含义输入的样本为（X,Y），其中X的维度为（m，T_x，n），m代表样本个数，T_x代表时间步，n代表字典长度。RNN中每个样本采用独热向量表示，在训练集中选择不重复数据作为字典，根据字典构造每个独热向量（当前词在字典中的位置标为1，其余标为0）。（1）网络结构RNN网络结果如图，在t时刻输入当前时刻的x值，经过隐藏层，通过计算前一层的激活值和输入值x得到预测的y。">
<meta name="keywords" content="神经网络,循环神经网络">
<meta property="og:type" content="article">
<meta property="og:title" content="序列模型RNN——基本网络结构">
<meta property="og:url" content="https:&#x2F;&#x2F;www.xiapf.com&#x2F;blogs&#x2F;rnn1&#x2F;index.html">
<meta property="og:site_name" content="八戒大强攻">
<meta property="og:description" content="RNN原理（0）标记符号含义输入的样本为（X,Y），其中X的维度为（m，T_x，n），m代表样本个数，T_x代表时间步，n代表字典长度。RNN中每个样本采用独热向量表示，在训练集中选择不重复数据作为字典，根据字典构造每个独热向量（当前词在字典中的位置标为1，其余标为0）。（1）网络结构RNN网络结果如图，在t时刻输入当前时刻的x值，经过隐藏层，通过计算前一层的激活值和输入值x得到预测的y。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;iamxpf&#x2F;pageImage&#x2F;images&#x2F;20200618165122.png">
<meta property="og:image" content="https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;iamxpf&#x2F;pageImage&#x2F;images&#x2F;20200618195021.png">
<meta property="og:image" content="https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;iamxpf&#x2F;pageImage&#x2F;images&#x2F;20200618195910.png">
<meta property="og:image" content="https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;iamxpf&#x2F;pageImage&#x2F;images&#x2F;20200618200114.png">
<meta property="og:image" content="https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;iamxpf&#x2F;pageImage&#x2F;images&#x2F;20200618165220.png">
<meta property="og:image" content="https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;iamxpf&#x2F;pageImage&#x2F;images&#x2F;20200618170023.png">
<meta property="og:image" content="https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;iamxpf&#x2F;pageImage&#x2F;images&#x2F;20200618170626.png">
<meta property="og:image" content="https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;iamxpf&#x2F;pageImage&#x2F;images&#x2F;20200618170550.png">
<meta property="og:image" content="https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;iamxpf&#x2F;pageImage&#x2F;images&#x2F;20200618170127.png">
<meta property="og:image" content="https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;iamxpf&#x2F;pageImage&#x2F;images&#x2F;20200618170142.png">
<meta property="og:image" content="https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;iamxpf&#x2F;pageImage&#x2F;images&#x2F;20200618170213.png">
<meta property="og:image" content="https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;iamxpf&#x2F;pageImage&#x2F;images&#x2F;20200618170231.png">
<meta property="og:image" content="https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;iamxpf&#x2F;pageImage&#x2F;images&#x2F;20200618170407.png">
<meta property="og:image" content="https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;iamxpf&#x2F;pageImage&#x2F;images&#x2F;20200618170430.png">
<meta property="og:updated_time" content="2020-06-18T14:13:17.534Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;iamxpf&#x2F;pageImage&#x2F;images&#x2F;20200618165122.png">

<link rel="canonical" href="https://www.xiapf.com/blogs/rnn1/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>序列模型RNN——基本网络结构 | 八戒大强攻</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">八戒大强攻</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <h1 class="site-subtitle" itemprop="description">好久没吃人肉了</h1>
      
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">38</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">11</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">65</span></a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-fw fa-sitemap"></i>站点地图</a>

  </li>
        <li class="menu-item menu-item-heart">

    <a href="/lover/" rel="section"><i class="fa fa-fw fa-heart"></i>heart</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/iamxpf" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="external nofollow noopener noreferrer" target="_blank"><svg width="80" height="80" viewbox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.xiapf.com/blogs/rnn1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/touxiang.jpg">
      <meta itemprop="name" content="Xiapf">
      <meta itemprop="description" content="好好学习，天天向上">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="八戒大强攻">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          序列模型RNN——基本网络结构
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-06-18 22:10:26 / 修改时间：22:13:17" itemprop="dateCreated datePublished" datetime="2020-06-18T22:10:26+08:00">2020-06-18</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" itemprop="url" rel="index">
                    <span itemprop="name">神经网络</span>
                  </a>
                </span>
            </span>

          
            <span id="/blogs/rnn1/" class="post-meta-item leancloud_visitors" data-flag-title="序列模型RNN——基本网络结构" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">评论数：</span>
    
    <a title="valine" href="/blogs/rnn1/#comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/blogs/rnn1/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>9.6k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>9 分钟</span>
            </span>
		  
			

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="RNN原理"><a href="#RNN原理" class="headerlink" title="RNN原理"></a>RNN原理</h2><p>（0）标记符号含义</p><p>输入的样本为（X,Y），其中X的维度为（m，T_x，n），m代表样本个数，T_x代表时间步，n代表字典长度。</p><p>RNN中每个样本采用独热向量表示，在训练集中选择不重复数据作为字典，根据字典构造每个独热向量（当前词在字典中的位置标为1，其余标为0）。</p><p>（1）网络结构</p><p>RNN网络结果如图，在t时刻输入当前时刻的x值，经过隐藏层，通过计算前一层的激活值和输入值x得到预测的y。</p><a id="more"></a>




<p>每个时刻的输出取决于当前时刻的输入和前一个时刻的激活值。</p>
<p>1° 多对多结构</p>
<p>很多输入对应很多输出，当Tx=Ty，即输入和输出数量相同，例如命名实体识别（根据一个句子识别出其中的主语）</p>
<p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618165122.png" alt></p>
<p>2° 多对多结构</p>
<p>很多输入对应很多输出，当Tx不等于Ty，即输入和输出数量不相同，例如机器翻译。</p>
<p>因为机器翻译需要考虑整个句子的前后联系，所以要将x全部输入后才能进行预测。因此，前半部分按照时间步输入x，作为编码器，后半部分输出预测的y，作为解码器。</p>
<p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618195021.png" alt></p>
<p>3° 一对多结构</p>
<p>输入部分片段或者不输入（0向量），输出形成的完整的输出，例如生成音乐。</p>
<p>根据输入的x和激活值得到预测的y，上一层预测的y作为下一层的输入。</p>
<p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618195910.png" alt></p>
<p>4° 多对一结构</p>
<p>很多输入对应有个输出，例如情感分类问题，给一段文字，给出评分。</p>
<p>每次输入一个时间步的x，但不输出预测值，当所有的x均按照时间步输入之后，最后得到一个评分，该评分综合考虑了签名所有的文字。</p>
<p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618200114.png" alt></p>
<p>（2）前向传播</p>
<p>以多对多中，Tx=Ty为例说明RNN的前向传播。</p>
<p>在t=1的时间步下：</p>
<p>1° 有输入a&lt;0&gt;激活值（伪激活值，常设置为0），和时间步为1的x值：x&lt;1&gt;，将两者乘以对应权重，加上偏置量得到线性值，再使用激活函数（tanh/Relu），得到当前单元输出的激活值。</p>
<p>2° 将激活值a&lt;1&gt;乘以对应权重，加上偏置量得到线性值，再使用激活函数（softmax）,得到最终预测的时间步为1的y的预测值。</p>
<p>后面的神经元步骤类似：</p>
<p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618165220.png" alt></p>
<p>编码时，首先根据前向传播公式得到一个基本RNN单元处理的结果，将前一个和当前激活值，当前输入的x和参数存成缓存，将当前输出的激活值、预测值、缓存作为最终的输出。</p>
<p>接着，按照时间步，传入当前输入和上一层的激活值输出，得到所有时间步的激活值和预测值，最终将所有时间步的激活值和预测值和缓存输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#搭建标准rnn单元</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_cell_forward</span><span class="params">(xt,a_prev,parameters)</span>:</span></span><br><span class="line">	Wax=parameters[<span class="string">"Wax"</span>]</span><br><span class="line">	Waa=parameters[<span class="string">"Waa"</span>]</span><br><span class="line">	Wya=parameters[<span class="string">"Wya"</span>]</span><br><span class="line"></span><br><span class="line">	ba=parameters[<span class="string">"ba"</span>]</span><br><span class="line">	by=parameters[<span class="string">"by"</span>]</span><br><span class="line"></span><br><span class="line">	a_next=np.tanh(np.dot(Waa,a_prev)+np.dot(Wax,xt)+ba)</span><br><span class="line"></span><br><span class="line">	y_next=ru.softmax(np.dot(Wya,a_next)+by)</span><br><span class="line"></span><br><span class="line">	cache=(a_next,a_prev,xt,parameters)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> a_next,y_next,cache</span><br><span class="line"></span><br><span class="line"><span class="comment">#将所有标准单元连接起来，进行前向传播</span></span><br><span class="line"><span class="comment">#这里Tx=Ty</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_forward</span><span class="params">(x,a0,parameters)</span>:</span></span><br><span class="line">	n_x,m,T_x=x.shape</span><br><span class="line">	Wya=parameters[<span class="string">"Wya"</span>]</span><br><span class="line">	n_y,n_a=Wya.shape</span><br><span class="line"></span><br><span class="line">	a_next=a0</span><br><span class="line">	a=np.zeros((n_a,m,T_x))</span><br><span class="line">	y_hat=np.zeros((n_y,m,T_x))</span><br><span class="line">	caches=[]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> t <span class="keyword">in</span> range(T_x):</span><br><span class="line">		a_next,y_next,cache=rnn_cell_forward(x[:,:,t],a_next,parameters)</span><br><span class="line"></span><br><span class="line">		a[:,:,t]=a_next</span><br><span class="line"></span><br><span class="line">		y_hat[:,:,t]=y_next</span><br><span class="line"></span><br><span class="line">		caches.append(cache)</span><br><span class="line">	caches=(caches,x)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> a,y_hat,caches</span><br></pre></td></tr></table></figure>

<p>（3）反向传播</p>
<p>反向传播需要从最后开始（for t in reversed(range(T_x))），求得所有参数的梯度。计算和激活值和输入值相关的梯度即可，通过损失调整相应权重和偏置量的值。</p>
<p>根据对tanh(a)求导等于(1-tanh^2) * da，易求得Wax,Waa,xt,ba,a&lt; t-1 &gt;的梯度如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618170023.png" alt></p>
<p>编码时，也是先得出一个基本神经元的反向传播过程，再根据时间步以此类推得到其他的，并将梯度修正并进行爆粗。</p>
<p>注：对激活值的推导一直要得到最开始的激活值即da0</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#标准rnn的反向传播</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#单个单元的反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_cell_backward</span><span class="params">(da_next,cache)</span>:</span></span><br><span class="line">	a_next,a_prev,xt,parameters=cache</span><br><span class="line"></span><br><span class="line">	Wax=parameters[<span class="string">"Wax"</span>]</span><br><span class="line">	Waa=parameters[<span class="string">"Waa"</span>]</span><br><span class="line">	ba=parameters[<span class="string">"ba"</span>]</span><br><span class="line">	<span class="comment"># Wya=parameters["Wya"]</span></span><br><span class="line">	<span class="comment"># by=parameters["by"]</span></span><br><span class="line"></span><br><span class="line">	dtanh=(<span class="number">1</span>-np.square(a_next))*da_next</span><br><span class="line"></span><br><span class="line">	dxt=np.dot(Wax.T,dtanh)</span><br><span class="line">	dWax=np.dot(dtanh,xt.T)</span><br><span class="line">	</span><br><span class="line">	da_prev=np.dot(Waa.T,dtanh)</span><br><span class="line">	dWaa=np.dot(dtanh,a_prev.T)</span><br><span class="line"></span><br><span class="line">	dba=np.sum(dtanh,axis=<span class="number">-1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">	<span class="comment"># dWya=np.dot(dy,dtanh.T)</span></span><br><span class="line">	<span class="comment"># dyb=np.sum(dtanh,axis=1,keepdims=True)</span></span><br><span class="line"></span><br><span class="line">	gradients=&#123;<span class="string">'dxt'</span>:dxt,<span class="string">'da_prev'</span>:da_prev,<span class="string">'dWaa'</span>:dWaa,<span class="string">'dWax'</span>:dWax,<span class="string">'dba'</span>:dba&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> gradients</span><br><span class="line"></span><br><span class="line"><span class="comment">#一个序列的反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_backward</span><span class="params">(da,caches)</span>:</span></span><br><span class="line">	caches,x=caches</span><br><span class="line">	a1,a0,x1,parameters=caches[<span class="number">0</span>]</span><br><span class="line">	n_x,m=x1.shape</span><br><span class="line">	n_a,m,T_x=da.shape</span><br><span class="line"></span><br><span class="line">	dx=np.zeros((n_x,m,T_x))</span><br><span class="line">	dWaa=np.zeros((n_a,n_a))</span><br><span class="line">	dWax=np.zeros((n_a,n_x))</span><br><span class="line">	dba=np.zeros((n_a,<span class="number">1</span>))</span><br><span class="line">	da0=np.zeros((n_a,m))</span><br><span class="line">	da_prevt=np.zeros((n_a,m))</span><br><span class="line"></span><br><span class="line">	<span class="comment">#倒序</span></span><br><span class="line">	<span class="keyword">for</span> t <span class="keyword">in</span> reversed(range(T_x)):</span><br><span class="line">		gradient=rnn_cell_backward(da[:,:,t]+da_prevt,caches[t])</span><br><span class="line">		dxt,da_prevt,dWaat,dWaxt,dbat=gradient[<span class="string">'dxt'</span>],gradient[<span class="string">'da_prev'</span>],gradient[<span class="string">'dWaa'</span>],gradient[<span class="string">'dWax'</span>],gradient[<span class="string">'dba'</span>]</span><br><span class="line"></span><br><span class="line">		dx[:,:,t]=dxt</span><br><span class="line">		dWaa+=dWaat</span><br><span class="line">		dWax+=dWaxt</span><br><span class="line">		dba+=dbat</span><br><span class="line">	da0=da_prevt</span><br><span class="line"></span><br><span class="line">	gradients=&#123;<span class="string">'dx'</span>:dx,<span class="string">'da0'</span>:da0,<span class="string">'dWaa'</span>:dWaa,<span class="string">'dWax'</span>:dWax,<span class="string">'dba'</span>:dba&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure>

<h2 id="基本RNN网络存在问题"><a href="#基本RNN网络存在问题" class="headerlink" title="基本RNN网络存在问题"></a>基本RNN网络存在问题</h2><p>当网络深度变深，权重将以指数形式扩大或减小，会出现以下几个问题：</p>
<p>（1）梯度爆炸——解决方法：梯度修剪</p>
<p>当出现梯度爆炸时，在使用当前梯度更新参数之后，对更新后的参数判断是否在[- 最大值，最大值]之间（这里的最大值和最小值需要用户给出），当不在时，按照最大值和最小值设置，这样保证梯度不会变的很大。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#梯度修剪</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clip</span><span class="params">(gradients,maxValue)</span>:</span></span><br><span class="line">	dWax=gradients[<span class="string">"dWax"</span>]</span><br><span class="line">	dWaa=gradients[<span class="string">"dWaa"</span>]</span><br><span class="line">	dWya=gradients[<span class="string">"dWya"</span>]</span><br><span class="line"></span><br><span class="line">	db=gradients[<span class="string">"db"</span>]</span><br><span class="line">	dby=gradients[<span class="string">"dby"</span>]</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> gradient <span class="keyword">in</span> [dWax,dWaa,dWya,db,dby]:</span><br><span class="line">		np.clip(gradient,-maxValue,maxValue,out=gradient)</span><br><span class="line">	gradients=&#123;<span class="string">'dWax'</span>:dWax,<span class="string">'dWaa'</span>:dWaa,<span class="string">'dWya'</span>:dWya,<span class="string">'db'</span>:db,<span class="string">'dby'</span>:dby&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure>

<p>（2）梯度消失——解决方法：修改网络的基本单元</p>
<p>采用GRU单元或者LSTM单元，能更好捕捉深层连接</p>
<h2 id="LSTM原理"><a href="#LSTM原理" class="headerlink" title="LSTM原理"></a>LSTM原理</h2><p>（1）网络结构</p>
<p>LSTM单元和基本RNN单元类似，但其中加入了记忆细胞，每个神经元都会有一个记忆细胞，并输出一个新记忆细胞值，这样保证即使很深，后面的值也会有前面神经元的值。</p>
<p>LSTM中设置更新门，遗忘门，根据更新门决定保留新的记忆细胞的部分，根据遗忘门决定保留前一个记忆细胞的部分，通过记忆细胞贯穿所有时间步，从而获得更远的连接。</p>
<p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618170626.png" alt></p>
<p>（2）前向传播</p>
<p>1° 记忆细胞根据当前时间步x&lt; t &gt;和上一个输出激活值得出新的记忆细胞</p>
<p>2° 将更新门、遗忘门、输出门按照权重得到最新的值</p>
<p>3° 根据更新门、遗忘门得到当前的记忆细胞</p>
<p>4° 根据输出门和当前记忆细胞德奥激活值</p>
<p>5° 最终经过softmax得到最终的预测</p>
<p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618170550.png" alt></p>
<p>编码同上面基本RNN单元，得出单一LSTM单元的前向传播，再对所有时间步求得预测值。</p>
<p>注：这里有个小技巧，将前一个激活值a_prev和当前x进行堆叠，减少参数个数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#加入LTSM解决梯度消失的问题</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#搭建单个lstm单元</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_cell_forward</span><span class="params">(xt,a_prev,c_prev,parameters)</span>:</span></span><br><span class="line">	Wf=parameters[<span class="string">'Wf'</span>]</span><br><span class="line">	bf=parameters[<span class="string">'bf'</span>]</span><br><span class="line">	Wu=parameters[<span class="string">'Wu'</span>]</span><br><span class="line">	bu=parameters[<span class="string">'bu'</span>]</span><br><span class="line">	Wc=parameters[<span class="string">'Wc'</span>]</span><br><span class="line">	bc=parameters[<span class="string">'bc'</span>]</span><br><span class="line">	Wo=parameters[<span class="string">'Wo'</span>]</span><br><span class="line">	bo=parameters[<span class="string">'bo'</span>]</span><br><span class="line">	Wy=parameters[<span class="string">'Wy'</span>]</span><br><span class="line">	by=parameters[<span class="string">'by'</span>]</span><br><span class="line"></span><br><span class="line">	n_a,m=a_prev.shape</span><br><span class="line">	n_xt,m=xt.shape</span><br><span class="line"></span><br><span class="line">	ax_prev=np.zeros(((n_a+n_xt),m))</span><br><span class="line"></span><br><span class="line">	<span class="comment">#将a_prev和xt堆叠起来</span></span><br><span class="line">	ax_prev[:n_a,:]=a_prev</span><br><span class="line">	ax_prev[n_a:,:]=xt</span><br><span class="line"></span><br><span class="line">	<span class="comment">#遗忘门</span></span><br><span class="line">	ft=ru.sigmoid(np.dot(Wf,ax_prev)+bf)</span><br><span class="line">	<span class="comment">#更新门</span></span><br><span class="line">	ut=ru.sigmoid(np.dot(Wu,ax_prev)+bu)</span><br><span class="line"></span><br><span class="line">	<span class="comment">#候选值</span></span><br><span class="line">	cct=np.tanh(np.dot(Wc,ax_prev)+bc)</span><br><span class="line">	<span class="comment">#记忆细胞</span></span><br><span class="line">	c_next=ut*cct+ft*c_prev</span><br><span class="line"></span><br><span class="line">	<span class="comment">#输出门</span></span><br><span class="line">	ot=ru.sigmoid(np.dot(Wo,ax_prev)+bo)</span><br><span class="line"></span><br><span class="line">	a_next=ot*np.tanh(c_next)</span><br><span class="line"></span><br><span class="line">	y_next=ru.softmax(np.dot(Wy,a_next)+by)</span><br><span class="line"></span><br><span class="line">	cache=(a_next,c_next,a_prev,c_prev,ft,ut,cct,ot,xt,parameters)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> a_next,c_next,y_next,cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#lstm的前向传播</span></span><br><span class="line"><span class="comment">#c0使用0来初始化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_forward</span><span class="params">(x,a0,parameters)</span>:</span></span><br><span class="line">	caches=[]</span><br><span class="line"></span><br><span class="line">	n_x,m,T_x=x.shape</span><br><span class="line">	Wy=parameters[<span class="string">'Wy'</span>]</span><br><span class="line">	n_y,n_a=Wy.shape</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	a=np.zeros((n_a,m,T_x))</span><br><span class="line">	c=np.zeros((n_a,m,T_x))</span><br><span class="line">	y_hat=np.zeros((n_y,m,T_x))</span><br><span class="line"></span><br><span class="line">	a_next=a0</span><br><span class="line">	c_next=np.zeros((n_a,m))</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> t <span class="keyword">in</span> range(T_x):</span><br><span class="line">		a_next,c_next,y_next,cache=lstm_cell_forward(x[:,:,t],a_next,c_next,parameters)</span><br><span class="line"></span><br><span class="line">		a[:,:,t]=a_next</span><br><span class="line">		c[:,:,t]=c_next</span><br><span class="line">		y_hat[:,:,t]=y_next</span><br><span class="line"></span><br><span class="line">		caches.append(cache)</span><br><span class="line"></span><br><span class="line">	caches=(caches,x)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> a,y_hat,c,caches</span><br></pre></td></tr></table></figure>

<p>（3）反向传播</p>
<p>反向从最后前向求得所有的梯度值（for t in reversed(range(T_x))），输出值y中的权重，偏置量不更新。</p>
<p>1° 求三个门和记忆细胞的梯度</p>
<p>输出门：根据最终输出的激活值公式以及对sigmoid的求导得出</p>
<p>记忆细胞候选值、更新门、遗忘门：一部分受求当前记忆细胞的影响，一部分受求激活值的影响</p>
<p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618170127.png" alt></p>
<p>2° 各个权重系数的梯度，根据1中求得的梯度能很容易求到</p>
<p>同理，各个偏置量梯度也可求得</p>
<p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618170142.png" alt></p>
<p>3° 上一个激活值，上一个记忆细胞候选值，当前输入值的梯度</p>
<p>上一个激活值和记忆细胞候选值、更新门、遗忘门、输出门均有联系，所以将各个梯度相加</p>
<p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618170213.png" alt></p>
<p>上一个记忆细胞候选值一部分受求当前记忆细胞的影响，一部分受求激活值的影响</p>
<p>当前输入值的梯度和记忆细胞候选值、更新门、遗忘门、输出门均有联系，所以将各个梯度相加</p>
<p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618170231.png" alt></p>
<p>编码同上面基本RNN单元，得出单一LSTM单元的反向传播，再对所有时间步求得梯度。</p>
<p>注：前向传播中用的小技巧，将前一个激活值a_prev和当前x进行堆叠，减少参数个数。求梯度的时候要将值进行拆分求梯度，即将权重前部分用来求a_prev，后部分用于求xt。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#lstm单个单元的反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_cell_backward</span><span class="params">(da_next,dc_next,cache)</span>:</span></span><br><span class="line">	a_next,c_next,a_prev,c_prev,ft,ut,cct,ot,xt,parameters=cache</span><br><span class="line">	n_a,m=a_next.shape</span><br><span class="line"></span><br><span class="line">	Wf=parameters[<span class="string">'Wf'</span>]</span><br><span class="line">	bf=parameters[<span class="string">'bf'</span>]</span><br><span class="line">	Wu=parameters[<span class="string">'Wu'</span>]</span><br><span class="line">	bu=parameters[<span class="string">'bu'</span>]</span><br><span class="line">	Wc=parameters[<span class="string">'Wc'</span>]</span><br><span class="line">	bc=parameters[<span class="string">'bc'</span>]</span><br><span class="line">	Wo=parameters[<span class="string">'Wo'</span>]</span><br><span class="line">	bo=parameters[<span class="string">'bo'</span>]</span><br><span class="line">	Wy=parameters[<span class="string">'Wy'</span>]</span><br><span class="line">	by=parameters[<span class="string">'by'</span>]</span><br><span class="line"></span><br><span class="line">	dot=da_next*np.tanh(c_next)*ot*(<span class="number">1</span>-ot)</span><br><span class="line">	dcct=(dc_next*ut+ot*(<span class="number">1</span>-np.square(np.tanh(c_next)))*ut*da_next)*(<span class="number">1</span>-np.square(cct))</span><br><span class="line">	dut=(dc_next*cct+ot*(<span class="number">1</span>-np.square(np.tanh(c_next)))*cct*da_next)*ut*(<span class="number">1</span>-ut)</span><br><span class="line">	dft=(dc_next*c_prev+ot*(<span class="number">1</span>-np.square(np.tanh(c_next)))*c_prev*da_next)*ft*(<span class="number">1</span>-ft)</span><br><span class="line"></span><br><span class="line">	concat=np.concatenate((a_prev,xt),axis=<span class="number">0</span>).T</span><br><span class="line"></span><br><span class="line">	dWf=np.dot(dft,concat)</span><br><span class="line">	dbf=np.sum(dft,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">	dWu=np.dot(dut,concat)</span><br><span class="line">	dbu=np.sum(dut,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">	dWc=np.dot(dcct,concat)</span><br><span class="line">	dbc=np.sum(dcct,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">	dWo=np.dot(dot,concat)</span><br><span class="line">	dbo=np.sum(dot,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">	da_prev=np.dot(Wf[:,:n_a].T,dft)+np.dot(Wu[:,:n_a].T,dut)+np.dot(Wc[:,:n_a].T,dcct)+np.dot(Wo[:,:n_a].T,dot)</span><br><span class="line">	dc_prev=dc_next*ft+ot*(<span class="number">1</span>-np.square(np.tanh(c_next)))*ft*da_next</span><br><span class="line">	dxt=np.dot(Wf[:,n_a:].T,dft)+np.dot(Wu[:,n_a:].T,dut)+np.dot(Wc[:,n_a:].T,dcct)+np.dot(Wo[:,n_a:].T,dot)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	gradients=&#123;<span class="string">'dxt'</span>:dxt,<span class="string">'da_prev'</span>:da_prev,<span class="string">'dc_prev'</span>:dc_prev,<span class="string">'dWf'</span>:dWf,<span class="string">'dbf'</span>:dbf,<span class="string">'dWu'</span>:dWu,<span class="string">'dbu'</span>:dbu,<span class="string">'dWc'</span>:dWc,<span class="string">'dbc'</span>:dbc,<span class="string">'dWo'</span>:dWo,<span class="string">'dbo'</span>:dbo&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> gradients</span><br><span class="line"></span><br><span class="line"><span class="comment">#lstm一个时间序列的反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_backward</span><span class="params">(da,caches)</span>:</span></span><br><span class="line">	caches,x=caches</span><br><span class="line">	a_next1,c_next1,a_prev1,c_prev1,ft1,ut1,cct1,ot1,xt1,parameters=caches[<span class="number">0</span>]</span><br><span class="line">	n_a,m,T_x=da.shape</span><br><span class="line">	n_x,m=xt1.shape</span><br><span class="line">	n_c,m=c_next1.shape</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	dx=np.zeros((n_x,m,T_x))</span><br><span class="line">	da0=np.zeros((n_a,m))</span><br><span class="line"></span><br><span class="line">	dWf=np.zeros((n_a,n_a+n_x))</span><br><span class="line">	dWu=np.zeros((n_a,n_a+n_x))</span><br><span class="line">	dWc=np.zeros((n_a,n_a+n_x))</span><br><span class="line">	dWo=np.zeros((n_a,n_a+n_x))</span><br><span class="line">	dbf=np.zeros((n_a,<span class="number">1</span>))</span><br><span class="line">	dbu=np.zeros((n_a,<span class="number">1</span>))</span><br><span class="line">	dbc=np.zeros((n_a,<span class="number">1</span>))</span><br><span class="line">	dbo=np.zeros((n_a,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">	da_prevt=np.zeros((n_a,m))</span><br><span class="line">	dc_prevt=np.zeros((n_c,m))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> t <span class="keyword">in</span> reversed(range(T_x)):</span><br><span class="line">		gradient=lstm_cell_backward(da[:,:,t]+da_prevt,dc_prevt,caches[t])</span><br><span class="line">		dxt,da_prevt,dc_prevt,dWft,dbft,dWut,dbut,dWct,dbct,dWot,dbot=gradient[<span class="string">'dxt'</span>],gradient[<span class="string">'da_prev'</span>],gradient[<span class="string">'dc_prev'</span>],gradient[<span class="string">'dWf'</span>],gradient[<span class="string">'dbf'</span>],gradient[<span class="string">'dWu'</span>],gradient[<span class="string">'dbu'</span>],gradient[<span class="string">'dWc'</span>],gradient[<span class="string">'dbc'</span>],gradient[<span class="string">'dWo'</span>],gradient[<span class="string">'dbo'</span>]</span><br><span class="line">		dx[:,:,t]=dxt</span><br><span class="line">		dWf+=dWft</span><br><span class="line">		dbf+=dbft</span><br><span class="line">		dWu+=dWut</span><br><span class="line">		dbu+=dbut</span><br><span class="line">		dWc+=dWct</span><br><span class="line">		dWo+=dWot</span><br><span class="line">		dbo+=dbot</span><br><span class="line">	da0=da_prevt</span><br><span class="line">	<span class="comment"># dc0=dc_prevt</span></span><br><span class="line">	gradients=&#123;<span class="string">'dx'</span>:dx,<span class="string">'da0'</span>:da0,<span class="string">'dWf'</span>:dWf,<span class="string">'dbf'</span>:dbf,<span class="string">'dWu'</span>:dWu,<span class="string">'dbu'</span>:dbu,<span class="string">'dWc'</span>:dWc,<span class="string">'dbc'</span>:dbc,<span class="string">'dWo'</span>:dWo,<span class="string">'dbo'</span>:dbo&#125;</span><br><span class="line">	<span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure>

<h2 id="GRU原理"><a href="#GRU原理" class="headerlink" title="GRU原理"></a>GRU原理</h2><p>（1）网络结构</p>
<p>GRU可以看成是简化版的LSTM，去除了遗忘门，并且将激活值就作为当前记忆细胞，简化神经单元。</p>
<p>GRU中同样使用记忆细胞，并使用更新门，当前的新的记忆细胞使用更新门保留新的候选值，使用（1-更新门）保留之前的记忆细胞，并将得到的新记忆细胞作为激活值输出，并经过softmax得到预测值y。</p>
<p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618170407.png" alt></p>
<p>（2）前向传播</p>
<p>GRU采用a&lt; t &gt;=c&lt; t &gt;</p>
<p>1° 得到记忆细胞候选值，使用相关门衡量前一个记忆细胞和当前候选值的相关性</p>
<p>2° 计算更新门、更新门</p>
<p>3° 按照更新门来计算新的记忆细胞</p>
<p>4° 将记忆细胞新的值作为激活值，采用softmax得到预测值</p>
<p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618170430.png" alt></p>
<h2 id="采样"><a href="#采样" class="headerlink" title="采样"></a>采样</h2><p>作用：合成数据，得到每个时间步的预测并传递给下一个神经元，从而得到训练模型后的预测结果。</p>
<p>当模型经过搭建、编译、训练之后，通过采样得到模型对新的数据（或者不输入数据）的预测值。</p>
<p>0° 初始采用零向量（独热向量）作为初始输入，根据模型训练好的参数进入循环</p>
<p>1° 根据RNN网络前向传播得到预测值y</p>
<p>2° 按照随机采样的方式取其中一个值作为预测值</p>
<p>3° 按照当前预测值的位置，生成独热向量作为下一个时间步的输入</p>
<p>4° 不断循环，就得到了合成数据，即使用训练好的模型预测的结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#采样</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(parameters,char_to_ix,seed)</span>:</span></span><br><span class="line">	Wax=parameters[<span class="string">'Wax'</span>]</span><br><span class="line">	Waa=parameters[<span class="string">'Waa'</span>]</span><br><span class="line">	b=parameters[<span class="string">'b'</span>]</span><br><span class="line">	Wya=parameters[<span class="string">'Wya'</span>]</span><br><span class="line">	by=parameters[<span class="string">'by'</span>]</span><br><span class="line"></span><br><span class="line">	vocab_size=by.shape[<span class="number">0</span>]</span><br><span class="line">	n_a=Waa.shape[<span class="number">1</span>]</span><br><span class="line">	a_prev=np.zeros((n_a,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">	<span class="comment">#创造独热向量</span></span><br><span class="line">	x=np.zeros((vocab_size,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">	counter=<span class="number">0</span></span><br><span class="line"></span><br><span class="line">	idx=<span class="number">-1</span></span><br><span class="line">	indies=[]</span><br><span class="line">	check_character=char_to_ix[<span class="string">'\n'</span>]</span><br><span class="line">	<span class="keyword">while</span> idx!=check_character <span class="keyword">and</span> counter&lt;<span class="number">50</span>:</span><br><span class="line">		a=np.tanh(np.dot(Waa,a_prev)+np.dot(Wax,x)+b)</span><br><span class="line">		z=np.dot(Wya,a)+by</span><br><span class="line">		y=cu.softmax(z)</span><br><span class="line"></span><br><span class="line">		<span class="comment">#按照概率采样</span></span><br><span class="line">		np.random.seed(seed+counter)</span><br><span class="line">		idx=np.random.choice(list(range(vocab_size)),p=y.ravel())</span><br><span class="line">		indies.append(idx)</span><br><span class="line"></span><br><span class="line">		<span class="comment">#按照选择的位置生成独热向量</span></span><br><span class="line">		x=np.zeros((vocab_size,<span class="number">1</span>))</span><br><span class="line">		x[idx]=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">		<span class="comment">#将上一个预测值直接作为下一个的输入</span></span><br><span class="line">		a_prev=a</span><br><span class="line">		seed+=<span class="number">1</span></span><br><span class="line">		counter+=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> counter==<span class="number">50</span>:</span><br><span class="line">		indies.append(char_to_ix[<span class="string">'\n'</span>])</span><br><span class="line">	<span class="keyword">return</span> indies</span><br></pre></td></tr></table></figure>



    </div>

    
    
    

    <div>
  
    <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢阅读-------------</div>
    
</div>
  
</div>
        <div class="reward-container">
  <div>大爷来玩啊</div>
  <button disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/wechat.png" alt="Xiapf 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/alipay.jpg" alt="Xiapf 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag"><i class="fa fa-tag"></i> 神经网络</a>
              <a href="/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag"><i class="fa fa-tag"></i> 循环神经网络</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
                <a href="/blogs/convNet5/" rel="next" title="卷积神经网络——神经风格转换">
                  <i class="fa fa-chevron-left"></i> 卷积神经网络——神经风格转换
                </a>
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
                <a href="/blogs/rnn1DN/" rel="prev" title="序列模型RNN——构建字符级语言模型">
                  序列模型RNN——构建字符级语言模型 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="comments"></div>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN原理"><span class="nav-number">1.</span> <span class="nav-text">RNN原理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#基本RNN网络存在问题"><span class="nav-number">2.</span> <span class="nav-text">基本RNN网络存在问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LSTM原理"><span class="nav-number">3.</span> <span class="nav-text">LSTM原理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GRU原理"><span class="nav-number">4.</span> <span class="nav-text">GRU原理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#采样"><span class="nav-number">5.</span> <span class="nav-text">采样</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Xiapf" src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/touxiang.jpg">
  <p class="site-author-name" itemprop="name">Xiapf</p>
  <div class="site-description" itemprop="description">好好学习，天天向上</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">65</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">38</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/iamxpf" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;iamxpf" rel="external nofollow noopener noreferrer" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/mailto:iamxpf@126.com" title="E-Mail → mailto:iamxpf@126.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-link"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.v2ex.com/" title="https:&#x2F;&#x2F;www.v2ex.com&#x2F;" rel="external nofollow noopener noreferrer" target="_blank">V2EX</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://leetcode-cn.com/problemset/all/" title="https:&#x2F;&#x2F;leetcode-cn.com&#x2F;problemset&#x2F;all&#x2F;" rel="external nofollow noopener noreferrer" target="_blank">LeetCode</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.nowcoder.com/activity/oj" title="https:&#x2F;&#x2F;www.nowcoder.com&#x2F;activity&#x2F;oj" rel="external nofollow noopener noreferrer" target="_blank">NowCoder</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        
  <div class="beian"><a href="http://www.beian.miit.gov.cn/" rel="external nofollow noopener noreferrer" target="_blank">苏ICP备19068825号-2 </a>
  </div>

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">All Rights Reserved</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">302k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">4:34</span>
</div>
  <div class="addthis_inline_share_toolbox">
    <script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5dd16697c9fe6c8a" async="async"></script>
  </div>

        






  <script>
  function leancloudSelector(url) {
    return document.getElementById(url).querySelector('.leancloud-visitors-count');
  }
  if (CONFIG.page.isPost) {
    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = visitors.getAttribute('id').trim();
      var title = visitors.getAttribute('data-flag-title').trim();

      Counter('get', `/classes/Counter?where=${JSON.stringify({ url })}`)
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .then(response => response.json())
              .then(() => {
                leancloudSelector(url).innerText = counter.time + 1;
              })
              .catch(error => {
                console.log('Failed to save visitor count', error);
              })
          } else {
              Counter('post', '/classes/Counter', { title: title, url: url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.log('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.log('LeanCloud Counter Error', error);
        });
    }
  } else {
    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return element.getAttribute('id').trim();
      });

      Counter('get', `/classes/Counter?where=${JSON.stringify({ url: { '$in': entries } })}`)
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length === 0) {
            document.querySelectorAll('.leancloud_visitors .leancloud-visitors-count').forEach(element => {
              element.innerText = 0;
            });
            return;
          }
          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.url;
            var time = item.time;
            leancloudSelector(url).innerText = time;
          }
          for (var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = leancloudSelector(url);
            if (element.innerText == '') {
              element.innerText = 0;
            }
          }
        })
        .catch(error => {
          console.log('LeanCloud Counter Error', error);
        });
    }
  }

  fetch('https://app-router.leancloud.cn/2/route?appId=e5fPNg6mJg8VLyXxWi6h7ItD-gzGzoHsz')
    .then(response => response.json())
    .then(({ api_server }) => {
      var Counter = (method, url, data) => {
        return fetch(`https://${api_server}/1.1${url}`, {
          method: method,
          headers: {
            'X-LC-Id': 'e5fPNg6mJg8VLyXxWi6h7ItD-gzGzoHsz',
            'X-LC-Key': 'NCOfwyk21HI5Snpwtgr9qkI5',
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        const localhost = /http:\/\/(localhost|127.0.0.1|0.0.0.0)/;
        if (localhost.test(document.URL)) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    });
  </script>






        
      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>




  <script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.getScript('https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/valine.js', () => {
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(item => {
    return GUEST.includes(item);
  });
  new Valine({
    el: '#comments',
    verify: true,
    notify: false,
    appId: 'e5fPNg6mJg8VLyXxWi6h7ItD-gzGzoHsz',
    appKey: 'NCOfwyk21HI5Snpwtgr9qkI5',
    placeholder: "吐槽一下",
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: false,
    lang: 'zh-cn' || 'zh-cn',
    path: location.pathname,
    recordIP: true,
    serverURLs: ''
  });
}, window.Valine);

//增加以下六行代码去除 power by valine
    var infoEle = document.querySelector('#comments .info');
    if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0){
      infoEle.childNodes.forEach(function(item) {
        item.parentNode.removeChild(item);
      });
    }
</script>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"react":{"opacity":0.7}});</script></body>
</html>
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/clicklove.js"></script>
<!-- 雪花特效 -->
<script type="text/javascript" src="/js/snow.js"></script>
<!--浏览器搞笑标题-->
<script type="text/javascript" src="/js/FunnyTitle.js"></script>
