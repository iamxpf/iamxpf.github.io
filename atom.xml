<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="https://www.w3.org/2005/Atom">
  <title>八戒大强攻</title>
  
  <subtitle>好久没吃人肉了</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.xiapf.com/"/>
  <updated>2021-06-16T05:55:20.580Z</updated>
  <id>https://www.xiapf.com/</id>
  
  <author>
    <name>Xiapf</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>用c++实现的一个简单消息中间件</title>
    <link href="https://www.xiapf.com/blogs/mqcplusplus/"/>
    <id>https://www.xiapf.com/blogs/mqcplusplus/</id>
    <published>2021-06-16T05:25:15.000Z</published>
    <updated>2021-06-16T05:55:20.580Z</updated>
    
    <content type="html"><![CDATA[<p>项目地址：<a href="https://github.com/iambajie/mq-cplusplus" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/iambajie/mq-cplusplus</a></p><h2 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h2><ul><li>step1:终端输入<code>make</code>生成服务端和客户端文件：通过makefile生成服务端和客户端文件：</li></ul><p>（1）按序生成mq_normal_producer mq_normal_consumer1 mq_normal_consumer2 mq_ack_producer mq_ack_consumer mq_multi_producer mq_priority_consumer mq_multi_consumer mq_priority_producer mq_pull_consumer mq_durable_producer mq_durable_consumer （客户端）</p><a id="more"></a><p>（2）mq_conSrv mq_logicSrv mq_persisSrv（服务端）</p><ul><li>step2:通过shell脚本调用程序:</li></ul><p>服务端</p><ul><li>启动：输入<code>./mq_server.start.sh</code>启动服务端</li><li>关闭：输入<code>./mq_server.stop.sh</code>+对应数字，关闭服务端</li></ul><p>数字含义同linux下kill命令的数字含义：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL</span><br><span class="line">5) SIGTRAP 6) SIGABRT 7) SIGEMT 8) SIGFPE</span><br><span class="line">9) SIGKILL 10) SIGBUS 11) SIGSEGV 12) SIGSYS</span><br><span class="line">13) SIGPIPE 14) SIGALRM 15) SIGTERM 16) SIGUSR1</span><br><span class="line">17) SIGUSR2 18) SIGCHLD 19) SIGPWR 20) SIGWINCH</span><br><span class="line">21) SIGURG 22) SIGIO 23) SIGSTOP 24) SIGTSTP</span><br><span class="line">25) SIGCONT 26) SIGTTIN 27) SIGTTOU 28) SIGVTALRM</span><br><span class="line">29) SIGPROF 30) SIGXCPU 31) SIGXFSZ 32) SIGWAITING</span><br><span class="line">33) SIGLWP 34) SIGFREEZE 35) SIGTHAW 36) SIGCANCEL</span><br><span class="line">37) SIGLOST 39) SIGRTMIN 40) SIGRTMIN+1 41) SIGRTMIN+2</span><br><span class="line">42) SIGRTMIN+3 43) SIGRTMAX-3 44) SIGRTMAX-2 45) SIGRTMAX-1</span><br><span class="line">46) SIGRTMAX</span><br></pre></td></tr></table></figure><p>客户端</p><ul><li>一个生产者对一个消费者：输入<code>./mq_normal_producer  ./mq_normal_consumer</code>启动客户端的生产者和消费者</li><li>一个生产者对多个消费者：工作队列（多个消费者同时处理，在多个工作人员之间分配耗时的任务）：输入<code>./multi_producer_start.sh 1 10</code>启动客户端，创建一个生产者，循环发送长度为10的消息</li><li>持久化消息：输入<code>./mq_durable_producer  ./mq_duravle_consumer</code>启动客户端的生产者和消费者</li></ul><h2 id="功能实现"><a href="#功能实现" class="headerlink" title="功能实现"></a>功能实现</h2><ul><li><p>服务器端是基于共享内存的队列，服务器端分为三部分，由接入层、逻辑层、持久化层构成</p><p>接入层负责与客户端建立连接，接收客户端创建的消息包及回复相应的网络包</p><p>逻辑层通过共享内存与接入层通信，主要负责消息处理与回复：接收接入层传入的订阅信息、出错信息、需要持久化信息等，将订阅的数据和需要回复的数据包回传给接入层</p><p>持久化层接收逻辑层传入共享内存的信息，将需要持久化的数据按照类别顺序落地磁盘</p></li><li><p>服务器端服务端采用epoll I/O多路复用技术</p><p>IO性能不会随着事件描述符的增大而线性减少，每轮处理数据，通过epoll_wait所有要处理的事件，如果是当前监听的套接字则添加到用户连接，接收数据</p></li><li><p>通过Compare &amp; Set实现无锁操作，避免多线程竞争</p><p>服务器端通过共享内存读写数据，为避免竞争，将队列结构中国设置使用人数，每次轮询使用人数如果是0，则设置为1，即获得了队列的使用权，反之则有其他线程在进行操作，继续等待。</p></li><li><p>通过服务器端的持久化层将数据顺序落地磁盘，实现消息持久化减少随机读写的寻址开销</p><p>当接收到需要持久化的数据包时，按照包的类型保存，同时保存每次写入的offsett提供下一次持久化写位置文件</p><p>当宕机重启时，逻辑层负责从本地读取持久化文件来恢复数据</p></li><li><p>服务器和客户端通过tcp连接</p></li><li><p>客户端有两种：消息生产者，消息消费者</p><p>可以实现三种exchange模式的处理，fanout，在绑定当前交换机的所有队列中广播消息，direct，转发给key匹配队列，topic，和每个key进行字符串匹配，匹配上的转发消息</p></li></ul><h2 id="框架"><a href="#框架" class="headerlink" title="框架"></a>框架</h2><h3 id="整体框架"><a href="#整体框架" class="headerlink" title="整体框架"></a>整体框架</h3><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20210602162238.png" alt></p><h3 id="服务器：消息队列服务器"><a href="#服务器：消息队列服务器" class="headerlink" title="服务器：消息队列服务器"></a>服务器：消息队列服务器</h3><ol><li><p>接入层ConnectServer.cpp</p><p>（1）通过socket和客户端建立tcp连接，见“Connection:网络连接”部分</p><p>（2）和业务逻辑层通信</p><p>m_pQueueToLogicLayer(12345) enqueue方法：当出错时传输出错信息</p><p>m_pQueueLogicToConnect(54321) deqeue方法：接收订阅的数据</p></li><li><p>业务逻辑层logicServer.cpp</p><p>（1）和接入层通信</p><p>m_pQueueFromConnect(12345) deqeue方法：处理共享内存的数据时接收业务逻辑层的数据<br>m_pQueueToConnect(54321)  enqueue方法：发送订阅的消息，以及需要回复的消息</p><p>（2）和持久化层通信</p><p>m_pQueueToPersis(65432) enqueue方法：发送需要持久化的数据</p></li><li><p>持久化层PersistenceServer.cpp</p><p>和业务逻辑层通信：m_pQueueFromLogic(65432) deqeue方法将需要持久化数据出队并保存</p></li></ol><p>通信通道：基于共享内存的队列</p><ol><li><p>初始化：创建信号量互斥锁，打开对应共享内存</p><p>信号量互斥锁</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">m_iSemId=semget(m_iSemKey,<span class="number">1</span>,IPC_CREAT|<span class="number">0666</span>);<span class="comment">//创建一个新的信号量或获取一个已经存在的信号量的键值。 IPC_CREAT如果信号量不存在，则创建一个信号量，否则获取</span></span><br><span class="line">semun arg;</span><br><span class="line">semid_ds semDs;</span><br><span class="line">arg.buf=&amp;semDs;</span><br><span class="line"><span class="keyword">int</span> ret=semctl(m_iSemId,<span class="number">0</span>,IPC_STAT,arg);<span class="comment">//从关联于 semid 的内核数据结构复制数据到 arg.buf 指向的semid_ds 数据结构</span></span><br><span class="line"><span class="comment">//未曾使用或者上次op操作超过3分钟则释放锁</span></span><br><span class="line"><span class="keyword">if</span>(semDs.sem_otime==<span class="number">0</span>||((semDs.sem_otime&gt;<span class="number">0</span>)&amp;&amp;(time(<span class="literal">NULL</span>)-semDs.sem_otime&gt;<span class="number">3</span>*<span class="number">60</span>)))</span><br><span class="line">&#123;</span><br><span class="line">  semun arg;</span><br><span class="line">  arg.val=<span class="number">1</span>;</span><br><span class="line">  ret=semctl(m_iSemId,<span class="number">0</span>,SETVAL,arg);<span class="comment">//值设置为1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>基于共享内存的队列</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建或者打开共享内存</span></span><br><span class="line"><span class="keyword">int</span> shmId=shmget(iShmKey,<span class="keyword">sizeof</span>(QueueHead)+iQueueSize,IPC_CREAT|IPC_EXCL|<span class="number">0666</span>);</span><br><span class="line">m_pMemAddr=(<span class="keyword">char</span> *)shmat(shmId,<span class="literal">NULL</span>,<span class="number">0</span>);<span class="comment">//启动共享内存</span></span><br><span class="line"><span class="comment">//初始化队列头</span></span><br><span class="line"><span class="keyword">if</span>(isExist)<span class="comment">//共享内存之前存在</span></span><br><span class="line">&#123;</span><br><span class="line">  m_pQueueHead=(QueueHead *)(m_pMemAddr);<span class="comment">//队列头部重新指向</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="built_in">memset</span>(m_pMemAddr,<span class="number">0</span>,iQueueSize+<span class="keyword">sizeof</span>(QueueHead));</span><br><span class="line">  m_pQueueHead=(QueueHead *)(m_pMemAddr);</span><br><span class="line">  m_pQueueHead-&gt;m_iBlockNum=<span class="number">0</span>;</span><br><span class="line">  m_pQueueHead-&gt;m_iLen=iQueueSize;</span><br><span class="line">  m_pQueueHead-&gt;m_iHead=<span class="number">0</span>;</span><br><span class="line">  m_pQueueHead-&gt;m_iTail=<span class="number">0</span>;</span><br><span class="line">  m_pQueueHead-&gt;m_iUsedNum=<span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>多线程下的入队和出队操作</p><p>（1）原因：多线程之间使用队列是一定需要做到同步的，在一个线程读写的时候，阻塞另一个线程。设置过程时间很短暂，所以没有必要使用锁。使用原子操作：要不操作全部完成要么一点都不开始。这和数据库的事务不同，因为原子操作没有回滚。也就是说原子操作，一旦开始某个操作，那么就必须要完全的完成。</p><p>（2）使用方法：<strong>CAS原子操作</strong>——Compare &amp; Set，或是 Compare &amp; Swap。</p><p>核心思想：队列结构设置使用人数，每次轮询使用人数如果是0，则设置为1，即获得了队列的使用权，反之则有其他线程在进行操作，继续等待。类似自旋锁</p><p>核心代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//获取使用权</span></span><br><span class="line"><span class="keyword">while</span>(<span class="literal">true</span>)</span><br><span class="line">&#123;</span><br><span class="line">  rmb();<span class="comment">//rmb() 確保 barrier 之前的 read operation 都能在 barrier 之後的 read operation 之前發生，簡單來說就是確保 barrier 前後的 read operation 的順序</span></span><br><span class="line">  <span class="keyword">if</span>(!CAS32(&amp;m_pQueueHead-&gt;m_iUsedNum,<span class="number">0</span>,<span class="number">1</span>)) ??</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">continue</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  wmb();<span class="comment">//wmb() 如同 rmb() 但是只針對 write operation</span></span><br><span class="line">  <span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//...</span></span><br><span class="line"><span class="comment">//操作队列</span></span><br><span class="line"><span class="comment">//...</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//释放使用权</span></span><br><span class="line"><span class="keyword">while</span>(<span class="literal">true</span>)</span><br><span class="line">&#123;</span><br><span class="line">  rmb();</span><br><span class="line">  <span class="keyword">if</span>(!CAS32(&amp;m_pQueueHead-&gt;m_iUsedNum,<span class="number">1</span>,<span class="number">0</span>))</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">continue</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  wmb();</span><br><span class="line">  <span class="keyword">break</span>;</span><br></pre></td></tr></table></figure></li></ol><h3 id="客户端"><a href="#客户端" class="headerlink" title="客户端"></a>客户端</h3><p>包含消息生产者和消费者</p><h4 id="生产者"><a href="#生产者" class="headerlink" title="生产者"></a>生产者</h4><p>（1）和服务端建立连接</p><p>（2）消息格式 CreatePublishMessage</p><ul><li>设置消息格式 4 = CMD_CREATE_PUBLISH</li><li>设置绑定的交换机名称：m_strExchangeName(istrExName)</li><li>绑定键名称：m_strRoutingKey(istrKey)</li><li>消息主体：m_strMsgBody(istrMsgBody)</li><li>优先级：m_iPriority(iPriority)</li><li>是否可持久化：m_bDurable(ibDurable)</li><li>消息序列号：m_iMsgSeq(-1)</li><li>确认级别：m_iConfirmLevel(iConfirmLevel)</li></ul><h4 id="交换机"><a href="#交换机" class="headerlink" title="交换机"></a>交换机</h4><p>交换器从生产者那收到消息后，根据Routing Key、Exchange Type和Binding key联合使用，分发消息到queue中。</p><p>消息格式：CreateExchangeMessage</p><ul><li><p>设置标志位为1 = CMD_CREATE_EXCNANGE</p></li><li><p>设置交换机名称：m_strExchangeName(istrName)</p></li><li><p>交换机类型：m_iExchangeType(iExchangeType)，三种类型可以选择EXCHANGE_TYPE_FANOUT=1（广播），EXCHANGE_TYPE_DIRECT=2（绑定键与该消息的路由键完全匹配的队列），EXCHANGE_TYPE_TOPIC=3（根据主题）;</p></li><li><p>是否持久化：m_bDurable(ibDurable)</p></li><li><p>是否自动删除m_bAutoDel(ibAutoDel)</p></li></ul><h4 id="队列"><a href="#队列" class="headerlink" title="队列"></a>队列</h4><p>消息最终被送到Queue中并等待consumer取走，一个message可以被同时拷贝到多个queue中。</p><p>消息格式：CreateQueueMessage</p><ul><li>设置标志位2 = CMD_CREATE_QUEUE</li><li>设置队列名称 m_strQueueName(istrName)</li><li>设置优先级m_iPriority(iPriority)</li><li>是否持久化：m_bDurable(ibDurable)</li><li>是否自动删除m_bAutoDel(ibAutoDel)</li></ul><h4 id="绑定"><a href="#绑定" class="headerlink" title="绑定"></a>绑定</h4><p>通过Binding将Exchange与Queue关联起来，在绑定（Binding）Exchange与Queue的同时，一般会指定一个binding key。可以将多个Queue绑定到同一个Exchange上，并且可以指定相同的Binding Key。</p><p>消息格式：CreateBindingMessage</p><ul><li>设置标志位3 = CMD_CREATE_BINDING</li><li>绑定的交换机的名称：m_strExchangeName(istrExName)</li><li>绑定的队列的名称m_strQueueName(istrQueueName)</li><li>绑定键的名称：m_strBindingKey(istrKey)</li></ul><p>1.Publisher:消息的生产者，向交换器Exchange发布消息，发送消息时还要指定Routing Key。</p><p>7.Consumer:消息的消费者，如果有多个消费者同时订阅同一个Queue中的消息，Queue中的消息会被平摊给多个消费者。</p><h3 id="Connection-网络连接"><a href="#Connection-网络连接" class="headerlink" title="Connection:网络连接"></a>Connection:网络连接</h3><p>客户端通过tcp进行网络连接（Producer和Consumer都是通过TCP连接到Server的）</p><h4 id="服务器端"><a href="#服务器端" class="headerlink" title="服务器端"></a>服务器端</h4><p>（1）初始化：int ConnectServer::Init()</p><p>套接字：socket-&gt;SetNonBlock-&gt;setsockopt</p><p>核心代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">m_iListenFd=socket(AF_INET,SOCK_STREAM,<span class="number">0</span>)；</span><br><span class="line">SetNonBlock(m_iListenFd)；</span><br><span class="line"><span class="keyword">int</span> ret=setsockopt(m_iListenFd,SOL_SOCKET,SO_REUSEADDR,(<span class="keyword">char</span> *)&amp;opt,<span class="keyword">sizeof</span>(opt));</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">serverAddr</span>;</span></span><br><span class="line"><span class="built_in">memset</span>(&amp;serverAddr,<span class="number">0</span>,<span class="keyword">sizeof</span>(serverAddr));</span><br><span class="line">serverAddr.sin_family=AF_INET;</span><br><span class="line">serverAddr.sin_port=htons(SERVER_DEFAULT_PORT);</span><br><span class="line">serverAddr.sin_addr.s_addr=htonl(INADDR_ANY);</span><br><span class="line">bind(m_iListenFd,(struct sockaddr *)&amp;serverAddr,<span class="keyword">sizeof</span>(serverAddr);</span><br><span class="line"><span class="built_in">listen</span>(m_iListenFd,<span class="number">512</span>);</span><br></pre></td></tr></table></figure><p>I/O多路复用：epoll_create（LT模式）-&gt;epoll_ctl（EPOLLIN读事件）</p><p>核心代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m_iEpollFd=epoll_create(CLIENT_EPOLL_COUNT);</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">epoll_event</span> <span class="title">event</span>;</span></span><br><span class="line">event.events=EPOLLIN;</span><br></pre></td></tr></table></figure><p>（2）运行：int ConnectServer::Run()</p><p>每轮处理数据：epoll_wait（所有要处理的事件）-&gt;读事件（ipEvents[i].events&amp;EPOLLIN）:如果是监听的套接字accept-&gt;SetNonBlock-&gt;添加到用户连接  -&gt;recv（接收数据）：处理接收到的数据</p><p>核心代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> num=epoll_wait(m_iEpollFd,pEvents,CLIENT_EPOLL_COUNT,<span class="number">0</span>);</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;iNum;++i) &#123;</span><br><span class="line">  <span class="keyword">if</span>(ipEvents[i].events&amp;EPOLLIN) &#123;</span><br><span class="line">    <span class="comment">//获取当前连接pClientConnect</span></span><br><span class="line">    <span class="keyword">int</span> sockfd=pClientConnect-&gt;m_iSockfd;</span><br><span class="line">    <span class="keyword">if</span>(sockfd==m_iListenFd)</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">cliAddr</span>;</span></span><br><span class="line">      <span class="built_in">memset</span>(&amp;cliAddr,<span class="number">0</span>,<span class="keyword">sizeof</span>(cliAddr));</span><br><span class="line">      <span class="keyword">socklen_t</span> len=<span class="keyword">sizeof</span>(cliAddr);</span><br><span class="line">      <span class="keyword">int</span> confd=accept(m_iListenFd,(struct sockaddr *)&amp;cliAddr,&amp;len);<span class="comment">//接收监听端口的连接</span></span><br><span class="line">      SetNonBlock(confd);</span><br><span class="line">      <span class="keyword">int</span> iSendBufSize=SOCK_SEND_BUFF_SIZE;</span><br><span class="line">      <span class="keyword">int</span> iRevBufSize=SOCK_RECV_BUFF_SIZE;</span><br><span class="line">      setsockopt(confd,SOL_SOCKET,SO_SNDBUF,(<span class="keyword">char</span> *)&amp;iSendBufSize,<span class="keyword">sizeof</span>(<span class="keyword">int</span>));</span><br><span class="line">      setsockopt(confd,SOL_SOCKET,SO_RCVBUF,(<span class="keyword">char</span> *)&amp;iRevBufSize,<span class="keyword">sizeof</span>(<span class="keyword">int</span>));</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">int</span> ret=recv(sockfd,pClientConnect-&gt;m_pRecvTail,iRecvBytes,<span class="number">0</span>);<span class="comment">//准备接收数据</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="comment">//收到其他事件，断开连接</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="客户端-1"><a href="#客户端-1" class="headerlink" title="客户端"></a>客户端</h4><p>Producer和Consumer生产者和消费者</p><p>（1）初始化：int Client::BuildConnection(const char *ipSeverIp,unsigned short iServerPort)</p><p>套接字：socket-&gt;connect-&gt;SetNonBlock-&gt;setsockopt</p><p>核心代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">m_iSockfd=socket(AF_INET,SOCK_STREAM,<span class="number">0</span>);</span><br><span class="line">sockaddr_in serverAddr;</span><br><span class="line"><span class="built_in">memset</span>(&amp;serverAddr,<span class="number">0</span>,<span class="keyword">sizeof</span>(serverAddr));</span><br><span class="line">serverAddr.sin_family=AF_INET;</span><br><span class="line">serverAddr.sin_port=htons(iServerPort);</span><br><span class="line">serverAddr.sin_addr.s_addr=inet_addr(ipSeverIp);</span><br><span class="line"><span class="keyword">int</span> ret=<span class="built_in">connect</span>(m_iSockfd,(struct sockaddr *)&amp;serverAddr,<span class="keyword">sizeof</span>(serverAddr));</span><br><span class="line">FuncTool::SetNonBlock(m_iSockfd);</span><br><span class="line"><span class="keyword">int</span> iSendBufSize=SOCK_SEND_BUFF_SIZE;</span><br><span class="line"><span class="keyword">int</span> iRevBufSize=SOCK_RECV_BUFF_SIZE;</span><br><span class="line">setsockopt(m_iSockfd,SOL_SOCKET,SO_SNDBUF,(<span class="keyword">char</span> *)&amp;iSendBufSize,<span class="keyword">sizeof</span>(<span class="keyword">int</span>));</span><br><span class="line">setsockopt(m_iSockfd,SOL_SOCKET,SO_RCVBUF,(<span class="keyword">char</span> *)&amp;iRevBufSize,<span class="keyword">sizeof</span>(<span class="keyword">int</span>));</span><br></pre></td></tr></table></figure><p>（2）运行：</p><p>以创建交换机为例</p><p><strong>初始化</strong>：初始化交换机消息头CreateExchangeMessage（oMsg），调用GetMessagePack，将要创建的内容写入消息包</p><p><strong>发送创建数据</strong>：I/O多路复用：poll（监听写事件）</p><p>poll等待套接字可写后发送数据包</p><p>核心代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//等待套接字可写</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">pollfd</span> <span class="title">fds</span>[1];</span><span class="comment">//定义pollfd结构的文件句柄</span></span><br><span class="line">fds[<span class="number">0</span>].fd=m_iSockfd;</span><br><span class="line">fds[<span class="number">0</span>].events=POLLOUT|POLLERR;<span class="comment">//监听写事件</span></span><br><span class="line"><span class="keyword">while</span>(<span class="literal">true</span>)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">int</span> ret=poll(fds,<span class="number">1</span>,<span class="number">-1</span>);</span><br><span class="line">  <span class="keyword">if</span>(fds[<span class="number">0</span>].revents&amp;POLLOUT)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">int</span> ret=send(iSockfd,pTemp,iLeft,<span class="number">0</span>);<span class="comment">//发送数据</span></span><br></pre></td></tr></table></figure><p><strong>接收创建回复</strong>：I/O多路复用：poll（监听读事件）</p><p>核心代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">do</span><br><span class="line">&#123;</span><br><span class="line">  struct pollfd fds[1];//定义pollfd结构的文件句柄</span><br><span class="line">  fds[0].fd=m_iSockfd;</span><br><span class="line">  fds[0].events=POLLIN|POLLERR;//监听读事件</span><br><span class="line">  while(true)</span><br><span class="line">  &#123;</span><br><span class="line">    int ret=poll(fds,1,-1);</span><br><span class="line">    if(fds[0].revents&amp;POLLIN)</span><br><span class="line">    &#123;</span><br><span class="line">      break;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  ret = recv(m_iSockfd,m_pRecvTail,iFreeSpace,0);</span><br><span class="line">&#125; while (ret!=SUCCESS);</span><br><span class="line">//读取消息，处理消息</span><br></pre></td></tr></table></figure><p>（发送数据只有发送，监听写事件即可，不需要回复，仅记录确认号即可）</p><h4 id="连接"><a href="#连接" class="headerlink" title="连接"></a>连接</h4><p>输入./mq_server.start.sh启动服务端</p><p>输入multi_producer_start.sh 1 10启动客户端，创建一个生产者，循环发送长度为10的消息</p><p>服务端日志</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20210524164800.png" alt></p><h2 id="后续改进的方向"><a href="#后续改进的方向" class="headerlink" title="后续改进的方向"></a>后续改进的方向</h2><p>（1） mq 得支持可伸缩性，需要的时候快速扩容，就可以增加吞吐量和容量?</p><p>设计个分布式的系统呗，参照一下 kafka 的设计理念，broker -&gt; topic -&gt; partition，每个 partition 放一个机器，就存一部分数据。如果现在资源不够了，给 topic 增加 partition，然后做数据迁移，增加机器，存放更多数据，提供更高的吞吐量了</p><p>（3） mq 的可用性啊?</p><p>参考 kafka 的高可用保障机制。多副本 -&gt; leader &amp; follower -&gt; broker 挂了重新选举 leader 即可对 外服务。</p><p>（4）能不能支持数据 0 丢失?</p><p>参考 kafka 数据零丢失方案。</p><h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><p>1、问题：tcp连接中socket端口号填的非本机，连接报错</p><p>tcp连接端口的可重用:设置SO_REUSEADDR</p><pre><code>int opt=1;int ret=setsockopt(m_iListenFd,SOL_SOCKET,SO_REUSEADDR,(char *)&amp;opt,sizeof(opt));//设置套接字</code></pre><p>2、poll调用问题：</p><p>当有生产者生产消息时，消费者才能读到（poll监听事件），反之poll会一直阻塞等待</p><p>所以需要先启动生产者，再启动消费者</p><h2 id="整体流程"><a href="#整体流程" class="headerlink" title="整体流程"></a>整体流程</h2><p>服务端类调用图</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20210514152704.png" alt></p><p>客户端（生产者和多个消费者）类调用图</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20210514152721.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;项目地址：&lt;a href=&quot;https://github.com/iambajie/mq-cplusplus&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;https://github.com/iambajie/mq-cplusplus&lt;/a&gt;&lt;/p&gt;&lt;h2 id=&quot;运行&quot;&gt;&lt;a href=&quot;#运行&quot; class=&quot;headerlink&quot; title=&quot;运行&quot;&gt;&lt;/a&gt;运行&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;step1:终端输入&lt;code&gt;make&lt;/code&gt;生成服务端和客户端文件：通过makefile生成服务端和客户端文件：&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;（1）按序生成mq_normal_producer mq_normal_consumer1 mq_normal_consumer2 mq_ack_producer mq_ack_consumer mq_multi_producer mq_priority_consumer mq_multi_consumer mq_priority_producer mq_pull_consumer mq_durable_producer mq_durable_consumer （客户端）&lt;/p&gt;
    
    </summary>
    
    
      <category term="mq" scheme="https://www.xiapf.com/categories/mq/"/>
    
    
      <category term="mq" scheme="https://www.xiapf.com/tags/mq/"/>
    
  </entry>
  
  <entry>
    <title>mq——mac下安装、使用</title>
    <link href="https://www.xiapf.com/blogs/mq/"/>
    <id>https://www.xiapf.com/blogs/mq/</id>
    <published>2021-06-16T05:24:58.000Z</published>
    <updated>2021-06-17T03:52:00.228Z</updated>
    
    <content type="html"><![CDATA[<p>c++使用rabbitmq例子见项目：<a href="https://github.com/iambajie/rabbitmq-tutorials-cplusplus" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/iambajie/rabbitmq-tutorials-cplusplus</a></p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p><a href="https://blog.csdn.net/keysilence1/article/details/80264882" target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/keysilence1/article/details/80264882</a></p><p>brew install rabbitmq安装 </p><a id="more"></a><p>安装目录在/usr/local/Cellar/rabbitmq/3.8.2/下，以root权限启动 sudo sbin/rabbitmq-server启动</p><p>输入<a href="http://localhost:15672/" target="_blank" rel="external nofollow noopener noreferrer">http://localhost:15672/</a>  即可进入mq，默认用户名和密码是guest</p><h2 id="简单使用"><a href="#简单使用" class="headerlink" title="简单使用"></a>简单使用</h2><p>进入sbin文件夹下</p><p>创建用户：sudo ./rabbitmqctl add_user bajie 123</p><p>设置用户身份：sudo ./rabbitmqctl set_user_tags bajie administrator</p><p>分配管理员权限：sudo ./rabbitmqctl set_permissions -p “/“ bajie “.*” “.*” “.*”</p><h2 id="c-使用rabbitmq"><a href="#c-使用rabbitmq" class="headerlink" title="c++使用rabbitmq"></a>c++使用rabbitmq</h2><p>参考：</p><p>​        例子</p><ul><li><p><a href="https://blog.csdn.net/qq78442761/article/details/93158659" target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/qq78442761/article/details/93158659</a> （简单实现）</p></li><li><p><a href="https://blog.csdn.net/qq78442761/article/details/94012784" target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/qq78442761/article/details/94012784</a> （公平分发）</p><p>api说明</p></li><li><p><a href="https://blog.csdn.net/u012861467/article/details/106517031" target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/u012861467/article/details/106517031</a></p></li><li><p><a href="https://blog.csdn.net/weixin_44353800/article/details/107733075" target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/weixin_44353800/article/details/107733075</a></p></li><li><p><a href="https://blog.csdn.net/Hopoxipo/article/details/103763001" target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/Hopoxipo/article/details/103763001</a></p></li><li><p><a href="https://blog.csdn.net/u013946356/article/details/82420489" target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/u013946356/article/details/82420489</a></p></li></ul><h3 id="一个生产者对一个消费者"><a href="#一个生产者对一个消费者" class="headerlink" title="一个生产者对一个消费者"></a>一个生产者对一个消费者</h3><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20210330153556.png" alt></p><p>生产者：声明一个队列，将数据amqp_basic_public存入队列中</p><p>消费者：根据队列名称用amqp_basic_consum取其中的数据（一大坨的读）</p><p>代码实现：Send.cpp，Recv.cpp</p><p>出现问题：生产者成功连接服务器，但是无法把消息传输进队列：没有通过amqp_queue_declare声明队列，所以传输的时候找不到相应名字的队列</p><h3 id="一个生产者对多个消费者：工作队列（多个消费者同时处理，在多个工作人员之间分配耗时的任务）"><a href="#一个生产者对多个消费者：工作队列（多个消费者同时处理，在多个工作人员之间分配耗时的任务）" class="headerlink" title="一个生产者对多个消费者：工作队列（多个消费者同时处理，在多个工作人员之间分配耗时的任务）"></a>一个生产者对多个消费者：工作队列（多个消费者同时处理，在多个工作人员之间分配耗时的任务）</h3><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20210330161732.png" alt></p><p>消息持久性：设置持久队列（生产者端声明队列时设置）</p><p>消息确认：设置ack应答（取了一条数据之后，消费者回复确认）</p><p>公平分发：amqp_basic_get()去读取RabbitMQ的数据，同时使用amqp_read_message()把数据给读出来。读完一条数据就发一个确认</p><p>代码实现：NewTask.cpp，Worker1.cpp，Worker2.cpp</p><h3 id="生产者把消息发到交换机"><a href="#生产者把消息发到交换机" class="headerlink" title="生产者把消息发到交换机"></a>生产者把消息发到交换机</h3><p><a href="https://blog.csdn.net/weixin_42148156/article/details/113445913" target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/weixin_42148156/article/details/113445913</a> （交换机函数）</p><p>如果没有队列绑定到交换，则消息将丢失，但这对我们来说是可以的。如果没有消费者在听，我们可以放心地丢弃该消息。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20210331153808.png" alt></p><p>生产者：声明交换机</p><p>fanout类型交换机：将接收到的所有消息广播到它知道的所有队列中</p><p><code>amqp_exchange_declare(connState, 1, amqp_cstring_bytes(exchange.c_str()), amqp_cstring_bytes(type.c_str()), **false**, **true**, **false**, **false**, amqp_empty_table);</code></p><p>消费者：创建随机队列（声明队列的时候把<strong><em>exclusive</em></strong>设置为true，不指定队列的名称，由系统随机生成），通过绑定告诉交换机将消息发送到我们的队列</p><p><code>amqp_queue_declare_ok_t_ *channel_id = amqp_queue_declare(connState, 1, amqp_cstring_bytes(&quot;&quot;), **false**, **true**, **true**, **false**, amqp_empty_table);</code></p><p><code>amqp_queue_bind(connState, 1, channel_id-&gt;queue, amqp_cstring_bytes(exchange.c_str()), amqp_cstring_bytes(&quot;&quot;), amqp_empty_table);</code></p><p>代码实现：EmitLog.cpp，ReceiveLogs.cpp，ReceiveLogs2.cpp（模拟有两个队列）</p><p>出现问题：不知道如何设置随机队列，以及生成的随机队列名称如何获取（声明队列后的返回值内包含了随机队列名称）</p><h3 id="选择性的接收消息"><a href="#选择性的接收消息" class="headerlink" title="选择性的接收消息"></a>选择性的接收消息</h3><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20210407135946.png" alt></p><p>生产者：直接交换的交换机：一条消息进入绑定键与该消息的路由键完全匹配的队列 。</p><p>消费者：根据绑定建有选择的接收消息</p><p>代码实现：EmitLogDirect.cpp，ReceiveLogsTopic.cpp，ReceiveLogsTopic2.cpp（模拟有两个不同绑定建的队列）</p><p>出现问题：生产者绑定键的类型从操作台接收的时候采用字符数组，后续转换为了amqp_bytes_t类型（是指针保存为地址），而消费者端的绑定建类型是字符串类型所以两者绑定建不同，消费者读不到数据：将生产者端通过getlin(cin,s)直接读入字符串类型的绑定建</p><h3 id="基于模式（主题）接收消息"><a href="#基于模式（主题）接收消息" class="headerlink" title="基于模式（主题）接收消息"></a>基于模式（主题）接收消息</h3><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20210407144146.png" alt></p><p>主题类型的交换机：routing-key必须是单词列表，以点分隔</p><p>*（星号）可以代替一个单词。</p><p>＃（哈希）可以替代零个或多个单词。</p><p>代码实现：EmitLogTopic.cpp，ReceiveLogsDirect.cpp，ReceiveLogsDirect2.cpp（模拟有两个不同绑定建的队列）</p><h3 id="使用rabbitmq构建rpc系统"><a href="#使用rabbitmq构建rpc系统" class="headerlink" title="使用rabbitmq构建rpc系统"></a>使用rabbitmq构建rpc系统</h3><p><a href="https://blog.csdn.net/singleroot/article/details/51547603" target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/singleroot/article/details/51547603</a> （例子参考）</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20210407154207.png" alt></p><p>RPC将像这样工作：</p><ul><li>对于RPC请求，客户端发送一条具有以下两个属性的消息： replyTo（设置为仅为该请求创建的匿名互斥队列）和correlationId（设置为每个请求的唯一值）。</li><li>该请求被发送到rpc_queue队列。</li><li>RPC工作程序（又名：服务器）正在等待该队列上的请求。出现请求时，它会使用replyTo字段中的队列来完成工作并将带有结果的消息发送回客户端。</li><li>客户端等待答复队列中的数据。出现消息时，它会检查correlationId属性。如果它与请求中的值匹配，则将响应返回给应用程序。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;c++使用rabbitmq例子见项目：&lt;a href=&quot;https://github.com/iambajie/rabbitmq-tutorials-cplusplus&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;https://github.com/iambajie/rabbitmq-tutorials-cplusplus&lt;/a&gt;&lt;/p&gt;&lt;h2 id=&quot;安装&quot;&gt;&lt;a href=&quot;#安装&quot; class=&quot;headerlink&quot; title=&quot;安装&quot;&gt;&lt;/a&gt;安装&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/keysilence1/article/details/80264882&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;https://blog.csdn.net/keysilence1/article/details/80264882&lt;/a&gt;&lt;/p&gt;&lt;p&gt;brew install rabbitmq安装 &lt;/p&gt;
    
    </summary>
    
    
      <category term="mq" scheme="https://www.xiapf.com/categories/mq/"/>
    
    
      <category term="mq" scheme="https://www.xiapf.com/tags/mq/"/>
    
  </entry>
  
  <entry>
    <title>redis——mac下安装、使用</title>
    <link href="https://www.xiapf.com/blogs/redis/"/>
    <id>https://www.xiapf.com/blogs/redis/</id>
    <published>2021-06-16T05:24:55.000Z</published>
    <updated>2021-06-16T05:42:20.054Z</updated>
    
    <content type="html"><![CDATA[<h2 id="redis安装"><a href="#redis安装" class="headerlink" title="redis安装"></a>redis安装</h2><p> <a href="https://blog.csdn.net/realize_dream/article/details/106227622" target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/realize_dream/article/details/106227622</a></p><p>brew install redis安装</p><p>配置文件在/usr/local/etc/redis.conf</p><p>启动：redis-server  /usr/local/etc/redis.conf</p><a id="more"></a><p>登录：redis-cli -h 127.0.0.1 -p 6379</p><h2 id="客户端安装"><a href="#客户端安装" class="headerlink" title="客户端安装"></a>客户端安装</h2><p><a href="https://github.com/qishibo/AnotherRedisDesktopManager" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/qishibo/AnotherRedisDesktopManager</a></p><p>输入ip和端口进入客户端</p><h2 id="c-连接操作redis"><a href="#c-连接操作redis" class="headerlink" title="c++连接操作redis"></a>c++连接操作redis</h2><p><a href="https://blog.csdn.net/wenfan0934/article/details/111475316" target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/wenfan0934/article/details/111475316</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;hiredis/hiredis.h&gt;</span></span></span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    redisContext* conn = redisConnect(<span class="string">"127.0.0.1"</span>, <span class="number">6379</span>); <span class="comment">//连接</span></span><br><span class="line">    <span class="keyword">if</span>(conn-&gt;err)</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span>&lt;&lt;<span class="string">"connect error:"</span>&lt;&lt;conn-&gt;errstr&lt;&lt;<span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">  </span><br><span class="line">    <span class="comment">//字符串</span></span><br><span class="line">    redisReply* reply = (redisReply*) redisCommand(conn, <span class="string">"set java 0.9"</span>);<span class="comment">//增加</span></span><br><span class="line">    redisReply* reply = (redisReply*) redisCommand(conn, <span class="string">"set java cat"</span>);<span class="comment">//修改</span></span><br><span class="line">    redisReply* reply = (redisReply*) redisCommand(conn, <span class="string">"del java"</span>);<span class="comment">//删除</span></span><br><span class="line">    redisReply* reply = (redisReply*) redisCommand(conn, <span class="string">"get bajie"</span>);<span class="comment">//查询</span></span><br><span class="line">    <span class="comment">//std::cout&lt;&lt;reply-&gt;str&lt;&lt;std::endl;//del不可以打印，不能打印不存在的键</span></span><br><span class="line">    freeReplyObject(reply);</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">//集合</span></span><br><span class="line">    redisReply* reply = (redisReply*) redisCommand(conn, <span class="string">"sadd l_bajie bl bg gb lb"</span>);<span class="comment">//增加</span></span><br><span class="line">    redisReply* reply = (redisReply*) redisCommand(conn, <span class="string">"smembers l_bajie"</span>);<span class="comment">//查询</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; reply-&gt;elements; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        redisReply* cur = reply-&gt;element[i];<span class="comment">//元素</span></span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span>&lt;&lt;cur-&gt;str&lt;&lt;<span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span>&lt;&lt;reply-&gt;elements&lt;&lt;<span class="built_in">std</span>::<span class="built_in">endl</span>;<span class="comment">//个数</span></span><br><span class="line">    redisReply* reply = (redisReply*) redisCommand(conn, <span class="string">"srem l_bajie bg"</span>);<span class="comment">//删除部分</span></span><br><span class="line">    redisReply* reply = (redisReply*) redisCommand(conn, <span class="string">"del l_bajie"</span>);<span class="comment">//删除</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment">//列表</span></span><br><span class="line">    redisReply* reply = (redisReply*) redisCommand(conn, <span class="string">"lpush l_bajie bl bg gb lb"</span>);<span class="comment">//增加  bl在最后</span></span><br><span class="line">    redisReply* reply = (redisReply*) redisCommand(conn, <span class="string">"rpush r_bajie bl bg gb lb"</span>);<span class="comment">//增加 bl在最前面</span></span><br><span class="line">    redisReply* reply = (redisReply*) redisCommand(conn, <span class="string">"lrange l_bajie 0 -1"</span>);<span class="comment">//查询</span></span><br><span class="line">    redisReply* reply = (redisReply*) redisCommand(conn, <span class="string">"lpush l_bajie xxx"</span>);<span class="comment">//头部增加</span></span><br><span class="line">    redisReply* reply = (redisReply*) redisCommand(conn, <span class="string">"lset l_bajie 0 123"</span>);<span class="comment">//index=0的位置修改</span></span><br><span class="line">    redisReply* reply = (redisReply*) redisCommand(conn, <span class="string">"lrem l_bajie 0 123"</span>);<span class="comment">//index=0的位置删除</span></span><br><span class="line">    </span><br><span class="line">  </span><br><span class="line">    <span class="comment">//哈希表</span></span><br><span class="line">    redisReply* reply = (redisReply*) redisCommand(conn, <span class="string">"hset h_bajie java cat"</span>);<span class="comment">//增加、</span></span><br><span class="line">    redisReply* reply = (redisReply*) redisCommand(conn, <span class="string">"hlen h_bajie"</span>);<span class="comment">//长度</span></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span>&lt;&lt;reply-&gt;integer&lt;&lt;<span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    redisReply* reply = (redisReply*) redisCommand(conn, <span class="string">"hkeys h_bajie"</span>);<span class="comment">//查询键</span></span><br><span class="line">    redisReply* reply = (redisReply*) redisCommand(conn, <span class="string">"hvals h_bajie"</span>);<span class="comment">//查询值</span></span><br><span class="line">    redisReply* reply = (redisReply*) redisCommand(conn, <span class="string">"hgetall h_bajie"</span>);<span class="comment">//查询所有</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//zset</span></span><br><span class="line">    redisReply* reply = (redisReply*) redisCommand(conn, <span class="string">"zadd z_bajie 11 d"</span>);<span class="comment">//增加 按照分数（从大到小排列）</span></span><br><span class="line">    redisReply* reply = (redisReply*) redisCommand(conn, <span class="string">"zrange z_bajie 0 -1"</span>);<span class="comment">//查询</span></span><br><span class="line"></span><br><span class="line">     </span><br><span class="line">    redisFree(conn);<span class="comment">//释放</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;redis安装&quot;&gt;&lt;a href=&quot;#redis安装&quot; class=&quot;headerlink&quot; title=&quot;redis安装&quot;&gt;&lt;/a&gt;redis安装&lt;/h2&gt;&lt;p&gt; &lt;a href=&quot;https://blog.csdn.net/realize_dream/article/details/106227622&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;https://blog.csdn.net/realize_dream/article/details/106227622&lt;/a&gt;&lt;/p&gt;&lt;p&gt;brew install redis安装&lt;/p&gt;&lt;p&gt;配置文件在/usr/local/etc/redis.conf&lt;/p&gt;&lt;p&gt;启动：redis-server  /usr/local/etc/redis.conf&lt;/p&gt;
    
    </summary>
    
    
      <category term="redis" scheme="https://www.xiapf.com/categories/redis/"/>
    
    
      <category term="redis" scheme="https://www.xiapf.com/tags/redis/"/>
    
  </entry>
  
  <entry>
    <title>在浏览器里输入一个URL到页面加载</title>
    <link href="https://www.xiapf.com/blogs/url/"/>
    <id>https://www.xiapf.com/blogs/url/</id>
    <published>2021-06-16T05:23:40.000Z</published>
    <updated>2021-06-16T05:34:16.579Z</updated>
    
    <content type="html"><![CDATA[<p>在浏览器里输入一个URL到页面加载，发生的顺序如下：</p><ol><li><p>DNS查询</p></li><li><p>TCP连接</p></li><li><p>发送HTTP请求</p></li><li><p>Server处理HTTP请求并返回HTTP报文</p></li><li><p>浏览器解析并render页面</p></li><li><p>HTTP连接断开</p></li><li><p>为什么要用域名？</p></li></ol><h2 id="DNS"><a href="#DNS" class="headerlink" title="DNS"></a>DNS</h2><ol><li><p>为什么要用域名？</p><p>域名的出现是为了方便记忆，因为域名比IP好记</p></li></ol><ol start="2"><li><p>DNS解析的基本流程？</p><p>首先，在<strong>本地域名服务器</strong>中根据域名查询IP地址，如果没有找到的情况下，本地域名服务器会向<strong>根域名服务器</strong>发送一个请求。</p><p>如果根域名服务器也不存在该域名时，本地域名会向com<strong>顶级域名服务</strong>器（TLD）发送一个请求，依次类推下去。</p><p>直到最后本地域名服务器得到google的IP地址并把它缓存到本地，供下次查询使用</p></li></ol><a id="more"></a><ol start="3"><li><p>DNS的根域名是什么，有几个Server？TLD DNS是什么？</p><p>Root DNS Server一般有13个 a~m.root-servers.net.</p><p>TLD:顶级域名服务器</p></li></ol><ol start="4"><li><p>DNS的优化策略是什么？在各个环节怎么做的？Chrome和各个操作系统怎么做的？</p><p>DNS是存在着多级缓存，从离浏览器的距离排序的话，有以下几种: </p><p>浏览器缓存:<em>chrome://net-internals/#dns</em></p><p>系统缓存:/etc/hosts下 more /etc/hosts进行查看</p><p>路由器缓存，IPS服务器缓存，根域名服务器缓存，顶级域名服务器缓存，主域名服务器缓存。</p></li></ol><ol start="5"><li><p>DNS负载均衡是什么，为什么要用？</p><p>根据你的位置或IP返回一个合适的IP给你用</p></li></ol><ol start="6"><li><p>DNS的记录类型有哪些？CNAME一般用在哪些场合？举例子说明一下。</p><p>A记录：用来指定主机名（或域名）对应的IP地址记录</p><p>CNAME记录：可以将一个域名或者子域名指向另外一个主机名，CDN就是这种方式。举个例子，例如公司A想把自己的图片放在Akamai的CDN上，A的子域名是img.abc.com, 而 Akamai的CDN服务域名是img.akaimacdn.com. 但是A公司期望用自己的域名吗，而不是Akamai的域名。为了实现这个目标，怎么办？是的，使用CNAME，只需要将子域名img.abc.com指向到img.akaimacdn.com。问题又来了，在哪里设置呢？肯定是在公司A这边的DNS server上，而不是Akamai那边。</p><p>NS记录：用来解析服务器记录，表明由哪台服务器对该域名进行解析</p></li></ol><ol start="7"><li><p>DNS的常用工具和命令有哪些？</p><p>dig查看域名记录累心中的cname和A记录</p><p>抓包工具Wireshark</p></li><li><p>DNS查询是用TCP还是UDP？一般用哪个端口？</p><p>当response的packet大于512字节时，就用TCP，反之，则用UDP。再回头看着89包，长度为70，所以用UDP了。那多问一个问题，什么情况下DNS查询包超过512？CNAME就有可能</p><p>DNS用的是53端口</p></li><li><p>DNS抓包抓过吗？Wireshark有用过吗？</p><p>有</p></li><li><p>请说明一下<a href="http://www.google.com" target="_blank" rel="external nofollow noopener noreferrer">www.google.com</a> 和google.com的区别，如何设置它们的DNS？</p><p>一样的</p><p>经常会去访问某个（测试）URL，例如<a href="http://192.168.1.8:8080/admin/login" target="_blank" rel="external nofollow noopener noreferrer">http://192.168.1.8:8080/admin/login</a>, 如果这样的话，我们极有可能存在一个问题，觉得每次输入ip真的很麻烦。可以利用DNS的原理，将某个伪hostname（abc-test）加入到hosts里，只需加入一条记录：</p><p><em>192.168.1.8． abc-test</em>  这样就可以用<a href="http://abc-test:8080/admin/login去访问了" target="_blank" rel="external nofollow noopener noreferrer">http://abc-test:8080/admin/login去访问了</a></p></li></ol><h2 id="TCP"><a href="#TCP" class="headerlink" title="TCP"></a>TCP</h2><ol><li><p>TCP/IP的4层模型了解吗？每层有哪些常见协议？</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20210601140752.png" alt></p></li><li><p>TCP/IP的三次握手了解吗？四次挥手是什么，了解多少？</p><p>三次握手：目的是连接服务器指定端口，建立 TCP 连接，并同步连接双方的序列号和确认号，交换 TCP 窗口大小信息。在 socket 编程中，<strong>客户端执行 connect()</strong> 时。将触发三次握手。</p><ul><li><p>第一次握手(SYN=1, seq=x):</p><p>客户端发送一个 TCP 的 SYN 标志位置1的包，指明客户端打算连接的服务器的端口，以及初始序号 X,保存在包头的序列号(Sequence Number)字段里。</p><p>发送完毕后，客户端进入 <strong>SYN_SEND</strong> 状态。</p></li><li><p>第二次握手(SYN=1, ACK=1, seq=y, ACKnum=x+1):</p><p>服务器发回确认包(ACK)应答。即 SYN 标志位和 ACK 标志位均为1。服务器端选择自己 ISN 序列号，放到 Seq 域里，同时将确认序号(Acknowledgement Number)设置为客户的 ISN 加1，即X+1。 发送完毕后，服务器端进入 <strong>SYN_RCVD</strong> 状态。</p></li><li><p>第三次握手(ACK=1，ACKnum=y+1)</p><p>客户端再次发送确认包(ACK)，SYN 标志位为0，ACK 标志位为1，并且把服务器发来 ACK 的序号字段+1，放在确定字段中发送给对方，并且在数据段放写ISN的+1</p><p>发送完毕后，客户端进入 <strong>ESTABLISHED</strong> 状态，当服务器端接收到这个包时，也进入 ESTABLISHED状态，TCP 握手结束。</p></li></ul><p>四次挥手：在 socket 编程中，<strong>任何一方执行 close()</strong> 操作即可产生挥手操作。</p><ul><li><p>第一次挥手(FIN=1，seq=x)</p><p>假设客户端想要关闭连接，客户端发送一个 FIN 标志位置为1的包，表示自己已经没有数据可以发送了，但是仍然可以接受数据。</p><p>发送完毕后，客户端进入 <strong>FIN_WAIT_1</strong> 状态。</p></li><li><p>第二次挥手(ACK=1，ACKnum=x+1)</p><p>服务器端确认客户端的 FIN 包，发送一个确认包，表明自己接受到了客户端关闭连接的请求，但还没有准备好关闭连接。</p><p>发送完毕后，服务器端进入 <strong>CLOSE_WAIT</strong> 状态，客户端接收到这个确认包之后，进入 <strong>FIN_WAIT_2</strong> 状态，等待服务器端关闭连接。</p></li><li><p>第三次挥手(FIN=1，seq=y)</p><p>服务器端准备好关闭连接时，向客户端发送结束连接请求，FIN 置为1。</p><p>发送完毕后，服务器端进入 <strong>LAST_ACK</strong> 状态，等待来自客户端的最后一个ACK。</p></li><li><p>第四次挥手(ACK=1，ACKnum=y+1)</p><p>客户端接收到来自服务器端的关闭请求，发送一个确认包，并进入 <strong>TIME_WAIT</strong>状态，等待可能出现的要求重传的 ACK 包。</p><p>服务器端接收到这个确认包之后，关闭连接，进入 <strong>CLOSED</strong> 状态。</p><p>客户端等待了某个固定时间（两个最大段生命周期，2MSL，2 Maximum Segment Lifetime）之后，没有收到服务器端的 ACK ，认为服务器端已经正常关闭连接，于是自己也关闭连接，进入 CLOSED 状态。</p></li></ul></li></ol><ol start="3"><li><p>HTTP和HTTPS在TCP握手上有什么不同？SSL/TLS握手流程了解吗？</p><p>HTTPS，需要有一个SSL/TLS的鉴权/认证，才能建立TCP链接。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20210602151030.png" alt></p><p>握手过程</p><p>第一步，客户端给出协议版本号、一个客户端生成的随机数（Client random），以及客户端支持的加密方法。</p><p>第二步，服务端确认双方使用的加密方法，并给出数字证书、以及一个服务器生成的随机数（Server random）。</p><p>第三步，客户端确认数字证书有效，然后生成一个新的随机数（Premaster secret），并使用数字证书中的公钥，加密这个随机数，发给服务端。</p><p>第四步，服务端使用自己的私钥，获取客户端发来的随机数（即Premaster secret）。</p><p>第五步，客户端和服务端根据约定的加密方法，使用前面的三个随机数，生成”对话密钥”（session key），用来加密接下来的整个对话过程。</p></li><li><p>SNI了解多少？如果SNI没有，该如何校验证书？</p><p>SNI（Server Name Indication）, server_name, 我们可以看见它的值是<a href="http://www.baidu.com/" target="_blank" rel="external nofollow noopener noreferrer">www.baidu.com</a>. 也就是说浏览器发送过来的证书是给baidu这个域名签发的。SNI用来校验该证书是不是为server name提供的域名签发的。如果不是，就会报错。</p><p>没有SNI：果Server端只有一个证书部署，那简单，就是按照部署的那个证书去判断。如果有多个证书部署呢？比如部署了aaa.com bbb.com ccc.com 三个域名的证书，那么就会按照缺省的去匹配</p><p>（</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在浏览器里输入一个URL到页面加载，发生的顺序如下：&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;DNS查询&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;TCP连接&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;发送HTTP请求&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Server处理HTTP请求并返回HTTP报文&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;浏览器解析并render页面&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;HTTP连接断开&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;为什么要用域名？&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;h2 id=&quot;DNS&quot;&gt;&lt;a href=&quot;#DNS&quot; class=&quot;headerlink&quot; title=&quot;DNS&quot;&gt;&lt;/a&gt;DNS&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;为什么要用域名？&lt;/p&gt;
&lt;p&gt;域名的出现是为了方便记忆，因为域名比IP好记&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;&lt;p&gt;DNS解析的基本流程？&lt;/p&gt;
&lt;p&gt;首先，在&lt;strong&gt;本地域名服务器&lt;/strong&gt;中根据域名查询IP地址，如果没有找到的情况下，本地域名服务器会向&lt;strong&gt;根域名服务器&lt;/strong&gt;发送一个请求。&lt;/p&gt;
&lt;p&gt;如果根域名服务器也不存在该域名时，本地域名会向com&lt;strong&gt;顶级域名服务&lt;/strong&gt;器（TLD）发送一个请求，依次类推下去。&lt;/p&gt;
&lt;p&gt;直到最后本地域名服务器得到google的IP地址并把它缓存到本地，供下次查询使用&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>linux下网络编程</title>
    <link href="https://www.xiapf.com/blogs/linuxNet/"/>
    <id>https://www.xiapf.com/blogs/linuxNet/</id>
    <published>2021-06-16T05:20:27.000Z</published>
    <updated>2021-06-16T05:30:47.198Z</updated>
    
    <content type="html"><![CDATA[<p>参考：<a href="https://blog.csdn.net/wteruiycbqqvwt/article/details/90313475" target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/wteruiycbqqvwt/article/details/90313475</a></p><p>Linux I/O多路复用技术在比较多的TCP网络服务器中有使用：epoll,select</p><h2 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h2><h3 id="select缺陷"><a href="#select缺陷" class="headerlink" title="select缺陷"></a>select缺陷</h3><p>(1) 在Linux内核中，select所用到的FD_SET是有限的：内核中有个参数FD_SETSIZE定义了每个FD_SET的句柄个数：#define FD_SETSIZE 1024，最大检测1024个句柄</p><a id="more"></a><p>(2) 内核中实现select是使用轮询方法 ：每次检测都会遍历所有FD_SET中的句柄，select要检测的句柄数越多就会越费时</p><h3 id="epoll优点"><a href="#epoll优点" class="headerlink" title="epoll优点"></a>epoll优点</h3><p>（1）支持一个进程打开大数目的socket描述符：上限是最大可以打开的文件数目（cat /proc/sys/fs/file-max ）</p><p>（2）IO性能不会随着fd增大而线性减少：select/poll每次调用都会线性扫描全部的集合，导致效率呈现线性下降。但是epoll不存在这个问题，它只会对”活跃”的socket进行操作。</p><h2 id="epoll有两种触发模式"><a href="#epoll有两种触发模式" class="headerlink" title="epoll有两种触发模式"></a>epoll有两种触发模式</h2><ul><li><p>ET（边缘模式）：只支持no-block socket，有数据过来后，<code>epoll_wait()</code>会返回一次，一段时间内，该套接字就算有数据源源不断地过来<code>，epoll_wait()</code>也不会返回了。</p><p>设置方法：epoll_event设置为EPOLLET</p></li><li><p>LT（水平模式）：每次epoll_wait()返回后，事件处理后，如果之后还有数据，会不断触发，也就是说，一个套接字上一次完整的数据，epoll_wait()可能会返回多次，直到没有数据为止。</p><p>设置方法：epoll_event设置为EPOLLLT</p></li></ul><p>二者的差异在于level-trigger模式下只要某个socket处于readable/writable状态，无论什么时候进行epoll_wait都会返回该socket；而edge-trigger模式下只有某个socket从unreadable变为readable或从unwritable变为writable时，epoll_wait才会返回该socket。</p><p>在epoll的ET模式下，正确的读写方式为:<br>读：只要可读，就一直读，直到返回0，或者 errno = EAGAIN 写:只要可写，就一直写，直到数据发送完，或者 errno = EAGAIN。</p><h2 id="阻塞和非阻塞socket"><a href="#阻塞和非阻塞socket" class="headerlink" title="阻塞和非阻塞socket"></a>阻塞和非阻塞socket</h2><p>对于阻塞socket，read/write返回-1代表网络出错了。但对于非阻塞socket，read/write返回-1不一定网络真的出错了。可能是Resource temporarily unavailable。这时应该再试，直到Resource available。</p><p>对于non-blocking的socket，正确的读写操作为:<br>读：忽略掉errno = EAGAIN的错误，下次继续读<br>写：忽略掉errno = EAGAIN的错误，下次继续写</p><h3 id="accept阻塞模式存在问题"><a href="#accept阻塞模式存在问题" class="headerlink" title="accept阻塞模式存在问题"></a>accept阻塞模式存在问题</h3><p>（1）问题：TCP连接被客户端关闭，即在服务器调用accept之前，客户端主动发送RST终止连接，导致刚刚建立的连接从就绪队列中移出，如果套接口被设置成阻塞模式，服务器就会一直阻塞在accept调用上，直到其他某个客户建立一个新的连接为止。但是在此期间，服务器单纯地阻塞在accept调用上，就绪队列中的其他描述符都得不到处理。</p><p>解决：把监听套接口设置为非阻塞，当客户在服务器调用accept之前中止某个连接时，accept调用可以立即返回-1</p><p>（2）ET模式下问题：多个连接同时到达，服务器的TCP就绪队列瞬间积累多个就绪连接，由于是边缘触发模式，epoll只会通知一次，accept只处理一个连接，导致TCP就绪队列中剩下的连接都得不到处理。</p><p>解决：用while循环包住accept调用，处理完TCP就绪队列中的所有连接后再退出循环。当accept返回-1并且errno设置为EAGAIN就表示所有连接都处理完了。</p><p>注意：</p><p>使用Linux epoll模型，水平触发模式；当socket可写时，会不停的触发socket可写的事件，如何处理？ </p><ul><li><p>第一种最普遍的方式：<br>需要向socket写数据的时候才把socket加入epoll，等待可写事件。接受到可写事件后，调用write或者send发送数据。当所有数据都写完后，把socket移出epoll。<br>这种方式的缺点是，即使发送很少的数据，也要把socket加入epoll，写完后在移出epoll，有一定操作代价。</p></li><li><p>第二种的方式： </p><p>开始不把socket加入epoll，需要向socket写数据的时候，直接调用write或者send发送数据。如果返回EAGAIN，把socket加入epoll，在epoll的驱动下写数据，全部数据发送完毕后，再移出epoll。<br>这种方式的优点是：数据不多的时候可以避免epoll的事件处理，提高效率。</p></li></ul><p>总结，ET模式下:<br>如果read返回0，那么说明已经接受所有数据<br>如果errno=EAGAIN，说明还有数据未接收，等待下一次通知<br>如果read返回-1，说明发生错误，停止处理</p><h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><p>使用epoll实现服务端：等待客户端连接，接收客户端数据并显示，将接收到的数据回传给客户端</p><p>使用select实现客户端：等待键盘输入，将输入传给服务器，并将服务器回传的数据显示出来</p><h3 id="服务端"><a href="#服务端" class="headerlink" title="服务端"></a>服务端</h3><p>套接字：socket-&gt;setNoBlock-&gt;setsockopt-&gt;bind-&gt;listen</p><p>I/O多路复用：epoll_create-&gt;epoll_ctl-&gt;epoll_wait</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//服务端：等待客户端连接；接收客户端数据并显示</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"> </span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/types.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/epoll.h&gt;  //epoll ways file</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/socket.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;fcntl.h&gt;    //block and noblock</span></span></span><br><span class="line"> </span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;error.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;arpa/inet.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;netinet/in.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;signal.h&gt;</span></span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SEV_PORT 8887</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAX_EVENTS 1000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//设置为非阻塞模式</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">setNonBlock</span><span class="params">(<span class="keyword">int</span> sock)</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> opt = fcntl(sock , F_SETFL);</span><br><span class="line"><span class="keyword">if</span>(opt &lt; <span class="number">0</span>) &#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"fcntl(sock , F_SETFL) fail\n"</span>);</span><br><span class="line">&#125;</span><br><span class="line">opt = opt | O_NONBLOCK;</span><br><span class="line"><span class="keyword">if</span>(fcntl(sock, F_SETFL, opt) &lt; <span class="number">0</span>) &#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"fctnl(sock, F_SETFL, opt) fail\n"</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">signal(SIGPIPE, SIG_IGN);</span><br><span class="line"><span class="keyword">int</span> epollfd, listenfd, nfds, connfd, sockfd;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">epoll_event</span> <span class="title">ev</span>, <span class="title">events</span>[20], <span class="title">ev_remv</span>;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">serveraddr</span>;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">clientadr</span>;</span></span><br><span class="line"><span class="keyword">socklen_t</span> clientlen;</span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; client;</span><br><span class="line"></span><br><span class="line"><span class="comment">//声明listenfd</span></span><br><span class="line">listenfd = socket(AF_INET, SOCK_STREAM, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//设置非阻塞</span></span><br><span class="line">setNonBlock(listenfd);</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> opt = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">int</span> ret = setsockopt(listenfd, SOL_SOCKET, SO_REUSEADDR, (<span class="keyword">char</span> *)&amp;opt, <span class="keyword">sizeof</span>(opt));<span class="comment">//地址重用</span></span><br><span class="line"><span class="keyword">if</span> (ret &lt; <span class="number">0</span>)</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"setsockopt fail\n"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">memset</span>(&amp;serveraddr, <span class="number">0</span>, <span class="keyword">sizeof</span>(serveraddr));</span><br><span class="line">serveraddr.sin_family = AF_INET;</span><br><span class="line">serveraddr.sin_port = htons(SEV_PORT);</span><br><span class="line">serveraddr.sin_addr.s_addr = htonl(INADDR_ANY);</span><br><span class="line"></span><br><span class="line">ret = bind(listenfd, (struct sockaddr*)&amp;serveraddr, <span class="keyword">sizeof</span>(serveraddr));</span><br><span class="line"><span class="keyword">if</span> (ret &lt; <span class="number">0</span>)</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"bind fail\n"</span>);</span><br><span class="line">&#125;</span><br><span class="line">ret = <span class="built_in">listen</span>(listenfd, SOMAXCONN);</span><br><span class="line"><span class="keyword">if</span> (ret &lt; <span class="number">0</span>)</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"listen fail\n"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//声明epollfd</span></span><br><span class="line">epollfd = epoll_create(MAX_EVENTS);<span class="comment">//最大文件描述符的个数</span></span><br><span class="line">ev.data.fd = listenfd;</span><br><span class="line">ev.events = EPOLLIN | EPOLLET;</span><br><span class="line"></span><br><span class="line">epoll_ctl(epollfd, EPOLL_CTL_ADD,listenfd, &amp;ev);<span class="comment">//注册事件</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//事件处理</span></span><br><span class="line"><span class="keyword">while</span>(<span class="literal">true</span>) &#123;</span><br><span class="line">nfds = epoll_wait(epollfd, events, MAX_EVENTS, <span class="number">-1</span>);<span class="comment">//等到epoll事件的发生</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nfds; i++) &#123;<span class="comment">//处理所有的事件</span></span><br><span class="line"><span class="keyword">if</span>(listenfd == events[i].data.fd) &#123;<span class="comment">//监听事件</span></span><br><span class="line">connfd = accept(listenfd, (struct sockaddr*)&amp;clientadr, &amp;clientlen);</span><br><span class="line"><span class="keyword">if</span>(connfd &lt; <span class="number">0</span>) &#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"accept error\n"</span>);</span><br><span class="line">&#125;</span><br><span class="line">setNonBlock(connfd);<span class="comment">//设置为非阻塞模式</span></span><br><span class="line"></span><br><span class="line">client.push_back(connfd);<span class="comment">//把连接加入</span></span><br><span class="line"></span><br><span class="line">ev.data.fd = connfd;</span><br><span class="line">ev.events = EPOLLIN | EPOLLET;</span><br><span class="line">epoll_ctl(epollfd, EPOLL_CTL_ADD, connfd, &amp;ev);<span class="comment">//注册ev事件</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span>(events[i].events &amp; EPOLLIN) &#123;<span class="comment">//读事件</span></span><br><span class="line"><span class="keyword">char</span> recvbufff[<span class="number">1024</span>];</span><br><span class="line"><span class="built_in">memset</span>(recvbufff, <span class="number">0</span>, <span class="keyword">sizeof</span>(recvbufff));</span><br><span class="line">sockfd = events[i].data.fd;</span><br><span class="line"><span class="keyword">if</span> (sockfd &lt; <span class="number">0</span>)</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"sockfd\n"</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">int</span> n = <span class="built_in">read</span>(sockfd, recvbufff, <span class="keyword">sizeof</span>(recvbufff));</span><br><span class="line"><span class="keyword">if</span> (n &lt; <span class="number">0</span>)</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"read error\n"</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (n == <span class="number">0</span>)<span class="comment">//处理完毕,客户端退出</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"client exit\n"</span>);</span><br><span class="line"><span class="built_in">close</span>(sockfd);</span><br><span class="line">ev_remv = events[i];</span><br><span class="line">epoll_ctl(epollfd, EPOLL_CTL_DEL, sockfd, &amp;ev_remv);</span><br><span class="line">client.erase(<span class="built_in">remove</span>(client.<span class="built_in">begin</span>(), client.<span class="built_in">end</span>(), sockfd) , client.<span class="built_in">end</span>());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">fputs</span>(recvbufff, <span class="built_in">stdout</span>);</span><br><span class="line"><span class="built_in">write</span>(sockfd, recvbufff,<span class="keyword">sizeof</span>(recvbufff));<span class="comment">//把接收到的数据回射</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行：输入g++ ./server.cpp -o server</p><h3 id="客户端"><a href="#客户端" class="headerlink" title="客户端"></a>客户端</h3><p>套接字：socket-&gt;connect</p><p>I/O多路复用：fd_set,maxfd-&gt;FD_SET-&gt;select</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//接收键盘输入，发送的服务器端</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"> </span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/types.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/epoll.h&gt;  //epoll ways file</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/socket.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;fcntl.h&gt;    //block and noblock</span></span></span><br><span class="line"> </span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;error.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;arpa/inet.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;netinet/in.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;signal.h&gt;</span></span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SEV_PORT 8887</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> <span class="keyword">const</span> *argv[])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">signal(SIGPIPE, SIG_IGN);<span class="comment">//忽略信号</span></span><br><span class="line"><span class="keyword">int</span> listenfd, stdinfd, conn, maxfd, rfd, selectfd;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">clientaddr</span>;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//初始化listenfd</span></span><br><span class="line">listenfd = socket(AF_INET, SOCK_STREAM, <span class="number">0</span>);</span><br><span class="line"><span class="keyword">if</span> (listenfd &lt; <span class="number">0</span>)</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"socket\n"</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">memset</span>(&amp;clientaddr, <span class="number">0</span>, <span class="keyword">sizeof</span>(clientaddr));</span><br><span class="line">clientaddr.sin_family = AF_INET;</span><br><span class="line">clientaddr.sin_port = htons(SEV_PORT);</span><br><span class="line">clientaddr.sin_addr.s_addr = inet_addr(<span class="string">"127.0.0.1"</span>);</span><br><span class="line"></span><br><span class="line">conn = <span class="built_in">connect</span>(listenfd, (struct sockaddr*)&amp;clientaddr, <span class="keyword">sizeof</span>(clientaddr));</span><br><span class="line"><span class="keyword">if</span> (conn &lt; <span class="number">0</span>)</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"connect\n"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">char</span> recvbuff[<span class="number">1024</span>] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line"><span class="keyword">char</span> sendbuff[<span class="number">1024</span>] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">fd_set rset;</span><br><span class="line">FD_ZERO(&amp;rset);</span><br><span class="line"></span><br><span class="line">stdinfd = fileno(<span class="built_in">stdin</span>);<span class="comment">//获取输入的所有文件</span></span><br><span class="line"></span><br><span class="line">maxfd = listenfd &gt;= stdinfd ? listenfd : stdinfd;</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span>(<span class="literal">true</span>) &#123;</span><br><span class="line">FD_SET(listenfd, &amp;rset);</span><br><span class="line">FD_SET(stdinfd, &amp;rset);</span><br><span class="line">selectfd = select(maxfd + <span class="number">1</span>, &amp;rset, <span class="literal">NULL</span>, <span class="literal">NULL</span>, <span class="literal">NULL</span>);</span><br><span class="line"><span class="keyword">if</span> (selectfd &lt; <span class="number">0</span>)<span class="comment">//信号被中断</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (selectfd == <span class="number">0</span>)<span class="comment">//没有设备准备好</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">continue</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (FD_ISSET(listenfd, &amp;rset))<span class="comment">//在文件描述符里查找是否存在</span></span><br><span class="line">&#123;</span><br><span class="line">rfd = <span class="built_in">read</span>(listenfd ,recvbuff, <span class="keyword">sizeof</span>(recvbuff));</span><br><span class="line"><span class="keyword">if</span> (rfd &lt; <span class="number">0</span>)</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"read\n"</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (rfd == <span class="number">0</span>)</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"server close\n"</span>);</span><br><span class="line"><span class="built_in">close</span>(listenfd);</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">//展示服务器回射数据</span></span><br><span class="line"><span class="built_in">fputs</span>(recvbuff, <span class="built_in">stdout</span>);</span><br><span class="line"><span class="built_in">memset</span>(recvbuff, <span class="number">0</span>, <span class="keyword">sizeof</span>(recvbuff));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (FD_ISSET(stdinfd, &amp;rset))</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">if</span> (fgets(sendbuff, <span class="keyword">sizeof</span>(sendbuff),<span class="built_in">stdin</span>))<span class="comment">//把键盘输入数据传给服务器端</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">write</span>(listenfd, sendbuff, <span class="keyword">sizeof</span>(sendbuff));</span><br><span class="line"><span class="built_in">memset</span>(sendbuff, <span class="number">0</span>, <span class="keyword">sizeof</span>(sendbuff));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125; </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行：输入g++ ./client.cpp -o client</p><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>输入./server启动服务端，输入./client启动客户端</p><p>在客户端输入hello world，服务器端显示hello world，并将该消息回传回客户端并显示</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20210520154808.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;参考：&lt;a href=&quot;https://blog.csdn.net/wteruiycbqqvwt/article/details/90313475&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;https://blog.csdn.net/wteruiycbqqvwt/article/details/90313475&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Linux I/O多路复用技术在比较多的TCP网络服务器中有使用：epoll,select&lt;/p&gt;&lt;h2 id=&quot;对比&quot;&gt;&lt;a href=&quot;#对比&quot; class=&quot;headerlink&quot; title=&quot;对比&quot;&gt;&lt;/a&gt;对比&lt;/h2&gt;&lt;h3 id=&quot;select缺陷&quot;&gt;&lt;a href=&quot;#select缺陷&quot; class=&quot;headerlink&quot; title=&quot;select缺陷&quot;&gt;&lt;/a&gt;select缺陷&lt;/h3&gt;&lt;p&gt;(1) 在Linux内核中，select所用到的FD_SET是有限的：内核中有个参数FD_SETSIZE定义了每个FD_SET的句柄个数：#define FD_SETSIZE 1024，最大检测1024个句柄&lt;/p&gt;
    
    </summary>
    
    
      <category term="linux" scheme="https://www.xiapf.com/categories/linux/"/>
    
    
      <category term="linux" scheme="https://www.xiapf.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux下进程通信</title>
    <link href="https://www.xiapf.com/blogs/process/"/>
    <id>https://www.xiapf.com/blogs/process/</id>
    <published>2021-06-16T05:20:09.000Z</published>
    <updated>2021-06-16T05:30:44.444Z</updated>
    
    <content type="html"><![CDATA[<p>参考：<a href="https://blog.csdn.net/ljianhui/article/details/10243617" target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/ljianhui/article/details/10243617</a></p><p><a href="https://blog.csdn.net/ljianhui/article/details/10253345" target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/ljianhui/article/details/10253345</a></p><h2 id="信号量"><a href="#信号量" class="headerlink" title="信号量"></a>信号量</h2><ul><li><p>为了防止出现因多个程序同时访问一个共享资源而引发的一系列问题，需要在任一时刻只能有一个执行线程访问代码的临界区域。</p></li><li><p><strong>临界区域</strong>是指执行数据更新的代码需要独占式地执行。而信号量就可以提供这样的一种访问机制，让一个临界区同一时间只有一个线程在访问它，也就是说信号量是用来调协进程对共享资源的访问的。</p></li><li><p>信号量是一个特殊的变量，程序对其访问都是原子操作，且只允许对它进行等待P(sv)和发送V(sv)操作。</p></li></ul><a id="more"></a><ol><li>P(sv)：如果sv的值大于零，就给它减1；如果它的值为零，就挂起（等待，阻塞）该进程的执行</li><li>V(sv)：如果有其他进程因等待sv而被挂起，就让它恢复运行，如果没有进程因等待sv而挂起，就给它加1.</li></ol><p>两个进程共享信号量sv，一旦其中一个进程执行了P(sv)操作，它将得到信号量，并可以进入临界区，使sv减1。而第二个进程将被阻止进入临界区，因为当它试图执行P(sv)时，sv为0，它会被挂起以等待第一个进程离开临界区域并执行V(sv)释放信号量，这时第二个进程就可以恢复执行。</p><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p>通过信号量控制两个进程向屏幕输出：</p><ul><li><p>通过semop操作struct sembuf中的sv的值来进行临界区操作</p></li><li><p>通过semctl操作union sem来初始化和删除信号量</p></li></ul><p>核心代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">union</span> sem &#123;</span><br><span class="line"><span class="keyword">int</span> val;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">semid_ds</span>* <span class="title">sem_buf</span>;</span></span><br><span class="line"><span class="keyword">unsigned</span> <span class="keyword">short</span>* <span class="built_in">array</span>;</span><br><span class="line">&#125;;<span class="comment">//用于控制信号量</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">int</span> sem_id = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">semaphore_p</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">sem_buf</span> <span class="title">sem_b</span>;</span></span><br><span class="line">sem_b.sem_num = <span class="number">0</span>;<span class="comment">//除非使用一组信号量，否则为0</span></span><br><span class="line">sem_b.sem_op = <span class="number">-1</span>;<span class="comment">//p操作减一</span></span><br><span class="line">sem_b.sem_flg = SEM_UNDO;</span><br><span class="line"></span><br><span class="line">sem_op(sem_id, &amp;sem_b, <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">semaphore_v</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">sem_buf</span> <span class="title">sem_b</span>;</span></span><br><span class="line">sem_b.sem_num = <span class="number">0</span>;</span><br><span class="line">sem_b.sem_op = <span class="number">1</span>;<span class="comment">//v操作加一</span></span><br><span class="line">sem_b.sem_flg = SEM_UNDO;</span><br><span class="line"></span><br><span class="line">semop(sem_id, &amp;sem_b, <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">init_semaphore</span><span class="params">()</span> </span>&#123;<span class="comment">//初始化信号量</span></span><br><span class="line"><span class="keyword">union</span> sem sem_union;</span><br><span class="line">sem_union.val = <span class="number">1</span>;</span><br><span class="line">semctl(sem_id, <span class="number">0</span>, SETVAL, sem_union);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">del_semaphore</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">union</span> sem sem_union;</span><br><span class="line">semctl(sem_id, <span class="number">0</span>, IPC_RMAD, sem_union);<span class="comment">//删除信号量</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> <span class="keyword">const</span> *argv[])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">char</span> message = <span class="string">'x'</span>;</span><br><span class="line"><span class="comment">//创建信号量</span></span><br><span class="line"><span class="keyword">int</span> ret = segmet(<span class="number">1234</span>, <span class="number">1</span>, IPC_CREAT | <span class="number">0666</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (argc &gt; <span class="number">1</span>)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">if</span> (!init_semaphore())<span class="comment">//初次调用信号量</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"init_semaphore\n"</span>);</span><br><span class="line"><span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line">message = argv[<span class="number">1</span>][<span class="number">0</span>];</span><br><span class="line">sleep(<span class="number">2</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//在临界区的操作</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; ++i)</span><br><span class="line">&#123;</span><br><span class="line"><span class="comment">//进入临界区</span></span><br><span class="line">semaphore_p();</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"%s\n"</span>, message);</span><br><span class="line">fflush(<span class="built_in">stdout</span>);<span class="comment">//清空缓冲区</span></span><br><span class="line">sleep(rand() % <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"%s\n"</span>, message);</span><br><span class="line">fflush(<span class="built_in">stdout</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//离开临界区</span></span><br><span class="line">semaphore_v()</span><br><span class="line">sleep(rand() % <span class="number">2</span>);</span><br><span class="line">&#125;</span><br><span class="line">sleep(<span class="number">10</span>);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"\n%d\n"</span>, getpid());</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (argc &gt; <span class="number">1</span>)</span><br><span class="line">&#123;</span><br><span class="line">sleep(<span class="number">3</span>);</span><br><span class="line">del_semaphore();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输入<code>g++ ./semDemo.cpp -o semDemo</code>生成可执行文件semDemo</p><p>启动两个进程<code>./semDemo o &amp; ./semDemo</code>，一个输出o，一个输出x，可见结果如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20210526151017.png" alt></p><h2 id="共享内存"><a href="#共享内存" class="headerlink" title="共享内存"></a>共享内存</h2><ul><li>共享内存就是允许两个不相关的进程访问同一个逻辑内存。</li><li>共享内存是在两个正在运行的进程之间共享和传递数据的一种非常有效的方式。</li><li>不同进程之间共享的内存通常安排为同一段物理内存</li></ul><h3 id="例子-1"><a href="#例子-1" class="headerlink" title="例子"></a>例子</h3><ol><li>设置共享内存结构中的标志位为1是可读不可写，0是可写不可读</li><li>通过shmRead.cpp一直查询共享内存，当有人（shmWrite.cpp）写入数据，将共享内存中当前数据输出在屏幕上</li><li>当共享内存的数据是最开始的字符是end，则退出将共享内存从当前进程分离</li></ol><p>输入<code>g++ ./shmRead.cpp -o shmr</code>和<code>g++ ./shmWrite.cpp -o shmw</code>生成可执行文件shmr（负责读）和shmw（负责写）</p><p>启动两个进程<code>./shmr  ./shmw</code>，结果如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20210527151019.png" alt></p><p>共享内存结构：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> BUF_SIZE 2048</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">share_used_st</span>//共享内存结构</span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">int</span> written;</span><br><span class="line"><span class="keyword">char</span> <span class="built_in">text</span>[BUF_SIZE];</span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">//written标志位</span></span><br><span class="line"><span class="comment">//非0即1 表示可读不可写</span></span><br><span class="line"><span class="comment">//0 表示可写不可读</span></span><br></pre></td></tr></table></figure><p>shmRead.cpp核心代码</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> <span class="keyword">const</span> *argv[])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">void</span> *shm = <span class="literal">NULL</span>;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">share_used_st</span> *<span class="title">share</span>;</span></span><br><span class="line"><span class="keyword">bool</span> <span class="built_in">running</span> = <span class="literal">true</span>;</span><br><span class="line"><span class="comment">//创建共享内存</span></span><br><span class="line"><span class="keyword">int</span> shmid = shmget(<span class="number">1234</span>, <span class="keyword">sizeof</span>(share_used_st), IPC_CREAT | IPC_EXCL | <span class="number">0666</span>);</span><br><span class="line"></span><br><span class="line">shm = shmat(shmid, <span class="literal">NULL</span>, <span class="number">0</span>);<span class="comment">//启动对共享内存的访问</span></span><br><span class="line">share = (struct share_used_st*)shm;</span><br><span class="line">share-&gt;written = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span>(<span class="built_in">running</span>) &#123;</span><br><span class="line"><span class="comment">//可读，读取内容</span></span><br><span class="line"><span class="keyword">if</span>(share-&gt;written != <span class="number">0</span>) &#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"you write:\n"</span>);</span><br><span class="line"><span class="built_in">fputs</span>(share-&gt;<span class="built_in">text</span>, <span class="built_in">stdout</span>);</span><br><span class="line"><span class="comment">//sleep(rand() % 3);</span></span><br><span class="line"><span class="comment">//读完设置可以写了，不再读了</span></span><br><span class="line">share-&gt;written = <span class="number">0</span>;</span><br><span class="line"><span class="comment">//判断是否退出</span></span><br><span class="line"><span class="keyword">if</span>(<span class="built_in">strncmp</span>(share-&gt;<span class="built_in">text</span>, <span class="string">"end"</span>, <span class="number">3</span>) == <span class="number">0</span>)</span><br><span class="line"><span class="built_in">running</span> = <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span>&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"waiting to read...\n"</span>);</span><br><span class="line">sleep(<span class="number">2</span>);<span class="comment">//否则等待</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//将共享内存从当前进程分离</span></span><br><span class="line"><span class="keyword">if</span> (shmdt(shm) &lt; <span class="number">0</span>)</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"shmdt\n"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//删除共享内存</span></span><br><span class="line"><span class="keyword">if</span> (shmctl(shmid, IPC_RMID, <span class="number">0</span>) &lt; <span class="number">0</span>)</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"shmid\n"</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>shmWrite.cpp核心代码</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> <span class="keyword">const</span> *argv[])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">void</span> *shm = <span class="literal">NULL</span>;</span><br><span class="line"><span class="keyword">bool</span> <span class="built_in">running</span> = <span class="literal">true</span>;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">share_used_st</span>* <span class="title">share</span>;</span></span><br><span class="line"><span class="keyword">char</span> buff[BUF_SIZE];</span><br><span class="line"><span class="comment">//创建共享内存</span></span><br><span class="line"><span class="keyword">int</span> shmid = shmget(<span class="number">1234</span>, <span class="keyword">sizeof</span>(share_used_st), IPC_CREAT|<span class="number">0666</span>);</span><br><span class="line"><span class="comment">//启动共享内存</span></span><br><span class="line">shm = shmat(shmid, <span class="literal">NULL</span>, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">share = (struct share_used_st*)shm;</span><br><span class="line"><span class="keyword">while</span>(<span class="built_in">running</span>) &#123;</span><br><span class="line"><span class="comment">//有人在读就不可写</span></span><br><span class="line"><span class="keyword">while</span>(share-&gt;written == <span class="number">1</span>) &#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"someone is read,waiting to wirte\n"</span>);</span><br><span class="line">sleep(<span class="number">2</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//可写</span></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"please write text:\n"</span>);</span><br><span class="line">fgets(buff, BUF_SIZE, <span class="built_in">stdin</span>);</span><br><span class="line"><span class="built_in">strncpy</span>(share-&gt;<span class="built_in">text</span>, buff, BUF_SIZE);</span><br><span class="line"></span><br><span class="line"><span class="comment">//写完了，设置可读</span></span><br><span class="line">share-&gt;written = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//判断是否退出</span></span><br><span class="line"><span class="keyword">if</span>(<span class="built_in">strncmp</span>(buff, <span class="string">"end"</span>, <span class="number">3</span>) == <span class="number">0</span>)</span><br><span class="line"><span class="built_in">running</span> = <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//将共享内存分离开</span></span><br><span class="line"><span class="keyword">if</span>(shmdt(shm) &lt; <span class="number">0</span>)&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"shmdt\n"</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="手动关闭信号量和共享内存"><a href="#手动关闭信号量和共享内存" class="headerlink" title="手动关闭信号量和共享内存"></a>手动关闭信号量和共享内存</h3><ul><li><p>查看当前信号量信息：ipcs -s</p></li><li><p>查看当前共享内存信息：ipcs -m</p></li><li><p>关闭信号量：ipcrm -s id（id是初始创建的返回值）</p></li><li><p>关闭信号量：ipcrm -m id</p></li></ul><h3 id="存在问题"><a href="#存在问题" class="headerlink" title="存在问题"></a>存在问题</h3><p>当written为0时，如果有两个进程同时访问共享内存，两个进程均可对其进行写操作，显然不行。</p><p>要想让程序安全地执行，就要有一种进程同步的进制，保证在进入临界区的操作是原子操作。可以使用前面所讲的信号量来进行进程的同步。因为信号量的操作都是原子性的。</p><h3 id="信号量互斥锁"><a href="#信号量互斥锁" class="headerlink" title="信号量互斥锁"></a>信号量互斥锁</h3><ul><li><p>step 1:semget创建信号量</p></li><li><p>step 2：segctl将信号量存储在union sem中</p></li><li><p>step 3：读取sem中的semid_ds时间，如果为0，即没有使用或者时间超过三分钟，则释放信号量（锁）sem.val 设置为1</p></li></ul><p>核心代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">union</span> semun</span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">int</span> val;</span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">semid_ds</span>*<span class="title">buf</span>;</span></span><br><span class="line">  <span class="keyword">unsigned</span> <span class="keyword">short</span> *<span class="built_in">array</span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">m_iSemId=semget(m_iSemKey,<span class="number">1</span>,IPC_CREAT|<span class="number">0666</span>);<span class="comment">//步骤1</span></span><br><span class="line">emun arg;</span><br><span class="line">semid_ds semDs;</span><br><span class="line">arg.buf=&amp;semDs;</span><br><span class="line"><span class="comment">//IPC_STAT读取一个信号量集的数据结构semid_ds，并将其存储在semun中的buf参数中</span></span><br><span class="line"><span class="keyword">int</span> ret=semctl(m_iSemId,<span class="number">0</span>,IPC_STAT,arg);<span class="comment">//步骤2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(semDs.sem_otime==<span class="number">0</span>||((semDs.sem_otime&gt;<span class="number">0</span>)&amp;&amp;(time(<span class="literal">NULL</span>)-semDs.sem_otime&gt;<span class="number">3</span>*<span class="number">60</span>)))</span><br><span class="line">&#123;</span><br><span class="line">  semun arg;</span><br><span class="line">  arg.val=<span class="number">1</span>;</span><br><span class="line">  ret=semctl(m_iSemId,<span class="number">0</span>,SETVAL,arg);<span class="comment">//步骤3</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;参考：&lt;a href=&quot;https://blog.csdn.net/ljianhui/article/details/10243617&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;https://blog.csdn.net/ljianhui/article/details/10243617&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/ljianhui/article/details/10253345&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;https://blog.csdn.net/ljianhui/article/details/10253345&lt;/a&gt;&lt;/p&gt;&lt;h2 id=&quot;信号量&quot;&gt;&lt;a href=&quot;#信号量&quot; class=&quot;headerlink&quot; title=&quot;信号量&quot;&gt;&lt;/a&gt;信号量&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;为了防止出现因多个程序同时访问一个共享资源而引发的一系列问题，需要在任一时刻只能有一个执行线程访问代码的临界区域。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;临界区域&lt;/strong&gt;是指执行数据更新的代码需要独占式地执行。而信号量就可以提供这样的一种访问机制，让一个临界区同一时间只有一个线程在访问它，也就是说信号量是用来调协进程对共享资源的访问的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;信号量是一个特殊的变量，程序对其访问都是原子操作，且只允许对它进行等待P(sv)和发送V(sv)操作。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="linux" scheme="https://www.xiapf.com/categories/linux/"/>
    
    
      <category term="linux" scheme="https://www.xiapf.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>贪心法——小区间并集覆盖大区间</title>
    <link href="https://www.xiapf.com/blogs/tanXin1/"/>
    <id>https://www.xiapf.com/blogs/tanXin1/</id>
    <published>2021-05-17T04:56:35.000Z</published>
    <updated>2021-05-17T04:59:02.457Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目内涵"><a href="#题目内涵" class="headerlink" title="题目内涵"></a>题目内涵</h2><p>给定数轴上的一个大区间 I 和 n 个小区间 i[0], i[1], …, i[n - 1]，问最少选择多少个小区间，使得这些小区间的并集可以覆盖整个大区间。</p><h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><p>45.跳跃游戏 II<a href="https://leetcode-cn.com/problems/jump-game-ii/" target="_blank" rel="external nofollow noopener noreferrer">https://leetcode-cn.com/problems/jump-game-ii/</a></p><p>1024.视频拼接 <a href="https://leetcode-cn.com/problems/video-stitching/" target="_blank" rel="external nofollow noopener noreferrer">https://leetcode-cn.com/problems/video-stitching/</a></p><a id="more"></a><p>1326.灌溉花园的最少水龙头数目 <a href="https://leetcode-cn.com/problems/minimum-number-of-taps-to-open-to-water-a-garden/" target="_blank" rel="external nofollow noopener noreferrer">https://leetcode-cn.com/problems/minimum-number-of-taps-to-open-to-water-a-garden/</a></p><p>视野争夺 <a href="https://www.nowcoder.com/profile/431148314/test/44341853/830864#summary" target="_blank" rel="external nofollow noopener noreferrer">https://www.nowcoder.com/profile/431148314/test/44341853/830864#summary</a></p><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>（1）预处理：得出当前位置能到的最远的距离（如果是给定小区间，则最远距离为右区间的值）</p><p>（2）贪心法遍历大区间：每次记录能到的最远的位置，从最远位置进行下一跳，记录每次跳的步数</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span><span class="comment">//45题</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">jump</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;<span class="comment">//贪心法，每次记录能到的最右边的位置</span></span><br><span class="line">        <span class="keyword">int</span> cur_can_right = <span class="number">0</span>;<span class="comment">//当前点能到的最右的位置</span></span><br><span class="line">        <span class="keyword">int</span> nxt_start = <span class="number">0</span>;<span class="comment">//下一个跳的位置/临界点</span></span><br><span class="line">        <span class="keyword">int</span> <span class="built_in">step</span> = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.<span class="built_in">size</span>() - <span class="number">1</span>; i++) &#123;</span><br><span class="line">            cur_can_right = <span class="built_in">max</span>(cur_can_right, i + nums[i]);</span><br><span class="line">            <span class="keyword">if</span>(i == nxt_start) &#123;<span class="comment">//到了要跳的位置</span></span><br><span class="line">                nxt_start = cur_can_right;<span class="comment">//下一个跳的位置，越远越好</span></span><br><span class="line">                <span class="built_in">step</span>++;</span><br><span class="line">            &#125;        </span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">step</span>;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span><span class="comment">//其余三题 大致思路相同</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">videoStitching</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; clips, <span class="keyword">int</span> time)</span> </span>&#123;<span class="comment">//贪心法</span></span><br><span class="line">        <span class="comment">//处理：每个点能跳的最远距离</span></span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; most_right(time);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; clips.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">            <span class="keyword">int</span> L = clips[i][<span class="number">0</span>];</span><br><span class="line">            <span class="keyword">int</span> R = clips[i][<span class="number">1</span>];</span><br><span class="line">            <span class="keyword">if</span>(L &lt; time)</span><br><span class="line">                most_right[L] = <span class="built_in">max</span>(most_right[L], R);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//贪心法</span></span><br><span class="line">        <span class="keyword">int</span> cur_can_right = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> nxt_start = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> <span class="built_in">step</span> = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i =<span class="number">0</span>; i &lt; most_right.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">            cur_can_right = <span class="built_in">max</span>(cur_can_right, most_right[i]);</span><br><span class="line">            <span class="keyword">if</span>(i == nxt_start) &#123;</span><br><span class="line">                <span class="keyword">if</span>(i &gt;= cur_can_right) <span class="keyword">return</span> <span class="number">-1</span>;<span class="comment">//不能往右边跳了</span></span><br><span class="line">                nxt_start = cur_can_right;</span><br><span class="line">                <span class="built_in">step</span>++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">step</span>;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;题目内涵&quot;&gt;&lt;a href=&quot;#题目内涵&quot; class=&quot;headerlink&quot; title=&quot;题目内涵&quot;&gt;&lt;/a&gt;题目内涵&lt;/h2&gt;&lt;p&gt;给定数轴上的一个大区间 I 和 n 个小区间 i[0], i[1], …, i[n - 1]，问最少选择多少个小区间，使得这些小区间的并集可以覆盖整个大区间。&lt;/p&gt;&lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h2&gt;&lt;p&gt;45.跳跃游戏 II&lt;a href=&quot;https://leetcode-cn.com/problems/jump-game-ii/&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;https://leetcode-cn.com/problems/jump-game-ii/&lt;/a&gt;&lt;/p&gt;&lt;p&gt;1024.视频拼接 &lt;a href=&quot;https://leetcode-cn.com/problems/video-stitching/&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;https://leetcode-cn.com/problems/video-stitching/&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="算法" scheme="https://www.xiapf.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="https://www.xiapf.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="贪心法" scheme="https://www.xiapf.com/tags/%E8%B4%AA%E5%BF%83%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>电影推荐系统项目实现（三）——协同过滤算法的实现</title>
    <link href="https://www.xiapf.com/blogs/mvRecommendSys3/"/>
    <id>https://www.xiapf.com/blogs/mvRecommendSys3/</id>
    <published>2020-12-01T07:11:03.000Z</published>
    <updated>2020-12-01T07:43:14.795Z</updated>
    
    <content type="html"><![CDATA[<h1 id="电影推荐系统项目实现（三）——协同过滤算法的实现"><a href="#电影推荐系统项目实现（三）——协同过滤算法的实现" class="headerlink" title="电影推荐系统项目实现（三）——协同过滤算法的实现"></a>电影推荐系统项目实现（三）——协同过滤算法的实现</h1><h2 id="整体设计"><a href="#整体设计" class="headerlink" title="整体设计"></a>整体设计</h2><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20201201115654.png" alt></p><h2 id="前期设置"><a href="#前期设置" class="headerlink" title="前期设置"></a>前期设置</h2><h3 id="接收页面回传的用户名"><a href="#接收页面回传的用户名" class="headerlink" title="接收页面回传的用户名"></a>接收页面回传的用户名</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">USERID = int(request.GET[<span class="string">"userIdd"</span>])</span><br></pre></td></tr></table></figure><h3 id="通过orm对象关系映射读取数据库内表并进行存储"><a href="#通过orm对象关系映射读取数据库内表并进行存储" class="headerlink" title="通过orm对象关系映射读取数据库内表并进行存储"></a>通过orm对象关系映射读取数据库内表并进行存储</h3><p>打开对应文件，用csv创建一个表格形式的文件对象，通过pymysql连接数据库，创建游标执行数据库查询语句，利用fetchall获取所有的结果，将查询的结果通过csv的writerow写入文件中</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#连接mysql数据库</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_conn</span><span class="params">()</span>:</span></span><br><span class="line">    conn = pymysql.connect(host=<span class="string">'127.0.0.1'</span>, port=<span class="number">3306</span>, user=<span class="string">'root'</span>, passwd=<span class="string">'xpf123321'</span>, db=<span class="string">'moviesys'</span>, charset=<span class="string">'utf8'</span>)</span><br><span class="line">    <span class="keyword">return</span> conn</span><br><span class="line">  </span><br><span class="line"><span class="comment">#将数据库内所有用户的评分写入excel文件中</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_mysql_to_csv</span><span class="params">(filename,filecsv)</span>:</span></span><br><span class="line">    file=filename+<span class="string">"/"</span>+filecsv+<span class="string">'.csv'</span></span><br><span class="line">    print(file)</span><br><span class="line">    <span class="keyword">with</span> codecs.open(filename=file, mode=<span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        write = csv.writer(f, dialect=<span class="string">'excel'</span>)</span><br><span class="line">        conn = get_conn()</span><br><span class="line">        cur = conn.cursor()</span><br><span class="line">        cur.execute(<span class="string">'select * from '</span>+filecsv)</span><br><span class="line">        <span class="comment">#sql = ('select * from users_resulttable WHERE userId = 1001')</span></span><br><span class="line">        rr = cur.fetchall()</span><br><span class="line">        <span class="comment">#results = query_all(cur=cur, sql=sql, args=None)</span></span><br><span class="line">        <span class="keyword">for</span> result <span class="keyword">in</span> rr:</span><br><span class="line">            <span class="comment">#print(result)</span></span><br><span class="line">            write.writerow(result[:])</span><br></pre></td></tr></table></figure><h3 id="初始化数据集"><a href="#初始化数据集" class="headerlink" title="初始化数据集"></a>初始化数据集</h3><p>根据文件名读入用户-电影-评分，按照随机数取值划分训练集和测试集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入数据</span></span><br><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadfile</span><span class="params">(filename)</span>:</span></span><br><span class="line">    fr=open(filename,<span class="string">'r'</span>,encoding=<span class="string">'UTF-8'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i ,line <span class="keyword">in</span> enumerate(fr):</span><br><span class="line">        <span class="keyword">yield</span> line.strip(<span class="string">'\r\n'</span>)</span><br><span class="line"></span><br><span class="line">    fr.close()</span><br><span class="line">    print(<span class="string">"load %s sucess"</span> % filename,file=sys.stderr)</span><br><span class="line"></span><br><span class="line"><span class="comment">#划分训练集和测试集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitDataset</span><span class="params">(self,filename,seed,m,k)</span>:</span></span><br><span class="line">    random.seed(seed)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> self.loadfile(filename):</span><br><span class="line">        id,user,movie,rating=line.split(<span class="string">','</span>)</span><br><span class="line">        <span class="keyword">if</span> random.randint(<span class="number">0</span>,m)==k:</span><br><span class="line">            self.testset.setdefault(user,&#123;&#125;)</span><br><span class="line">            self.testset[user][movie]=rating</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.trainset.setdefault(user, &#123;&#125;)</span><br><span class="line">            self.trainset[user][movie] = rating</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#初始数据集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initial_dataset</span><span class="params">(self,filename1)</span>:</span></span><br><span class="line">    initialset_len=<span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> lines <span class="keyword">in</span> self.loadfile(filename1):</span><br><span class="line">        id,users,movies,ratings=lines.split(<span class="string">','</span>)</span><br><span class="line">        <span class="comment"># print(users,movies,ratings)</span></span><br><span class="line">        self.initialset.setdefault(users,&#123;&#125;)</span><br><span class="line">        self.initialset[users][movies]=(ratings)</span><br><span class="line">        initialset_len+=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(self.initialset)</span></span><br><span class="line">    print(<span class="string">"load data set sucess"</span> , file=sys.stderr)</span><br><span class="line">    print(<span class="string">'datase=%s'</span> % initialset_len,file=sys.stderr)</span><br></pre></td></tr></table></figure><h3 id="构建用户-电影矩阵、电影-用户矩阵、电影-用户-评分矩阵"><a href="#构建用户-电影矩阵、电影-用户矩阵、电影-用户-评分矩阵" class="headerlink" title="构建用户-电影矩阵、电影-用户矩阵、电影-用户-评分矩阵"></a>构建用户-电影矩阵、电影-用户矩阵、电影-用户-评分矩阵</h3><p>（1）根据电影表，得到所有电影id数组、电影id和标题的对应数组、用户id数组</p><p>（2）按照电影和用户在原始数组中的位置索引构造用户-电影矩阵及电影用户矩阵</p><p>（3）同样按照每个电影对应每个用户的位置的评分构建电影-用户-评分矩阵</p><p>（4）构建图</p><p>遍历用户-电影矩阵，当当前用户与电影有评分即有连线，则两者直接的重要度加1，得到每个节点与其他节点重要度矩阵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 得到用户-电影矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bulid_users_movies_matrix</span><span class="params">(self, filename1, filename2)</span>:</span></span><br><span class="line">    movie_len = <span class="number">0</span></span><br><span class="line">    user_len = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> lines <span class="keyword">in</span> self.loadfile(filename2):</span><br><span class="line">        imdbid = lines.split(<span class="string">','</span>)[<span class="number">0</span>]</span><br><span class="line">        title = lines.split(<span class="string">','</span>)[<span class="number">1</span>]  <span class="comment"># 读入电影名称</span></span><br><span class="line">        self.movie_matrix.append(imdbid)<span class="comment">#所有电影id</span></span><br><span class="line">        self.movie_list.append(title)</span><br><span class="line">        self.movie_tltle[imdbid]=title</span><br><span class="line">        movie_len += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> lines <span class="keyword">in</span> self.loadfile(filename1):</span><br><span class="line">        id = lines.split(<span class="string">','</span>)[<span class="number">0</span>]</span><br><span class="line">        self.user_matrix.append(id)<span class="comment">#所有用户id</span></span><br><span class="line">        user_len += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">"user:%d,movie:%d"</span> % (user_len, movie_len), file=sys.stderr)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># self.user_movie_matrix=np.zeros((user_len,movie_len))</span></span><br><span class="line">    dataset=self.trainset</span><br><span class="line"></span><br><span class="line">    self.r = np.zeros((movie_len, user_len))</span><br><span class="line">    self.y = np.zeros((movie_len, user_len))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> key, value <span class="keyword">in</span> dataset.items():</span><br><span class="line">        self.user_movie_matrix.setdefault(int(key)- <span class="number">1</span>, &#123;&#125;)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> value:</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> range(len(self.movie_matrix)):  <span class="comment"># 找到电影在原数组中的对应位置</span></span><br><span class="line">                <span class="keyword">if</span> (self.movie_matrix[k] == i):</span><br><span class="line">                    mvid = k</span><br><span class="line">                    self.user_movie_matrix[int(key)- <span class="number">1</span>].setdefault(mvid, <span class="number">0</span>)</span><br><span class="line">                    self.movie_user_matrix.setdefault(mvid,&#123;&#125;)</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            self.movie_user_matrix[mvid].setdefault(int(key)<span class="number">-1</span>,<span class="number">0</span>)</span><br><span class="line">            self.user_movie_matrix[int(key)- <span class="number">1</span>][mvid] = float(dataset[key][i])</span><br><span class="line">            self.movie_user_matrix[mvid][int(key)- <span class="number">1</span>] = float(dataset[key][i])</span><br><span class="line"></span><br><span class="line">            self.r[mvid][int(key)- <span class="number">1</span>] = self.initialset[key][i]</span><br><span class="line">            self.y[mvid][int(key)- <span class="number">1</span>]=<span class="number">1</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#构建图矩阵 利用初始数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">construct_graph</span><span class="params">(self)</span>:</span></span><br><span class="line">    user_movie_matrix=self.user_movie_matrix</span><br><span class="line">    <span class="keyword">for</span> user,related_mv <span class="keyword">in</span> user_movie_matrix.items():</span><br><span class="line">        self.graph.setdefault(user,&#123;&#125;)</span><br><span class="line">        <span class="keyword">for</span> movie,rating <span class="keyword">in</span> related_mv.items():</span><br><span class="line">            self.graph[user].setdefault(movie, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">            self.graph.setdefault(movie, &#123;&#125;)</span><br><span class="line">            self.graph[movie].setdefault(user, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">            self.graph[user][movie]+=<span class="number">1</span></span><br><span class="line">            self.graph[movie][user]+=<span class="number">1</span></span><br></pre></td></tr></table></figure><h3 id="判断有无冷启动"><a href="#判断有无冷启动" class="headerlink" title="判断有无冷启动"></a>判断有无冷启动</h3><p>遍历用户-电影矩阵，当该用户给电影评分过，返回true，否则返回false。返回true说明不存在冷启动，直接按照历史数据应用协同过滤算法得到推荐结果，反之，则推荐最热门的电影（遍历用户-电影矩阵，对每个被评分的矩阵进行计数，找到计数次数最多的前n个电影的位置，再根据电影表找到对应电影返回推荐结果）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#冷启动</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cold_start</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="comment"># 获取当前最热门的电影</span></span><br><span class="line">    N =&#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> user, related_mv <span class="keyword">in</span> self.user_movie_matrix.items():</span><br><span class="line">        <span class="keyword">for</span> mv, rating <span class="keyword">in</span> related_mv.items():</span><br><span class="line">            N.setdefault(mv,<span class="number">0</span>)</span><br><span class="line">            N[mv] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    rankN=sorted(N.items(),key=<span class="keyword">lambda</span> j:j[<span class="number">1</span>],reverse=<span class="literal">True</span>)[:self.n_rec_movie]</span><br><span class="line">    print(rankN)</span><br><span class="line"></span><br><span class="line">    rec_list=[]</span><br><span class="line">    <span class="keyword">for</span> i,rating <span class="keyword">in</span> rankN:</span><br><span class="line">        rec_list.append(self.movie_matrix[i])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> rec_list</span><br><span class="line"></span><br><span class="line"><span class="comment"># 判断是否评价过电影，即新用户</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(self, id)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> user, mv <span class="keyword">in</span> self.user_movie_matrix.items():</span><br><span class="line">        <span class="keyword">if</span> id == user:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure><p>当不存在冷启动的时候，应用下面三种算法进行推荐，并评估推荐的结果</p><h2 id="基于邻域的算法"><a href="#基于邻域的算法" class="headerlink" title="基于邻域的算法"></a>基于邻域的算法</h2><h3 id="基于用户的协同过滤算法"><a href="#基于用户的协同过滤算法" class="headerlink" title="基于用户的协同过滤算法"></a>基于用户的协同过滤算法</h3><p>（1）计算用户之间的相似度（应用余弦相似度）</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20201201133725.png" alt></p><p>改进相似度的计算：两个用户对冷门物品采取过相同行为说明他们之间相似度更高，惩罚共同兴趣列表中饿热门物品</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20201201133638.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#建立用户相似度表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_users_sim_matrix</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="comment">#建立电影用户倒排表</span></span><br><span class="line">    buy=&#123;&#125;</span><br><span class="line">    user2user = &#123;&#125;</span><br><span class="line">    movie_user_matrix = self.movie_user_matrix</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> movie,user <span class="keyword">in</span> movie_user_matrix.items():</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> user.keys():</span><br><span class="line">            buy.setdefault(i,<span class="number">0</span>)</span><br><span class="line">            buy[i]+=<span class="number">1</span></span><br><span class="line">            user2user.setdefault(i,&#123;&#125;)</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> user.keys():</span><br><span class="line">                <span class="keyword">if</span>(i==j):</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                user2user[i].setdefault(j,<span class="number">0</span>)</span><br><span class="line">                user2user[i][j]+=<span class="number">1</span></span><br><span class="line">                <span class="comment"># user_user_sim[i][j]+=np.log(1/(1+len(user))) #对用户相似度的改进</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> u1,related_u <span class="keyword">in</span> user2user.items():</span><br><span class="line">        self.user_sim.setdefault(u1,&#123;&#125;)</span><br><span class="line">        <span class="keyword">for</span> u2,cij <span class="keyword">in</span> related_u.items():</span><br><span class="line">            self.user_sim[u1][u2]=cij/(np.sqrt(buy[u1])*np.sqrt(buy[u2])) <span class="comment">#余弦相似度</span></span><br></pre></td></tr></table></figure><p>（2）推荐</p><p>根据用户之间的相似度，找到与当前用户最近的k个用户，从他们评分的电影中找到该用户未看过的进行推荐，推荐理由=相似度 * 评分</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 推荐</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recommend_linyu_users</span><span class="params">(self, userid)</span>:</span></span><br><span class="line">    K=self.k_sim_user</span><br><span class="line">    N=self.n_rec_movie</span><br><span class="line">    user_user_sim=self.user_sim</span><br><span class="line"></span><br><span class="line">    watched_movie = self.user_movie_matrix[userid]</span><br><span class="line">    rankSim = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> v, sim <span class="keyword">in</span> sorted(user_user_sim[userid].items(), key=<span class="keyword">lambda</span> j: j[<span class="number">1</span>], reverse=<span class="literal">True</span>)[:K]:</span><br><span class="line">        <span class="keyword">for</span> mv, rating <span class="keyword">in</span> self.user_movie_matrix[v].items():</span><br><span class="line">            <span class="keyword">if</span> mv <span class="keyword">in</span> watched_movie:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            rankSim.setdefault(mv,<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">            rankSim[mv] += sim * rating</span><br><span class="line"></span><br><span class="line">    rankN = sorted(rankSim.items(), key=<span class="keyword">lambda</span> j: j[<span class="number">1</span>], reverse=<span class="literal">True</span>)[:N]  <span class="comment"># 得到了推荐的n个电影</span></span><br><span class="line"></span><br><span class="line">    rec_list=[]</span><br><span class="line">    <span class="keyword">for</span> i,rating <span class="keyword">in</span> rankN:</span><br><span class="line">        rec_list.append(self.movie_matrix[i])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> rec_list</span><br></pre></td></tr></table></figure><h3 id="基于物品的协同过滤算法"><a href="#基于物品的协同过滤算法" class="headerlink" title="基于物品的协同过滤算法"></a>基于物品的协同过滤算法</h3><p>（1）计算物品之间的相似度（应用余弦相似度）</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20201201133939.png" alt></p><p>改进相似度的计算：活跃用户的贡献度应小于不活跃用户，惩罚活跃用户</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20201201134007.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#计算任意两个物品之间的相似度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_movies_sim_matrix</span><span class="params">(self)</span>:</span></span><br><span class="line">    buy=&#123;&#125;</span><br><span class="line">    movie2movie=&#123;&#125;</span><br><span class="line">    user_movie_matrix=self.user_movie_matrix</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> user,movie <span class="keyword">in</span> user_movie_matrix.items():</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> movie.keys():</span><br><span class="line">            buy.setdefault(i,<span class="number">0</span>)</span><br><span class="line">            buy[i]+=<span class="number">1</span></span><br><span class="line">            movie2movie.setdefault(i,&#123;&#125;)</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> movie.keys():</span><br><span class="line">                <span class="keyword">if</span>(i==j):</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                movie2movie[i].setdefault(j, <span class="number">0</span>)</span><br><span class="line">                movie2movie[i][j]+=<span class="number">1</span></span><br><span class="line">                <span class="comment"># movie2movie[i][j]+=1/(np.log(1+len(movie))) #用户活跃度对物品相似度的影响</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> m1,relateM <span class="keyword">in</span> movie2movie.items():</span><br><span class="line">        self.movie_sim.setdefault(m1,&#123;&#125;)</span><br><span class="line">        <span class="keyword">for</span> m2,score <span class="keyword">in</span> relateM.items():</span><br><span class="line">            self.movie_sim[m1][m2]=score/(np.sqrt(buy[m1])*np.sqrt(buy[m2]))</span><br><span class="line">            <span class="comment"># self.movie_sim[m1][m2]/=max(self.movie_movie_sim[m1][m2]) #物品相似度的归一化</span></span><br></pre></td></tr></table></figure><p>（2）推荐</p><p>在当前用户看过的电影中，根据物品之间的相似度，找到与看过电影最近的k个用户，从中找到该用户未看过的进行推荐，推荐理由=相似度 * 评分</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recommend_linyu_items</span><span class="params">(self,user)</span>:</span></span><br><span class="line">    K=self.k_sim_movie</span><br><span class="line">    N=self.n_rec_movie</span><br><span class="line"></span><br><span class="line">    movie_movie_sim=self.movie_sim</span><br><span class="line"></span><br><span class="line">    watched_movie=self.user_movie_matrix[user]</span><br><span class="line">    rankSim=&#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> movie,rating <span class="keyword">in</span> watched_movie.items():</span><br><span class="line">        <span class="keyword">for</span> related_movie,sim <span class="keyword">in</span> sorted(movie_movie_sim[movie].items(),key=<span class="keyword">lambda</span> j:j[<span class="number">1</span>],reverse=<span class="literal">True</span>)[:K]:</span><br><span class="line">            <span class="keyword">if</span>(related_movie <span class="keyword">in</span> watched_movie):</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment">#因数据集太小，将上一句删除</span></span><br><span class="line">            rankSim.setdefault(related_movie,<span class="number">0</span>)</span><br><span class="line">            rankSim[related_movie]+=sim*rating</span><br><span class="line">            <span class="comment"># rankSim[related_movie].reason[movie]=sim*rating #对推荐该电影的解释</span></span><br><span class="line"></span><br><span class="line">    rankN=sorted(rankSim.items(),key=<span class="keyword">lambda</span> j:j[<span class="number">1</span>],reverse=<span class="literal">True</span>)[:N] <span class="comment">#得到推荐电影的索引</span></span><br><span class="line">    print(rankN)</span><br><span class="line"></span><br><span class="line">    rec_list=[]</span><br><span class="line">    <span class="keyword">for</span> i,rating <span class="keyword">in</span> rankN:</span><br><span class="line">        rec_list.append(self.movie_matrix[i])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> rec_list</span><br></pre></td></tr></table></figure><h2 id="隐语义模型"><a href="#隐语义模型" class="headerlink" title="隐语义模型"></a>隐语义模型</h2><p>（对于某个用户，首先得到他的兴趣分类，然后从分类中挑选他可能喜欢的物品。）</p><p>将电影按照类别进行分类，通过用户u的兴趣和电影第k个隐类之间的关系，以及电影第k个隐类和物品i之间的关系来计算用户u对物品i的兴趣</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20201201140407.png" alt></p><h3 id="总体思路"><a href="#总体思路" class="headerlink" title="总体思路"></a>总体思路</h3><p>对每个用户应用一个不同的线性回归的方程，使用用户的参数向量乘以电影的特征向量就是当前的预测，将预测减去实际值，目标是缩小预测和实际之间的误差，同时加上使得特征向量和参数向量最小的正则项</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200911155633.png" alt></p><p>r（i，j）：表示用户j评价了电影i</p><p>y（i，j）：用户j对电影i的评价等级（如果用户评价过该电影）</p><p>θ（j）：用户j对电影评价的参数</p><p>x（i）：电影i的特征向量</p><h3 id="数据预处理，导入数据的评分矩阵和用户评价矩阵，并将评分矩阵进行均值化"><a href="#数据预处理，导入数据的评分矩阵和用户评价矩阵，并将评分矩阵进行均值化" class="headerlink" title="数据预处理，导入数据的评分矩阵和用户评价矩阵，并将评分矩阵进行均值化"></a>数据预处理，导入数据的评分矩阵和用户评价矩阵，并将评分矩阵进行均值化</h3><p>由于有用户没有评分任何一个电影，则该用户的参数向量为0，则不管算法如何运行，最终的参数向量仍为0，最终预测也都是0，这样的预测没有意义。</p><p>因此，需要对电影评分Y矩阵进行均值归一化，将Y=Y-Y.mean()（Y的均值），这样保证了没有任何评分的用户最终预测也不是0。</p><p>预处理后的数据在模型训练后如何进行预测：将预测值θ * x再加上Y的均值Y.mean()，因为一开始的预处理把所有评分都减去了均值，最后的预测需要加上</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200911153057.png" alt></p><p>将电影-用户=评分表进行均值归一化：y_mean=y-y.mean()</p><h3 id="随机初始化电影的特征向量和用户的参数向量：使用正态分布函数进行初始化"><a href="#随机初始化电影的特征向量和用户的参数向量：使用正态分布函数进行初始化" class="headerlink" title="随机初始化电影的特征向量和用户的参数向量：使用正态分布函数进行初始化"></a>随机初始化电影的特征向量和用户的参数向量：使用正态分布函数进行初始化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_origin = np.random.standard_normal((n_movies, n_features))</span><br><span class="line">theta_origin = np.random.standard_normal((n_users, n_features))</span><br></pre></td></tr></table></figure><h3 id="建立优化目标函数（如上）"><a href="#建立优化目标函数（如上）" class="headerlink" title="建立优化目标函数（如上）"></a>建立优化目标函数（如上）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2.计算损失</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(self,param, y, r, n_features)</span>:</span></span><br><span class="line">    n_movies, n_users = y.shape  <span class="comment"># y的大小是电影乘以用户</span></span><br><span class="line">    x, theta = self.separate(param, n_movies, n_users, n_features)</span><br><span class="line"></span><br><span class="line">    inner = np.multiply(np.dot(x, theta.T) - y, r)  <span class="comment"># 为什么要乘以r</span></span><br><span class="line">    inner_cost = (<span class="number">1</span> / <span class="number">2</span>) * np.sum(np.power(inner, <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> inner_cost</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加上正则化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regular_cost</span><span class="params">(self,param, y, r, n_features, l=<span class="number">1</span>)</span>:</span></span><br><span class="line">    n_movies, n_users = y.shape</span><br><span class="line">    x, theta = self.separate(param, n_movies, n_users, n_features)</span><br><span class="line"></span><br><span class="line">    inner_cost = self.cost(param, y, r, n_features)</span><br><span class="line"></span><br><span class="line">    regular = (l / <span class="number">2</span>) * np.sum(np.power(param, <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> inner_cost + regular</span><br></pre></td></tr></table></figure><h3 id="计算函数梯度"><a href="#计算函数梯度" class="headerlink" title="计算函数梯度"></a>计算函数梯度</h3><p>分别更新特征向量和参数向量</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200911155655.png" alt></p><h3 id="使用优化算法，按照梯度下降的方向求得优化函数最小的参数值"><a href="#使用优化算法，按照梯度下降的方向求得优化函数最小的参数值" class="headerlink" title="使用优化算法，按照梯度下降的方向求得优化函数最小的参数值"></a>使用优化算法，按照梯度下降的方向求得优化函数最小的参数值</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3.计算梯度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad</span><span class="params">(self,param, y, r, n_features)</span>:</span></span><br><span class="line">    n_movies, n_users = y.shape</span><br><span class="line">    x, theta = self.separate(param, n_movies, n_users, n_features)</span><br><span class="line"></span><br><span class="line">    inner = np.multiply(np.dot(x, theta.T) - y, r)</span><br><span class="line">    x_grad = np.dot(inner, theta)</span><br><span class="line"></span><br><span class="line">    theta_grad = np.dot(inner.T, x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self.merge(x_grad, theta_grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加上正则化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regular_grad</span><span class="params">(self,param, y, r, n_features, l=<span class="number">1</span>)</span>:</span></span><br><span class="line">    inner_grad = self.grad(param, y, r, n_features)</span><br><span class="line">    regular = l * param  <span class="comment"># param是x和theta的连接</span></span><br><span class="line">    <span class="keyword">return</span> inner_grad + regular</span><br><span class="line"><span class="comment"># 对梯度进行优化</span></span><br><span class="line">res = opt.minimize(fun=self.regular_cost, x0=param, args=(y_mean, r, n_features, l), jac=self.regular_grad, method=<span class="string">'TNC'</span>)</span><br><span class="line">param_train = res.x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用得到的电影特征和用户偏好进行预测</span></span><br><span class="line">x_train, theta_train = self.separate(param_train, n_movies, n_users, n_features)</span><br></pre></td></tr></table></figure><h3 id="得到要求的用户的参数矩阵，乘以所有电影的特征向量，再加上预处理中减去的评分矩阵的均值，按照排序的方法，取前几个电影即为需要推荐的"><a href="#得到要求的用户的参数矩阵，乘以所有电影的特征向量，再加上预处理中减去的评分矩阵的均值，按照排序的方法，取前几个电影即为需要推荐的" class="headerlink" title="得到要求的用户的参数矩阵，乘以所有电影的特征向量，再加上预处理中减去的评分矩阵的均值，按照排序的方法，取前几个电影即为需要推荐的"></a>得到要求的用户的参数矩阵，乘以所有电影的特征向量，再加上预处理中减去的评分矩阵的均值，按照排序的方法，取前几个电影即为需要推荐的</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">real_predict=predict[:,userid]+y.mean()</span><br><span class="line"><span class="comment"># 以降序形式排列</span></span><br><span class="line">idx = np.argsort(real_predict)[::<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">print(<span class="string">'推荐的电影为：'</span>)</span><br><span class="line"><span class="comment">#获得推荐的电影的名称</span></span><br><span class="line">movie_list=np.array(movie_list)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> movie_list[idx][:N]:</span><br><span class="line">    rec_list.append(i)</span><br><span class="line">    print(i)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> imdbid,title <span class="keyword">in</span> self.movie_tltle.items():</span><br><span class="line">    <span class="keyword">if</span> title <span class="keyword">in</span> rec_list:</span><br><span class="line">        rec_id.append(imdbid)</span><br><span class="line">        print(rec_id)</span><br></pre></td></tr></table></figure><h2 id="基于图的随机游走算法"><a href="#基于图的随机游走算法" class="headerlink" title="基于图的随机游走算法"></a>基于图的随机游走算法</h2><p>（用户行为很容易用二分图表示）</p><p>之前已经将用户的行为表示为二分图模型，根据用户顶点直接有无边连接来确定节点的相关性。</p><p>对用户u进行推荐，则从该点对应的节点开始随机游走，游走到任意节点，根据参数alpha决定是否继续游走，如果继续，从当前节点指向的节点中随机选一个作为下一个走的节点，停止游走则从根节点重新走，多次之后，每个节点被访问的概率会收敛，最终推荐的权重就是节点访问的概率。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20201201150236.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#pk算法 计算图定点的相关性</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recommend_personalrank</span><span class="params">(self,root,alpha=<span class="number">0.5</span>,max_step=<span class="number">50</span>)</span>:</span></span><br><span class="line">    Graph=self.graph</span><br><span class="line">    rank=dict()</span><br><span class="line">    rank=&#123;x:<span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> Graph.keys()&#125;</span><br><span class="line">    rank[root]=<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(max_step):</span><br><span class="line">        tmp=&#123;x:<span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> Graph.keys()&#125;</span><br><span class="line">        <span class="keyword">for</span> u,ev <span class="keyword">in</span> Graph.items():</span><br><span class="line">            <span class="keyword">for</span> v,e <span class="keyword">in</span> ev.items():</span><br><span class="line">                <span class="keyword">if</span> v <span class="keyword">not</span> <span class="keyword">in</span> tmp:</span><br><span class="line">                    tmp[v]=<span class="number">0</span></span><br><span class="line">                tmp[v]+=alpha*rank[u]/len(ev)  <span class="comment">#ev:u的出度</span></span><br><span class="line">                <span class="keyword">if</span> v==root:</span><br><span class="line">                    tmp[root]+=<span class="number">1</span>-alpha</span><br><span class="line">            rank=tmp</span><br><span class="line">    <span class="comment">#得到从root走的所有节点的权重</span></span><br><span class="line">    <span class="keyword">for</span> v,e <span class="keyword">in</span> Graph[root].items():</span><br><span class="line">        <span class="keyword">for</span> item,weight <span class="keyword">in</span> rank.items():</span><br><span class="line">            <span class="keyword">if</span> v==item <span class="keyword">or</span> item==root:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            self.rank_list[item]=weight</span><br><span class="line"></span><br><span class="line">    <span class="comment">#返回没有和root产生行为的节点中，最优的几个节点</span></span><br><span class="line">    self.rank_list=sorted(self.rank_list.items(),key=<span class="keyword">lambda</span> j:j[<span class="number">1</span>],reverse=<span class="literal">True</span>)[:self.n_rec_movie]</span><br><span class="line">    print(self.rank_list)</span><br><span class="line">    rec_list = []</span><br><span class="line">    <span class="keyword">for</span> item, weight <span class="keyword">in</span> self.rank_list:</span><br><span class="line">        rec_list.append(self.movie_matrix[item])</span><br></pre></td></tr></table></figure><h2 id="评价指标（结果待续）"><a href="#评价指标（结果待续）" class="headerlink" title="评价指标（结果待续）"></a>评价指标（结果待续）</h2><p>（1）准确率：描述最终的推荐列表中有多少比例是发生过的用户—物品评分记录</p><p>（2）召回率：描述有多少比例的用户—物品评分记录包含在最终的推荐列表中</p><p>（3）覆盖率：反映了推荐算法发掘长尾的能力，覆盖率越高，说明推荐算法越能够将长尾中的物品推荐给用户</p><p>（4）新颖度：列表中物品的平均流行度度量推荐结果的新颖度。如果推荐出的物品都很热门，说明推荐的新颖度较低，否则说明推荐结果比较新颖</p><p>user:673,movie:39514</p><table><thead><tr><th>算法</th><th>准确率</th><th>召回率</th><th>覆盖率</th><th>新颖度</th><th>运行时间</th></tr></thead><tbody><tr><td>基于邻域的协同过滤（用户）</td><td>0</td><td>0</td><td>1</td><td>4.941765770489811</td><td>148.95515394210815</td></tr><tr><td>基于邻域的协同过滤（物品）</td><td>0</td><td>0</td><td>1</td><td>4.843394485517513</td><td>992.0291049480438</td></tr><tr><td>隐语义模型</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>基于图的随机游走</td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table><p>结论：当网站是物品较多，则适用于基于邻域的协同过滤（用户），当网站用户较多，则适用于基于邻域的协同过滤（物品），物品的相似度相对于用户的兴趣一般比较稳定，基于邻域的协同过滤（物品）效果更好</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;电影推荐系统项目实现（三）——协同过滤算法的实现&quot;&gt;&lt;a href=&quot;#电影推荐系统项目实现（三）——协同过滤算法的实现&quot; class=&quot;headerlink&quot; title=&quot;电影推荐系统项目实现（三）——协同过滤算法的实现&quot;&gt;&lt;/a&gt;电影推荐系统项目实现（三）——协同过滤算法的实现&lt;/h1&gt;&lt;h2 id=&quot;整体设计&quot;&gt;&lt;a href=&quot;#整体设计&quot; class=&quot;headerlink&quot; title=&quot;整体设计&quot;&gt;&lt;/a&gt;整体设计&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20201201115654.png&quot; alt&gt;&lt;/p&gt;&lt;h2 id=&quot;前期设置&quot;&gt;&lt;a href=&quot;#前期设置&quot; class=&quot;headerlink&quot; title=&quot;前期设置&quot;&gt;&lt;/a&gt;前期设置&lt;/h2&gt;&lt;h3 id=&quot;接收页面回传的用户名&quot;&gt;&lt;a href=&quot;#接收页面回传的用户名&quot; class=&quot;headerlink&quot; title=&quot;接收页面回传的用户名&quot;&gt;&lt;/a&gt;接收页面回传的用户名&lt;/h3&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;USERID = int(request.GET[&lt;span class=&quot;string&quot;&gt;&quot;userIdd&quot;&lt;/span&gt;])&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;h3 id=&quot;通过orm对象关系映射读取数据库内表并进行存储&quot;&gt;&lt;a href=&quot;#通过orm对象关系映射读取数据库内表并进行存储&quot; class=&quot;headerlink&quot; title=&quot;通过orm对象关系映射读取数据库内表并进行存储&quot;&gt;&lt;/a&gt;通过orm对象关系映射读取数据库内表并进行存储&lt;/h3&gt;&lt;p&gt;打开对应文件，用csv创建一个表格形式的文件对象，通过pymysql连接数据库，创建游标执行数据库查询语句，利用fetchall获取所有的结果，将查询的结果通过csv的writerow写入文件中&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="推荐算法" scheme="https://www.xiapf.com/tags/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>电影推荐系统项目实现（二）——基于用户行为数据的电影推荐系统</title>
    <link href="https://www.xiapf.com/blogs/mvRecommendSys2/"/>
    <id>https://www.xiapf.com/blogs/mvRecommendSys2/</id>
    <published>2020-11-26T08:51:44.000Z</published>
    <updated>2020-11-26T08:52:24.662Z</updated>
    
    <content type="html"><![CDATA[<h2 id="项目目的"><a href="#项目目的" class="headerlink" title="项目目的"></a>项目目的</h2><p>对<a href="https://github.com/JaniceWuo/MovieRecommend" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/JaniceWuo/MovieRecommend</a> 推荐系统的改进</p><p>1、增加隐语义模型和基于图的算法</p><p>2、同时增加评价指标准确率、召回率、覆盖度、新颖度来评估算法性能</p><p>3、针对冷启动问题采用最热门推荐</p><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><h3 id="电影表（从网上搜集，自行导入）"><a href="#电影表（从网上搜集，自行导入）" class="headerlink" title="电影表（从网上搜集，自行导入）"></a>电影表（从网上搜集，自行导入）</h3><p>包含电影对应的编号（不是从1开始）、电影名称、电影海报的地址</p><a id="more"></a><h3 id="用户评分表（由django模型生成）"><a href="#用户评分表（由django模型生成）" class="headerlink" title="用户评分表（由django模型生成）"></a>用户评分表（由django模型生成）</h3><p>包含编号（主键），评分的用户id，电影对应的编号，打分</p><h3 id="用户表（（由django模型中注册页面自动生成）"><a href="#用户表（（由django模型中注册页面自动生成）" class="headerlink" title="用户表（（由django模型中注册页面自动生成）"></a>用户表（（由django模型中注册页面自动生成）</h3><p>包含编号（主键），密码，最后一次登录时间，用户名称，邮箱等其他信息</p><h2 id="实验设计"><a href="#实验设计" class="headerlink" title="实验设计"></a>实验设计</h2><h3 id="总体实验设计流程"><a href="#总体实验设计流程" class="headerlink" title="总体实验设计流程"></a>总体实验设计流程</h3><p>如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20201125110624.png" alt></p><p>（1）推荐系统的数据可以分为显性反馈数据（有用户的评分）和隐性反馈数据集（没有评分，只看用户的行为），项目的数据集为显性反馈数据</p><p>（2）划分数据集</p><p>在训练集上建立用户—兴趣模型，并在测试集上对用户行为进行预测，统计出相应的评测指标</p><p>（3）确定评价指标</p><h3 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h3><p>（1）准确率：描述最终的推荐列表中有多少比例是发生过的用户—物品评分记录</p><p>（2）召回率：描述有多少比例的用户—物品评分记录包含在最终的推荐列表中</p><p>（3）覆盖率：反映了推荐算法发掘长尾的能力，覆盖率越高，说明推荐算法越能够将长尾中的物品推荐给用户</p><p>（4）新颖度：列表中物品的平均流行度度量推荐结果的新颖度。如果推荐出的物品都很热门，说明推荐的新颖度较低，否则说明推荐结果比较新颖</p><h3 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h3><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20201126162343.png" alt></p><p>（1）基于邻域的算法</p><p>基于领域的算法分为基于用户和基于物品，基于用户是给用户推荐和他兴趣相似的其他用户喜欢的物品，基于物品是给用户推荐和他之前喜欢的物品相似的物品</p><p>（2）隐语义模型</p><p>基于机器学习的方法，利用最终得到的电影特征和用户偏好进行预测</p><p>具体流程 <a href="https://www.xiapf.com/blogs/tfRecommendSys/">https://www.xiapf.com/blogs/tfRecommendSys/</a></p><p>（3）基于图的随机游走算法</p><p>算法中不区分user节点和item节点，这样一来问题就转化成：对与任意节点来说，其他节点的重要度各是多少，根据当前节点计算出其他节点的重要度来进行计算</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;项目目的&quot;&gt;&lt;a href=&quot;#项目目的&quot; class=&quot;headerlink&quot; title=&quot;项目目的&quot;&gt;&lt;/a&gt;项目目的&lt;/h2&gt;&lt;p&gt;对&lt;a href=&quot;https://github.com/JaniceWuo/MovieRecommend&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;https://github.com/JaniceWuo/MovieRecommend&lt;/a&gt; 推荐系统的改进&lt;/p&gt;&lt;p&gt;1、增加隐语义模型和基于图的算法&lt;/p&gt;&lt;p&gt;2、同时增加评价指标准确率、召回率、覆盖度、新颖度来评估算法性能&lt;/p&gt;&lt;p&gt;3、针对冷启动问题采用最热门推荐&lt;/p&gt;&lt;h2 id=&quot;数据集&quot;&gt;&lt;a href=&quot;#数据集&quot; class=&quot;headerlink&quot; title=&quot;数据集&quot;&gt;&lt;/a&gt;数据集&lt;/h2&gt;&lt;h3 id=&quot;电影表（从网上搜集，自行导入）&quot;&gt;&lt;a href=&quot;#电影表（从网上搜集，自行导入）&quot; class=&quot;headerlink&quot; title=&quot;电影表（从网上搜集，自行导入）&quot;&gt;&lt;/a&gt;电影表（从网上搜集，自行导入）&lt;/h3&gt;&lt;p&gt;包含电影对应的编号（不是从1开始）、电影名称、电影海报的地址&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="推荐算法" scheme="https://www.xiapf.com/tags/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>电影推荐系统项目实现（一）——web框架搭建及连接数据库</title>
    <link href="https://www.xiapf.com/blogs/mvRecommendSys1/"/>
    <id>https://www.xiapf.com/blogs/mvRecommendSys1/</id>
    <published>2020-11-24T08:16:57.000Z</published>
    <updated>2020-11-24T08:17:56.801Z</updated>
    
    <content type="html"><![CDATA[<h2 id="web框架搭建——django的使用"><a href="#web框架搭建——django的使用" class="headerlink" title="web框架搭建——django的使用"></a>web框架搭建——django的使用</h2><h3 id="django框架"><a href="#django框架" class="headerlink" title="django框架"></a>django框架</h3><p>采用MTV模型</p><p>M：模型，负责业务对象和数据库之间的映射</p><p>T模板，把页面展示给用户</p><p>V：视图，负责业务逻辑，可调用模型和模板</p><p>此外，还需要url分发器，将url请求分发给视图处理，视图再调用模型和模板</p><p>使用pycharm新建django项目如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20201124151653.png" alt></p><p>其中manage.py为项目入口（其中包含了项目设置从何处读入，即需要去setting下进行查找），templates文件夹下为模板，项目名字下的urls.py为url分发器，需要自行新建应用app，使用app下的model.py，设置数据库中表的名称和各列的数据类型。</p><a id="more"></a><h3 id="设置项目起始运行端口"><a href="#设置项目起始运行端口" class="headerlink" title="设置项目起始运行端口"></a>设置项目起始运行端口</h3><p>在edit configurations里设置程序运行的参数</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20201124153212.jpg" alt></p><h2 id="连接数据库——django与数据库连接"><a href="#连接数据库——django与数据库连接" class="headerlink" title="连接数据库——django与数据库连接"></a>连接数据库——django与数据库连接</h2><h3 id="系统设置"><a href="#系统设置" class="headerlink" title="系统设置"></a>系统设置</h3><p>（1）导入依赖</p><p>与mysql数据库相连，需要导入pymysql依赖：pip3 install pymysql</p><p>同时在项目同名下的__ init __.py下设置导入pymysql及版本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line">pymysql.version_info = (<span class="number">1</span>, <span class="number">4</span>, <span class="number">13</span>, <span class="string">"final"</span>, <span class="number">0</span>)</span><br><span class="line">pymysql.install_as_MySQLdb()</span><br></pre></td></tr></table></figure><p>（2）在项目同名的文件夹下的setting设置连接的数据库的名字、端口、用户名、密码，即连接上了clTest数据库</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#mysql数据库的名字，进入数据库的端口、用户名、密码等</span></span><br><span class="line">DATABASES = &#123;</span><br><span class="line">    <span class="string">'default'</span>: &#123;</span><br><span class="line">        <span class="string">'ENGINE'</span>: <span class="string">'django.db.backends.mysql'</span>,</span><br><span class="line">        <span class="comment">#'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),</span></span><br><span class="line">        <span class="string">'NAME'</span>: <span class="string">'clTest'</span>,</span><br><span class="line">        <span class="string">'USER'</span>: <span class="string">'root'</span>,</span><br><span class="line">        <span class="string">'PASSWORD'</span>: <span class="string">'xpf123321'</span>,</span><br><span class="line">        <span class="string">'HOST'</span>: <span class="string">'127.0.0.1'</span>,</span><br><span class="line">        <span class="string">'PORT'</span>: <span class="string">'3306'</span>,<span class="comment">#端口</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="如何与数据库连接"><a href="#如何与数据库连接" class="headerlink" title="如何与数据库连接"></a>如何与数据库连接</h3><p>（1）利用ORM（对象关系映射）</p><p>django模型使用自带的ORM（对象关系映射），ORM将python对象和sql语句互相转换，使得项目能够访问sql数据库，ORM中的models类对应数据库中的表，对象实例对应表中的一条记录，属性对应表中的字段，对应关系如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20201124154431.png" alt></p><p>（2）建立app</p><p>django中要使用模型就需要创建app</p><p>在终端输入python3 manage.py startapp bajie，同时在系统设置setting下加入app的名字，则创建了应用程序为bajie的app</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20201124153930.png" alt></p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20201124154031.png" alt></p><p>（3）通过模型在数据库中创建表</p><p>利用app下的models.py创建类（对应就会在数据库中创建表）:设置user表，其中有对象有两个属性（对应两个字段），长度分别为20，10，都是字符类型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">user</span><span class="params">(models.Model)</span>:</span></span><br><span class="line">    name=models.CharField(max_length=<span class="number">20</span>)</span><br><span class="line">    sex=models.CharField(max_length=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>通过：python3 manage.py makemigrations 让django知道模型的变化</p><p>通过：python3 manage.py migrate 创建表结构（项目会根据model.py下设置的表结构在对应数据库下创建表）</p><p>打开数据库clTest，可以看到使用该应用程序创建的表（其余的都是系统表）</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20201124161200.png" alt></p><p>以上就是使用django框架搭建web应用，同时连接数据库，主要使用了django框架下的MTV模型中的M，即模型，下一个笔记主要记录如何使用V，即视图，使用数据库中的数据，并应用相关推荐算法得到推荐内容，并将结果通过T，即模板展示出来。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;web框架搭建——django的使用&quot;&gt;&lt;a href=&quot;#web框架搭建——django的使用&quot; class=&quot;headerlink&quot; title=&quot;web框架搭建——django的使用&quot;&gt;&lt;/a&gt;web框架搭建——django的使用&lt;/h2&gt;&lt;h3 id=&quot;django框架&quot;&gt;&lt;a href=&quot;#django框架&quot; class=&quot;headerlink&quot; title=&quot;django框架&quot;&gt;&lt;/a&gt;django框架&lt;/h3&gt;&lt;p&gt;采用MTV模型&lt;/p&gt;&lt;p&gt;M：模型，负责业务对象和数据库之间的映射&lt;/p&gt;&lt;p&gt;T模板，把页面展示给用户&lt;/p&gt;&lt;p&gt;V：视图，负责业务逻辑，可调用模型和模板&lt;/p&gt;&lt;p&gt;此外，还需要url分发器，将url请求分发给视图处理，视图再调用模型和模板&lt;/p&gt;&lt;p&gt;使用pycharm新建django项目如下：&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20201124151653.png&quot; alt&gt;&lt;/p&gt;&lt;p&gt;其中manage.py为项目入口（其中包含了项目设置从何处读入，即需要去setting下进行查找），templates文件夹下为模板，项目名字下的urls.py为url分发器，需要自行新建应用app，使用app下的model.py，设置数据库中表的名称和各列的数据类型。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="推荐算法" scheme="https://www.xiapf.com/tags/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Leecode做题记录（七）</title>
    <link href="https://www.xiapf.com/blogs/LC7/"/>
    <id>https://www.xiapf.com/blogs/LC7/</id>
    <published>2020-11-20T09:03:12.000Z</published>
    <updated>2020-11-20T09:04:08.163Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目：402-移掉K位数字"><a href="#题目：402-移掉K位数字" class="headerlink" title="题目：402. 移掉K位数字"></a>题目：402. 移掉K位数字</h2><p>描述：给定一个以字符串表示的非负整数 num，移除这个数中的 k 位数字，使得剩下的数字最小。</p><p>注意:</p><p>num 的长度小于 10002 且 ≥ k。<br>num 不会包含任何前导零。<br>示例 1 :</p><p>输入: num = “1432219”, k = 3<br>输出: “1219”<br>解释: 移除掉三个数字 4, 3, 和 2 形成一个新的最小的数字 1219。</p><a id="more"></a><p>思路：从最高位开始，每次找到第一个比前面一个数字大的位置，移除该数字，此过程执行k次。</p><p>（1）将原始字符串复制给结果字符串中</p><p>（2）遍历字符串，找到第一个比前面一个数字大的位置，移除该位置的字符</p><p>（3）当移除完字符后，字符串首位为0，则将首位的字符删除，直至首位不是0为止</p><p>（4）以上两步（2）、（3）步骤执行k次，最终得到移除k个数字后的最小的数字</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="built_in">string</span> <span class="title">removeKdigits</span><span class="params">(<span class="built_in">string</span> num, <span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//找到第一个比前面的数字大的位置移除</span></span><br><span class="line">       <span class="keyword">if</span>(k==num.<span class="built_in">size</span>())</span><br><span class="line">           <span class="keyword">return</span> <span class="string">"0"</span>;</span><br><span class="line">       <span class="built_in">string</span> res=num;</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;k;i++)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;res.<span class="built_in">size</span>();j++)</span><br><span class="line">           &#123;</span><br><span class="line">               <span class="keyword">if</span>(j!=res.<span class="built_in">size</span>()<span class="number">-1</span>&amp;&amp;res[j]&lt;=res[j+<span class="number">1</span>])</span><br><span class="line">                   <span class="keyword">continue</span>;</span><br><span class="line">               res.erase(j,<span class="number">1</span>);</span><br><span class="line">               <span class="keyword">break</span>;</span><br><span class="line">           &#125;</span><br><span class="line">           <span class="keyword">while</span>(res.<span class="built_in">size</span>()&gt;<span class="number">1</span>&amp;&amp;res.<span class="built_in">find</span>(<span class="string">'0'</span>)==<span class="number">0</span>)</span><br><span class="line">               res.erase(<span class="number">0</span>,<span class="number">1</span>);</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> res;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：1122-数组的相对排序"><a href="#题目：1122-数组的相对排序" class="headerlink" title="题目：1122. 数组的相对排序"></a>题目：1122. 数组的相对排序</h2><p>描述：</p><p>给你两个数组，arr1 和 arr2，</p><p>arr2 中的元素各不相同<br>arr2 中的每个元素都出现在 arr1 中<br>对 arr1 中的元素进行排序，使 arr1 中项的相对顺序和 arr2 中的相对顺序相同。未在 arr2 中出现过的元素需要按照升序放在 arr1 的末尾。</p><p> 示例：</p><p>输入：arr1 = [2,3,1,3,2,4,6,7,9,2,19], arr2 = [2,1,4,3,9,6]<br>输出：[2,2,2,1,4,3,3,9,6,7,19]</p><p>思路：类似桶排序的思想</p><p>（1）使用排序哈希表，存储数组arr1中每个数字-出现次数的映射</p><p>（2）遍历数组arr2，找到该数字在哈希表中出现的次数，按照出现的次数插入到结果数组中</p><p>（3）经过步骤（2）此时已经将 arr1 中项的相对顺序和 arr2 中的相对顺序相同，剩下处理未在 arr2 中出现过的元素</p><p>遍历哈希表，当当前元素出现次数不为0，则说明是没有在arr2中出现，按照出现的次数插入到结果数组中（因为哈希表排序过了，所以此时是按照升序放在 arr1 的末尾的。）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="built_in">string</span> <span class="title">removeKdigits</span><span class="params">(<span class="built_in">string</span> num, <span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//找到第一个比前面的数字大的位置移除</span></span><br><span class="line">       <span class="keyword">if</span>(k==num.<span class="built_in">size</span>())</span><br><span class="line">           <span class="keyword">return</span> <span class="string">"0"</span>;</span><br><span class="line">       <span class="built_in">string</span> res=num;</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;k;i++)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;res.<span class="built_in">size</span>();j++)</span><br><span class="line">           &#123;</span><br><span class="line">               <span class="keyword">if</span>(j!=res.<span class="built_in">size</span>()<span class="number">-1</span>&amp;&amp;res[j]&lt;=res[j+<span class="number">1</span>])</span><br><span class="line">                   <span class="keyword">continue</span>;</span><br><span class="line">               res.erase(j,<span class="number">1</span>);</span><br><span class="line">               <span class="keyword">break</span>;</span><br><span class="line">           &#125;</span><br><span class="line">           <span class="keyword">while</span>(res.<span class="built_in">size</span>()&gt;<span class="number">1</span>&amp;&amp;res.<span class="built_in">find</span>(<span class="string">'0'</span>)==<span class="number">0</span>)</span><br><span class="line">               res.erase(<span class="number">0</span>,<span class="number">1</span>);</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> res;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：1030-距离顺序排列矩阵单元格"><a href="#题目：1030-距离顺序排列矩阵单元格" class="headerlink" title="题目：1030. 距离顺序排列矩阵单元格"></a>题目：1030. 距离顺序排列矩阵单元格</h2><p>描述：给出 R 行 C 列的矩阵，其中的单元格的整数坐标为 (r, c)，满足 0 &lt;= r &lt; R 且 0 &lt;= c &lt; C。</p><p>另外，我们在该矩阵中给出了一个坐标为 (r0, c0) 的单元格。</p><p>返回矩阵中的所有单元格的坐标，并按到 (r0, c0) 的距离从最小到最大的顺序排，其中，两单元格(r1, c1) 和 (r2, c2) 之间的距离是曼哈顿距离，|r1 - r2| + |c1 - c2|。（你可以按任何满足此条件的顺序返回答案。）</p><p> 示例 1：</p><p>输入：R = 1, C = 2, r0 = 0, c0 = 0<br>输出：[[0,0],[0,1]]<br>解释：从 (r0, c0) 到其他单元格的距离为：[0,1]</p><p>思路：</p><p>1、自定义排序    时间复杂度O(RClogRC)（排序时间RClogRC），空间复杂度O(logRC)（需要额外的栈存储）</p><p>（1）将横纵坐标与原点的差值以及当前位置保存下来</p><p>（2）对差值之和进行自定义排序</p><p>（3）按照排序之后的数组，保存第2和第3个索引上的位置</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">cmp</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; a,<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a[<span class="number">0</span>]+a[<span class="number">1</span>]&lt;b[<span class="number">0</span>]+b[<span class="number">1</span>];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; allCellsDistOrder(<span class="keyword">int</span> R, <span class="keyword">int</span> C, <span class="keyword">int</span> r0, <span class="keyword">int</span> c0) &#123;</span><br><span class="line">        <span class="comment">//存储所有的点自定义排序</span></span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; tmp;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; res;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;R;i++)</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;C;j++)</span><br><span class="line">                tmp.push_back(&#123;<span class="built_in">abs</span>(i-r0),<span class="built_in">abs</span>(j-c0),i,j&#125;);</span><br><span class="line">        </span><br><span class="line">        sort(tmp.<span class="built_in">begin</span>(), tmp.<span class="built_in">end</span>(), cmp);</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;tmp.<span class="built_in">size</span>();i++)</span><br><span class="line">            res.push_back(&#123;tmp[i][<span class="number">2</span>],tmp[i][<span class="number">3</span>]&#125;);</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>2、优先队列   时间复杂度O(RC)，空间复杂度O(RC)（需要额外的存储）</p><p>默认情况下，优先队列是大顶堆（降序），队头元素最大，要得到小顶堆（升序）需要添加保存数据的容器以及比较方式greater&lt;&gt;</p><p>（1）定义小顶堆，保存距离以及当前位置</p><p>（2）在数据插入小顶堆的过程中，就按照距离从小到大进行了排序</p><p>（3）只需要从队首一个个取出当前距离下的位置即可</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; allCellsDistOrder(<span class="keyword">int</span> R, <span class="keyword">int</span> C, <span class="keyword">int</span> r0, <span class="keyword">int</span> c0) &#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; res;</span><br><span class="line"><span class="comment">//        priority_queue&lt;pair&lt;int, pair&lt;int, int&gt;&gt;&gt; p; //大顶堆（降序），队头元素最大</span></span><br><span class="line">        priority_queue&lt;pair&lt;<span class="keyword">int</span>, pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt;&gt;,<span class="built_in">vector</span>&lt;pair&lt;<span class="keyword">int</span>, pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt;&gt;&gt;,greater&lt;pair&lt;<span class="keyword">int</span>, pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt;&gt;&gt;&gt; p; <span class="comment">//小顶堆（升序） 数据类型，保存数据容器，比较方式</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;R;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;C;j++)</span><br><span class="line">            &#123;</span><br><span class="line">                pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; pos=make_pair(i, j);</span><br><span class="line">                <span class="keyword">int</span> dis=<span class="built_in">abs</span>(i-r0)+<span class="built_in">abs</span>(j-c0);</span><br><span class="line">                p.push(&#123;dis,pos&#125;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span>(!p.empty())</span><br><span class="line">        &#123;</span><br><span class="line">            pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; pos=p.top().second;</span><br><span class="line">            res.push_back(&#123;pos.first,pos.second&#125;);</span><br><span class="line">            p.pop();</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>3、桶排序  时间复杂度O(RC)，空间复杂度O(RC)（需要额外的桶存储）</p><p>（1）定义一个桶用于存储所有的距离</p><p>（2）遍历所有的坐标，得到当前坐标下的距离，按照距离的大小作为索引存储位置</p><p>则最终得到的桶是按照距离排序，每个距离桶里存储了当前得到该距离的位置</p><p>（3）遍历桶，得到每个桶下的位置进行保存即可</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; allCellsDistOrder(<span class="keyword">int</span> R, <span class="keyword">int</span> C, <span class="keyword">int</span> r0, <span class="keyword">int</span> c0) &#123;</span><br><span class="line">       <span class="comment">//桶排序</span></span><br><span class="line">       <span class="keyword">int</span> maxlen=R*C;</span><br><span class="line">       <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; res;</span><br><span class="line">       <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&gt; bucket(maxlen+<span class="number">1</span>);</span><br><span class="line">       </span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;R;i++)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;C;j++)</span><br><span class="line">           &#123;</span><br><span class="line">               <span class="keyword">int</span> d=dist(i, j, r0, c0);</span><br><span class="line">               bucket[d].push_back(&#123;i,j&#125;);</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;bucket.<span class="built_in">size</span>();i++)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">for</span>(<span class="keyword">auto</span> c:bucket[i])</span><br><span class="line">               res.push_back(c);</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       <span class="keyword">return</span> res;</span><br><span class="line"></span><br><span class="line">   &#125;</span><br><span class="line">   <span class="function"><span class="keyword">int</span> <span class="title">dist</span><span class="params">(<span class="keyword">int</span> a,<span class="keyword">int</span> b,<span class="keyword">int</span> x,<span class="keyword">int</span> y)</span></span></span><br><span class="line"><span class="function">   </span>&#123;</span><br><span class="line">       <span class="keyword">return</span> <span class="built_in">abs</span>(a-x)+<span class="built_in">abs</span>(b-y);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：面试题-10-01-合并排序的数组"><a href="#题目：面试题-10-01-合并排序的数组" class="headerlink" title="题目：面试题 10.01. 合并排序的数组"></a>题目：面试题 10.01. 合并排序的数组</h2><p>描述：给定两个排序后的数组 A 和 B，其中 A 的末端有足够的缓冲空间容纳 B。 编写一个方法，将 B 合并入 A 并排序。</p><p>初始化 A 和 B 的元素数量分别为 m 和 n。</p><p>示例:</p><p>输入:<br>A = [1,2,3,0,0,0], m = 3<br>B = [2,5,6],       n = 3</p><p>输出: [1,2,2,3,5,6]</p><p>思路：</p><p>1、合并后用sort排序   时间复杂度O((m+n)log(m+n))，空间复杂度O(log(m+n))</p><p>2、双指针，分别指向两个数组的首部，比较对应元素，小的那个放到新数组中，移动指针，比较下一个，依次循环</p><p>时间复杂度O(m+n)，空间复杂度O(m+n)</p><p>3、三指针 从尾部开始比较 原地算法   时间复杂度O(m+n)，空间复杂度O(1)</p><p>（1）定义三个指针，分别指向数组A的尾部，数组A的最后一个元素，数组B的尾部</p><p>（2）当数组A和数组B的指针没有移动到首部时，继续循环</p><p>比较两个元素中大的那个，将其值赋值给数据A尾部的指针，并移动指针</p><p>（3）当循环结束后，数组B中还有元素，则将其全部赋值给数组A尾部的指针</p><p>（为什么不需要判断数组A中的元素：因为如果数组A中还有没有比较的元素，直接不处理，因为是原地算法）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">merge</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; A, <span class="keyword">int</span> m, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; B, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//三指针 从尾部开始比较</span></span><br><span class="line">       <span class="keyword">if</span>(m==<span class="number">0</span>)</span><br><span class="line">           A=B;</span><br><span class="line">       <span class="keyword">int</span> i=m<span class="number">-1</span>;</span><br><span class="line">       <span class="keyword">int</span> j=n<span class="number">-1</span>;</span><br><span class="line">       <span class="keyword">int</span> k=m+n<span class="number">-1</span>;</span><br><span class="line">       <span class="keyword">while</span>(i&gt;=<span class="number">0</span>&amp;&amp;j&gt;=<span class="number">0</span>)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">if</span>(A[i]&gt;B[j])</span><br><span class="line">           &#123;</span><br><span class="line">               A[k]=A[i];</span><br><span class="line">               k--;</span><br><span class="line">               i--;</span><br><span class="line">           &#125;</span><br><span class="line">           <span class="keyword">else</span></span><br><span class="line">           &#123;</span><br><span class="line">               A[k]=B[j];</span><br><span class="line">               k--;</span><br><span class="line">               j--;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">while</span>(j&gt;=<span class="number">0</span>)</span><br><span class="line">       &#123;</span><br><span class="line">          A[k]=B[j];</span><br><span class="line">          k--;</span><br><span class="line">          j--;</span><br><span class="line">        &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：414-第三大的数"><a href="#题目：414-第三大的数" class="headerlink" title="题目：414. 第三大的数"></a>题目：414. 第三大的数</h2><p>描述：给定一个非空数组，返回此数组中第三大的数。如果不存在，则返回数组中最大的数。要求算法时间复杂度必须是O(n)。</p><p>示例 1:</p><p>输入: [2, 2, 3, 1]</p><p>输出: 1</p><p>解释: 注意，要求返回第三大的数，是指第三大且唯一出现的数。<br>存在两个值为2的数，它们都排第二。</p><p>思路：</p><p>1、每次输出当前最大的值</p><p>（1）当当前数组内元素没有遍历结束，继续循环</p><p>（2）找到数组当前最大的值，删除该值，并进行最大值赋值</p><p>（3）如果当前最大值和上一次一样，不加入计数中，反之进行计数，并重新设置最大值</p><p>（4）当计数值等于3，则弹出，返回当前第三大的值</p><p>（5）当遍历结束，计数值小于3，则重新遍历，找到原数组中的最大值返回</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">thirdMax</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">int</span> maxnum=INT_MIN;</span><br><span class="line">       <span class="keyword">int</span> maxpos=<span class="number">-1</span>;</span><br><span class="line">       <span class="keyword">int</span> count=<span class="number">0</span>;</span><br><span class="line">       <span class="keyword">int</span> res;</span><br><span class="line">       <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; tmp=nums;</span><br><span class="line">       <span class="keyword">int</span> len=nums.<span class="built_in">size</span>();</span><br><span class="line">       <span class="keyword">while</span>(len&gt;<span class="number">0</span>)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;nums.<span class="built_in">size</span>();i++)</span><br><span class="line">           &#123;</span><br><span class="line">               <span class="keyword">if</span>(nums[i]&gt;=maxnum)</span><br><span class="line">               &#123;</span><br><span class="line">                   maxnum=nums[i];</span><br><span class="line">                   maxpos=i;</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">           nums.erase(nums.<span class="built_in">begin</span>()+maxpos);</span><br><span class="line">           len--;</span><br><span class="line"></span><br><span class="line">           <span class="keyword">if</span>(maxnum==res)</span><br><span class="line">           &#123;</span><br><span class="line">               maxnum=INT_MIN;</span><br><span class="line">               <span class="keyword">continue</span>;</span><br><span class="line">           &#125;</span><br><span class="line">           res=maxnum;</span><br><span class="line">           maxnum=INT_MIN;</span><br><span class="line">           count++;</span><br><span class="line">           <span class="keyword">if</span>(count&gt;=<span class="number">3</span>)</span><br><span class="line">               <span class="keyword">break</span>;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">if</span>(count&lt;<span class="number">3</span>)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;tmp.<span class="built_in">size</span>();i++)</span><br><span class="line">               maxnum=<span class="built_in">max</span>(maxnum,tmp[i]);</span><br><span class="line">           <span class="keyword">return</span> maxnum;</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       <span class="keyword">return</span> res;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>2、利用set，维护大小为3的set（默认排序是从小到大）</p><p>（1）遍历数组中的元素将其插入set中（此时自动去重）</p><p>当set的大小大于3时，删除最前面一个元素</p><p>（2）当最后得到的set大小等于3，直接返回最前面一个元素的值，即第三大的值（s.begin）</p><p>当大小小于3，返回数组内最大的值，即最后一个元素的值，即s.rbegin()</p><p>注：</p><p>c.begin() 返回一个迭代器，它指向容器c的第一个元素</p><p>c.end() 返回一个迭代器，它指向容器c的最后一个元素的下一个位置</p><p>c.rbegin() 返回一个逆序迭代器，它指向容器c的最后一个元素</p><p>c.rend() 返回一个逆序迭代器，它指向容器c的第一个元素前面的位置</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">thirdMax</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//维护一个大小为3的set</span></span><br><span class="line">       <span class="built_in">set</span>&lt;<span class="keyword">int</span>&gt; s;</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;nums.<span class="built_in">size</span>();i++)</span><br><span class="line">       &#123;</span><br><span class="line">           s.insert(nums[i]);</span><br><span class="line">           <span class="keyword">if</span>(s.<span class="built_in">size</span>()&gt;<span class="number">3</span>)</span><br><span class="line">               s.erase(*(s.<span class="built_in">begin</span>()));</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> s.<span class="built_in">size</span>()&lt;<span class="number">3</span>?*(s.rbegin()):*(s.<span class="built_in">begin</span>());<span class="comment">//c.begin() 返回一个迭代器，它指向容器c的第一个元素;c.end() 返回一个迭代器，它指向容器c的最后一个元素的下一个位置;c.rbegin() 返回一个逆序迭代器，它指向容器c的最后一个元素;c.rend() 返回一个逆序迭代器，它指向容器c的第一个元素前面的位置</span></span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：509-斐波那契数"><a href="#题目：509-斐波那契数" class="headerlink" title="题目：509. 斐波那契数"></a>题目：509. 斐波那契数</h2><p>描述：斐波那契数，通常用 F(n) 表示，形成的序列称为斐波那契数列。该数列由 0 和 1 开始，后面的每一项数字都是前面两项数字的和。也就是：</p><p>F(0) = 0,   F(1) = 1<br>F(N) = F(N - 1) + F(N - 2), 其中 N &gt; 1.<br>给定 N，计算 F(N)。</p><p>示例 1：</p><p>输入：2<br>输出：1<br>解释：F(2) = F(1) + F(0) = 1 + 0 = 1.</p><p>思路：动态规划</p><p>设置dp数组大小为N+1，保存从0开始的N+1个数字的斐波那契和</p><p>（1）定义初始状态</p><p>F(0) = 0,   F(1) = 1</p><p>（2）定义转移方程</p><p>F(N) = F(N - 1) + F(N - 2)</p><p>最后返回dp数组最后一个值即为F(N)的结果</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fib</span><span class="params">(<span class="keyword">int</span> N)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//输入：2 输出：1  解释：F(2) = F(1) + F(0) = 1 + 0 = 1.</span></span><br><span class="line">       <span class="comment">//F(0) = 0,   F(1) = 1 F(N) = F(N - 1) + F(N - 2), 其中 N &gt; 1.</span></span><br><span class="line">       <span class="keyword">if</span>(N==<span class="number">0</span>||N==<span class="number">1</span>)</span><br><span class="line">           <span class="keyword">return</span> N;</span><br><span class="line">       <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; dp(N+<span class="number">1</span>);</span><br><span class="line">       dp[<span class="number">0</span>]=<span class="number">0</span>;</span><br><span class="line">       dp[<span class="number">1</span>]=<span class="number">1</span>;</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">2</span>;i&lt;dp.<span class="built_in">size</span>();i++)</span><br><span class="line">           dp[i]=dp[i<span class="number">-1</span>]+dp[i<span class="number">-2</span>];        </span><br><span class="line">       <span class="keyword">return</span> dp[N];</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：283-移动零"><a href="#题目：283-移动零" class="headerlink" title="题目：283. 移动零"></a>题目：283. 移动零</h2><p>描述：给定一个数组 nums，编写一个函数将所有 0 移动到数组的末尾，同时保持非零元素的相对顺序。</p><p>示例:</p><p>输入: [0,1,0,3,12]<br>输出: [1,3,12,0,0]<br>说明:</p><p>必须在原数组上操作，不能拷贝额外的数组。<br>尽量减少操作次数。</p><p>思路：</p><p>1、两次遍历：非0元素前移，0元素放置最后</p><p>（1）第一次遍历数组：找到非0的值前移即可</p><p>（2）第二次遍历数组：在非0元素最后加上0</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> index=<span class="number">0</span>;</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;nums.<span class="built_in">size</span>();i++)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">if</span>(nums[i]!=<span class="number">0</span>)</span><br><span class="line">           &#123;</span><br><span class="line">               nums[index]=nums[i];</span><br><span class="line">               index++;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=index;i&lt;nums.<span class="built_in">size</span>();i++)</span><br><span class="line">           nums[i]=<span class="number">0</span>;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>2、一次遍历：双指针，保证左指针的左边为非0，右指针和左指针之间为0</p><p>（1）设置两个指针都指向首部</p><p>（2）当移动右指针不为0时，将左右指针的值交换，保证左指针左边非0，移动左右指针</p><p>当右指针为0，则继续找不为0的值</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">moveZeroes</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//283. 移动零:给定一个数组 nums，编写一个函数将所有 0 移动到数组的末尾，同时保持非零元素的相对顺序。</span></span><br><span class="line">       <span class="comment">//双指针 左指针左边为非0，左指针到右指针之间为0</span></span><br><span class="line">       <span class="keyword">int</span> left=<span class="number">0</span>;</span><br><span class="line">       <span class="keyword">int</span> right=<span class="number">0</span>;</span><br><span class="line">       <span class="keyword">while</span>(right&lt;nums.<span class="built_in">size</span>())</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">if</span>(nums[right]!=<span class="number">0</span>)</span><br><span class="line">           &#123;</span><br><span class="line">               swap(nums[left], nums[right]);</span><br><span class="line">               left++;</span><br><span class="line">           &#125;</span><br><span class="line">           right++;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：496-下一个更大元素-I"><a href="#题目：496-下一个更大元素-I" class="headerlink" title="题目：496. 下一个更大元素 I"></a>题目：496. 下一个更大元素 I</h2><p>描述：给定两个 没有重复元素 的数组 nums1 和 nums2 ，其中nums1 是 nums2 的子集。找到 nums1 中每个元素在 nums2 中的下一个比其大的值。</p><p>nums1 中数字 x 的下一个更大元素是指 x 在 nums2 中对应位置的右边的第一个比 x 大的元素。如果不存在，对应位置输出 -1 。</p><p>示例 1:</p><p>输入: nums1 = [4,1,2], nums2 = [1,3,4,2].<br>输出: [-1,3,-1]<br>解释:<br>    对于num1中的数字4，你无法在第二个数组中找到下一个更大的数字，因此输出 -1。<br>    对于num1中的数字1，第二个数组中数字1右边的下一个较大数字是 3。<br>    对于num1中的数字2，第二个数组中没有下一个更大的数字，因此输出 -1。<br>示例 2:</p><p>输入: nums1 = [2,4], nums2 = [1,2,3,4].<br>输出: [3,-1]<br>解释:<br>    对于 num1 中的数字 2 ，第二个数组中的下一个较大数字是 3 。<br>    对于 num1 中的数字 4 ，第二个数组中没有下一个更大的数字，因此输出 -1 。</p><p>思路：</p><p>1、暴力法：遍历nums1中的每个元素，首先找到其在nums2中所在的位置，再判断该位置之后有无比其更大的元素       时间复杂度O(m * n )(m为nums1大小，n为nums2大小)，空间复杂度O(1)</p><p>（1）遍历nums1中的每个元素</p><p>（2）找到其在nums2中所在的位置</p><p>（3）判断该位置之后有无比其更大的元素 </p><p>有则保存比其大的数，没有就保存-1</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; nextGreaterElement(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums1, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums2) &#123;</span><br><span class="line">       <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res;</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;nums1.<span class="built_in">size</span>();i++)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">int</span> num=nums1[i];</span><br><span class="line">           <span class="keyword">int</span> nextmax=INT_MIN;</span><br><span class="line">           <span class="comment">//找到在nums2中的位置</span></span><br><span class="line">           <span class="keyword">int</span> idx=<span class="number">-1</span>;</span><br><span class="line">           <span class="keyword">for</span>(<span class="keyword">int</span> k=<span class="number">0</span>;k&lt;nums2.<span class="built_in">size</span>();k++)</span><br><span class="line">           &#123;</span><br><span class="line">               <span class="keyword">if</span>(nums2[k]==num)</span><br><span class="line">               &#123;</span><br><span class="line">                   idx=k;</span><br><span class="line">                   <span class="keyword">break</span>;</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">           <span class="comment">//判断下一个大的元素</span></span><br><span class="line">           <span class="keyword">for</span>(<span class="keyword">int</span> j=idx;j&lt;nums2.<span class="built_in">size</span>();j++)</span><br><span class="line">           &#123;</span><br><span class="line">               <span class="keyword">if</span>(nums2[j]&gt;num)</span><br><span class="line">               &#123;</span><br><span class="line">                   nextmax=nums2[j];</span><br><span class="line">                   <span class="keyword">break</span>;</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">           res.push_back(nextmax==INT_MIN?<span class="number">-1</span>:nextmax);</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> res;</span><br><span class="line"></span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>2、单调栈：对nums2维护一个单调栈，保存每个元素和其下一个更大的元素的映射      时间复杂度O(m+n )(m为nums1大小，n为nums2大小)，空间复杂度O(n)（用到的哈希表和栈所占的空间）</p><p>（1）对nums2维护一个单调栈，保存每个元素和其下一个更大的元素的映射 ：单调栈内的元素是从栈顶到栈底是升序排列，即栈底的元素大于上面的元素</p><p>先将第0个元素压入单调栈中，遍历nums2中的每个元素（从第1个元素开始），当当前元素比栈顶的元素大，则栈内所有元素的下一个更大的值为该遍历到的元素（因为入栈是则说明之前都没有遇到更大的元素），将其保存在哈希表中；</p><p>反之，即当前元素比栈顶元素小，则入栈，等待后面的元素比其大</p><p>（2）当遍历结束，单调栈内还有元素：说明栈内的元素到最后都没有遇到比它大的元素：则将栈内所有元素都映射到-1</p><p>（3）遍历nums2数组，找到每个元素在哈希表中的映射，即为下一个更大的元素</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; nextGreaterElement(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums1, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums2) &#123;</span><br><span class="line">       <span class="comment">//单调栈</span></span><br><span class="line">       <span class="keyword">if</span>(nums2.empty()||nums1.empty())</span><br><span class="line">           <span class="keyword">return</span> &#123;&#125;;</span><br><span class="line">       <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res;</span><br><span class="line">       <span class="comment">//对nums2构造单调栈，找到每个元素下一个大的元素</span></span><br><span class="line">       <span class="built_in">unordered_map</span>&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; <span class="built_in">map</span>;</span><br><span class="line">       <span class="built_in">stack</span>&lt;<span class="keyword">int</span>&gt; s;</span><br><span class="line">       s.push(nums2[<span class="number">0</span>]);</span><br><span class="line">       <span class="keyword">int</span> i=<span class="number">1</span>;</span><br><span class="line">       <span class="keyword">while</span>(i&lt;nums2.<span class="built_in">size</span>())</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">while</span>(!s.empty()&amp;&amp;nums2[i]&gt;s.top())</span><br><span class="line">           &#123;</span><br><span class="line">               <span class="built_in">map</span>[s.top()]=nums2[i];</span><br><span class="line">               s.pop();</span><br><span class="line">           &#125;</span><br><span class="line">           s.push(nums2[i]);</span><br><span class="line">           i++;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">while</span>(!s.empty())</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="built_in">map</span>[s.top()]=<span class="number">-1</span>;</span><br><span class="line">           s.pop();</span><br><span class="line">       &#125;  </span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;nums1.<span class="built_in">size</span>();i++)</span><br><span class="line">           res.push_back(<span class="built_in">map</span>[nums1[i]]);</span><br><span class="line">       <span class="keyword">return</span> res;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：541-反转字符串-II"><a href="#题目：541-反转字符串-II" class="headerlink" title="题目：541. 反转字符串 II"></a>题目：541. 反转字符串 II</h2><p>描述：给定一个字符串 s 和一个整数 k，你需要对从字符串开头算起的每隔 2k 个字符的前 k 个字符进行反转。</p><p>如果剩余字符少于 k 个，则将剩余字符全部反转。<br>如果剩余字符小于 2k 但大于或等于 k 个，则反转前 k 个字符，其余字符保持原样。</p><p>示例:</p><p>输入: s = “abcdefg”, k = 2<br>输出: “bacdfeg”</p><p>思路：</p><p>1、分组法   时间复杂度O(n)，空间复杂度O(n)（用到额外字符串数组所占的空间）</p><p>（1）将原数组按照每2k个进行分组，存入到字符串数组中</p><p>设置起始位置，当当前位置向后2k个字符没有超出范围，则将从起始位置开始的2k个字符存储下来；</p><p>反之如果超出范围，则将当前位置到末尾的字符串存储下来</p><p>（2）读入分组存储的字符串数组</p><p>如果长度大于等于k，则反转前k个字符；</p><p>反之，即长度小于k，则将整个字符串反转</p><p>处理完成后保存到结果中即可。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="built_in">string</span> <span class="title">reverseStr</span><span class="params">(<span class="built_in">string</span> s, <span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line">       <span class="built_in">string</span> res;</span><br><span class="line">       <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; tmp;</span><br><span class="line">       <span class="keyword">int</span> start=<span class="number">0</span>;</span><br><span class="line">       <span class="keyword">while</span>(start&lt;s.<span class="built_in">size</span>())</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">if</span>((start+<span class="number">2</span>*k)&lt;s.<span class="built_in">size</span>())</span><br><span class="line">               tmp.push_back(s.substr(start,<span class="number">2</span>*k));</span><br><span class="line">           <span class="keyword">else</span></span><br><span class="line">               tmp.push_back(s.substr(start));</span><br><span class="line">           start=start+<span class="number">2</span>*k;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;tmp.<span class="built_in">size</span>();i++)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">if</span>(tmp[i].<span class="built_in">size</span>()&gt;=k)</span><br><span class="line">               reverse(tmp[i].<span class="built_in">begin</span>(), tmp[i].<span class="built_in">begin</span>()+k);</span><br><span class="line">           <span class="keyword">else</span></span><br><span class="line">               reverse(tmp[i].<span class="built_in">begin</span>(), tmp[i].<span class="built_in">end</span>());</span><br><span class="line">           res.append(tmp[i]);</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> res;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>2、优化1</p><p>直接在1中每次分组的时候反转前k个字符即可</p><p>（1）确定起始位置，控制步长为2 * k</p><p>（2）当从起始位置开始的k个位置不超出范围，则进行反转；</p><p>反之，则将剩余的字符串全部反转</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="built_in">string</span> <span class="title">reverseStr</span><span class="params">(<span class="built_in">string</span> s, <span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line">       <span class="built_in">string</span> res;</span><br><span class="line">       <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; tmp;</span><br><span class="line">       <span class="keyword">int</span> start=<span class="number">0</span>;</span><br><span class="line">       <span class="keyword">while</span>(start&lt;s.<span class="built_in">size</span>())</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">if</span>((start+k)&lt;s.<span class="built_in">size</span>())</span><br><span class="line">               reverse(s.<span class="built_in">begin</span>()+start, s.<span class="built_in">begin</span>()+start+k);</span><br><span class="line">           <span class="keyword">else</span></span><br><span class="line">               reverse(s.<span class="built_in">begin</span>()+start, s.<span class="built_in">end</span>());</span><br><span class="line">           start=start+<span class="number">2</span>*k;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> s;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：147-对链表进行插入排序"><a href="#题目：147-对链表进行插入排序" class="headerlink" title="题目：147. 对链表进行插入排序"></a>题目：147. 对链表进行插入排序</h2><p>描述：插入排序的为：从第一个元素开始，该链表可以被认为已经部分排序（用黑色表示）。<br>每次迭代时，从输入数据中移除一个元素（用红色表示），并原地将其插入到已排好序的链表中。</p><p>插入排序算法：</p><p>插入排序是迭代的，每次只移动一个元素，直到所有元素可以形成一个有序的输出列表。<br>每次迭代中，插入排序只从输入数据中移除一个待排序的元素，找到它在序列中适当的位置，并将其插入。<br>重复直到所有输入数据插入完为止。</p><p>示例 1：</p><p>输入: 4-&gt;2-&gt;1-&gt;3<br>输出: 1-&gt;2-&gt;3-&gt;4</p><p>思路：遍历链表，找到破坏升序的位置，再从头找到让其插入的位置    时间复杂度O(n * n)，空间复杂度O(1)（原地算法）</p><p>（1）设置哑结点，next指针指向原链表</p><p>（2）遍历链表</p><p>当当前节点的值比下一个节点的值小，则继续比较下一个节点；</p><p>当当前节点的值比下一个节点的值大，则找到了需要找到其插入位置的节点cur=当前节点的下一个节点：</p><p>利用哑结点从头开始遍历，找到其需要插入的位置，将当前节点指向下下个节点，cur节点指向需要插入的位置，并将哑结点当前位置的下一个设置为cur</p><p>（3）返回哑结点的下一个</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">ListNode* <span class="title">insertionSortList</span><span class="params">(ListNode* head)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">if</span>(head==<span class="literal">NULL</span>)</span><br><span class="line">           <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">       ListNode* dummy=<span class="keyword">new</span> ListNode(<span class="number">-1</span>);</span><br><span class="line">       dummy-&gt;next=head;</span><br><span class="line">       <span class="keyword">while</span>(head!=<span class="literal">NULL</span>&amp;&amp;head-&gt;next!=<span class="literal">NULL</span>)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">if</span>(head-&gt;val&lt;head-&gt;next-&gt;val)</span><br><span class="line">               head=head-&gt;next;</span><br><span class="line">           <span class="keyword">else</span></span><br><span class="line">           &#123;</span><br><span class="line">               <span class="comment">//当找到小于的那个位置，从头找到他插入的位置</span></span><br><span class="line">               ListNode* p=dummy;</span><br><span class="line">               <span class="keyword">while</span>(p-&gt;next-&gt;val&lt;head-&gt;next-&gt;val)</span><br><span class="line">                   p=p-&gt;next;</span><br><span class="line">               </span><br><span class="line">               ListNode* cur=head-&gt;next;<span class="comment">//要插入的点</span></span><br><span class="line">               head-&gt;next=head-&gt;next-&gt;next;</span><br><span class="line">               cur-&gt;next=p-&gt;next;</span><br><span class="line">               p-&gt;next=cur;</span><br><span class="line">           &#125;</span><br><span class="line"></span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> dummy-&gt;next;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：LCP-01-猜数字"><a href="#题目：LCP-01-猜数字" class="headerlink" title="题目：LCP 01. 猜数字"></a>题目：LCP 01. 猜数字</h2><p>描述：小A 和 小B 在玩猜数字。小B 每次从 1, 2, 3 中随机选择一个，小A 每次也从 1, 2, 3 中选择一个猜。他们一共进行三次这个游戏，请返回 小A 猜对了几次？</p><p>输入的guess数组为 小A 每次的猜测，answer数组为 小B 每次的选择。guess和answer的长度都等于3。</p><p>示例 1：</p><p>输入：guess = [1,2,3], answer = [1,2,3]<br>输出：3<br>解释：小A 每次都猜对了。</p><p>示例 2：</p><p>输入：guess = [2,2,3], answer = [3,2,1]<br>输出：1<br>解释：小A 只猜对了第二次。</p><p>思路：遍历数组，对应位置相同则计数加1，最后返回计数值即可</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">game</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; guess, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; answer)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">int</span> res=<span class="number">0</span>;</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;guess.<span class="built_in">size</span>();i++)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">if</span>(guess[i]==answer[i])</span><br><span class="line">               res++;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> res;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：1539-第-k-个缺失的正整数"><a href="#题目：1539-第-k-个缺失的正整数" class="headerlink" title="题目：1539. 第 k 个缺失的正整数"></a>题目：1539. 第 k 个缺失的正整数</h2><p>描述：给你一个 严格升序排列 的正整数数组 arr 和一个整数 k 。</p><p>请你找到这个数组里第 k 个缺失的正整数。</p><p> 示例 1：</p><p>输入：arr = [2,3,4,7,11], k = 5<br>输出：9<br>解释：缺失的正整数包括 [1,5,6,8,9,10,12,13,…] 。第 5 个缺失的正整数为 9 。</p><p>思路：</p><p>（1）设置i从1开始，并且遍历数组内所有元素</p><p>当数组内元素和i不相等时，继续计数，并移动i，同时判断计数值是否等于k，相等则返回当前的i值即缺失的值</p><p>当组内元素和i相等，则移动i，遍历下一个元素</p><p>（2）当遍历到数组末尾，计数值仍然小于k，则说明缺失值在末尾的值之后</p><p>将数组最后一个值++，直至计数值等于k</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">findKthPositive</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; arr, <span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">int</span> idx=<span class="number">0</span>;</span><br><span class="line">       <span class="keyword">int</span> i=<span class="number">1</span>;</span><br><span class="line">       <span class="keyword">int</span> count=<span class="number">0</span>;</span><br><span class="line">       <span class="keyword">while</span>(idx&lt;arr.<span class="built_in">size</span>())</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">while</span>(arr[idx]!=i)</span><br><span class="line">           &#123;</span><br><span class="line">               count++;</span><br><span class="line">               <span class="keyword">if</span>(count==k)</span><br><span class="line">                   <span class="keyword">return</span> i;</span><br><span class="line">               i++;</span><br><span class="line">           &#125;</span><br><span class="line">           idx++;</span><br><span class="line">           i++;</span><br><span class="line">           </span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">int</span> res=arr[idx<span class="number">-1</span>];</span><br><span class="line">       <span class="keyword">while</span>(count&lt;k)</span><br><span class="line">       &#123;</span><br><span class="line">           res++;</span><br><span class="line">           count++;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> res;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：384-打乱数组"><a href="#题目：384-打乱数组" class="headerlink" title="题目：384. 打乱数组"></a>题目：384. 打乱数组</h2><p>描述：给你一个整数数组 nums ，设计算法来打乱一个没有重复元素的数组。</p><p>实现 Solution class:</p><p>Solution(int[] nums) 使用整数数组 nums 初始化对象<br>int[] reset() 重设数组到它的初始状态并返回<br>int[] shuffle() 返回数组随机打乱后的结果</p><p>示例：</p><p>输入<br>[“Solution”, “shuffle”, “reset”, “shuffle”]<br>[[[1, 2, 3]], [], [], []]<br>输出<br>[null, [3, 1, 2], [1, 2, 3], [1, 3, 2]]</p><p>思路：</p><p>（1）初始化</p><p>可以直接设置一个私有变量，将传入的数组的值赋值给它</p><p>（2）重置</p><p>将原来的私有变量的值返回</p><p>（3）打乱</p><p>1.暴力法  时间复杂度O(n^2) (乘方时间来自erase)，空间复杂度O(n)（用到额外字符串数组所占的空间）</p><p>（1）每次从数组中随机选择一个位置赋值给新数组</p><p>（2）为了避免重复，每次选择的数字需要删除</p><p>一直到新数组被填满</p><p>2.洗牌算法  时间复杂度O(n)，空间复杂度O(n)（用到额外数组所占的空间）</p><p>每次从前i-1张洗好的牌中，选择一张和当前第i张牌交换即可</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; data;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">//初始化</span></span><br><span class="line">    Solution(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums) &#123;</span><br><span class="line">        data=nums;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/** Resets the array to its original configuration and return it. */</span></span><br><span class="line">    <span class="comment">//重置</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; reset() &#123;</span><br><span class="line">        <span class="keyword">return</span> data;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/** Returns a random shuffling of the array. */</span></span><br><span class="line">    <span class="comment">//打乱</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; shuffle() &#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; tmp=data;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res(tmp.<span class="built_in">size</span>());</span><br><span class="line">        <span class="comment">//暴力算法，每次随机取出一个值</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;res.<span class="built_in">size</span>();i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">int</span> pos=rand()%(tmp.<span class="built_in">size</span>());</span><br><span class="line">            res[i]=tmp[pos];</span><br><span class="line">            tmp.erase(tmp.<span class="built_in">begin</span>()+pos);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; shuffle() &#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; shdata=data;</span><br><span class="line">        <span class="comment">//洗牌算法，在前i-1张牌洗好的基础上，随机选一张进行交换</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;shdata.<span class="built_in">size</span>();i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">int</span> pos=rand()%(i+<span class="number">1</span>);<span class="comment">//产生一个0~i之间的数</span></span><br><span class="line">            swap(shdata[i], shdata[pos]);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> shdata;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;题目：402-移掉K位数字&quot;&gt;&lt;a href=&quot;#题目：402-移掉K位数字&quot; class=&quot;headerlink&quot; title=&quot;题目：402. 移掉K位数字&quot;&gt;&lt;/a&gt;题目：402. 移掉K位数字&lt;/h2&gt;&lt;p&gt;描述：给定一个以字符串表示的非负整数 num，移除这个数中的 k 位数字，使得剩下的数字最小。&lt;/p&gt;&lt;p&gt;注意:&lt;/p&gt;&lt;p&gt;num 的长度小于 10002 且 ≥ k。&lt;br&gt;num 不会包含任何前导零。&lt;br&gt;示例 1 :&lt;/p&gt;&lt;p&gt;输入: num = “1432219”, k = 3&lt;br&gt;输出: “1219”&lt;br&gt;解释: 移除掉三个数字 4, 3, 和 2 形成一个新的最小的数字 1219。&lt;/p&gt;
    
    </summary>
    
    
      <category term="算法" scheme="https://www.xiapf.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="https://www.xiapf.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Leecode做题记录（六）</title>
    <link href="https://www.xiapf.com/blogs/LC6/"/>
    <id>https://www.xiapf.com/blogs/LC6/</id>
    <published>2020-11-13T07:08:22.000Z</published>
    <updated>2020-11-13T07:09:25.195Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目：349-两个数组的交集"><a href="#题目：349-两个数组的交集" class="headerlink" title="题目：349. 两个数组的交集"></a>题目：349. 两个数组的交集</h2><p>描述：示例 1：</p><p>输入：nums1 = [1,2,2,1], nums2 = [2,2]<br>输出：[2]</p><p>思路：</p><p>1、排序+双指针 时间复杂度O(mlogm+nlogn)（主要是排序的时间+查找的时间），空间复杂度O(mlogm+nlogn)（排序的空间）</p><p>（1）将数组从小到大排序</p><p>（2）用双指针法分别指向两个数组，当两者不相等的时候分别移动各自指针，当相同的时候将值插入set表中去重</p><a id="more"></a><p>（3）将set表中不重复的值插入结果中</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; intersection(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums1, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums2) &#123;</span><br><span class="line">       <span class="comment">//1.排序</span></span><br><span class="line">       sort(nums1.<span class="built_in">begin</span>(),nums1.<span class="built_in">end</span>());</span><br><span class="line">       sort(nums2.<span class="built_in">begin</span>(),nums2.<span class="built_in">end</span>());</span><br><span class="line">       </span><br><span class="line">       <span class="comment">//2.双指针</span></span><br><span class="line">       <span class="keyword">int</span> i=<span class="number">0</span>,j=<span class="number">0</span>;</span><br><span class="line">       <span class="built_in">set</span>&lt;<span class="keyword">int</span>&gt; s;</span><br><span class="line"></span><br><span class="line">       <span class="keyword">while</span>((i&lt;nums1.<span class="built_in">size</span>())&amp;&amp;(j&lt;nums2.<span class="built_in">size</span>()))</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">if</span>(nums1[i]&lt;nums2[j])</span><br><span class="line">               i++;</span><br><span class="line">           <span class="keyword">else</span> <span class="keyword">if</span> (nums2[j]&lt;nums1[i])</span><br><span class="line">               j++;</span><br><span class="line">           <span class="keyword">else</span></span><br><span class="line">           &#123;</span><br><span class="line">               s.insert(nums1[i]);</span><br><span class="line">               i++;</span><br><span class="line">               j++;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       </span><br><span class="line">       <span class="keyword">int</span> k=<span class="number">0</span>;</span><br><span class="line">       <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res(s.<span class="built_in">size</span>());</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">auto</span> num:s)</span><br><span class="line">       &#123;</span><br><span class="line">           res[k]=num;</span><br><span class="line">           k++;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> res;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>2、哈希表 时间复杂度O(m+n)，空间复杂度O(m+n)</p><p>（1）将第一个数组的值插入哈希表中</p><p>（2）遍历第二个数组，当第二个数组中的元素存在哈希表的键中，则将其值插入set表中</p><p>（3）将set表中不重复的值插入结果中</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; intersection(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums1, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums2) &#123;</span><br><span class="line">       <span class="comment">//哈希表</span></span><br><span class="line">       <span class="built_in">unordered_map</span>&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; map1;</span><br><span class="line">       <span class="built_in">set</span>&lt;<span class="keyword">int</span>&gt; map2;</span><br><span class="line">       <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res;</span><br><span class="line">       </span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;nums1.<span class="built_in">size</span>();i++)</span><br><span class="line">           map1[nums1[i]]++;</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;nums2.<span class="built_in">size</span>();i++)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">if</span>(map1.<span class="built_in">find</span>(nums2[i])!=map1.<span class="built_in">end</span>())</span><br><span class="line">               map2.insert(nums2[i]);</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">auto</span> c:map2)</span><br><span class="line">           res.push_back(c);</span><br><span class="line">       </span><br><span class="line">       <span class="keyword">return</span> res;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：134-加油站"><a href="#题目：134-加油站" class="headerlink" title="题目：134. 加油站"></a>题目：134. 加油站</h2><p>描述：在一条环路上有 N 个加油站，其中第 i 个加油站有汽油 gas[i] 升。</p><p>你有一辆油箱容量无限的的汽车，从第 i 个加油站开往第 i+1 个加油站需要消耗汽油 cost[i] 升。你从其中的一个加油站出发，开始时油箱为空。</p><p>如果你可以绕环路行驶一周，则返回出发时加油站的编号，否则返回 -1。</p><p>说明: </p><p>如果题目有解，该答案即为唯一答案。<br>输入数组均为非空数组，且长度相同。<br>输入数组中的元素均为非负数。<br>示例 1:</p><p>输入:<br>gas  = [1,2,3,4,5]<br>cost = [3,4,5,1,2]</p><p>输出: 3</p><p>解释:<br>从 3 号加油站(索引为 3 处)出发，可获得 4 升汽油。此时油箱有 = 0 + 4 = 4 升汽油<br>开往 4 号加油站，此时油箱有 4 - 1 + 5 = 8 升汽油<br>开往 0 号加油站，此时油箱有 8 - 2 + 1 = 7 升汽油<br>开往 1 号加油站，此时油箱有 7 - 3 + 2 = 6 升汽油<br>开往 2 号加油站，此时油箱有 6 - 4 + 3 = 5 升汽油<br>开往 3 号加油站，你需要消耗 5 升汽油，正好足够你返回到 3 号加油站。<br>因此，3 可为起始索引。</p><p>思路：思路来源于例子，遍历每个位置，判断从该位置出发路上加减的油量是否能够回到该点</p><p>双指针控制当前遍历的位置，和具体判断</p><p>（1）指针i指向遍历每个位置</p><p>（2）指针j指向当前判断</p><p>将当前油量设置为j指向的位置（即i），当当前的油量-去下一个加油站的耗费&gt;=0，说明可以到达下一个加油站，则继续走：</p><p>当走到下一个加油站时，根据当前加油站加的油和路上耗费的油，得到当前新的油量，指针j指向下一个位置（注意：由于j一开始赋值为i，当j&gt;数组大小，即需要到数组开头，此时的j应该为（j+1)%n，保证能到数组开头）</p><p>当遍历之后j的值和i相同，则说明回到了原位置，返回当前位置i</p><p>当所有i都遍历过，没有符合要求的则返回-1</p><p>优化：当当前遍历的i最远能够到j的位置，则i后面的位置都到不了j+1,即无法回到i，则将i的值赋值为j；</p><p>当j&lt;i，说明j在i前面，到不了i，直接返回-1</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">canCompleteCircuit</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; gas, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; cost)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//对所有元素检查遍历一圈能否回到该点</span></span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;gas.<span class="built_in">size</span>();i++)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">int</span> j=i;</span><br><span class="line">           <span class="keyword">int</span> remain=gas[i];</span><br><span class="line">           <span class="keyword">while</span>(remain-cost[j]&gt;=<span class="number">0</span>)<span class="comment">//能继续往下跑</span></span><br><span class="line">           &#123;</span><br><span class="line">               <span class="comment">//到新的地方的油</span></span><br><span class="line">               remain=remain-cost[j]+gas[(j+<span class="number">1</span>)%gas.<span class="built_in">size</span>()];</span><br><span class="line">               j=(j+<span class="number">1</span>)%gas.<span class="built_in">size</span>();</span><br><span class="line">               <span class="keyword">if</span>(j==i)</span><br><span class="line">                   <span class="keyword">return</span> i;</span><br><span class="line">           &#125;</span><br><span class="line">         <span class="comment">//优化</span></span><br><span class="line">           <span class="keyword">if</span>(j&lt;i)</span><br><span class="line">               <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">         <span class="comment">//优化</span></span><br><span class="line">           i=j;</span><br><span class="line">       &#125;</span><br><span class="line">       </span><br><span class="line">       <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：941-有效的山脉数组"><a href="#题目：941-有效的山脉数组" class="headerlink" title="题目：941. 有效的山脉数组"></a>题目：941. 有效的山脉数组</h2><p>描述：给定一个整数数组 A，如果它是有效的山脉数组就返回 true，否则返回 false。</p><p>让我们回顾一下，如果 A 满足下述条件，那么它是一个山脉数组：</p><p>A.length &gt;= 3<br>在 0 &lt; i &lt; A.length - 1 条件下，存在 i 使得：<br>A[0] &lt; A[1] &lt; … A[i-1] &lt; A[i]<br>A[i] &gt; A[i+1] &gt; … &gt; A[A.length - 1]</p><p>示例 1：</p><p>输入：[2,1]<br>输出：false</p><p>示例 2：</p><p>输入：[3,5,5]<br>输出：false</p><p>示例 3：</p><p>输入：[0,3,2,1]<br>输出：true</p><p>思路：</p><p>1、线性扫描：先上山，再下山</p><p>（1）当数组大小小于3，直接排除</p><p>（2）从左边开始，找到最高的点</p><p>当该最高的点在首部和尾部，直接排除</p><p>（3）从最末尾向最高点走</p><p>当出现比前一个值大的情况，说明不满足条件，遍历到最后，则满足条件</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">validMountainArray</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; A)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">if</span>(A.<span class="built_in">size</span>()&lt;<span class="number">3</span>)</span><br><span class="line">           <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">       <span class="keyword">int</span> i=<span class="number">0</span>;</span><br><span class="line">       <span class="keyword">while</span>(i&lt;A.<span class="built_in">size</span>()<span class="number">-1</span>&amp;&amp;A[i]&lt;A[i+<span class="number">1</span>])</span><br><span class="line">           i++;</span><br><span class="line">       <span class="keyword">if</span>(i==A.<span class="built_in">size</span>()<span class="number">-1</span>||i==<span class="number">0</span>)</span><br><span class="line">           <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> j=A.<span class="built_in">size</span>()<span class="number">-1</span>;j&gt;i;j--)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">if</span>(A[j]&gt;=A[j<span class="number">-1</span>])</span><br><span class="line">               <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>2、双指针：从两头往中间靠</p><p>（1）当数组大小小于3，直接排除</p><p>（2）指针1：从左边开始，找到最高的点的位置</p><p>（3）指针2：从右边开始，找到最高的点的位置</p><p>（4）判断两次的位置是否相同，并且这两个位置是否在首末尾</p><p>相同不在首末尾则满足条件，反之不满足</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">validMountainArray</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; A)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">if</span>(A.<span class="built_in">size</span>()&lt;<span class="number">3</span>)</span><br><span class="line">           <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">       <span class="keyword">int</span> i=<span class="number">0</span>;</span><br><span class="line">       <span class="keyword">int</span> j=A.<span class="built_in">size</span>()<span class="number">-1</span>;</span><br><span class="line">       <span class="keyword">while</span>(i&lt;A.<span class="built_in">size</span>()<span class="number">-1</span>&amp;&amp;A[i]&lt;A[i+<span class="number">1</span>])</span><br><span class="line">           i++;</span><br><span class="line">       <span class="keyword">while</span>(j&gt;<span class="number">0</span>&amp;&amp;A[j]&lt;A[j<span class="number">-1</span>])</span><br><span class="line">           j--;</span><br><span class="line">       <span class="keyword">if</span>(i==j&amp;&amp;i!=<span class="number">0</span>&amp;&amp;j!=A.<span class="built_in">size</span>()<span class="number">-1</span>)</span><br><span class="line">           <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">       <span class="keyword">else</span></span><br><span class="line">           <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">       </span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：138-复制带随机指针的链表"><a href="#题目：138-复制带随机指针的链表" class="headerlink" title="题目：138. 复制带随机指针的链表"></a>题目：138. 复制带随机指针的链表</h2><p>描述：给定一个链表，每个节点包含一个额外增加的随机指针，该指针可以指向链表中的任何节点或空节点。</p><p>要求返回这个链表的 深拷贝。 </p><p>我们用一个由 n 个节点组成的链表来表示输入/输出中的链表。每个节点用一个 [val, random_index] 表示：</p><p>val：一个表示 Node.val 的整数。<br>random_index：随机指针指向的节点索引（范围从 0 到 n-1）；如果不指向任何节点，则为  null 。</p><p>示例 1：</p><p>输入：head = [[7,null],[13,0],[11,4],[10,2],[1,0]]<br>输出：[[7,null],[13,0],[11,4],[10,2],[1,0]]</p><p>思路：</p><p>题目的要求是对原始链表中所有节点的值和关系的复制，但是由于链表的节点中有next指针和random指针，关系复杂，直接原地复制困难，可以借助哈希表存储每个节点的值，后续再遍历一遍链表增加节点的关系</p><p>（1）建立哈希表，将节点的值复制到新节点中，形成原节点-&gt;新节点（只包含原节点的值）的映射</p><p>（2）重新遍历一遍原始链表，增加节点间的关系</p><p>map中key对应的即为新的节点</p><p>将map中key对应的新的节点的next指针指向map中原节点的下一个值</p><p>将map中key对应的新的节点的random指针指向map中原节点的random值</p><p>最后返回map中头节点对应的新的节点即为新的复制的链表</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Node* <span class="title">copyRandomList</span><span class="params">(Node* head)</span> </span>&#123;</span><br><span class="line">      <span class="comment">//哈希表:存储每个节点</span></span><br><span class="line">      <span class="built_in">unordered_map</span>&lt;Node*, Node*&gt; <span class="built_in">map</span>;</span><br><span class="line">      </span><br><span class="line">      <span class="comment">//复制值</span></span><br><span class="line">      Node* cur=head;</span><br><span class="line">      <span class="keyword">while</span>(cur!=<span class="literal">NULL</span>)</span><br><span class="line">      &#123;</span><br><span class="line">          Node* tmp=<span class="keyword">new</span> Node(cur-&gt;val);</span><br><span class="line">          <span class="built_in">map</span>[cur]=tmp;</span><br><span class="line">          cur=cur-&gt;next;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">//复制next和random的关系</span></span><br><span class="line">      cur=head;</span><br><span class="line">      <span class="keyword">while</span>(cur!=<span class="literal">NULL</span>)</span><br><span class="line">      &#123;</span><br><span class="line">          Node* newNode=<span class="built_in">map</span>[cur];</span><br><span class="line">          newNode-&gt;next=<span class="built_in">map</span>[cur-&gt;next];</span><br><span class="line">          newNode-&gt;<span class="built_in">random</span>=<span class="built_in">map</span>[cur-&gt;<span class="built_in">random</span>];</span><br><span class="line">          cur=cur-&gt;next;</span><br><span class="line">      &#125;</span><br><span class="line">      </span><br><span class="line">      Node* res=<span class="built_in">map</span>[head];</span><br><span class="line">      </span><br><span class="line">      <span class="keyword">return</span> res;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：56-合并区间"><a href="#题目：56-合并区间" class="headerlink" title="题目：56. 合并区间"></a>题目：56. 合并区间</h2><p>描述：给出一个区间的集合，请合并所有重叠的区间。</p><p> 示例 1:</p><p>输入: intervals = [[1,3],[2,6],[8,10],[15,18]]<br>输出: [[1,6],[8,10],[15,18]]<br>解释: 区间 [1,3] 和 [2,6] 重叠, 将它们合并为 [1,6].</p><p>思路：</p><p>（1）将数组按照左边区间排序</p><p>（2）将首个区间压入结果中，遍历剩余的区间</p><p>每次从结果中取出最后一个区间进行比较：</p><p>1°当当前遍历的区间的左边大于最后一个区间的右边，则不存在重复，直接压入结果</p><p>2°反之，则说明存在重复，将最后一个区间的右边更新为更大的右边（不更新左边区间的原因是：区间已经按照从小到大排序了，最后一个区间的左边肯定小于等于当前遍历的区间的左边）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; merge(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; intervals) &#123;</span><br><span class="line">       <span class="keyword">if</span>(intervals.<span class="built_in">size</span>()==<span class="number">0</span>)</span><br><span class="line">           <span class="keyword">return</span> &#123;&#125;;</span><br><span class="line">       <span class="comment">//按照左边起点排序</span></span><br><span class="line">       sort(intervals.<span class="built_in">begin</span>(), intervals.<span class="built_in">end</span>());</span><br><span class="line">       <span class="comment">//区间比较</span></span><br><span class="line">       <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; res;</span><br><span class="line">       res.push_back(intervals[<span class="number">0</span>]);</span><br><span class="line">       </span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;intervals.<span class="built_in">size</span>();i++)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">int</span> rr=res.back()[<span class="number">1</span>];</span><br><span class="line">           <span class="keyword">int</span> il=intervals[i][<span class="number">0</span>];</span><br><span class="line">           <span class="keyword">int</span> ir=intervals[i][<span class="number">1</span>];</span><br><span class="line">           <span class="keyword">if</span>(il&gt;rr)<span class="comment">//要插入的区间比已有的大，说明不需要合并</span></span><br><span class="line">               res.push_back(intervals[i]);</span><br><span class="line">           <span class="keyword">else</span></span><br><span class="line">           &#123;</span><br><span class="line">               res.back()[<span class="number">1</span>]=<span class="built_in">max</span>(rr,ir);</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> res;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：57-插入区间"><a href="#题目：57-插入区间" class="headerlink" title="题目：57. 插入区间"></a>题目：57. 插入区间</h2><p>描述：给出一个无重叠的 ，按照区间起始端点排序的区间列表。</p><p>在列表中插入一个新的区间，你需要确保列表中的区间仍然有序且不重叠（如果有必要的话，可以合并区间）。</p><p>示例 1：</p><p>输入：intervals = [[1,3],[6,9]], newInterval = [2,5]<br>输出：[[1,5],[6,9]]</p><p>思路：</p><p>1、按照合并区间的思路  时间复杂度O(nlogn)（排序logn，线性查找n），空间复杂度O(nlogn)（排序logn）</p><p>将新区间插入列表中，对合并后的区间排序，再进行合并</p><p>2、遍历原区间，遇到有合并的再进行合并 时间复杂度O(n)，空间复杂度O(1)</p><p>题目目的就是为了插入一个新的区间，则需要判断该区间是否插入，插入后是否需要合并：</p><p>遍历原区间：</p><p>（1）当需插入的区间在当前遍历区间的右边（即插入区间的左边&gt;当前区间的右边）</p><p>则结果中插入当前遍历的区间</p><p>（2）当需插入的区间在当前遍历区间的左边（即插入区间的右边&gt;当前区间的左边）</p><p>则结果中插入当前遍历的区间，并且判断该新区间是否插入过，没有插入，则插入该新区间，将插入新区间的标记设置为已插入-&gt;防止多次插入新的区间</p><p>（3）其余情况就是，当需插入的区间和当前遍历区间存在交集</p><p>则选择两者中小的左区间和大的右区间，作为新的需要插入的区间</p><p>（4)当所有区间遍历结束，需要插的区间还未插入，则说明该区间在所有区间的右侧，则直接将新区间插入</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; insert(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; intervals, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; newInterval) &#123;</span><br><span class="line">       <span class="comment">//以此判断新区间是否能够插入</span></span><br><span class="line">       <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; res;</span><br><span class="line">       <span class="keyword">bool</span> flag=<span class="literal">false</span>;<span class="comment">//判断新区间是否插入</span></span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;intervals.<span class="built_in">size</span>();i++)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">if</span>(newInterval[<span class="number">0</span>]&gt;intervals[i][<span class="number">1</span>])</span><br><span class="line">               res.push_back(intervals[i]);</span><br><span class="line">           <span class="keyword">else</span> <span class="keyword">if</span>(newInterval[<span class="number">1</span>]&lt;intervals[i][<span class="number">0</span>])</span><br><span class="line">           &#123;</span><br><span class="line">               <span class="keyword">if</span>(flag==<span class="literal">false</span>)<span class="comment">//防止多次插入新的区间</span></span><br><span class="line">               &#123;</span><br><span class="line">                   res.push_back(newInterval);</span><br><span class="line">                   flag=!flag;</span><br><span class="line">               &#125;</span><br><span class="line">               res.push_back(intervals[i]);</span><br><span class="line">           &#125;</span><br><span class="line">           <span class="keyword">else</span>&#123;</span><br><span class="line">               newInterval[<span class="number">0</span>]=<span class="built_in">min</span>(intervals[i][<span class="number">0</span>],newInterval[<span class="number">0</span>]);</span><br><span class="line">               newInterval[<span class="number">1</span>]=<span class="built_in">max</span>(intervals[i][<span class="number">1</span>],newInterval[<span class="number">1</span>]);</span><br><span class="line">           &#125;</span><br><span class="line">           </span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">if</span>(flag==<span class="literal">false</span>)</span><br><span class="line">           res.push_back(newInterval);</span><br><span class="line">       <span class="keyword">return</span> res;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：454-四数相加-II"><a href="#题目：454-四数相加-II" class="headerlink" title="题目：454. 四数相加 II"></a>题目：454. 四数相加 II</h2><p>描述：给定四个包含整数的数组列表 A , B , C , D ,计算有多少个元组 (i, j, k, l) ，使得 A[i] + B[j] + C[k] + D[l] = 0。</p><p>为了使问题简单化，所有的 A, B, C, D 具有相同的长度 N，且 0 ≤ N ≤ 500 。所有整数的范围在 -228 到 228 - 1 之间，最终结果不会超过 231 - 1 。</p><p>例如:</p><p>输入:<br>A = [ 1, 2]<br>B = [-2,-1]<br>C = [-1, 2]<br>D = [ 0, 2]</p><p>输出:<br>2</p><p>解释:<br>两个元组如下:</p><ol><li>(0, 0, 0, 1) -&gt; A[0] + B[0] + C[0] + D[1] = 1 + (-2) + (-1) + 2 = 0</li><li>(1, 1, 0, 0) -&gt; A[1] + B[1] + C[0] + D[0] = 2 + (-1) + (-1) + 0 = 0</li></ol><p>思路：</p><p>1、暴力法</p><p>遍历四个数组，找出所有四元组的组合，找到值为0的</p><p>但是这样时间复杂度很高</p><p>2、哈希表：将数组两两组合的和及次数映射存储到哈希表</p><p>（1）将前两个数组中元素两两组合的和出现的次数存储到哈希表中</p><p>（2）遍历后两个数组中的元素</p><p>在哈希表中找到值为-c-d(后两个数组中元素和的负数)，即负数+之前两两之和=0，出现的次数即为组合的个数</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fourSumCount</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; A, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; B, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; C, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; D)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//哈希表，记录前两个组合的和</span></span><br><span class="line">        <span class="keyword">int</span> count=<span class="number">0</span>;</span><br><span class="line">        <span class="built_in">unordered_map</span>&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; <span class="built_in">map</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">auto</span> a:A)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">auto</span> b:B)</span><br><span class="line">                <span class="built_in">map</span>[a+b]++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">auto</span> c:C)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">auto</span> d:D)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span>(<span class="built_in">map</span>.<span class="built_in">find</span>(-c-d)!=<span class="built_in">map</span>.<span class="built_in">end</span>())</span><br><span class="line">                    count+=<span class="built_in">map</span>[-c-d];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> count;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>时间复杂度比暴力法减少一半，空间上只额外增加了哈希表</p><h2 id="题目：328-奇偶链表"><a href="#题目：328-奇偶链表" class="headerlink" title="题目：328. 奇偶链表"></a>题目：328. 奇偶链表</h2><p>描述：给定一个单链表，把所有的奇数节点和偶数节点分别排在一起。请注意，这里的奇数节点和偶数节点指的是节点编号的奇偶性，而不是节点的值的奇偶性。</p><p>请尝试使用原地算法完成。你的算法的空间复杂度应为 O(1)，时间复杂度应为 O(nodes)，nodes 为节点总数。</p><p>示例 1:</p><p>输入: 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;NULL<br>输出: 1-&gt;3-&gt;5-&gt;2-&gt;4-&gt;NULL</p><p>思路：当没有空间复杂度限制的时候，可以顺序读取链表，当奇数节点赋值给一个新的链表，偶数节点赋值给另一个新的链表，再将这两个链表连接即可。</p><p>当有空间复杂度限制的时候（即原地完成），则思路相同，只要增加两个指针即可，将这两个指针分别指向不同的节点，再将两个指针的链表连接即可。</p><p>（1）双指针，设置一个指针指向奇数节点，另一个指针指向偶数节点</p><p>（2）链表至少要有两个节点，否则直接返回空（链表为空），或者返回原链表（链表只有一个节点）</p><p>将奇数指针指向链表头结点，偶数指针指向链表头节点的下一个节点</p><p>（3）当奇数指针和偶数指针的下一个next不为空（说明没有到链表末尾），则继续构造新链表</p><p>将偶数指针的next赋值给奇数指针的next，这样奇数指针就成功连接到下一个奇数点</p><p>移动奇数指针，此时将奇数指针的next赋值给偶数指针的next，这样偶数指针就成功连接到下一个奇数点</p><p>再移动偶数指针，不断循环就构造了两个链表</p><p>（4）最后，将两个链表连接起来即可</p><p>这样，只遍历了一遍链表，时间复杂度为 O(nodes)，nodes 为节点总数,同时没有用其他空间，只用了两个指针，空间复杂度为O(1)</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">ListNode* <span class="title">oddEvenList</span><span class="params">(ListNode* head)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//双指针</span></span><br><span class="line">       <span class="keyword">if</span>(head==<span class="literal">NULL</span>)</span><br><span class="line">           <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">       <span class="keyword">if</span>(head-&gt;next==<span class="literal">NULL</span>)</span><br><span class="line">           <span class="keyword">return</span> head;</span><br><span class="line">       <span class="comment">//奇数节点</span></span><br><span class="line">       ListNode* odd=head;</span><br><span class="line">       <span class="comment">//偶数节点</span></span><br><span class="line">       ListNode* even=head-&gt;next;</span><br><span class="line">       </span><br><span class="line">       ListNode* res=odd;</span><br><span class="line">       ListNode* tmp=even;</span><br><span class="line">       <span class="keyword">while</span>(odd-&gt;next!=<span class="literal">NULL</span>&amp;&amp;even-&gt;next!=<span class="literal">NULL</span>)</span><br><span class="line">       &#123;</span><br><span class="line">           odd-&gt;next=even-&gt;next;</span><br><span class="line">           odd=odd-&gt;next;</span><br><span class="line">           even-&gt;next=odd-&gt;next;</span><br><span class="line">           even=even-&gt;next;</span><br><span class="line">       &#125;</span><br><span class="line">       odd-&gt;next=tmp;</span><br><span class="line">       <span class="keyword">return</span> res;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：371-两整数之和"><a href="#题目：371-两整数之和" class="headerlink" title="题目：371. 两整数之和"></a>题目：371. 两整数之和</h2><p>描述：不使用运算符 + 和 - ，计算两整数 a 、b 之和。</p><p>示例 1:</p><p>输入: a = 1, b = 2<br>输出: 3</p><p>思路：当不能使用运算符时，考虑使用位运算</p><p>（1）不考虑进位</p><p>位运算中只有两个数字：0+0=0，0+1=1,1+0=1,1+1=0，运算规则同异或，即不考虑进位的情况下，数字的值等于两者异或运算</p><p>（2）进位</p><p>当出现进位时，即时1+1，需要进位1，即判断两者相与，再左移一位即为进位</p><p>最后将不考虑进位的值+需要进位的数字即为最终的结果：</p><p>则每次将a&amp;b(进行相与运算)，得到进位，在通过a^b(异或操作)，得到新的值，再将新的值和进位异或，这样就进行了一次加法运算，一直循环当最终的进位为0，说明已经相加结束</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">getSum</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">while</span>(b)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">auto</span> carry=((<span class="keyword">unsigned</span> <span class="keyword">int</span>)(a&amp;b))&lt;&lt;<span class="number">1</span>;</span><br><span class="line">           a=a^b;</span><br><span class="line">           b=carry;</span><br><span class="line"></span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> a;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：1356-根据数字二进制下-1-的数目排序"><a href="#题目：1356-根据数字二进制下-1-的数目排序" class="headerlink" title="题目：1356. 根据数字二进制下 1 的数目排序"></a>题目：1356. 根据数字二进制下 1 的数目排序</h2><p>描述：给你一个整数数组 arr 。请你将数组中的元素按照其二进制表示中数字 1 的数目升序排序。</p><p>如果存在多个数字二进制中 1 的数目相同，则必须将它们按照数值大小升序排列。</p><p>请你返回排序后的数组。</p><p>示例 1：</p><p>输入：arr = [0,1,2,3,4,5,6,7,8]<br>输出：[0,1,2,4,8,3,5,6,7]<br>解释：[0] 是唯一一个有 0 个 1 的数。<br>[1,2,4,8] 都有 1 个 1 。<br>[3,5,6] 有 2 个 1 。<br>[7] 有 3 个 1 。<br>按照 1 的个数排序得到的结果数组为 [0,1,2,4,8,3,5,6,7]</p><p>思路：按照每个数字转换为二进制后1的数目进行排序，即需要自定义排序方法</p><p>（1）定义自定义排序方法</p><p>1°计算数组中每个元素转换为二进制之后1的个数</p><p>方法1：将元素每次对2取余的结果相加，即得到转换为二进制后1的个数</p><p>方法2：将元素对自生-1相与，即每次去掉最后一个1</p><p>2°自定义排序方法</p><p>将元素转换为二进制中1的个数：当个数相同，按照原始数据大小返回，反之按照转换为二进制中1的个数大小返回</p><p>（2）将数组按照自定义的排序进行排列</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//得到二进制的位置</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">convert_to_binary</span><span class="params">(<span class="keyword">int</span> a)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> res=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(a!=<span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        res+=a%<span class="number">2</span>;</span><br><span class="line">        a=a/<span class="number">2</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">custom_compare</span><span class="params">(<span class="keyword">int</span> a,<span class="keyword">int</span> b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> binary_a=convert_to_binary(a);</span><br><span class="line">    <span class="keyword">int</span> binary_b=convert_to_binary(b);</span><br><span class="line">    <span class="keyword">if</span>(binary_a==binary_b)</span><br><span class="line">        <span class="keyword">return</span> a&lt;b;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> binary_a&lt;binary_b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; sortByBits(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; arr) &#123;</span><br><span class="line">        sort(arr.<span class="built_in">begin</span>(),arr.<span class="built_in">end</span>(),custom_compare);</span><br><span class="line">        <span class="keyword">return</span> arr;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="题目：973-最接近原点的-K-个点"><a href="#题目：973-最接近原点的-K-个点" class="headerlink" title="题目：973. 最接近原点的 K 个点"></a>题目：973. 最接近原点的 K 个点</h2><p>描述：我们有一个由平面上的点组成的列表 points。需要从中找出 K 个距离原点 (0, 0) 最近的点。</p><p>（这里，平面上两点之间的距离是欧几里德距离。）</p><p>你可以按任何顺序返回答案。除了点坐标的顺序之外，答案确保是唯一的。</p><p> 示例 1：</p><p>输入：points = [[1,3],[-2,2]], K = 1<br>输出：[[-2,2]]<br>解释：<br>(1, 3) 和原点之间的距离为 sqrt(10)，<br>(-2, 2) 和原点之间的距离为 sqrt(8)，<br>由于 sqrt(8) &lt; sqrt(10)，(-2, 2) 离原点更近。<br>我们只需要距离原点最近的 K = 1 个点，所以答案就是 [[-2,2]]。</p><p>思路：</p><p>1、自定义排序——超出时间限制</p><p>（1）将原数组中的元素按照欧几里得计算公式的和排序</p><p>（2）将前k个的值赋值给结果</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; kClosest(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; points, <span class="keyword">int</span> K) &#123;</span><br><span class="line">       <span class="comment">//自定义排序 超出时间限制</span></span><br><span class="line">       <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; res;</span><br><span class="line">       sort(points.<span class="built_in">begin</span>(),points.<span class="built_in">end</span>(),custom_compare);</span><br><span class="line"> </span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;K;i++)</span><br><span class="line">           res.push_back(&#123;points[i][<span class="number">0</span>],points[i][<span class="number">1</span>]&#125;);</span><br><span class="line"></span><br><span class="line">       <span class="keyword">return</span> res;</span><br><span class="line"></span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>2、优先级队列——维护K个大小的大顶堆</p><p>使用优先级队列（会按照第一个元素自动排序，堆顶为最大值），维护K个大小的大顶堆，对剩余的元素计算的和与堆顶比较，小的则插入，反之不插入，最终得到k大小的优先级队列即为结果</p><p>（1）使用优先级队列，将数组内前k个元素的欧几里得距离和元素索引存储在大顶推中</p><p>（2）对剩余的元素计算其欧几里得距离，与堆顶比较</p><p>比堆顶的计算和小，则将堆顶元素弹出，插入新的元素，最终得到k大小的堆</p><p>（3）遍历堆，取出每个元素的第二个位置保存的索引，对应在原数组中的值即为结果</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; kClosest(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; points, <span class="keyword">int</span> K) &#123;</span><br><span class="line">       <span class="comment">//优先级队列</span></span><br><span class="line">       priority_queue&lt;pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt;&gt; p;</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;K;i++)</span><br><span class="line">           p.push(&#123;points[i][<span class="number">0</span>]*points[i][<span class="number">0</span>]+points[i][<span class="number">1</span>]*points[i][<span class="number">1</span>],i&#125;);</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=K;i&lt;points.<span class="built_in">size</span>();i++)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">int</span> tmp=points[i][<span class="number">0</span>]*points[i][<span class="number">0</span>]+points[i][<span class="number">1</span>]*points[i][<span class="number">1</span>];</span><br><span class="line">           <span class="keyword">if</span>(tmp&lt;p.top().first)</span><br><span class="line">           &#123;</span><br><span class="line">               p.pop();</span><br><span class="line">               p.push(&#123;tmp,i&#125;);</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       </span><br><span class="line">       <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; res;</span><br><span class="line">       <span class="keyword">while</span>(!p.empty())</span><br><span class="line">       &#123;</span><br><span class="line">           res.push_back(points[p.top().second]);</span><br><span class="line">           p.pop();</span><br><span class="line">           </span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> res;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：31-下一个排列"><a href="#题目：31-下一个排列" class="headerlink" title="题目：31. 下一个排列"></a>题目：31. 下一个排列</h2><p>描述：实现获取下一个排列的函数，算法需要将给定数字序列重新排列成字典序中下一个更大的排列。</p><p>如果不存在下一个更大的排列，则将数字重新排列成最小的排列（即升序排列）。</p><p>必须原地修改，只允许使用额外常数空间。</p><p>以下是一些例子，输入位于左侧列，其相应输出位于右侧列。<br>1,2,3 → 1,3,2<br>3,2,1 → 1,2,3<br>1,1,5 → 1,5,1</p><p>思路：假设需要找到123465的下一个排列，实际的下一个排列为123546，可以看出是需要找到后面一个最小的大数和前面的小数交换：即从后往前找到破坏降序的第一个数字即为前面的小数，为了保证上升幅度尽可能小，则需要从后往前找比前面小数大的数字，交换后，再将后面数字倒序即可</p><p>（1）从后往前找到破坏降序的第一个数字即为前面的小数</p><p>（2）从后往前找比前面小数大的数字，即为后面尽可能小的大数</p><p>（3）交换小数和大数</p><p>（4）将交换位置后倒序</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">nextPermutation</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//找小数</span></span><br><span class="line">       <span class="keyword">int</span> pos=nums.<span class="built_in">size</span>()<span class="number">-2</span>;</span><br><span class="line">       <span class="keyword">while</span>(pos&gt;=<span class="number">0</span>&amp;&amp;nums[pos]&gt;=nums[pos+<span class="number">1</span>])</span><br><span class="line">           pos--;</span><br><span class="line">       <span class="keyword">if</span>(pos&gt;=<span class="number">0</span>)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">int</span> i=nums.<span class="built_in">size</span>()<span class="number">-1</span>;</span><br><span class="line">           <span class="keyword">for</span>(;i&gt;<span class="number">0</span>;i--)</span><br><span class="line">           &#123;</span><br><span class="line">               <span class="keyword">if</span>(nums[i]&gt;nums[pos])</span><br><span class="line">                   <span class="keyword">break</span>;</span><br><span class="line">           &#125;</span><br><span class="line">           swap(nums[i], nums[pos]);</span><br><span class="line">       &#125;</span><br><span class="line">       reverse(nums.<span class="built_in">begin</span>()+pos+<span class="number">1</span>, nums.<span class="built_in">end</span>());</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：190-颠倒二进制位"><a href="#题目：190-颠倒二进制位" class="headerlink" title="题目：190. 颠倒二进制位"></a>题目：190. 颠倒二进制位</h2><p>描述：颠倒给定的 32 位无符号整数的二进制位。</p><p>示例 1：</p><p>输入: 00000010100101000001111010011100<br>输出: 00111001011110000010100101000000<br>解释: 输入的二进制串 00000010100101000001111010011100 表示无符号整数 43261596，<br>     因此返回 964176192，其二进制表示形式为 00111001011110000010100101000000。</p><p>思路：按位翻转，题目要求是将最后一位向前翻转到最前面，可以看出规律是第0位-&gt;第31位，第1位-&gt;第30位，第2位-&gt;第29位，…，因此，第i位需要左移到第31-i位</p><p>（1）当原数不为0的时候，继续翻转</p><p>（2）使用(n&amp;1)取出数字的最后一位</p><p>（3）将最后一位左移到第31-i的位置</p><p>（4）将原来的数字去掉最后一位，继续循环</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">uint32_t</span> reverseBits(<span class="keyword">uint32_t</span> n) &#123;</span><br><span class="line">       <span class="keyword">uint32_t</span> res=<span class="number">0</span>;</span><br><span class="line">       <span class="keyword">int</span> pos=<span class="number">31</span>;</span><br><span class="line">       <span class="keyword">while</span>(n!=<span class="number">0</span>)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="comment">//取出最后一位放到最前面</span></span><br><span class="line">           res+=(n&amp;<span class="number">1</span>)&lt;&lt;pos;</span><br><span class="line">           <span class="comment">//去掉最后一位</span></span><br><span class="line">           n=n&gt;&gt;<span class="number">1</span>;</span><br><span class="line">           pos--;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> res;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：922-按奇偶排序数组-II"><a href="#题目：922-按奇偶排序数组-II" class="headerlink" title="题目：922. 按奇偶排序数组 II"></a>题目：922. 按奇偶排序数组 II</h2><p>描述：给定一个非负整数数组 A， A 中一半整数是奇数，一半整数是偶数。</p><p>对数组进行排序，以便当 A[i] 为奇数时，i 也是奇数；当 A[i] 为偶数时， i 也是偶数。</p><p>你可以返回任何满足上述条件的数组作为答案。</p><p>示例：</p><p>输入：[4,2,5,7]<br>输出：[4,5,2,7]<br>解释：[4,7,2,5]，[2,5,4,7]，[2,7,4,5] 也会被接受。</p><p>思路：</p><p>1、两个数组：时间复杂度O(n)，空间复杂度O(1)，需要额外使用两个数组，非原地算法</p><p>（1）一遍遍历原数组</p><p>（2）当读到偶数时插入偶数数组</p><p>（3）读到奇数时插入奇数数组</p><p>（4）最后按照索引合并两个数组</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; sortArrayByParityII(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; A) &#123;</span><br><span class="line">       <span class="comment">//偶数放入偶数序列，奇数放入奇数序列，再合并</span></span><br><span class="line">       <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; odd;<span class="comment">//奇数序列</span></span><br><span class="line">       <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; even;<span class="comment">//偶数序列</span></span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;A.<span class="built_in">size</span>();i++)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">if</span>(A[i]%<span class="number">2</span>==<span class="number">0</span>)</span><br><span class="line">               even.push_back(A[i]);</span><br><span class="line">           <span class="keyword">else</span></span><br><span class="line">               odd.push_back(A[i]);</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res(A.<span class="built_in">size</span>());</span><br><span class="line">       <span class="keyword">int</span> j=<span class="number">0</span>;</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;res.<span class="built_in">size</span>();i=i+<span class="number">2</span>)</span><br><span class="line">       &#123;</span><br><span class="line">           res[i]=even[j];</span><br><span class="line">           res[i+<span class="number">1</span>]=odd[j];</span><br><span class="line">           j++;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> res;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>2、双指针：时间复杂度O(n)，空间复杂度O(1)，需要额外使用两个指针并将结果赋值给最终数组，非原地算法</p><p>（1）当读到偶数时结果数组的偶数指针移动（每次移动两位）</p><p>（2）读到奇数时结果数组的奇数指针移动（每次移动两位）</p><p>（3）直到遍历完毕，输出结果即可</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; sortArrayByParityII(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; A) &#123;</span><br><span class="line">       <span class="comment">//双指针</span></span><br><span class="line">       <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res(A.<span class="built_in">size</span>());</span><br><span class="line">       <span class="keyword">int</span> index1=<span class="number">0</span>;</span><br><span class="line">       <span class="keyword">int</span> index2=<span class="number">1</span>;</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;A.<span class="built_in">size</span>();i++)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">if</span>(A[i]%<span class="number">2</span>==<span class="number">0</span>)</span><br><span class="line">           &#123;</span><br><span class="line">               res[index1]=A[i];</span><br><span class="line">               index1=index1+<span class="number">2</span>;</span><br><span class="line">           &#125;</span><br><span class="line">           <span class="keyword">else</span></span><br><span class="line">           &#123;</span><br><span class="line">               res[index2]=A[i];</span><br><span class="line">               index2=index2+<span class="number">2</span>;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> res;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>3、原地算法：时间复杂度O(n)，空间复杂度O(1)，因为是在原数组上进行的修改</p><p>（1）定义两个指针分别指向偶数位置和奇数位置</p><p>（2）当当前读到的是奇数，则不断移动奇数指针（每次移动两位）</p><p>（3）当在奇数位置读到偶数的时候，将该值和偶数位置的奇数交换即可</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; sortArrayByParityII(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; A) &#123;</span><br><span class="line">       <span class="comment">//原地修改</span></span><br><span class="line">       <span class="keyword">int</span> j=<span class="number">1</span>;</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;A.<span class="built_in">size</span>();i=i+<span class="number">2</span>)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">if</span>(A[i]%<span class="number">2</span>==<span class="number">1</span>)</span><br><span class="line">           &#123;</span><br><span class="line">               <span class="keyword">while</span>(A[j]%<span class="number">2</span>==<span class="number">1</span>)</span><br><span class="line">                   j=j+<span class="number">2</span>;</span><br><span class="line">               swap(A[i], A[j]);</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> A;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：剑指-Offer-42-连续子数组的最大和"><a href="#题目：剑指-Offer-42-连续子数组的最大和" class="headerlink" title="题目：剑指 Offer 42. 连续子数组的最大和"></a>题目：剑指 Offer 42. 连续子数组的最大和</h2><p>描述：输入一个整型数组，数组中的一个或连续多个整数组成一个子数组。求所有子数组的和的最大值。</p><p>要求时间复杂度为O(n)。 </p><p>示例1:</p><p>输入: nums = [-2,1,-3,4,-1,2,1,-5,4]<br>输出: 6<br>解释: 连续子数组 [4,-1,2,1] 的和最大，为 6。</p><p>思路：</p><p>1、贪心法</p><p>（1）遍历数组中每个元素</p><p>（2）当当前和加上当前遍历的元素的值比当前元素的值大，则将和更新为加上该值后的和</p><p>（3）反之，则从当前数字开始重新累加。（原理为：当累加的和加上本身之后都没有本身大，说明之前存在负数，则之前的序列肯定不存在最大的连续和，则从当前遍历的位置开始重新累加）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxSubArray</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//暴力求解</span></span><br><span class="line">       <span class="keyword">int</span> tmp=nums[<span class="number">0</span>];</span><br><span class="line">       <span class="keyword">int</span> res=tmp;</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;nums.<span class="built_in">size</span>();i++)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">if</span>(tmp+nums[i]&gt;nums[i])</span><br><span class="line">           &#123;</span><br><span class="line">               res=<span class="built_in">max</span>(res,tmp+nums[i]);</span><br><span class="line">               tmp=tmp+nums[i];</span><br><span class="line">           &#125;</span><br><span class="line">           <span class="keyword">else</span></span><br><span class="line">           &#123;</span><br><span class="line">               res=<span class="built_in">max</span>(res,nums[i]);</span><br><span class="line">               tmp=nums[i];</span><br><span class="line">           &#125;</span><br><span class="line">       &#125; </span><br><span class="line">       <span class="keyword">return</span> res;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>2、动态规划</p><p>由贪心法得到的思路，定义状态dp[ i ]是以nums[ i ]结尾的连续子数组的最大和，由此确定的转移方程（即求dp[ i ]的递推式）为当dp[ i-1 ]为正数，即前面数字加起来的最大连续和是正的贡献，则当前位置的累加和将前面的和加上本身，当dp[ i-1 ]为负数，即前面数字加起来的最大连续和是负的贡献，则当前位置的累加和直接舍弃前面的值，从当前位置重新开始累加。</p><p>（1）定义状态</p><p>定义状态dp[ i ]是以nums[ i ]结尾的连续子数组的最大和</p><p>（2）确定转移方程</p><p>dp[ i ]=dp[ i-1 ]+nums[ i ] （dp[ i-1 ]&gt;0,即为正贡献）</p><p>dp[ i ]=nums[ i ] （dp[ i-1 ]&lt;=0,即为负贡献，舍弃前面的值，从当前位置开始重新累加）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxSubArray</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//动态规划 dp[i]表示以nums[i]结尾的连续子数组的最大和</span></span><br><span class="line">       <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; dp(nums.<span class="built_in">size</span>());</span><br><span class="line">       dp[<span class="number">0</span>]=nums[<span class="number">0</span>];</span><br><span class="line">       <span class="keyword">int</span> res=dp[<span class="number">0</span>];</span><br><span class="line">       </span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;nums.<span class="built_in">size</span>();i++)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">if</span>(dp[i<span class="number">-1</span>]&gt;<span class="number">0</span>)</span><br><span class="line">               dp[i]=dp[i<span class="number">-1</span>]+nums[i];</span><br><span class="line">           <span class="keyword">else</span></span><br><span class="line">               dp[i]=nums[i];</span><br><span class="line">           res=<span class="built_in">max</span>(res,dp[i]);</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> res;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：剑指-Offer-22-链表中倒数第k个节点"><a href="#题目：剑指-Offer-22-链表中倒数第k个节点" class="headerlink" title="题目：剑指 Offer 22. 链表中倒数第k个节点"></a>题目：剑指 Offer 22. 链表中倒数第k个节点</h2><p>描述：输入一个链表，输出该链表中倒数第k个节点。为了符合大多数人的习惯，本题从1开始计数，即链表的尾节点是倒数第1个节点。例如，一个链表有6个节点，从头节点开始，它们的值依次是1、2、3、4、5、6。这个链表的倒数第3个节点是值为4的节点。</p><p> 示例：</p><p>给定一个链表: 1-&gt;2-&gt;3-&gt;4-&gt;5, 和 k = 2.</p><p>返回链表 4-&gt;5.</p><p>思路：双指针（快慢指针）</p><p>（1）定义两个指针：快指针和慢指针</p><p>（2）先让快指针走k步，此时快指针和慢指针距离k步</p><p>（3）同时移动快慢指针，当快指针到末尾的时候，此时慢指针在倒数第k个节点的位置</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">ListNode* <span class="title">getKthFromEnd</span><span class="params">(ListNode* head, <span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//链表中倒数的节点</span></span><br><span class="line">       <span class="comment">//快慢指针，快指针先走k步</span></span><br><span class="line">       ListNode* slow=head;</span><br><span class="line">       ListNode* fast=head;</span><br><span class="line">       <span class="comment">//快指针先走k步</span></span><br><span class="line">       <span class="keyword">while</span>(fast!=<span class="literal">NULL</span>&amp;&amp;k&gt;<span class="number">0</span>)</span><br><span class="line">       &#123;</span><br><span class="line">           fast=fast-&gt;next;</span><br><span class="line">           k--;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">while</span>(fast!=<span class="literal">NULL</span>)</span><br><span class="line">       &#123;</span><br><span class="line">           slow=slow-&gt;next;</span><br><span class="line">           fast=fast-&gt;next;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> slow;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;题目：349-两个数组的交集&quot;&gt;&lt;a href=&quot;#题目：349-两个数组的交集&quot; class=&quot;headerlink&quot; title=&quot;题目：349. 两个数组的交集&quot;&gt;&lt;/a&gt;题目：349. 两个数组的交集&lt;/h2&gt;&lt;p&gt;描述：示例 1：&lt;/p&gt;&lt;p&gt;输入：nums1 = [1,2,2,1], nums2 = [2,2]&lt;br&gt;输出：[2]&lt;/p&gt;&lt;p&gt;思路：&lt;/p&gt;&lt;p&gt;1、排序+双指针 时间复杂度O(mlogm+nlogn)（主要是排序的时间+查找的时间），空间复杂度O(mlogm+nlogn)（排序的空间）&lt;/p&gt;&lt;p&gt;（1）将数组从小到大排序&lt;/p&gt;&lt;p&gt;（2）用双指针法分别指向两个数组，当两者不相等的时候分别移动各自指针，当相同的时候将值插入set表中去重&lt;/p&gt;
    
    </summary>
    
    
      <category term="算法" scheme="https://www.xiapf.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="https://www.xiapf.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Leecode做题记录（五）</title>
    <link href="https://www.xiapf.com/blogs/LC5/"/>
    <id>https://www.xiapf.com/blogs/LC5/</id>
    <published>2020-10-30T04:39:35.000Z</published>
    <updated>2020-10-30T04:40:22.652Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目：844-比较含退格的字符串"><a href="#题目：844-比较含退格的字符串" class="headerlink" title="题目：844. 比较含退格的字符串"></a>题目：844. 比较含退格的字符串</h2><p>描述：给定 S 和 T 两个字符串，当它们分别被输入到空白的文本编辑器后，判断二者是否相等，并返回结果。 # 代表退格字符。</p><p>注意：如果对空文本输入退格字符，文本继续为空。</p><p>示例 1：</p><p>输入：S = “ab#c”, T = “ad#c”<br>输出：true<br>解释：S 和 T 都会变成 “ac”。</p><a id="more"></a><p>思路：</p><p>利用栈的性质，符合要求则出栈</p><p>（1）定义两个栈分别读入S,T的字符</p><p>（2）当遇到非退格字符‘#’的时候则入栈，反之当遇到该退格字符，并且栈不为空的时候进行退栈操作，将字符退格</p><p>（3）最后将退格结束的两个字符串进行比较即可</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">backspaceCompare</span><span class="params">(<span class="built_in">string</span> S, <span class="built_in">string</span> T)</span> </span>&#123;</span><br><span class="line">      <span class="comment">//使用栈</span></span><br><span class="line">      <span class="built_in">stack</span>&lt;<span class="keyword">char</span>&gt; p;</span><br><span class="line">      <span class="built_in">stack</span>&lt;<span class="keyword">char</span>&gt; q;</span><br><span class="line">      p=storeStr(S);</span><br><span class="line">      q=storeStr(T);</span><br><span class="line">      <span class="keyword">if</span>(p.<span class="built_in">size</span>()!=q.<span class="built_in">size</span>())</span><br><span class="line">          <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">      <span class="keyword">int</span> len=p.<span class="built_in">size</span>();</span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;len;i++)</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="keyword">if</span>(p.top()!=q.top())</span><br><span class="line">              <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">          <span class="keyword">else</span></span><br><span class="line">          &#123;</span><br><span class="line">              p.pop();</span><br><span class="line">              q.pop();</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="built_in">stack</span>&lt;<span class="keyword">char</span>&gt; storeStr(<span class="built_in">string</span> S)</span><br><span class="line">  &#123;</span><br><span class="line">      <span class="built_in">stack</span>&lt;<span class="keyword">char</span>&gt; p;</span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;S.<span class="built_in">size</span>();i++)</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="keyword">if</span>(S[i]!=<span class="string">'#'</span>)</span><br><span class="line">              p.push(S[i]);</span><br><span class="line">          <span class="keyword">if</span>(S[i]==<span class="string">'#'</span>&amp;&amp;!p.empty())</span><br><span class="line">              p.pop();</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> p;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：143-重排链表"><a href="#题目：143-重排链表" class="headerlink" title="题目：143. 重排链表"></a>题目：143. 重排链表</h2><p>描述：给定一个单链表 L：L0→L1→…→Ln-1→Ln ，<br>将其重新排列后变为： L0→Ln→L1→Ln-1→L2→Ln-2→…</p><p>你不能只是单纯的改变节点内部的值，而是需要实际的进行节点交换。</p><p>示例 1:</p><p>给定链表 1-&gt;2-&gt;3-&gt;4, 重新排列为 1-&gt;4-&gt;2-&gt;3.</p><p>思路：总体就是将链表后部分节点有间隔的插入前面节点中</p><p>（1）利用快慢指针找到中点，将链表分为两部分</p><p>（2）将后半部分链表节点利用栈进行逆序操作</p><p>（3）将逆序好的后半部分节点依次插入前半部分的缝隙中</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">reorderList</span><span class="params">(ListNode* head)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//重排链表：给定一个单链表 L：L0→L1→…→Ln-1→Ln ，将其重新排列后变为： L0→Ln→L1→Ln-1→L2→Ln-2→…</span></span><br><span class="line">        <span class="comment">//快慢指针找到中点，将链表分为两部分，把后面的链表插入前面的链表中</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span>(head==<span class="literal">NULL</span>)</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">                </span><br><span class="line">        <span class="comment">//0.定义快慢指针，找到中点</span></span><br><span class="line">        ListNode* slow=head;</span><br><span class="line">        ListNode* fast=head;</span><br><span class="line">        <span class="keyword">while</span>(fast!=<span class="literal">NULL</span>&amp;&amp;fast-&gt;next!=<span class="literal">NULL</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            slow=slow-&gt;next;<span class="comment">//慢指针走一步</span></span><br><span class="line">            fast=fast-&gt;next-&gt;next;<span class="comment">//快指针走两步</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//1.slow将链表分为了两部分</span></span><br><span class="line">        ListNode* behind=slow-&gt;next;</span><br><span class="line">        slow-&gt;next=<span class="literal">NULL</span>;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//2.将后半部分链表逆序:利用栈进行存储节点，再依次弹出节点</span></span><br><span class="line">        behind=reverseb(behind);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//3.将后半部分插入前半部分</span></span><br><span class="line">        ListNode* cur=head;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(behind!=<span class="literal">NULL</span>&amp;&amp;cur!=<span class="literal">NULL</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">//3.0每次指向下一个需要插入位置的节点</span></span><br><span class="line">            ListNode* nextNode=cur-&gt;next;</span><br><span class="line">            </span><br><span class="line">            <span class="comment">//3.1每次指向后面需要向前面链表插的节点</span></span><br><span class="line">            ListNode* behindNode=behind;</span><br><span class="line">            behind=behind-&gt;next;<span class="comment">//因为后面会改原来behind指向的节点，所以要提前把behind向后移动</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">//3.2将后面节点插入前面</span></span><br><span class="line">            behindNode-&gt;next=cur-&gt;next;<span class="comment">//此时behid-&gt;next发生了变化，指向了前半部分链表，所以前门把节点存储起来使用</span></span><br><span class="line">            cur-&gt;next=behindNode;</span><br><span class="line">            </span><br><span class="line">            cur=nextNode;     </span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function">ListNode* <span class="title">reverseb</span><span class="params">(ListNode* l)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="comment">//将链表存入栈中</span></span><br><span class="line">        <span class="built_in">stack</span>&lt;ListNode*&gt; s;</span><br><span class="line">        ListNode* r=l;</span><br><span class="line">        <span class="keyword">while</span> (r!=<span class="literal">NULL</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            s.push(r);</span><br><span class="line">            r=r-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//利用栈的先进后出逆序输出节点</span></span><br><span class="line">        ListNode* res=<span class="keyword">new</span> ListNode(<span class="number">-1</span>);</span><br><span class="line">        ListNode* result=res;</span><br><span class="line">        <span class="keyword">while</span>(!s.empty())</span><br><span class="line">        &#123;</span><br><span class="line">            res-&gt;next=s.top();</span><br><span class="line">            s.pop();</span><br><span class="line">            res=res-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        res-&gt;next=<span class="literal">NULL</span>;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> result-&gt;next;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：344-反转字符串"><a href="#题目：344-反转字符串" class="headerlink" title="题目：344. 反转字符串"></a>题目：344. 反转字符串</h2><p>描述：编写一个函数，其作用是将输入的字符串反转过来。输入字符串以字符数组 char[] 的形式给出。</p><p>不要给另外的数组分配额外的空间，你必须原地修改输入数组、使用 O(1) 的额外空间解决这一问题。</p><p>你可以假设数组中的所有字符都是 ASCII 码表中的可打印字符。。</p><p>思路：双指针</p><p>（1）定义两个指针，分别指向字符串的首部和尾部</p><p>（2）交换两个指针内的值</p><p>（3）首部指针后移，尾部指针前移，直至重叠</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">reverseString</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">char</span>&gt;&amp; s)</span> </span>&#123;</span><br><span class="line">      <span class="comment">//定义两个指针，首末调换</span></span><br><span class="line">      <span class="keyword">int</span> left=<span class="number">0</span>;</span><br><span class="line">      <span class="keyword">int</span> right=s.<span class="built_in">size</span>()<span class="number">-1</span>;</span><br><span class="line">      <span class="keyword">char</span> tmp;</span><br><span class="line">      <span class="keyword">while</span>(left&lt;right)</span><br><span class="line">      &#123;</span><br><span class="line">          tmp=s[left];</span><br><span class="line">          s[left]=s[right];</span><br><span class="line">          s[right]=tmp;</span><br><span class="line">          </span><br><span class="line">          left++;</span><br><span class="line">          right--;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：925-长按键入"><a href="#题目：925-长按键入" class="headerlink" title="题目：925. 长按键入"></a>题目：925. 长按键入</h2><p>描述：你的朋友正在使用键盘输入他的名字 name。偶尔，在键入字符 c 时，按键可能会被长按，而字符可能被输入 1 次或多次。</p><p>你将会检查键盘输入的字符 typed。如果它对应的可能是你的朋友的名字（其中一些字符可能被长按），那么就返回 True。</p><p>示例 1：</p><p>输入：name = “alex”, typed = “aaleex”<br>输出：true<br>解释：’alex’ 中的 ‘a’ 和 ‘e’ 被长按。</p><p>思路：因为需要按照位置查看重复的字符，所以不能使用哈希表。</p><p>采用双指针，当字符相同则移动指针，对指针指向内容不同，需要判断是出现的长按字符还是没有出现。</p><p>（1）定义两个指针，分别指向name和typed</p><p>（2）当两个指针指向相同，则移动两个指针，比较下一个字符</p><p>（3）当指针指向不同，则判断是由什么引起的</p><p>当是第一个typed的字符就不同，则两个字符完全不同，返回false</p><p>当不是第一个字符，则判断typed当前的字符和前一个是否一样：</p><p>一样说明是出现了长按字符，则移动typed的指针，直到读完所有的长按字符，再比较此时两个指针是否相同，相同则继续移动指针反之说明不是出现长按字符，两个字符串不同，返回false</p><p>（4）判断指针是否读完</p><p>判断name指针是否读完所有name字符，没有全部读完，出现有多余字符，返回false</p><p>判断typed指针是否读完所有typed字符，没有全部读完，则判断typed当前的字符和前一个是否一样，继续移动其指针，当与前一个不一样，说明和name字符串不匹配，返回false</p><p>当全部判断完，说明符合条件，返回true</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//位置需要对应,定义两个指针</span></span><br><span class="line">    <span class="keyword">int</span> i=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> j=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(j&lt;typed.<span class="built_in">size</span>()&amp;&amp;i&lt;name.<span class="built_in">size</span>())</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(typed[j]==name[i])</span><br><span class="line">        &#123;</span><br><span class="line">            i++;</span><br><span class="line">            j++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(j==<span class="number">0</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            <span class="keyword">while</span>(typed[j]==typed[j<span class="number">-1</span>])</span><br><span class="line">                j++;</span><br><span class="line">            <span class="keyword">if</span>(typed[j]==name[i])</span><br><span class="line">            &#123;</span><br><span class="line">                i++;</span><br><span class="line">                j++;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(i&lt;name.<span class="built_in">size</span>())</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">while</span>(j&lt;typed.<span class="built_in">size</span>())</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(typed[j]==typed[j<span class="number">-1</span>])</span><br><span class="line">            j++;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br></pre></td></tr></table></figure><h2 id="题目：763-划分字母区间"><a href="#题目：763-划分字母区间" class="headerlink" title="题目：763. 划分字母区间"></a>题目：763. 划分字母区间</h2><p>描述：字符串 S 由小写字母组成。我们要把这个字符串划分为尽可能多的片段，同一个字母只会出现在其中的一个片段。返回一个表示每个字符串片段的长度的列表。</p><p> 示例 1：</p><p>输入：S = “ababcbacadefegdehijhklij”<br>输出：[9,7,8]<br>解释：<br>划分结果为 “ababcbaca”, “defegde”, “hijhklij”。<br>每个字母最多出现在一个片段中。<br>像 “ababcbacadefegde”, “hijhklij” 的划分是错误的，因为划分的片段数较少。</p><p>思路：本题可以看出每个字符最后出现的位置会影响区间的分布，因此记录每个字符最终出现的位置，以此作为依据，不断更新区间的右边界。</p><p>（1）将字符转换为数字，记录其在数组中最后出现的位置</p><p>（2）重新遍历原字符串，定义两个指针，一个指针a指向上一个区间的右区间，另一个指针b指向当前字符最后出现的位置</p><p>得出当前遍历的字符最后出现的位置，并更新当前指针b，即当前区间需要向右延展</p><p>当当前的区间位置和当前字符最后出现位置一致，说明将最多的字符包含在区间内了，此时更新区间的右区间，即指针b，并记录区间长度</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; num(<span class="number">26</span>,<span class="number">-1</span>);</span><br><span class="line">    <span class="comment">//保存每个字符出现的最终位置</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;S.<span class="built_in">size</span>();i++)</span><br><span class="line">        num[S[i]-<span class="string">'a'</span>]=i;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//更新每次的区间</span></span><br><span class="line">    <span class="keyword">int</span> preIndex=<span class="number">-1</span>;<span class="comment">//上一个的右区间</span></span><br><span class="line">    <span class="keyword">int</span> maxIndex=<span class="number">0</span>;<span class="comment">//当前字符出现的最后位置</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;S.<span class="built_in">size</span>();i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> index=num[S[i]-<span class="string">'a'</span>];</span><br><span class="line">        <span class="keyword">if</span>(index&gt;maxIndex)</span><br><span class="line">            maxIndex=index;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(i==maxIndex)</span><br><span class="line">        &#123;</span><br><span class="line">            res.push_back(maxIndex-preIndex);</span><br><span class="line">            preIndex=maxIndex;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> res;</span><br></pre></td></tr></table></figure><h2 id="题目：557-反转字符串中的单词-III"><a href="#题目：557-反转字符串中的单词-III" class="headerlink" title="题目：557. 反转字符串中的单词 III"></a>题目：557. 反转字符串中的单词 III</h2><p>描述：给定一个字符串，你需要反转字符串中每个单词的字符顺序，同时仍保留空格和单词的初始顺序。</p><p> 示例：</p><p>输入：”Let’s take LeetCode contest”<br>输出：”s’teL ekat edoCteeL tsetnoc”</p><p>思路：</p><p>1、拆分字符串一一翻转</p><p>（1）遍历当前字符串，当遇到空格的时候，取出当前遇到的单词</p><p>（2）将取出的单词，使用双指针进行翻转</p><p>（3）将翻转后的结果存入res字符串中，并找下一个单词</p><p>（4）最后，对字符串中的最后一个单词单独处理翻转</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="built_in">string</span> <span class="title">reverseStr</span><span class="params">(<span class="built_in">string</span> c)</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">      <span class="keyword">int</span> left=<span class="number">0</span>;</span><br><span class="line">      <span class="keyword">int</span> right=c.<span class="built_in">size</span>()<span class="number">-1</span>;</span><br><span class="line">      <span class="keyword">char</span> tmp;</span><br><span class="line">      <span class="keyword">while</span>(left&lt;right)</span><br><span class="line">      &#123;</span><br><span class="line">          tmp=c[left];</span><br><span class="line">          c[left]=c[right];</span><br><span class="line">          c[right]=tmp;</span><br><span class="line">          </span><br><span class="line">          left++;</span><br><span class="line">          right--;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> c;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="built_in">string</span> <span class="title">reverseWords</span><span class="params">(<span class="built_in">string</span> s)</span> </span>&#123;</span><br><span class="line">      <span class="comment">//在遇到第一个空格之间的字符进行单词翻转</span></span><br><span class="line">      <span class="built_in">string</span> tmp;</span><br><span class="line">      <span class="built_in">string</span> res;</span><br><span class="line">      <span class="keyword">int</span> start=<span class="number">0</span>;</span><br><span class="line">      <span class="keyword">int</span> i=<span class="number">0</span>;</span><br><span class="line">      <span class="keyword">for</span>(;i&lt;s.<span class="built_in">size</span>();i++)</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="keyword">if</span>(s[i]==<span class="string">' '</span>)</span><br><span class="line">          &#123;</span><br><span class="line">              tmp=s.substr(start,i-start);</span><br><span class="line">              res=res+reverseStr(tmp)+<span class="string">" "</span>;</span><br><span class="line">              start=i+<span class="number">1</span>;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      </span><br><span class="line">      <span class="comment">//处理最后一个单词</span></span><br><span class="line">      tmp=s.substr(start,i-start);</span><br><span class="line">      res=res+reverseStr(tmp);</span><br><span class="line">      </span><br><span class="line">      <span class="keyword">return</span> res;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>2、原地翻转</p><p>（1）遍历当前字符串</p><p>（2）当出现空格或者到达字符串末尾的时候（为了能到达尾部，需要用i&lt;s.size()+1）</p><p>确定当前单词的首末位置，直接使用双指针进行原地翻转</p><p>（3）最后返回原始字符串（此时不需要额外的字符串，实现了原地翻转）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="built_in">string</span> <span class="title">reverseWords</span><span class="params">(<span class="built_in">string</span> s)</span> </span>&#123;</span><br><span class="line">      <span class="comment">//在遇到第一个空格之间的字符进行单词翻转</span></span><br><span class="line">      <span class="built_in">string</span> tmp;</span><br><span class="line">      <span class="built_in">string</span> res;</span><br><span class="line">      <span class="keyword">int</span> start=<span class="number">0</span>;</span><br><span class="line">      <span class="keyword">int</span> i=<span class="number">0</span>;</span><br><span class="line">      <span class="keyword">for</span>(;i&lt;s.<span class="built_in">size</span>()+<span class="number">1</span>;i++) <span class="comment">//+1是为了能到达尾部</span></span><br><span class="line">      &#123;</span><br><span class="line">          <span class="keyword">if</span>(s[i]==<span class="string">' '</span>||i==s.<span class="built_in">size</span>())</span><br><span class="line">          &#123;</span><br><span class="line">              <span class="keyword">int</span> <span class="built_in">end</span>=i<span class="number">-1</span>;</span><br><span class="line">              <span class="keyword">while</span>(start&lt;<span class="built_in">end</span>)</span><br><span class="line">              &#123;</span><br><span class="line">                  <span class="keyword">char</span> c=s[start];</span><br><span class="line">                  s[start]=s[<span class="built_in">end</span>];</span><br><span class="line">                  s[<span class="built_in">end</span>]=c;</span><br><span class="line">                  </span><br><span class="line">                  start++;</span><br><span class="line">                  <span class="built_in">end</span>--;</span><br><span class="line">              &#125;</span><br><span class="line">              start=i+<span class="number">1</span>;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> s;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：1365-有多少小于当前数字的数字"><a href="#题目：1365-有多少小于当前数字的数字" class="headerlink" title="题目：1365. 有多少小于当前数字的数字"></a>题目：1365. 有多少小于当前数字的数字</h2><p>描述：给你一个数组 nums，对于其中每个元素 nums[i]，请你统计数组中比它小的所有数字的数目。</p><p>换而言之，对于每个 nums[i] 你必须计算出有效的 j 的数量，其中 j 满足 j != i 且 nums[j] &lt; nums[i] 。</p><p>以数组形式返回答案。</p><p>示例 1：</p><p>输入：nums = [8,1,2,2,3]<br>输出：[4,0,1,1,3]<br>解释：<br>对于 nums[0]=8 存在四个比它小的数字：（1，2，2 和 3）。<br>对于 nums[1]=1 不存在比它小的数字。<br>对于 nums[2]=2 存在一个比它小的数字：（1）。<br>对于 nums[3]=2 存在一个比它小的数字：（1）。<br>对于 nums[4]=3 存在三个比它小的数字：（1，2 和 2）。</p><p>提示：</p><p>2 &lt;= nums.length &lt;= 500<br>0 &lt;= nums[i] &lt;= 100</p><p>思路：</p><p>1、暴力法：时间复杂度O(n^2)，空间复杂度O(1) </p><p>遍历数组内所有元素，找到比其小的进行计数</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; smallerNumbersThanCurrent(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums) &#123;</span><br><span class="line">      <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res;</span><br><span class="line">      <span class="keyword">int</span> num;</span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;nums.<span class="built_in">size</span>();i++)</span><br><span class="line">      &#123;</span><br><span class="line">          num=searchNum(i,nums);</span><br><span class="line">          res.push_back(num);</span><br><span class="line">      &#125;</span><br><span class="line">      </span><br><span class="line">      </span><br><span class="line">      <span class="keyword">return</span> res;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">int</span> <span class="title">searchNum</span><span class="params">(<span class="keyword">int</span> m,<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; nums)</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">      <span class="keyword">int</span> count=<span class="number">0</span>;</span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;nums.<span class="built_in">size</span>();i++)</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="keyword">if</span>(i!=m&amp;&amp;nums[i]&lt;nums[m])</span><br><span class="line">              count++;</span><br><span class="line">      &#125;</span><br><span class="line">      </span><br><span class="line">      <span class="keyword">return</span> count;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>2、计数法/前缀和：时间复杂度O(n+k)，空间复杂度O(k) (k为前缀和数组大小)</p><p>由于数组内出现的元素大小为0 ~ 100，则可以用一个数组countNum按照索引记录数组内元素出现的次数，即countNum [  i ]表示数组内值=i的数字出现的次数，将其值作为在countNum中的索引</p><p>（1）使用countNum数组（长度为101）记录原数组内每个元素的值在countNum内作为索引，记录出现的次数</p><p>（2）每个元素按照大小做好了计数，则countNum [ i ]=前面0 ~ i-1个相加（即前缀和）</p><p>（3）当原数组内元素值不为0，则将countNum中的前缀和加入结果中，元素为0则直接将0加入结果 </p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; smallerNumbersThanCurrent(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums) &#123;</span><br><span class="line">     <span class="comment">//计数法 nums的大小为0-100</span></span><br><span class="line">     <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; countNum(<span class="number">101</span>);</span><br><span class="line">     <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;nums.<span class="built_in">size</span>();i++)</span><br><span class="line">         countNum[nums[i]]++;</span><br><span class="line">     </span><br><span class="line">     <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;countNum.<span class="built_in">size</span>();i++)<span class="comment">//从i=1开始</span></span><br><span class="line">         countNum[i]+=countNum[i<span class="number">-1</span>];</span><br><span class="line">     </span><br><span class="line">     <span class="keyword">int</span> tmp;</span><br><span class="line">     <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res;</span><br><span class="line">     <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;nums.<span class="built_in">size</span>();i++)</span><br><span class="line">     &#123;</span><br><span class="line">         tmp=nums[i]==<span class="number">0</span>?<span class="number">0</span>:countNum[nums[i]<span class="number">-1</span>];<span class="comment">//值为0，则没有数组比它小</span></span><br><span class="line">         res.push_back(tmp);</span><br><span class="line">     &#125;</span><br><span class="line">     </span><br><span class="line">     <span class="keyword">return</span> res;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：89-格雷编码"><a href="#题目：89-格雷编码" class="headerlink" title="题目：89. 格雷编码"></a>题目：89. 格雷编码</h2><p>描述：格雷编码是一个二进制数字系统，在该系统中，两个连续的数值仅有一个位数的差异。</p><p>给定一个代表编码总位数的非负整数 n，打印其格雷编码序列。即使有多个不同答案，你也只需要返回其中一种。</p><p>格雷编码序列必须以 0 开头。</p><p>示例 1:</p><p>输入: 2<br>输出: [0,1,3,2]<br>解释:<br>00 - 0<br>01 - 1<br>11 - 3<br>10 - 2</p><p>对于给定的 n，其格雷编码序列并不唯一。<br>例如，[0,2,3,1] 也是一个有效的格雷编码序列。</p><p>00 - 0<br>10 - 2<br>11 - 3<br>01 - 1</p><p>思路：</p><p>（1）按照给定的n的长度，得出格雷编码</p><p>定义0，1字符串，每次分为从前到后，从后到前两个方向将字符串加入目标字符数组中，下一次在上一次的基础上增加0，1，直到达到n的长度</p><p>（2）读出每一位的格雷编码</p><p>从最后一位开始读入，利用2的阶乘将二进制转换为十进制</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; grayCode(<span class="keyword">int</span> n) &#123;</span><br><span class="line">      <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res;</span><br><span class="line">      <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; t1&#123;<span class="string">"0"</span>,<span class="string">"1"</span>&#125;;</span><br><span class="line">      <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; t;</span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">2</span>;i&lt;=n;i++)</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;t1.<span class="built_in">size</span>();j++)</span><br><span class="line">              t.push_back(<span class="string">"0"</span>+t1[j]);</span><br><span class="line">          <span class="keyword">for</span>(<span class="keyword">int</span> j=t1.<span class="built_in">size</span>()<span class="number">-1</span>;j&gt;=<span class="number">0</span>;j--)</span><br><span class="line">              t.push_back(<span class="string">"1"</span>+t1[j]);</span><br><span class="line">          t1=t;</span><br><span class="line">          t.<span class="built_in">clear</span>();</span><br><span class="line">      &#125;</span><br><span class="line">      </span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;t1.<span class="built_in">size</span>();i++)</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="built_in">string</span> tmp=t1[i];</span><br><span class="line">          <span class="keyword">int</span> val=<span class="number">0</span>;</span><br><span class="line">          <span class="keyword">int</span> cnt=<span class="number">1</span>;</span><br><span class="line">          <span class="keyword">for</span>(<span class="keyword">int</span> j=tmp.<span class="built_in">size</span>()<span class="number">-1</span>;j&gt;=<span class="number">0</span>;j--)</span><br><span class="line">          &#123;</span><br><span class="line">              val+=cnt*(tmp[j]-<span class="string">'0'</span>);</span><br><span class="line">              cnt*=<span class="number">2</span>;</span><br><span class="line">          &#125;</span><br><span class="line">          res.push_back(val);</span><br><span class="line">      &#125;</span><br><span class="line">      </span><br><span class="line">      <span class="keyword">return</span> res;</span><br><span class="line">      </span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：144-二叉树的前序遍历"><a href="#题目：144-二叉树的前序遍历" class="headerlink" title="题目：144. 二叉树的前序遍历"></a>题目：144. 二叉树的前序遍历</h2><p>描述：给定一个二叉树，返回它的 前序 遍历。</p><p> 示例:</p><p>输入: [1,null,2,3]<br>   1<br>    <br>     2<br>    /<br>   3 </p><p>输出: [1,2,3]</p><p>思路：迭代法：利用栈遍历整棵树，中序遍历是先遍历根，再遍历左子树和右子树</p><p>（1）将根节点入栈，当栈不为空或者还有节点的时候循环进行遍历</p><p>（2）当节点不为空的时候，将当前节点加入结果中（即遍历了根），由于栈是先进后出，因此再将右子树入栈，继续一直访问左子树</p><p>（3）直到节点为空，则所有左子树都遍历完毕，此时取栈顶元素，继续遍历右子树</p><p>（4）栈内元素出栈，继续循环</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; preorderTraversal(TreeNode* root) &#123;</span><br><span class="line">       <span class="comment">//二叉树的前序遍历：利用栈，先进后出，存储暂时不用的右子树</span></span><br><span class="line">       <span class="comment">//先访问节点，压入右子树，再遍历左子树</span></span><br><span class="line">       </span><br><span class="line">       <span class="comment">//0.考虑树为空的情况</span></span><br><span class="line">       <span class="keyword">if</span>(root==<span class="literal">NULL</span>)</span><br><span class="line">           <span class="keyword">return</span> &#123;&#125;;</span><br><span class="line">       <span class="comment">//1.利用栈进行遍历</span></span><br><span class="line">       <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; result;</span><br><span class="line">       <span class="built_in">stack</span>&lt;TreeNode*&gt; s;</span><br><span class="line">       TreeNode* p=root;</span><br><span class="line">       <span class="keyword">while</span> (p!=<span class="literal">NULL</span>||s.<span class="built_in">size</span>()) &#123;</span><br><span class="line">           <span class="keyword">while</span> (p!=<span class="literal">NULL</span>) &#123;</span><br><span class="line">               <span class="comment">//2.访问当前节点</span></span><br><span class="line">               result.push_back(p-&gt;val);</span><br><span class="line">               <span class="comment">//3.右子树入栈</span></span><br><span class="line">               s.push(p-&gt;right);</span><br><span class="line">               <span class="comment">//4.访问左子树</span></span><br><span class="line">               p=p-&gt;left;</span><br><span class="line">           &#125;</span><br><span class="line">           <span class="comment">//5.取栈顶的右子树的根节点，即为当前最小的右子树</span></span><br><span class="line">           p=s.top();</span><br><span class="line">           s.pop();</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> result;</span><br><span class="line">       </span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：29-两数相除"><a href="#题目：29-两数相除" class="headerlink" title="题目：29. 两数相除"></a>题目：29. 两数相除</h2><p>描述：给定两个整数，被除数 dividend 和除数 divisor。将两数相除，要求不使用乘法、除法和 mod 运算符。</p><p>返回被除数 dividend 除以除数 divisor 得到的商。</p><p>整数除法的结果应当截去（truncate）其小数部分，例如：truncate(8.345) = 8 以及 truncate(-2.7335) = -2</p><p> 示例 1:</p><p>输入: dividend = 10, divisor = 3<br>输出: 3<br>解释: 10/3 = truncate(3.33333..) = truncate(3) = 3</p><p>提示：</p><p>被除数和除数均为 32 位有符号整数。<br>除数不为 0。<br>假设我们的环境只能存储 32 位有符号整数，其数值范围是 [−231,  231 − 1]。本题中，如果除法结果溢出，则返回 231 − 1。</p><p>思路：</p><p>思路来源于例子：当除数为10，被除数为3的时候，不能使用乘法和除法，则从（最小的值3+自身）开始试</p><p>1、计算div(10,3)</p><p>3+3=6&lt;10 此时的商为1+1=2，继续试</p><p>6+6=12&gt;10 此时的商为2+2=4，但是12比11大，则上一步应该是用10-6=5作为剩余的被除数，和3进行相除</p><p>所以总体计算变为商为2+div(5,3)</p><p>2、计算div(5,3)</p><p>3+3=6&gt;5 此时的商为1+1=2，但是6比5大，则总体计算变为商为1+div(5-3,3)，即1+div(2,3)</p><p>3、计算div(2,3)</p><p>因为2&lt;3，则对结果进行截断，直接返回0</p><p>总体思路：为了防止溢出，所有的数据全部使用long类型，在最后返回的结果判断是否溢出</p><p>（1）处理特殊情况</p><p>当除数为0，直接返回0；被除数为1，直接返回除数的值</p><p>（2）单独计算符号</p><p>根据除数和被除数的值将最后计算的符号取出</p><p>（3）将除数和被除数全部转换为正数计算</p><p>（4）调用递归计算商的函数，判断结果是否溢出</p><p>注：当不能使用long类型时，所有数据类型转换为int，数据全部转换为负数进行操作</p><p>在递归计算商时，条件改为：while((tb-a+tb)&gt;=0)，tb-a放在前面，防止tb+tb出现溢出</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">divide</span><span class="params">(<span class="keyword">int</span> dividend, <span class="keyword">int</span> divisor)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">if</span>(dividend==<span class="number">0</span>)</span><br><span class="line">          <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">      <span class="keyword">if</span>(divisor==<span class="number">1</span>)</span><br><span class="line">          <span class="keyword">return</span> dividend;</span><br><span class="line">      </span><br><span class="line">      <span class="keyword">long</span> a=dividend;</span><br><span class="line">      <span class="keyword">long</span> b=divisor;</span><br><span class="line">      <span class="keyword">int</span> sign=<span class="number">1</span>;</span><br><span class="line">      </span><br><span class="line">      <span class="comment">//设置结果的符号</span></span><br><span class="line">      <span class="keyword">if</span>((a&lt;<span class="number">0</span>&amp;&amp;b&gt;<span class="number">0</span>)||(a&gt;<span class="number">0</span>&amp;&amp;b&lt;<span class="number">0</span>))</span><br><span class="line">          sign=<span class="number">-1</span>;</span><br><span class="line">      </span><br><span class="line">      <span class="comment">//全部转换为正数进行运算</span></span><br><span class="line">      a=a&gt;<span class="number">0</span>?a:-a;</span><br><span class="line">      b=b&gt;<span class="number">0</span>?b:-b;</span><br><span class="line">      </span><br><span class="line">      <span class="comment">//递归求解值</span></span><br><span class="line">      <span class="keyword">long</span> res=div(a,b);</span><br><span class="line">      </span><br><span class="line">      <span class="keyword">if</span>(sign&gt;<span class="number">0</span>)</span><br><span class="line">          res=res&gt;INT_MAX?INT_MAX:sign*res;</span><br><span class="line">      <span class="keyword">else</span></span><br><span class="line">          res=res&lt;INT_MIN?INT_MIN:sign*res;</span><br><span class="line">      <span class="keyword">return</span> res;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">long</span> <span class="title">div</span><span class="params">(<span class="keyword">long</span> a,<span class="keyword">long</span> b)</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">      <span class="comment">//最后进行截断</span></span><br><span class="line">      <span class="keyword">if</span>(a&lt;b)</span><br><span class="line">          <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">      <span class="keyword">long</span> tb=b;</span><br><span class="line">      <span class="keyword">long</span> count=<span class="number">1</span>;</span><br><span class="line">      <span class="keyword">while</span>((tb+tb)&lt;=a)</span><br><span class="line">      &#123;</span><br><span class="line">          count=count+count;</span><br><span class="line">          tb=tb+tb;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> count+div(a-tb,b);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：1207-独一无二的出现次数"><a href="#题目：1207-独一无二的出现次数" class="headerlink" title="题目：1207. 独一无二的出现次数"></a>题目：1207. 独一无二的出现次数</h2><p>描述：给你一个整数数组 arr，请你帮忙统计数组中每个数的出现次数。</p><p>如果每个数的出现次数都是独一无二的，就返回 true；否则返回 false。</p><p>示例 1：</p><p>输入：arr = [1,2,2,1,1,3]<br>输出：true<br>解释：在该数组中，1 出现了 3 次，2 出现了 2 次，3 只出现了 1 次。没有两个数的出现次数相同。</p><p>思路：利用哈希表存储每个元素出现的次数，及利用set的去重性</p><p>（1）将数组中的元素出现的次数存储到哈希表中（每个元素对应一个出现的次数）</p><p>（2）将元素出现的次数存储到set（去重数组）中</p><p>（3）如果出现次数独一无二，则哈希表和set数组的大小一样，反之不一样</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">uniqueOccurrences</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; arr)</span> </span>&#123;</span><br><span class="line">      <span class="built_in">unordered_map</span>&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; <span class="built_in">map</span>;</span><br><span class="line">      <span class="built_in">set</span>&lt;<span class="keyword">int</span>&gt; s;</span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;arr.<span class="built_in">size</span>();i++)</span><br><span class="line">          <span class="built_in">map</span>[arr[i]]++;</span><br><span class="line">      </span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">auto</span> c:<span class="built_in">map</span>)</span><br><span class="line">          s.insert(c.second);</span><br><span class="line">      </span><br><span class="line">      <span class="keyword">if</span>(s.<span class="built_in">size</span>()!=<span class="built_in">map</span>.<span class="built_in">size</span>())</span><br><span class="line">          <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：91-解码方法"><a href="#题目：91-解码方法" class="headerlink" title="题目：91. 解码方法"></a>题目：91. 解码方法</h2><p>描述：一条包含字母 A-Z 的消息通过以下方式进行了编码：</p><p>‘A’ -&gt; 1<br>‘B’ -&gt; 2<br>…<br>‘Z’ -&gt; 26<br>给定一个只包含数字的非空字符串，请计算解码方法的总数。</p><p>题目数据保证答案肯定是一个 32 位的整数。</p><p>示例 1：</p><p>输入：”12”<br>输出：2<br>解释：它可以解码为 “AB”（1 2）或者 “L”（12）。</p><p>思路：递归法，因为是包含26位字母，则编码的数字不可能超过两位</p><p>思路来源于例子：假设输入的字符串为“226123”，则根据编码不超过两位的性质可得</p><p>“226123”的解码结果=情况1：（2对应的解码+剩余26123解码的结果）加上 情况2：（22对应的解码+剩余6123解码的结果）</p><p>即一个字符串的解码可以分为：开头的一个数字对应解码+剩余的 再加上开头的两个数字对应的解码+剩余的，但是当开头是两个数字的时候需要判断其是否小于26</p><p>注：由于递归法容易超时，这里加上记忆数组，将每个位置得到的解码个数记录下来，便于后续查找</p><p>总体思路：</p><p>（1）输入原始字符串和开始寻找解码的初始位置</p><p>（2）递归结束的条件</p><p>当解码的位置和字符串位置相等，说明有一种解码方式，返回1</p><p>当当前字符串的解码位置是0，不对应任何字母，返回0</p><p>（3）递归</p><p>1°在哈希表中得到当前寻找解码位置的解码个数，不为0，说明之前计算过，直接返回，反之则需要计算</p><p>2°递归得到去掉当前第一个数字的剩余数字的解码方式</p><p>3°设置去掉当前前两个数字的剩余数字的解码方式=0</p><p>当当前寻找解码的位置比源字符串少1，说明可以去掉前两个数字</p><p>得到去掉前两个数字的大小，如果小于等于26，则递归得到去掉当前前两个数字的剩余数字的解码方式，反之则去掉当前前两个数字的剩余数字的解码方式=0</p><p>（4）记忆数组</p><p>将当前寻找解码位置的解码个数存储到哈希表中</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">unordered_map</span>&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; <span class="built_in">map</span>;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">numDecodings</span><span class="params">(<span class="built_in">string</span> s)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//递归 带记忆的</span></span><br><span class="line">        <span class="keyword">int</span> res=dfs(s,<span class="number">0</span>);</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">dfs</span><span class="params">(<span class="built_in">string</span> s,<span class="keyword">int</span> start)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="comment">//递归结束</span></span><br><span class="line">        <span class="keyword">if</span>(start==s.length())</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span>(s[start]==<span class="string">'0'</span>)</span><br><span class="line">           <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//记忆法</span></span><br><span class="line">        <span class="keyword">int</span> m=<span class="built_in">map</span>[start];</span><br><span class="line">        <span class="keyword">if</span>(m!=<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span> m;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//字符串最多为两位</span></span><br><span class="line">        <span class="keyword">int</span> ans1=dfs(s, start+<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">int</span> ans2=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span>(start&lt;s.<span class="built_in">size</span>()<span class="number">-1</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">int</span> ten=s[start]-<span class="string">'0'</span>;</span><br><span class="line">            <span class="keyword">int</span> one=s[start+<span class="number">1</span>]-<span class="string">'0'</span>;</span><br><span class="line">            <span class="keyword">int</span> tmp=ten*<span class="number">10</span>+one;</span><br><span class="line">            <span class="keyword">if</span>(tmp&lt;=<span class="number">26</span>)</span><br><span class="line">                ans2=dfs(s, start+<span class="number">2</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">map</span>[start]=ans1+ans2;</span><br><span class="line">        <span class="keyword">return</span> ans1+ans2;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="题目：387-字符串中的第一个唯一字符"><a href="#题目：387-字符串中的第一个唯一字符" class="headerlink" title="题目：387. 字符串中的第一个唯一字符"></a>题目：387. 字符串中的第一个唯一字符</h2><p>描述：给定一个字符串，找到它的第一个不重复的字符，并返回它的索引。如果不存在，则返回 -1。</p><p>示例：</p><p>s = “leetcode”<br>返回 0</p><p>思路：题目中出现和次数相关的，用哈希表</p><p>（1）用哈希表存储每个字符出现的次数</p><p>（2）验证：遍历字符串中的字符，找到在哈希表中出现次数为1的即为需要找的字符的位置</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">firstUniqChar</span><span class="params">(<span class="built_in">string</span> s)</span> </span>&#123;</span><br><span class="line">       <span class="built_in">unordered_map</span>&lt;<span class="keyword">char</span>, <span class="keyword">int</span>&gt; <span class="built_in">map</span>;</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">auto</span> c:s)</span><br><span class="line">           <span class="built_in">map</span>[c]++; <span class="comment">//可以转换为数字存储 map[c-'a']++;</span></span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;s.<span class="built_in">size</span>();i++)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">char</span> c=s[i];</span><br><span class="line">           <span class="keyword">if</span>(<span class="built_in">map</span>[c]==<span class="number">1</span>)</span><br><span class="line">               <span class="keyword">return</span> i;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：129-求根到叶子节点数字之和"><a href="#题目：129-求根到叶子节点数字之和" class="headerlink" title="题目：129. 求根到叶子节点数字之和"></a>题目：129. 求根到叶子节点数字之和</h2><p>描述：给定一个二叉树，它的每个结点都存放一个 0-9 的数字，每条从根到叶子节点的路径都代表一个数字。</p><p>例如，从根到叶子节点路径 1-&gt;2-&gt;3 代表数字 123。</p><p>计算从根到叶子节点生成的所有数字之和。</p><p>说明: 叶子节点是指没有子节点的节点。</p><p>示例 1:</p><p>输入: [1,2,3]<br>    1<br>   / <br>  2   3<br>输出: 25<br>解释:<br>从根到叶子节点路径 1-&gt;2 代表数字 12.<br>从根到叶子节点路径 1-&gt;3 代表数字 13.<br>因此，数字总和 = 12 + 13 = 25.</p><p>思路：层次遍历，建立两个队列，一个用于存储每层遍历的节点，一个用于存储每层叠加的和，当遇到叶子节点则将当前累加和加入到结果中</p><p>（1）建立两个队列a、b，一个用于存储每层遍历的节点（将根节点入栈），一个用于存储每层叠加的和（初始和为0）</p><p>（2）当队列内还有节点的时候，说明树没有遍历结束，则循环继续</p><p>每次从a队首取当前元素，在b队首取之前累加和，并将累加和 * 10再加上当前节点的值</p><p>1°两个队列操作完毕后，将原元素出栈</p><p>2°当当前节点的左右节点均为空，说明已经到叶子节点，则将当前操作完后的累加和赋值给结果</p><p>3°当当前节点还有左子树，则将左子树入队列a，将累加和入队列b</p><p>4°当当前节点还有右子树，则将右子树入队列a，将累加和入队列b</p><p>一直循环，直到队列为空</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">sumNumbers</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//二叉树问题一般解题思路都是用层次遍历或者递归</span></span><br><span class="line">       <span class="comment">//给定一个二叉树，它的每个结点都存放一个 0-9 的数字，每条从根到叶子节点的路径都代表一个数字。例如，从根到叶子节点路径 1-&gt;2-&gt;3 代表数字 123。计算从根到叶子节点生成的所有数字之和。</span></span><br><span class="line">       </span><br><span class="line">       <span class="comment">//层次遍历，将每层的节点都乘以10，当遇到叶子节点的时候，将相加结果放入result中去</span></span><br><span class="line">       </span><br><span class="line">       <span class="comment">//0.处理树为空的情况</span></span><br><span class="line">       <span class="keyword">if</span>(root==<span class="literal">NULL</span>)</span><br><span class="line">           <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">       </span><br><span class="line">       <span class="comment">//1.当树不为空</span></span><br><span class="line">       <span class="comment">//1.0新建队列用于层次遍历</span></span><br><span class="line">       <span class="built_in">queue</span>&lt;TreeNode*&gt; p;</span><br><span class="line">       p.push(root);</span><br><span class="line">       </span><br><span class="line">       <span class="keyword">int</span> result=<span class="number">0</span>;</span><br><span class="line">       </span><br><span class="line">       <span class="comment">//1.1新建队列，存储每层需要相加的值</span></span><br><span class="line">       <span class="built_in">queue</span>&lt;<span class="keyword">int</span>&gt; numQue;</span><br><span class="line">       numQue.push(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">       <span class="keyword">while</span>(!p.empty())</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">int</span> <span class="built_in">width</span>=p.<span class="built_in">size</span>();</span><br><span class="line">           <span class="comment">//1.2临时存储每层节点相加的值</span></span><br><span class="line">           <span class="keyword">int</span> val=<span class="number">0</span>;</span><br><span class="line">           <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="built_in">width</span>;i++)</span><br><span class="line">           &#123;</span><br><span class="line">               TreeNode* r=p.front();</span><br><span class="line">               <span class="comment">//把上一次相加的和取出来</span></span><br><span class="line">               <span class="keyword">int</span> temp=numQue.front();</span><br><span class="line">               val=temp*<span class="number">10</span>+r-&gt;val;</span><br><span class="line">               p.pop();</span><br><span class="line">               numQue.pop();</span><br><span class="line">               </span><br><span class="line">               <span class="comment">//1.3当时叶子节点时，存储此时的和</span></span><br><span class="line">               <span class="keyword">if</span>(r-&gt;left==<span class="literal">NULL</span>&amp;&amp;r-&gt;right==<span class="literal">NULL</span>)</span><br><span class="line">                   result+=val;</span><br><span class="line">               </span><br><span class="line">               <span class="keyword">if</span>(r-&gt;left)</span><br><span class="line">               &#123;</span><br><span class="line">                   p.push(r-&gt;left);</span><br><span class="line">                   <span class="comment">//把上一层相加的和存储起来</span></span><br><span class="line">                   numQue.push(val);</span><br><span class="line">               &#125;</span><br><span class="line">               </span><br><span class="line">               <span class="keyword">if</span>(r-&gt;right)</span><br><span class="line">               &#123;</span><br><span class="line">                   p.push(r-&gt;right);</span><br><span class="line">                   <span class="comment">//把上一层相加的和存储起来</span></span><br><span class="line">                   numQue.push(val);</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> result;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：127-单词接龙"><a href="#题目：127-单词接龙" class="headerlink" title="题目：127. 单词接龙"></a>题目：127. 单词接龙</h2><p>描述：给定两个单词（beginWord 和 endWord）和一个字典，找到从 beginWord 到 endWord 的最短转换序列的长度。转换需遵循如下规则：</p><p>每次转换只能改变一个字母。<br>转换过程中的中间单词必须是字典中的单词。<br>说明:</p><p>如果不存在这样的转换序列，返回 0。<br>所有单词具有相同的长度。<br>所有单词只由小写字母组成。<br>字典中不存在重复的单词。<br>你可以假设 beginWord 和 endWord 是非空的，且二者不相同。<br>示例 1:</p><p>输入:<br>beginWord = “hit”,<br>endWord = “cog”,<br>wordList = [“hot”,”dot”,”dog”,”lot”,”log”,”cog”]</p><p>输出: 5</p><p>解释: 一个最短转换序列是 “hit” -&gt; “hot” -&gt; “dot” -&gt; “dog” -&gt; “cog”,<br>     返回它的长度 5。 </p><p>思路：<strong><u>题目可以转换为求图中两点之间最短距离，其中每个点由单词组成，相差一位的单词之间才有路径</u></strong></p><p>使用层次遍历+邻接矩阵（存储每个节点的邻居），将最初起点单词入队，每次读入其的邻居，当该邻居没有访问过，即有新的路径，将其压入队列中，继续找该条路径，当最终的邻居和endword一样说明找到了当前的最短路径。即<strong><u>图的广度优先遍历找到图的最短路径问题。</u></strong></p><p>（1）构建单词表的邻接矩阵</p><p>将初始单词压入单词表，遍历每个单词，当当前单词和遍历的单词只有一位不同时，则在这两个单词的邻接的邻居中加上各自</p><p>附：需要自己定义判断两个字符串只有一位不同：可以遍历单词，当每一个对应中，只有一位不同则返回true，反之返回false</p><p>（2）层次遍历找到图的一条路径</p><p>1°定义一个队列存储每次遍历的单词，将初始单词压入队列，并定义一个标记数组，存储以及走过的单词，避免形成环</p><p>°每次取队首的单词，利用邻接矩阵找到其邻居，当其邻居没有被遍历过，则加入遍历队列，并在标记数组中标记</p><p>当邻居为endword，则找到了最短路径</p><p>每层遍历的时候步数加一，当队列为空后都没有找到邻居和endword相同，说明stratword无法变成endword，返回0</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">ladderLength</span><span class="params">(<span class="built_in">string</span> beginWord, <span class="built_in">string</span> endWord, <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; wordList)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//求图中两点之间最短距离，其中每个点由单词组成，相差一位的单词之间才有路径</span></span><br><span class="line">       <span class="comment">//层次遍历+邻接矩阵（存储每个节点的邻居）</span></span><br><span class="line">       </span><br><span class="line">       <span class="built_in">unordered_map</span>&lt;<span class="built_in">string</span>, <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&gt; graph;<span class="comment">//每个字符串对应的邻居</span></span><br><span class="line">       construct_graph(beginWord, wordList, graph);<span class="comment">//构建邻接矩阵</span></span><br><span class="line">       </span><br><span class="line">       <span class="comment">//层次遍历</span></span><br><span class="line">       <span class="built_in">queue</span>&lt;<span class="built_in">string</span>&gt; p;</span><br><span class="line">       p.push(beginWord);</span><br><span class="line">       </span><br><span class="line">       <span class="comment">//标记已访问单词</span></span><br><span class="line">       <span class="built_in">unordered_map</span>&lt;<span class="built_in">string</span>, <span class="keyword">int</span>&gt; visit;</span><br><span class="line">       visit[beginWord]++;</span><br><span class="line">       <span class="keyword">int</span> res=<span class="number">0</span>;</span><br><span class="line">       </span><br><span class="line">       <span class="keyword">while</span>(!p.empty())</span><br><span class="line">       &#123;</span><br><span class="line">           res++;</span><br><span class="line">           <span class="keyword">int</span> <span class="built_in">width</span>=p.<span class="built_in">size</span>();</span><br><span class="line">           <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="built_in">width</span>;i++)</span><br><span class="line">           &#123;</span><br><span class="line">               <span class="built_in">string</span> r=p.front();</span><br><span class="line">               p.pop();</span><br><span class="line">               <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; neighbours=graph[r];</span><br><span class="line">               <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;neighbours.<span class="built_in">size</span>();j++)</span><br><span class="line">               &#123;</span><br><span class="line">                   <span class="keyword">if</span>(visit.<span class="built_in">find</span>(neighbours[j])==visit.<span class="built_in">end</span>())<span class="comment">//没有访问过</span></span><br><span class="line">                   &#123;</span><br><span class="line">                       p.push(neighbours[j]);</span><br><span class="line">                       visit[neighbours[j]]++;</span><br><span class="line">                   &#125;</span><br><span class="line">                   </span><br><span class="line">                   <span class="keyword">if</span>(neighbours[j]==endWord)</span><br><span class="line">                       <span class="keyword">return</span> res+<span class="number">1</span>;</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">          </span><br><span class="line">       &#125;</span><br><span class="line">       </span><br><span class="line">       <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">       </span><br><span class="line">   &#125;</span><br><span class="line">   <span class="comment">//判断两个字符串是否只有一位不同</span></span><br><span class="line">   <span class="function"><span class="keyword">bool</span> <span class="title">compare</span><span class="params">(<span class="keyword">const</span> <span class="built_in">string</span>&amp; a,<span class="keyword">const</span> <span class="built_in">string</span>&amp; b)</span></span></span><br><span class="line"><span class="function">   </span>&#123;</span><br><span class="line">       <span class="keyword">int</span> count=<span class="number">0</span>;</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;a.<span class="built_in">size</span>();i++)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">if</span>(a[i]!=b[i])</span><br><span class="line">               count++;</span><br><span class="line">           <span class="keyword">if</span>(count&gt;<span class="number">1</span>)</span><br><span class="line">               <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">           </span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> count==<span class="number">1</span>;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="comment">//建立邻接矩阵</span></span><br><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">construct_graph</span><span class="params">(<span class="built_in">string</span> <span class="built_in">begin</span>,<span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; wordlist,<span class="built_in">unordered_map</span>&lt;<span class="built_in">string</span>, <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&gt;&amp; graph)</span></span></span><br><span class="line"><span class="function">   </span>&#123;</span><br><span class="line">       wordlist.push_back(<span class="built_in">begin</span>);</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;wordlist.<span class="built_in">size</span>();i++)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">for</span>(<span class="keyword">int</span> j=i+<span class="number">1</span>;j&lt;wordlist.<span class="built_in">size</span>();j++)<span class="comment">//避免重复</span></span><br><span class="line">           &#123;</span><br><span class="line">               <span class="keyword">if</span>(compare(wordlist[i], wordlist[j]))</span><br><span class="line">               &#123;</span><br><span class="line">                   graph[wordlist[i]].push_back(wordlist[j]);</span><br><span class="line">                   graph[wordlist[j]].push_back(wordlist[i]);</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：463-岛屿的周长"><a href="#题目：463-岛屿的周长" class="headerlink" title="题目：463. 岛屿的周长"></a>题目：463. 岛屿的周长</h2><p>描述：给定一个包含 0 和 1 的二维网格地图，其中 1 表示陆地 0 表示水域。</p><p>网格中的格子水平和垂直方向相连（对角线方向不相连）。整个网格被水完全包围，但其中恰好有一个岛屿（或者说，一个或多个表示陆地的格子相连组成的岛屿）。</p><p>岛屿中没有“湖”（“湖” 指水域在岛屿内部且不和岛屿周围的水相连）。格子是边长为 1 的正方形。网格为长方形，且宽度和高度均不超过 100 。计算这个岛屿的周长。</p><p>思路：</p><p>（1）遍历整个网格</p><p>（2）当当前遍历的点时陆地，则使用方向数组，判断其上下左右的是陆地还是海洋</p><p>当周围遇到边界或者周围是海洋的时候，周长+1，其余情况周长不变</p><p>以此循环，最终得到陆地的周长</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">islandPerimeter</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; grid)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//暴力法</span></span><br><span class="line">       <span class="keyword">int</span> res=<span class="number">0</span>;</span><br><span class="line">       <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; direction&#123;&#123;<span class="number">0</span>,<span class="number">1</span>&#125;,&#123;<span class="number">0</span>,<span class="number">-1</span>&#125;,&#123;<span class="number">1</span>,<span class="number">0</span>&#125;,&#123;<span class="number">-1</span>,<span class="number">0</span>&#125;&#125;;</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;grid.<span class="built_in">size</span>();i++)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;grid[<span class="number">0</span>].<span class="built_in">size</span>();j++)</span><br><span class="line">           &#123;</span><br><span class="line">               <span class="keyword">if</span>(grid[i][j]==<span class="number">1</span>)</span><br><span class="line">               &#123;</span><br><span class="line">                   <span class="keyword">for</span>(<span class="keyword">int</span> k=<span class="number">0</span>;k&lt;direction.<span class="built_in">size</span>();k++)</span><br><span class="line">                   &#123;</span><br><span class="line">                       <span class="keyword">int</span> a=i+direction[k][<span class="number">0</span>];</span><br><span class="line">                       <span class="keyword">int</span> b=j+direction[k][<span class="number">1</span>];</span><br><span class="line">                       </span><br><span class="line">                       <span class="keyword">if</span>(a&lt;<span class="number">0</span>||a&gt;=grid.<span class="built_in">size</span>()||b&lt;<span class="number">0</span>||b&gt;=grid[<span class="number">0</span>].<span class="built_in">size</span>()||grid[a][b]==<span class="number">0</span>)</span><br><span class="line">                           res++;</span><br><span class="line">                   &#125;</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> res;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：412-Fizz-Buzz"><a href="#题目：412-Fizz-Buzz" class="headerlink" title="题目：412. Fizz Buzz"></a>题目：412. Fizz Buzz</h2><p>描述：写一个程序，输出从 1 到 n 数字的字符串表示。</p><ol><li><p>如果 n 是3的倍数，输出“Fizz”；</p></li><li><p>如果 n 是5的倍数，输出“Buzz”；</p></li></ol><p>3.如果 n 同时是3和5的倍数，输出 “FizzBuzz”。</p><p>示例：</p><p>n = 15,</p><p>返回:<br>[<br>    “1”,<br>    “2”,<br>    “Fizz”,<br>    “4”,<br>    “Buzz”,<br>    “Fizz”,<br>    “7”,<br>    “8”,<br>    “Fizz”,<br>    “Buzz”,<br>    “11”,<br>    “Fizz”,<br>    “13”,<br>    “14”,<br>    “FizzBuzz”<br>]</p><p>思路：</p><p>1、根据条件判断进行字符串连接</p><p>遍历1 ~ n个数字</p><p>判断每个数字是否是3的倍数（取余3是否为0），如果是，则在字符串中增加Fizz</p><p>（2）判断每个数字是否是5的倍数（取余5是否为0），如果是，则在字符串中增加Buizz</p><p>（3）如果字符串为空，说明既不是3的倍数也不是5的倍数，则将当前数字加入字符串</p><p>一直循环，直至结束</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; fizzBuzz(<span class="keyword">int</span> n) &#123;</span><br><span class="line">       <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; res;</span><br><span class="line">       </span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=n;i++)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="built_in">string</span> tmp;</span><br><span class="line">           <span class="keyword">if</span>(i%<span class="number">3</span>==<span class="number">0</span>)</span><br><span class="line">               tmp+=<span class="string">"Fizz"</span>;</span><br><span class="line">           <span class="keyword">if</span>(i%<span class="number">5</span>==<span class="number">0</span>)</span><br><span class="line">               tmp+=<span class="string">"Buzz"</span>;</span><br><span class="line">           <span class="keyword">if</span>(tmp==<span class="string">""</span>)</span><br><span class="line">               tmp+=to_string(i);</span><br><span class="line">           res.push_back(tmp);</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> res;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>2、哈希表维护映射</p><p>当需要判断的条件很多的时候，则if else 语句过于累赘，则建立一个排序哈希表（默认按照第一个数据类型从小到大排序），存储需要判断的倍数以及是倍数时的字符串，每次用当前元素除以map中的key值，如果符合条件则字符串加上map中的value值</p><p>（1）建立一个排序哈希表map，存储需要判断的倍数以及是倍数时的字符串</p><p>（2）遍历1 ~ n个数字，每次取当前一个数字判断</p><p>1°遍历哈希表map，取map中的key值，判断当前数字是否能够整除key值，能整除则在字符串加上map中的value值</p><p>2°当遍历哈希表结束后，字符串仍为空，说明说明既不是3的倍数也不是5的倍数，则将当前数字加入字符串</p><p>一直循环，直至结束</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; fizzBuzz(<span class="keyword">int</span> n) &#123;</span><br><span class="line">       <span class="comment">//哈希表存储</span></span><br><span class="line">       <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; res;</span><br><span class="line">       <span class="built_in">map</span>&lt;<span class="keyword">int</span>, <span class="built_in">string</span>&gt; <span class="built_in">map</span>;</span><br><span class="line">       <span class="built_in">map</span>[<span class="number">3</span>]=<span class="string">"Fizz"</span>;</span><br><span class="line">       <span class="built_in">map</span>[<span class="number">5</span>]=<span class="string">"Buzz"</span>;</span><br><span class="line">       </span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=n;i++)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="built_in">string</span> tmp;</span><br><span class="line">           <span class="keyword">for</span>(<span class="keyword">auto</span> c:<span class="built_in">map</span>)</span><br><span class="line">           &#123;</span><br><span class="line">               <span class="keyword">if</span>(i%c.first==<span class="number">0</span>)</span><br><span class="line">                   tmp+=c.second;</span><br><span class="line">           &#125;</span><br><span class="line">           <span class="keyword">if</span>(tmp==<span class="string">""</span>)</span><br><span class="line">               tmp+=to_string(i);</span><br><span class="line">           </span><br><span class="line">           res.push_back(tmp);</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> res;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;题目：844-比较含退格的字符串&quot;&gt;&lt;a href=&quot;#题目：844-比较含退格的字符串&quot; class=&quot;headerlink&quot; title=&quot;题目：844. 比较含退格的字符串&quot;&gt;&lt;/a&gt;题目：844. 比较含退格的字符串&lt;/h2&gt;&lt;p&gt;描述：给定 S 和 T 两个字符串，当它们分别被输入到空白的文本编辑器后，判断二者是否相等，并返回结果。 # 代表退格字符。&lt;/p&gt;&lt;p&gt;注意：如果对空文本输入退格字符，文本继续为空。&lt;/p&gt;&lt;p&gt;示例 1：&lt;/p&gt;&lt;p&gt;输入：S = “ab#c”, T = “ad#c”&lt;br&gt;输出：true&lt;br&gt;解释：S 和 T 都会变成 “ac”。&lt;/p&gt;
    
    </summary>
    
    
      <category term="算法" scheme="https://www.xiapf.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="https://www.xiapf.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>其他工具——使用svd简化数据</title>
    <link href="https://www.xiapf.com/blogs/svd/"/>
    <id>https://www.xiapf.com/blogs/svd/</id>
    <published>2020-10-21T06:51:52.000Z</published>
    <updated>2020-10-21T06:57:54.044Z</updated>
    
    <content type="html"><![CDATA[<h1 id="其他工具——使用svd简化数据"><a href="#其他工具——使用svd简化数据" class="headerlink" title="其他工具——使用svd简化数据"></a>其他工具——使用svd简化数据</h1><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>（1）svd可以用于简化数据，减少噪声</p><p>（2）svd将原始数据分解为三个矩阵U，∑，VT，假设原始数据维度为（m,n），这三个矩阵维度分别为（m,m）,（n,）,（n,n）,其中∑是对角矩阵，对角元素从大到小排列，即为奇异值（奇异值=原始矩阵 * 原始矩阵的转置的特征值的平方根）</p><a id="more"></a><h2 id="推荐系统——基于协同过滤"><a href="#推荐系统——基于协同过滤" class="headerlink" title="推荐系统——基于协同过滤"></a>推荐系统——基于协同过滤</h2><p>协同过滤是通过将用户和其他用户的数据进行对比来实现推荐的</p><p>则需要知道两个用户或者物品之间的相似度，用已有的数据来预测未来的喜好，则需要对相似度的计算进行定义</p><p><u><strong>以给定一个用户，返回给该用户推荐的N个菜为例：</strong></u></p><p><u><strong>需要得到评级最高的N个商品进行推荐，但是用户不可能对所有商品都评级，则此时需要将未评级的商品利用已有数据计算相似度作为评级依据</strong></u></p><h3 id="相似度的计算（输入的数据均为列向量）"><a href="#相似度的计算（输入的数据均为列向量）" class="headerlink" title="相似度的计算（输入的数据均为列向量）"></a>相似度的计算（输入的数据均为列向量）</h3><p>常用的similarity计算有欧式距离，皮尔逊系数，余弦系数，为了比较这三个系数，将其都控制在0 ~ 1之间</p><p>（1）欧式距离（即二阶范数）</p><p>利用numpy下的linalg库下的norm</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#欧式距离</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">edSim</span><span class="params">(inA,inB)</span>:</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.linalg.norm(inA-inB)) <span class="comment">#norm 二阶范数</span></span><br></pre></td></tr></table></figure><p>（2）皮尔逊系数</p><p>利用numpy下的corrcoef</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#皮尔逊系数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pearSim</span><span class="params">(inA,inB)</span>:</span></span><br><span class="line"><span class="keyword">if</span>(len(inA)&lt;<span class="number">3</span>):</span><br><span class="line"><span class="keyword">return</span> <span class="number">1</span> <span class="comment">#不存在三个或者更多的点，说明两个向量完全相关</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">0.5</span>+<span class="number">0.5</span>*np.corrcoef(inA,inB,rowvar=<span class="number">0</span>)[<span class="number">0</span>][<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p>（3）余弦系数</p><p>利用公式</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20201021131645.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#余弦相似度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cosSim</span><span class="params">(inA,inB)</span>:</span></span><br><span class="line">num=inA.T*inB</span><br><span class="line">norm=np.linalg.norm(inA)*np.linalg.norm(inB)</span><br><span class="line"><span class="keyword">return</span> <span class="number">0.5</span>+<span class="number">0.5</span>*(num/norm)</span><br></pre></td></tr></table></figure><p>讨论：基于什么的相似度？</p><p>基于用户的相似度（行与行）、基于商品的相似度（列与列），取决于哪个数量少就基于哪个的相似度</p><h3 id="评级估计方法"><a href="#评级估计方法" class="headerlink" title="评级估计方法"></a>评级估计方法</h3><p>a）标准方法</p><p>输入用户、未评级的物品、相似度的计算、原数据</p><p>（1）遍历所有的物品</p><p>（2）得到用户对当前遍历物品的评级</p><p>如果评级为0或者该物品就是要求的物品，则跳过，继续找下一个物品</p><p>找到未评级物品和当前物品重叠部分，即有用户对这两个物品都评级了（这样可以计算相似度）</p><p>将得到的相似度累加，并且用相似度乘以当前的评分，用乘积除以相似度总和（归一化操作），得到最终的评级</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#估计方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">standEst</span><span class="params">(dataset,user,SimMethod,item)</span>:</span></span><br><span class="line">m,n=np.shape(dataset)</span><br><span class="line"></span><br><span class="line">simTotal=<span class="number">0</span>;</span><br><span class="line">rateSimTotal=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#遍历每个已经评级的物品</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">usrRate=dataset[user,j]</span><br><span class="line"><span class="keyword">if</span> usrRate==<span class="number">0</span> <span class="keyword">or</span> j==item:</span><br><span class="line"><span class="keyword">continue</span></span><br><span class="line">overlap=np.nonzero(np.logical_and(dataset[:,item]&gt;<span class="number">0</span>,dataset[:,j]&gt;<span class="number">0</span>))[<span class="number">0</span>]  <span class="comment">#找到评级过的两个物品</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> len(overlap)==<span class="number">0</span>:</span><br><span class="line">similarity=<span class="number">0</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">similarity=SimMethod(dataset[overlap,item],dataset[overlap,j])</span><br><span class="line">simTotal+=similarity</span><br><span class="line">rateSimTotal+=similarity*usrRate</span><br><span class="line"><span class="keyword">if</span> simTotal==<span class="number">0</span>:</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">return</span> rateSimTotal/simTotal</span><br></pre></td></tr></table></figure><p>b）应用svd</p><p>因为原始数据会存在很多0，则需要对原始数据进行降维操作，这里使用svd，但是又要保证准确率，需要对保留的特征向量的k进行设置，这里使用保留下的数据是原始数据方差的90%</p><p>在标准方法前增加对原始数据的降维：</p><p>（1）使用svd分解原始数据为U,sigema,VT，计奇异值矩阵中的和，即主对角线元素相加，随着i的增加，分别取对应行的主对角元素，当满足上述式子后得到的i值就是能够保留90%的方差的k的取值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">U,sigma,VT=np.linalg.svd(dataset)</span><br><span class="line">sig2=sigma**<span class="number">2</span></span><br><span class="line">m=np.shape(sigma)[<span class="number">0</span>]</span><br><span class="line">k=<span class="number">0</span></span><br><span class="line">tmp_s=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line"><span class="keyword">if</span> np.sum(tmp_s)&gt;(np.sum(sig2)*<span class="number">0.9</span>):</span><br><span class="line">k=i</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">tmp_s=sig2[:i]</span><br><span class="line">print(k)</span><br></pre></td></tr></table></figure><p>（2）因为这里的sigma不是用对角矩阵形式存储（以数组形式存储），因为取前k个sigma的值形成对角矩阵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sig4=np.mat(np.eye(k)*sigma[:k])</span><br></pre></td></tr></table></figure><p>（3）将数据映射到低维度空间</p><p><strong>其中U矩阵可以将物品转换到低维空间，而V矩阵可以将用户转换到低维空间</strong></p><p>原始数据的维度为（m,n）；特征向量即低维子空间向量的维度为（m,m），取前k个，维度为（m,k）,所以需要将原始矩阵转置乘以特征向量，再算上原来的奇异值。最终投影后的数据维度为（n,k）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xtransform=dataset.T*U[:,:k]*sig4.I <span class="comment">#数据投影</span></span><br></pre></td></tr></table></figure><p>（4）按照标准方法，遍历每个物品，计算投影后两个重叠物品的相似度</p><p>因为相似度是输入的列向量，因此，需要把投影后的数据进行转置操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#指定用户对一些物品的评分</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svdEst</span><span class="params">(dataset,user,SimMethod,item)</span>:</span></span><br><span class="line"><span class="comment">#遍历已经评级的物品</span></span><br><span class="line">m,n=np.shape(dataset)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将原始数据映射到低维度空间</span></span><br><span class="line"><span class="comment">#选择合适的特征保留下来</span></span><br><span class="line">U,sigma,VT=np.linalg.svd(dataset)</span><br><span class="line">sig2=sigma**<span class="number">2</span></span><br><span class="line">m=np.shape(sigma)[<span class="number">0</span>]</span><br><span class="line">k=<span class="number">0</span></span><br><span class="line">tmp_s=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line"><span class="keyword">if</span> np.sum(tmp_s)&gt;(np.sum(sig2)*<span class="number">0.9</span>):</span><br><span class="line">k=i</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">tmp_s=sig2[:i]</span><br><span class="line">print(k)</span><br><span class="line">sig4=np.mat(np.eye(k)*sigma[:k])</span><br><span class="line">xtransform=dataset.T*U[:,:k]*sig4.I <span class="comment">#数据投影</span></span><br><span class="line"></span><br><span class="line">simTotal=<span class="number">0</span>;</span><br><span class="line">rateSimTotal=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">usrRate=dataset[user,j]</span><br><span class="line"><span class="keyword">if</span> usrRate==<span class="number">0</span> <span class="keyword">or</span> j==item:</span><br><span class="line"><span class="keyword">continue</span></span><br><span class="line">similarity=SimMethod(xtransform[item,:].T,xtransform[j,:].T)</span><br><span class="line">print(<span class="string">"%d and %d similarity is:%f"</span> % (item,j,similarity))</span><br><span class="line">simTotal+=similarity</span><br><span class="line">rateSimTotal+=usrRate*similarity</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> simTotal==<span class="number">0</span>:</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">return</span> rateSimTotal/simTotal</span><br></pre></td></tr></table></figure><h3 id="推荐商品"><a href="#推荐商品" class="headerlink" title="推荐商品"></a>推荐商品</h3><p>（1）得到该用户没有评级过的商品</p><p>（2）如果所有商都评级过，则之间返回</p><p>（3）对没有评级的每个商品，计算其在给定评级方法下的得分</p><p>（4）最终，利用sorted对每个商品逆序排序，得到前n个</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#推荐系统</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recomend</span><span class="params">(dataset,user,N=<span class="number">3</span>,SimMethod=edSim,estMethod=standEst)</span>:</span></span><br><span class="line"><span class="comment">#得到没有评级的物品</span></span><br><span class="line">nonest=np.nonzero(dataset[user,:]==<span class="number">0</span>)[<span class="number">1</span>] <span class="comment">#[1]得到列向量</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(len(nonest)==<span class="number">0</span>):</span><br><span class="line"><span class="keyword">return</span> <span class="string">"you all rated"</span></span><br><span class="line"><span class="comment">#将没有评级的物品按照给定估计方法进行评级</span></span><br><span class="line">itemScore=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> nonest:</span><br><span class="line">score=estMethod(dataset,user,SimMethod,i)</span><br><span class="line">itemScore.append((i,score))</span><br><span class="line"></span><br><span class="line"><span class="comment">#得到前n个评级的物品</span></span><br><span class="line"><span class="keyword">return</span> sorted(itemScore,key=<span class="keyword">lambda</span> j:j[<span class="number">1</span>],reverse=<span class="literal">True</span>)[:N]</span><br></pre></td></tr></table></figure><h3 id="推荐效果"><a href="#推荐效果" class="headerlink" title="推荐效果"></a>推荐效果</h3><p>（1）原始数据为：</p><blockquote><p>​           [[0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 5],<br>​           [0, 0, 0, 3, 0, 4, 0, 0, 0, 0, 3],<br>​           [0, 0, 0, 0, 4, 0, 0, 1, 0, 4, 0],<br>​           [3, 3, 4, 0, 0, 0, 0, 2, 2, 0, 0],<br>​           [5, 4, 5, 0, 0, 0, 0, 5, 5, 0, 0],<br>​           [0, 0, 0, 0, 5, 0, 1, 0, 0, 5, 0],<br>​           [4, 3, 4, 0, 0, 0, 0, 5, 5, 0, 1],<br>​           [0, 0, 0, 4, 0, 4, 0, 0, 0, 0, 4],<br>​           [0, 0, 0, 2, 0, 2, 5, 0, 0, 1, 2],<br>​           [0, 0, 0, 0, 5, 0, 0, 0, 0, 4, 0],<br>​           [1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0]]</p></blockquote><p>相似度计算方式均使用皮尔逊系数，一个评级方法使用svd，一个使用标准方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">r=recomend(mymat2,<span class="number">1</span>,SimMethod=pearSim,estMethod=svdEst)</span><br><span class="line">print(r)</span><br><span class="line">r=recomend(mymat2,<span class="number">1</span>,SimMethod=pearSim,estMethod=standEst)</span><br><span class="line">print(r)</span><br></pre></td></tr></table></figure><p>两种不同方法下推荐的结果如下，同时打印出每次计算的相似度过程，可以看出使用svd方法相似度的度量更准确些，推荐结果也会更好些。</p><blockquote><p>0 and 3 similarity is:0.341942<br>0 and 5 similarity is:0.124132<br>0 and 10 similarity is:0.116698<br>1 and 3 similarity is:0.345560<br>1 and 5 similarity is:0.126456<br>1 and 10 similarity is:0.118892<br>…</p><p>[(4, 3.3469521867021728), (9, 3.33537965732747), (6, 3.3071930278130375)]</p><p>0 and 3 similarity is:0.000000<br>0 and 5 similarity is:0.000000<br>0 and 10 similarity is:1.000000<br>1 and 3 similarity is:0.000000<br>1 and 5 similarity is:0.000000<br>1 and 10 similarity is:1.000000<br>…</p><p>[(6, 3.3333333333333335), (9, 3.3333333333333335), (0, 3.0)]</p></blockquote><h2 id="图像压缩"><a href="#图像压缩" class="headerlink" title="图像压缩"></a>图像压缩</h2><h3 id="打印图像"><a href="#打印图像" class="headerlink" title="打印图像"></a>打印图像</h3><p>对图像中的数值按照阈值进行打印输出，小于阈值输出0，大于阈值输出1</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printmat</span><span class="params">(mydata,threshold)</span>:</span></span><br><span class="line">m,n=np.shape(mydata)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line"><span class="keyword">if</span>(float(mydata[i,j])&lt;threshold):</span><br><span class="line">print(<span class="string">'0'</span>,end=<span class="string">''</span>)<span class="comment">#去掉回车</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">print(<span class="string">'1'</span>,end=<span class="string">''</span>)</span><br><span class="line">print(<span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure><h3 id="使用svd降维"><a href="#使用svd降维" class="headerlink" title="使用svd降维"></a>使用svd降维</h3><p>（1）使用svd分解原始数据为U,sigema,VT</p><p>（2）因为这里的sigma不是用对角矩阵形式存储（以数组形式存储），因为取前k个sigma的值形成对角矩阵</p><p>（3）利用U,V矩阵同时进行降维</p><p>xtransform=U[:,:numSVD]<em>sigcon</em>VT[:numSVD,:]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用svd</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compressImg</span><span class="params">(filename,numSVD=<span class="number">3</span>,threshold=<span class="number">0.8</span>)</span>:</span></span><br><span class="line"><span class="comment">#读入数据</span></span><br><span class="line">fr=open(filename)</span><br><span class="line">curpt=[]</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">m=len(line)</span><br><span class="line">newrow=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m<span class="number">-1</span>):<span class="comment">#去掉最后的回车</span></span><br><span class="line">newrow.append(int(line[i]))</span><br><span class="line">curpt.append(newrow)</span><br><span class="line">print(curpt)</span><br><span class="line">curpt=np.mat(curpt)</span><br><span class="line">print(<span class="string">'origin mat'</span>)</span><br><span class="line">printmat(curpt,threshold)</span><br><span class="line"></span><br><span class="line">U,sigma,VT=np.linalg.svd(curpt)</span><br><span class="line"></span><br><span class="line">sigcon=np.zeros((numSVD,numSVD))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(numSVD):</span><br><span class="line">sigcon[i][i]=sigma[i]</span><br><span class="line">xtransform=U[:,:numSVD]*sigcon*VT[:numSVD,:]</span><br><span class="line">print(<span class="string">'svd mat'</span>)</span><br><span class="line">printmat(xtransform,threshold)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;其他工具——使用svd简化数据&quot;&gt;&lt;a href=&quot;#其他工具——使用svd简化数据&quot; class=&quot;headerlink&quot; title=&quot;其他工具——使用svd简化数据&quot;&gt;&lt;/a&gt;其他工具——使用svd简化数据&lt;/h1&gt;&lt;h2 id=&quot;应用&quot;&gt;&lt;a href=&quot;#应用&quot; class=&quot;headerlink&quot; title=&quot;应用&quot;&gt;&lt;/a&gt;应用&lt;/h2&gt;&lt;p&gt;（1）svd可以用于简化数据，减少噪声&lt;/p&gt;&lt;p&gt;（2）svd将原始数据分解为三个矩阵U，∑，VT，假设原始数据维度为（m,n），这三个矩阵维度分别为（m,m）,（n,）,（n,n）,其中∑是对角矩阵，对角元素从大到小排列，即为奇异值（奇异值=原始矩阵 * 原始矩阵的转置的特征值的平方根）&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="降维" scheme="https://www.xiapf.com/tags/%E9%99%8D%E7%BB%B4/"/>
    
  </entry>
  
  <entry>
    <title>其他工具——使用pca简化数据</title>
    <link href="https://www.xiapf.com/blogs/pca/"/>
    <id>https://www.xiapf.com/blogs/pca/</id>
    <published>2020-10-21T06:51:48.000Z</published>
    <updated>2020-10-21T06:55:10.611Z</updated>
    
    <content type="html"><![CDATA[<h2 id="应用背景"><a href="#应用背景" class="headerlink" title="应用背景"></a>应用背景</h2><p>（1）目的：对数据进行降维，即对输入的数据存在冗余，对输入的数目削减</p><p>（2）场景：当求协方差的特征值是，得出很多特征值为0，说明特征存在冗余</p><p>（3）实际使用：实际应用中，数据集中存在很多nan空值，则将nan值替换为平均值</p><p>遍历所有的特征，利用numpy下的isnan找到非nan值对应的行，取均值，赋值给该特征对应的空值中</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#实际应用中，将nan值替换为平均值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">replaceNan</span><span class="params">(filename)</span>:</span></span><br><span class="line">dataset=loadDataset(filename,<span class="string">' '</span>)</span><br><span class="line">m,n=np.shape(dataset)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">meanVal=np.mean(dataset[np.nonzero(~np.isnan(dataset[:,i].A))[<span class="number">0</span>],i])</span><br><span class="line">dataset[np.nonzero(np.isnan(dataset[:,i].A))[<span class="number">0</span>],i]=meanVal</span><br><span class="line"><span class="keyword">return</span> dataset</span><br></pre></td></tr></table></figure><h2 id="算法简介"><a href="#算法简介" class="headerlink" title="算法简介"></a>算法简介</h2><p>原始数据为n维</p><p>（1）找到一个向量u属于Rn空间的向量，找该向量的原则就是数据投影的误差最小化的方向</p><p>（2）将数据从n维-&gt;k维，找到k个方向的向量u(i),…u(k)，此时这些向量定义了一个低维度的平面</p><p>（3）将数据投影到这k个向量展开的线性子空间上，即k维平面上</p><p>总的来说，pca是找到一个低维度的空间，通过最小化投影距离的方法来对数据投影，以此实现数据的降维。投影就是两个向量的乘积</p><h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><p>（1）计算均值：减少不同量纲之间的影响</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">meanData=np.mean(dataset,axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>使用归一化后的数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">subMeanData=dataset-meanData</span><br></pre></td></tr></table></figure><p>（2）计算协方差：低维度平面的向量所对应的矩阵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">covData=np.cov(subMeanData,rowvar=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>（3）计算特征值、特征向量：选择前K个特征向量即为低维度平面对应的向量</p><p>利用numpy下的linalg.eig求得特征值和特征向量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">feaVal,feaVec=np.linalg.eig(np.mat(covData))</span><br></pre></td></tr></table></figure><p>（4）选择前K个来定义低维度空间</p><p>利用numpy下的argsort对特征值进行排序，按照排序好的索引，取前k个，得到对应的特征向量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sortIndex=np.argsort(feaVal)</span><br><span class="line">sortIndex=sortIndex[:-(topFeat+<span class="number">1</span>):<span class="number">-1</span>]</span><br><span class="line">sortVec=feaVec[:,sortIndex]</span><br></pre></td></tr></table></figure><p>（5）公式计算：将数据投影到低维度空间上</p><p>低维度空间对应的矩阵维度为n * k，待处理的数据对应的矩阵维度为m * n，将两者相乘，即得到了待处理数据投影到低维度空间的值，即投影后的数据维度为m * k</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">resData=subMeanData*sortVec</span><br></pre></td></tr></table></figure><p>（6）将投影的数据还原在原来维度上显示</p><p>用投影后的数据（维度为m * k）乘以 低维度空间对应的矩阵（维度为n * k）的转置，得到的即为原来的数据，再还原均值归一化得到还原的数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">returnData=resData*sortVec.T+meanData</span><br></pre></td></tr></table></figure><p>注：上述求特征向量还可以使用numpy.linalg下的svd对协方差矩阵直接求得，此时的特征向量矩阵是已经按照降序排列的</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;应用背景&quot;&gt;&lt;a href=&quot;#应用背景&quot; class=&quot;headerlink&quot; title=&quot;应用背景&quot;&gt;&lt;/a&gt;应用背景&lt;/h2&gt;&lt;p&gt;（1）目的：对数据进行降维，即对输入的数据存在冗余，对输入的数目削减&lt;/p&gt;&lt;p&gt;（2）场景：当求协方差的特征值是，得出很多特征值为0，说明特征存在冗余&lt;/p&gt;&lt;p&gt;（3）实际使用：实际应用中，数据集中存在很多nan空值，则将nan值替换为平均值&lt;/p&gt;&lt;p&gt;遍历所有的特征，利用numpy下的isnan找到非nan值对应的行，取均值，赋值给该特征对应的空值中&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="降维" scheme="https://www.xiapf.com/tags/%E9%99%8D%E7%BB%B4/"/>
    
  </entry>
  
  <entry>
    <title>Leecode做题记录（四）</title>
    <link href="https://www.xiapf.com/blogs/LC4/"/>
    <id>https://www.xiapf.com/blogs/LC4/</id>
    <published>2020-10-16T05:16:20.000Z</published>
    <updated>2020-10-16T05:17:25.669Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目：461-汉明距离"><a href="#题目：461-汉明距离" class="headerlink" title="题目：461. 汉明距离"></a>题目：461. 汉明距离</h2><p>描述：两个整数之间的汉明距离指的是这两个数字对应二进制位不同的位置的数目。给出两个整数 x 和 y，计算它们之间的汉明距离。</p><p>思路：</p><p>两个数的二进制位不同即可以使用x&amp;1判断或者是x%2来判断数字的每一位</p><p>（1）对两个数对2取余来判断每一位的二进制数是否相同</p><p>（2）移动到下一位判断（使用除法或者是右移操作即可）</p><a id="more"></a><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">hammingDistance</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//汉明距离指的是这两个数字对应二进制位不同的位置的数目。</span></span><br><span class="line">       <span class="keyword">int</span> res=<span class="number">0</span>;</span><br><span class="line">       <span class="keyword">while</span>((x!=<span class="number">0</span>)||(y!=<span class="number">0</span>))</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">if</span>(x%<span class="number">2</span>!=y%<span class="number">2</span>)</span><br><span class="line">               res++;</span><br><span class="line">           x=x/<span class="number">2</span>;<span class="comment">//x&gt;&gt;=1</span></span><br><span class="line">           y=y/<span class="number">2</span>;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> res;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：501-二叉搜索树中的众数"><a href="#题目：501-二叉搜索树中的众数" class="headerlink" title="题目：501. 二叉搜索树中的众数"></a>题目：501. 二叉搜索树中的众数</h2><p>描述：给定一个有相同值的二叉搜索树（BST），找出 BST 中的所有众数（出现频率最高的元素）。</p><p>假定 BST 有如下定义：结点左子树中所含结点的值小于等于当前结点的值；结点右子树中所含结点的值大于等于当前结点的值；左子树和右子树都是二叉搜索树</p><p>思路：</p><p>要找到树中出现频率最高的元素可以使用中序遍历，因为中序遍历树之后，元素按照从小到大的顺序排列，当出现重复数字的时候也是连续出现的。因此只需要每次记录之前出现的最大次数的数字和当前数字的比较，就能找到出现次数最多的数字。</p><p>注意重复出现次数的数的处理。（见3°的（3））</p><p>（1）建立栈来存储树</p><p>（2）利用栈中序遍历树：</p><p>1°每次当当前的节点不为空的时候，就将该点入栈</p><p>2°并不断找到其最左边的节点</p><p>3°此时的栈顶就是最左边的节点，出栈后下一个节点为根节点，再遍历该节点的右节点</p><p>最终能够中序遍历该树</p><p>（3）在中序遍历出栈时，进行比较操作：</p><p>1°比较当前的数字和之前众数的数字是否相同，相同次数加1，说明有连续数字出现，不相同，则当前次数计数为1</p><p>2°比较当前次数和最大次数，如果大于最大次数，之前的数全部清空，将当前节点的值加入结果中，最大次数进行更新；如果等于最大次数，说明出现了重次数字，重复的次数的数需要都加入结果中，则将当前节点的值加入结果中。</p><p>最后，将当前节点的值给出现最多的值，继续遍历下一个节点。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; findMode(TreeNode* root) &#123;</span><br><span class="line">      <span class="comment">//中序遍历之后，重复的数字都是连续出现的</span></span><br><span class="line">      <span class="comment">//用一个数字记录当前数字出现的次数，一个数字记录最大的重复次数</span></span><br><span class="line">      <span class="comment">//当出现次数一样如何处理</span></span><br><span class="line">      <span class="keyword">if</span>(root==<span class="literal">NULL</span>)</span><br><span class="line">          <span class="keyword">return</span> &#123;&#125;;</span><br><span class="line">      <span class="keyword">int</span> count=<span class="number">0</span>;</span><br><span class="line">      <span class="keyword">int</span> maxCount=<span class="number">-1</span>;</span><br><span class="line">      <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res;</span><br><span class="line">      TreeNode* p=root;</span><br><span class="line">      <span class="keyword">int</span> curRes=p-&gt;val;</span><br><span class="line">      <span class="built_in">stack</span>&lt;TreeNode*&gt; s;</span><br><span class="line">      <span class="keyword">while</span>(p!=<span class="literal">NULL</span>||!s.empty())</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="comment">//根节点入栈，找到最左边的节点</span></span><br><span class="line">          <span class="keyword">while</span>(p!=<span class="literal">NULL</span>)</span><br><span class="line">          &#123;</span><br><span class="line">              s.push(p);</span><br><span class="line">              p=p-&gt;left;</span><br><span class="line">          &#125;</span><br><span class="line">          p=s.top();</span><br><span class="line">          s.pop();</span><br><span class="line">          </span><br><span class="line">          <span class="keyword">int</span> val=p-&gt;val;</span><br><span class="line">          <span class="keyword">if</span>(curRes==val)</span><br><span class="line">              count++;</span><br><span class="line">          <span class="keyword">else</span></span><br><span class="line">              count=<span class="number">1</span>;</span><br><span class="line">          <span class="keyword">if</span>(count&gt;maxCount)</span><br><span class="line">          &#123;</span><br><span class="line">              res.<span class="built_in">clear</span>();</span><br><span class="line">              res.push_back(val);</span><br><span class="line">              maxCount=count;</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">else</span> <span class="keyword">if</span> (count==maxCount)</span><br><span class="line">              res.push_back(val);</span><br><span class="line">          curRes=val;</span><br><span class="line">          p=p-&gt;right;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> res;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：106-从中序与后序遍历序列构造二叉树"><a href="#题目：106-从中序与后序遍历序列构造二叉树" class="headerlink" title="题目：106. 从中序与后序遍历序列构造二叉树"></a>题目：106. 从中序与后序遍历序列构造二叉树</h2><p>描述：根据一棵树的中序遍历与后序遍历构造二叉树。</p><p>思路：</p><p>中序遍历中节点总体顺序为：[ 左子树 根节点 右子树 ]</p><p>后序遍历中节点总体顺序为：[ 左子树  右子树 根节点  ]</p><p>所以根据后序遍历从最后开始的节点为根节点，按照得到的根节点可以在中序遍历中确定左子树和右子树的位置，以此来构建二叉树。</p><p>总体思路：按照确定的根节点的位置，在中序遍历中确定左右子树的节点，递归构建二叉树。</p><p>（1）递归结束的条件：当在中序遍历中的左边节点的位置大于右边节点位置，说明构建结束</p><p>（2）根据当前后序遍历中最后一个节点，即为根节点的位置，确定该根节点在中序遍历中的位置</p><p>并且以此值构建根的节点。在中序遍历中确定根的位置后，则确定左子树的长度，便于判断后序遍历中左子树的位置。</p><p>（3）递归构建左子树：输入中序遍历中左子树的起始结束位置，即为原中序中左节点位置 ~ 当前确定的根节点-1的位置，输入后序遍历的左子树的起始结束位置，即为原后序左节点位置 ~ 中序确定的左子树的长度+左节点位置-1</p><p>（4）递归构建右子树：输入中序遍历中右子树的起始结束位置，即为当前确定的根节点+1的位置 ~ 原中序中右节点位置，输入后序遍历的左子树的起始结束位置，即为中序确定的左子树的长度+左节点位置 ~ 原后序右节点位置-1</p><p>注意：输入左右子树的饿位置的时候，需要把根节点的位置去除。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">TreeNode* <span class="title">buildTree</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; inorder, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; postorder)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//中序确定左右子树的位置，后序确定根的位置</span></span><br><span class="line">        TreeNode* root;</span><br><span class="line">        root=preorder(<span class="number">0</span>,inorder.<span class="built_in">size</span>()<span class="number">-1</span>,<span class="number">0</span>,postorder.<span class="built_in">size</span>()<span class="number">-1</span>,inorder,postorder);</span><br><span class="line">        <span class="keyword">return</span> root;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//中序位置，后序位置，中序数组，后序数组</span></span><br><span class="line">    <span class="function">TreeNode* <span class="title">preorder</span><span class="params">(<span class="keyword">int</span> leftin,<span class="keyword">int</span> rightin,<span class="keyword">int</span> leftp,<span class="keyword">int</span> rightp,<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; in,<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; post)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="comment">//结束条件</span></span><br><span class="line">        <span class="keyword">if</span>(leftin&gt;rightin)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">        <span class="comment">//确定根的位置</span></span><br><span class="line">        TreeNode* root=<span class="keyword">new</span> TreeNode(post[rightp]);</span><br><span class="line">        <span class="keyword">int</span> rootin=leftin;</span><br><span class="line">        <span class="keyword">while</span>(rootin&lt;=rightin&amp;&amp;in[rootin]!=post[rightp])</span><br><span class="line">            rootin++;</span><br><span class="line">        <span class="keyword">int</span> left=rootin-leftin;</span><br><span class="line">        <span class="comment">//左子树位置 中序遍历去掉根，后序遍历去掉根</span></span><br><span class="line">        root-&gt;left=preorder(leftin, rootin<span class="number">-1</span>, leftp, leftp+left<span class="number">-1</span>, in, post);</span><br><span class="line">        <span class="comment">//右子树位置 中序遍历去掉根，后序遍历去掉根</span></span><br><span class="line">        root-&gt;right=preorder(rootin+<span class="number">1</span>, rightin, leftp+left, rightp<span class="number">-1</span>, in, post);</span><br><span class="line">        <span class="keyword">return</span> root;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：322-零钱兑换"><a href="#题目：322-零钱兑换" class="headerlink" title="题目：322. 零钱兑换"></a>题目：322. 零钱兑换</h2><p>描述：给定不同面额的硬币 coins 和一个总金额 amount。编写一个函数来计算可以凑成总金额所需的最少的硬币个数。如果没有任何一种硬币组合能组成总金额，返回 -1。</p><p>思路：</p><p>看到题目很容易想到贪心法，但是贪心法得到的结果不一定是最优解，如[1,7,10] 这种用例，按照贪心思路 10 + 1 + 1 + 1 + 1 会比 7 + 7 更早找到，因此还是需要将所有情况都遍历一遍。</p><p>（1）按照贪心法的思路，从大的面额开始取，因此首先将数组降序排列（利用反向迭代器）</p><p>（2）从第一个面额开始搜索，ans中记录需要的最少次数</p><p>（3）搜索时，从大到小，按照每个面额能取的最多的次数来取值，则最多取值为amount/当前面额，并且要保证之前取的次数加上现在取的次数要小于最少次数（进行剪枝）</p><p>下一次取值，就是将总价值减去当前取的次数 * 面额，继续遍历下一个小的面额。</p><p>（4）最后当总面值等于0的时候，取当前计数值和原始最小值的较小者返回，当数组遍历完也进行返回操作。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">coinChange</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; coins, <span class="keyword">int</span> amount)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//贪心法：每次都找面值最大的</span></span><br><span class="line">       <span class="comment">//但是贪心得到的解并不一定是最优的，因此还是需要全部递归一遍</span></span><br><span class="line">       <span class="keyword">if</span>(amount==<span class="number">0</span>)</span><br><span class="line">           <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">       </span><br><span class="line">       <span class="comment">//从大的开始</span></span><br><span class="line">       sort(coins.rbegin(), coins.rend());<span class="comment">//反向迭代器</span></span><br><span class="line">       <span class="keyword">int</span> ans=INT_MAX;</span><br><span class="line">       coinSearch(coins,amount,<span class="number">0</span>,<span class="number">0</span>,ans);</span><br><span class="line">       <span class="keyword">return</span> ans==INT_MAX?<span class="number">-1</span>:ans;</span><br><span class="line">   &#125;</span><br><span class="line">   </span><br><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">coinSearch</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; coins,<span class="keyword">int</span> amout,<span class="keyword">int</span> c_index,<span class="keyword">int</span> count,<span class="keyword">int</span> &amp;ans)</span></span></span><br><span class="line"><span class="function">   </span>&#123;</span><br><span class="line">       <span class="keyword">if</span>(amout==<span class="number">0</span>)</span><br><span class="line">       &#123;</span><br><span class="line">           ans=<span class="built_in">min</span>(ans,count);</span><br><span class="line">           <span class="keyword">return</span>;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">if</span>(c_index&gt;=coins.<span class="built_in">size</span>())</span><br><span class="line">           <span class="keyword">return</span>;</span><br><span class="line">       </span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> k=(<span class="keyword">int</span>)amout/coins[c_index];k&gt;=<span class="number">0</span>&amp;&amp;count+k&lt;ans;k--)</span><br><span class="line">       &#123;</span><br><span class="line">           coinSearch(coins, amout-k*coins[c_index], c_index+<span class="number">1</span>, count+k, ans);</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：530-二叉搜索树的最小绝对差、783-二叉搜索树节点最小距离"><a href="#题目：530-二叉搜索树的最小绝对差、783-二叉搜索树节点最小距离" class="headerlink" title="题目：530. 二叉搜索树的最小绝对差、783. 二叉搜索树节点最小距离"></a>题目：530. 二叉搜索树的最小绝对差、783. 二叉搜索树节点最小距离</h2><p>描述：1.给你一棵所有节点为非负值的二叉搜索树，请你计算树中任意两节点的差的绝对值的最小值。</p><p>2.给定一个二叉搜索树的根节点 root，返回树中任意两节点的差的最小值。</p><p>思路：看到二叉搜索树，由其性质可以想到要用中序搜索</p><p>（1）利用中序遍历时，能将树的节点按照从小到大进行排序的性质</p><p>（2）在中序遍历树过程中，计算相邻两个节点的差值，取最小值即可</p><p>注：可以设置一个pre前置节点存储上一个节点的值，用当前节点减去该节点的值与当前最小的差值比较，取较小者。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> res=INT_MAX;</span><br><span class="line">    <span class="built_in">stack</span>&lt;TreeNode*&gt; s;</span><br><span class="line">    TreeNode* p=root;</span><br><span class="line">    TreeNode* pre=<span class="literal">NULL</span>;</span><br><span class="line">    <span class="keyword">while</span>(p!=<span class="literal">NULL</span>||!s.empty())</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">while</span>(p!=<span class="literal">NULL</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            s.push(p);</span><br><span class="line">            p=p-&gt;left;</span><br><span class="line">        &#125;</span><br><span class="line">        p=s.top();</span><br><span class="line">        <span class="keyword">if</span>(pre!=<span class="literal">NULL</span>)</span><br><span class="line">            res=<span class="built_in">min</span>(res,p-&gt;val-pre-&gt;val);</span><br><span class="line">        pre=p;</span><br><span class="line">        s.pop();</span><br><span class="line">        </span><br><span class="line">        p=p-&gt;right;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：24-两两交换链表中的节点"><a href="#题目：24-两两交换链表中的节点" class="headerlink" title="题目：24. 两两交换链表中的节点"></a>题目：24. 两两交换链表中的节点</h2><p>描述：给定一个链表，两两交换其中相邻的节点，并返回交换后的链表。</p><p>你不能只是单纯的改变节点内部的值，而是需要实际的进行节点交换。</p><p>示例:</p><p>给定 1-&gt;2-&gt;3-&gt;4, 你应该返回 2-&gt;1-&gt;4-&gt;3.</p><p>思路：</p><p>使用三个指针分别指向前一个、当前、后一个节点</p><p>（1）当后一个节点为空，说明链表处理完毕，此时跳出循环</p><p>（2）每遇到一个节点，则将当前节点和后一个节点进行交换：</p><p>前一个节点的next指向后一个节点；</p><p>当前节点的next指向后一个节点的next；</p><p>后一个节点的next指向当前节点；</p><p>此时变换前为：pre-&gt;cur-&gt;next，变换后为：pre-&gt;next-&gt;cur</p><p>（3）不断循环，依次移动三个指针</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">ListNode* l3=<span class="keyword">new</span> ListNode(<span class="number">0</span>);</span><br><span class="line">    ListNode* result=l3;</span><br><span class="line">    <span class="keyword">if</span>(head==<span class="literal">NULL</span>) <span class="keyword">return</span> &#123;&#125;;</span><br><span class="line">    ListNode* pre=l3;</span><br><span class="line">    l3-&gt;next=head;</span><br><span class="line">    </span><br><span class="line">    ListNode* cur=head;</span><br><span class="line">    ListNode* next;<span class="comment">//=cur-&gt;next;//同理，不能现在给next赋值</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>(cur!=<span class="literal">NULL</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        next=cur-&gt;next;<span class="comment">//当cur到末尾了，next的赋值将毫无意义，会报错，因此把这句话放至上面，不能放在cur=cur-&gt;next后面</span></span><br><span class="line">        <span class="keyword">if</span>(next==<span class="literal">NULL</span>) <span class="keyword">break</span>;<span class="comment">//当next到末尾需跳出</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//交换</span></span><br><span class="line">        pre-&gt;next=next;</span><br><span class="line">        cur-&gt;next=next-&gt;next;</span><br><span class="line">        next-&gt;next=cur;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//不断循环</span></span><br><span class="line">        pre=cur;</span><br><span class="line">        cur=cur-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> result-&gt;next;</span><br></pre></td></tr></table></figure><h2 id="题目：406-根据身高重建队列"><a href="#题目：406-根据身高重建队列" class="headerlink" title="题目：406. 根据身高重建队列"></a>题目：406. 根据身高重建队列</h2><p>描述：假设有打乱顺序的一群人站成一个队列。 每个人由一个整数对(h, k)表示，其中h是这个人的身高，k是排在这个人前面且身高大于或等于h的人数。 编写一个算法来重建这个队列。</p><p>注意：<br>总人数少于1100人。</p><p>思路：每个人由（身高，比自己身高高的人数）整数对即（h,k）组成，当每个人的身高相同，则k的值代表该人在数组中的实际索引位置。</p><p>可以看出数组整体是按照身高排序的，再根据k的值调整每个人的位置。</p><p>（1）排序：利用sort中自定义比较方法，将数组按照身高降序排列，比自己身高高的人数升序排列。</p><p>（2）插入：按照最高的身高先插入结果中，插入的位置为res.begin()+当前的k值，res.begin()一直在动态变化，所以身高的降序排列保证了整体上是按照身高排列。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//1.排序，从最大的开始排序</span></span><br><span class="line">    sort(people.<span class="built_in">begin</span>(),people.<span class="built_in">end</span>(),[](<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; a,<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; b)</span><br><span class="line">         &#123;</span><br><span class="line">        <span class="keyword">if</span>(a[<span class="number">0</span>]&gt;b[<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">if</span>(a[<span class="number">0</span>]==b[<span class="number">0</span>]&amp;&amp;a[<span class="number">1</span>]&lt;b[<span class="number">1</span>])</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.插入，从从大的开始，根据前面比自己高的人数确定插入的位置</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">auto</span> e:people)</span><br><span class="line">    &#123;</span><br><span class="line">        res.insert(res.<span class="built_in">begin</span>()+e[<span class="number">1</span>], e);</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> res;</span><br></pre></td></tr></table></figure><h2 id="题目：1002-查找常用字符"><a href="#题目：1002-查找常用字符" class="headerlink" title="题目：1002. 查找常用字符"></a>题目：1002. 查找常用字符</h2><p>描述：给定仅有小写字母组成的字符串数组 A，返回列表中的每个字符串中都显示的全部字符（包括重复字符）组成的列表。例如，如果一个字符在每个字符串中出现 3 次，但不是 4 次，则需要在最终答案中包含该字符 3 次。</p><p>你可以按任意顺序返回答案。</p><p>思路：<strong><u>从范例入手基本的解题思路：</u></strong></p><p>假设字符数组中的字符串为{bella,label,roller}</p><p>则对于第一个字符串所有字符出现的次数</p><p>b 1  e 1  l 2  a 1</p><p>第二个字符串</p><p>l 2  a 1 b 1 e 1</p><p>第三个字符串</p><p>r 2 o 1 l 2 e 1</p><p>第一个和第二个重叠的字符为：b 1 e 1  l 2  a 1，该重叠部分和第三个字符串比较，重叠部分为： e 1 l2</p><p><u>总结：<strong>需要找的部分为每个字符串中字符出现次数的交集</strong></u></p><p><u><strong>注：当遇到字符问题时，可以将所有字符通过减去‘a’变成数字方便处理，最后仅需要通过数字+’a’即可变为字符</strong></u></p><p>（1）将第一个字符串中的字符转换为对应的26个数字中的一个，并计算出现次数</p><p>（2）从第二个字符串开始，计算其与上一个重叠部分字符中再次重叠的部分，即取出现次数最少的字符</p><p>（3）对于26个字符，当其有出现的次数时，通过字符对应数字+‘a’进行转换</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//每个字符串中字符出现次数的交集</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; res;</span><br><span class="line">    <span class="built_in">unordered_map</span>&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; <span class="built_in">map</span>;</span><br><span class="line">    <span class="built_in">unordered_map</span>&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; tmp;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">auto</span> c:A[<span class="number">0</span>])</span><br><span class="line">        <span class="built_in">map</span>[c-<span class="string">'a'</span>]++;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;A.<span class="built_in">size</span>();i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">auto</span> c:A[i])</span><br><span class="line">            tmp[c-<span class="string">'a'</span>]++;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> k=<span class="number">0</span>;k&lt;<span class="number">26</span>;k++)</span><br><span class="line">            <span class="built_in">map</span>[k]=<span class="built_in">min</span>(<span class="built_in">map</span>[k],tmp[k]);</span><br><span class="line">        tmp.<span class="built_in">clear</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="number">26</span>;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;<span class="built_in">map</span>[i];j++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="function"><span class="built_in">string</span> <span class="title">s</span><span class="params">(<span class="number">1</span>,i+<span class="string">'a'</span>)</span></span>;</span><br><span class="line">            res.push_back(s);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br></pre></td></tr></table></figure><h2 id="题目：116-填充每个节点的下一个右侧节点指针"><a href="#题目：116-填充每个节点的下一个右侧节点指针" class="headerlink" title="题目：116. 填充每个节点的下一个右侧节点指针"></a>题目：116. 填充每个节点的下一个右侧节点指针</h2><p>描述：给定一个完美二叉树，其所有叶子节点都在同一层，每个父节点都有两个子节点。二叉树定义如下：</p><p>struct Node {<br>  int val;<br>  Node *left;<br>  Node *right;<br>  Node *next;<br>}<br>填充它的每个 next 指针，让这个指针指向其下一个右侧节点。如果找不到下一个右侧节点，则将 next 指针设置为 NULL。</p><p>初始状态下，所有 next 指针都被设置为 NULL。</p><p>思路：</p><p>利用树的层次遍历</p><p>（1）从根节点开始入队</p><p>（2）每次将下一层的节点入队</p><p>（3）取队列第一个节点，除了最后一个元素，其他节点的next都指向后一个元素，最后一个元素的next指向null</p><p>（4）继续将下一层节点入队</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">//<span class="number">0.</span>处理为空的情况</span><br><span class="line">    <span class="keyword">if</span>(root==NULL) <span class="keyword">return</span> NULL;</span><br><span class="line">    //<span class="number">1.</span>不为空，建立队列，将根节点入队</span><br><span class="line">    queue&lt;Node*&gt; p;</span><br><span class="line">    p.push(root);</span><br><span class="line">    </span><br><span class="line">    //<span class="number">2.</span>每次入队的是同一层元素</span><br><span class="line">    <span class="keyword">while</span> (!p.empty())</span><br><span class="line">    &#123;</span><br><span class="line">        int width=p.size();</span><br><span class="line">        <span class="keyword">for</span>(int i=<span class="number">0</span>;i&lt;width;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            //<span class="number">3.</span>取当前节点</span><br><span class="line">            Node* cur=p.front();</span><br><span class="line">            p.pop();</span><br><span class="line">            //<span class="number">4.</span>除了最后一个元素，其他节点的next都指向后一个元素</span><br><span class="line">            <span class="keyword">if</span>(i&lt;width<span class="number">-1</span>)</span><br><span class="line">                cur-&gt;next=p.front();</span><br><span class="line">            //<span class="number">5.</span>最后一个元素的next指向null</span><br><span class="line">            <span class="keyword">if</span>(i==width<span class="number">-1</span>)</span><br><span class="line">                cur-&gt;next=NULL;</span><br><span class="line">            </span><br><span class="line">            //<span class="number">6.</span>将左右子树节点作为下一层需要处理的</span><br><span class="line">            if(cur-&gt;left)</span><br><span class="line">                p.push(cur-&gt;left);</span><br><span class="line">            if(cur-&gt;right)</span><br><span class="line">                p.push(cur-&gt;right);</span><br><span class="line">        &#125;</span><br><span class="line">        //res-&gt;next=NULL;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> root;</span><br></pre></td></tr></table></figure><h2 id="题目：437-路径总和-III"><a href="#题目：437-路径总和-III" class="headerlink" title="题目：437. 路径总和 III"></a>题目：437. 路径总和 III</h2><p>描述：给定一个二叉树，它的每个结点都存放着一个整数值。</p><p>找出路径和等于给定数值的路径总数。</p><p>路径不需要从根节点开始，也不需要在叶子节点结束，但是路径方向必须是向下的（只能从父节点到子节点）。</p><p>二叉树不超过1000个节点，且节点数值范围是 [-1000000,1000000] 的整数。</p><p>思路：</p><p>先序遍历每个节点，从当前遍历的节点出发找是否有满足条件的路径</p><p>（1）先序遍历树，当遇到当前节点，则从该节点出发</p><p>用一个临时存储和累加当前节点的值，不断递归加上左子树和右子树的值，遇到存储和和目标值相同，则计数加1；反之则减去当前节点的值，重新递归</p><p>（2）继续先序遍历左子树，重复第（1）步</p><p>（3）继续先序遍历左子树，重复第（1）步</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">int</span> num;</span><br><span class="line">    <span class="keyword">int</span> cur_sum;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">pathSum</span><span class="params">(TreeNode* root, <span class="keyword">int</span> sum)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//先序遍历得到每个节点，以该节点找是否有满足条件的路径</span></span><br><span class="line">        <span class="keyword">if</span>(root==<span class="literal">NULL</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        preorder(root,sum);</span><br><span class="line">        <span class="keyword">return</span> num;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">preorder</span><span class="params">(TreeNode* root,<span class="keyword">int</span> sum)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(root==<span class="literal">NULL</span>)</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        <span class="comment">//以当前节点找路径</span></span><br><span class="line">        cur_sum=<span class="number">0</span>;</span><br><span class="line">        dfs(root,sum);</span><br><span class="line">        <span class="comment">//左子树 找路径</span></span><br><span class="line">        preorder(root-&gt;left, sum);</span><br><span class="line">        <span class="comment">//右子树 找路径</span></span><br><span class="line">        preorder(root-&gt;right, sum);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(TreeNode* root,<span class="keyword">int</span> sum)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(root==<span class="literal">NULL</span>)</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">        cur_sum+=root-&gt;val;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span>(cur_sum==sum)</span><br><span class="line">            num++;</span><br><span class="line">        </span><br><span class="line">        dfs(root-&gt;left,sum);</span><br><span class="line">        dfs(root-&gt;right,sum);</span><br><span class="line">        </span><br><span class="line">        cur_sum-=root-&gt;val;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：977-有序数组的平方"><a href="#题目：977-有序数组的平方" class="headerlink" title="题目：977. 有序数组的平方"></a>题目：977. 有序数组的平方</h2><p>描述：给定一个按非递减顺序排序的整数数组 <code>A</code>，返回每个数字的平方组成的新数组，要求也按非递减顺序排序。</p><p>思路：</p><p>采用双指针的方法，使用两个指针分别指向原数组的首部和尾部，每次比较绝对值大的那个，将其平方插入到结果数组的目前结果的前面（从末尾开始插入），并移动该指针，重复上述比较过程。</p><p>（1）定义两个指针a,b指向原数组的首部和尾部，一个指针指向结果数组的尾部，每次都找到最大值来依次插入结果数组中</p><p>（2）比较a,b指针中绝对值大的数字，将该值的平方插入到结果数组的当前指针指向位置，同时移动比较过的数组的指针和结果数组的指针</p><p>（3）循环以上比较过程</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//双指针 从末尾开始插入</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res(A.<span class="built_in">size</span>());</span><br><span class="line">    <span class="keyword">int</span> left=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> right=A.<span class="built_in">size</span>()<span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">int</span> i=right;</span><br><span class="line">    <span class="keyword">while</span>(left&lt;=right)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(<span class="built_in">abs</span>(A[right]&gt;<span class="built_in">abs</span>(A[left])))</span><br><span class="line">        &#123;</span><br><span class="line">            res[i]=A[right]*A[right];</span><br><span class="line">            right--;</span><br><span class="line">            i--;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            res[i]=A[left]*A[left];</span><br><span class="line">            left++;</span><br><span class="line">            i--;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;题目：461-汉明距离&quot;&gt;&lt;a href=&quot;#题目：461-汉明距离&quot; class=&quot;headerlink&quot; title=&quot;题目：461. 汉明距离&quot;&gt;&lt;/a&gt;题目：461. 汉明距离&lt;/h2&gt;&lt;p&gt;描述：两个整数之间的汉明距离指的是这两个数字对应二进制位不同的位置的数目。给出两个整数 x 和 y，计算它们之间的汉明距离。&lt;/p&gt;&lt;p&gt;思路：&lt;/p&gt;&lt;p&gt;两个数的二进制位不同即可以使用x&amp;amp;1判断或者是x%2来判断数字的每一位&lt;/p&gt;&lt;p&gt;（1）对两个数对2取余来判断每一位的二进制数是否相同&lt;/p&gt;&lt;p&gt;（2）移动到下一位判断（使用除法或者是右移操作即可）&lt;/p&gt;
    
    </summary>
    
    
      <category term="算法" scheme="https://www.xiapf.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="https://www.xiapf.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>树回归</title>
    <link href="https://www.xiapf.com/blogs/treeRegression/"/>
    <id>https://www.xiapf.com/blogs/treeRegression/</id>
    <published>2020-10-14T06:55:11.000Z</published>
    <updated>2020-10-14T06:56:05.425Z</updated>
    
    <content type="html"><![CDATA[<p>当遇到非线性的数据的时候可以使用树回归，树回归可用于分类和回归</p><h2 id="回归树和模型树"><a href="#回归树和模型树" class="headerlink" title="回归树和模型树"></a>回归树和模型树</h2><h3 id="回归树"><a href="#回归树" class="headerlink" title="回归树"></a>回归树</h3><p>回归树的叶节点的模型使用的是分段常数</p><h3 id="模型树"><a href="#模型树" class="headerlink" title="模型树"></a>模型树</h3><p>模型树的叶节点的模型使用的是线性回归方程</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>两种树的构建方式都相同，只需要定义两种叶子节点的模型，根据情况进行修改即可。</p><h2 id="树的构建（CART）"><a href="#树的构建（CART）" class="headerlink" title="树的构建（CART）"></a>树的构建（CART）</h2><h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><p>决策树（ID3算法）每次用最佳特征来划分树，并且只能处理离散的特征，而树回归中（CART算法）可以处理连续的特征，当特征值大于定值就走左子树，反之走右子树</p><a id="more"></a><p><u><strong>回归树和模型树的区别在于划分函数中叶子模型的设置以及误差的计算</strong></u></p><p>（1）根据定值选择数据哪条子树</p><p>找到小于当前特征的所有样本（即所在行），将这些样本作为左子树的值，反之作为右子树的值 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitTwoData</span><span class="params">(dataSet,feature,value)</span>:</span></span><br><span class="line"><span class="comment">#np.nonzero返回非0元素所在的行和列</span></span><br><span class="line">mat0=dataSet[np.nonzero(dataSet[:,feature]&gt;value)[<span class="number">0</span>],:]</span><br><span class="line">mat1=dataSet[np.nonzero(dataSet[:,feature]&lt;=value)[<span class="number">0</span>],:]</span><br><span class="line"><span class="keyword">return</span> mat0,mat1</span><br></pre></td></tr></table></figure><p>（2）切分函数，选择最佳定值来进行切分</p><p>1.对每个特征的每个特征值，计算以当前特征的特征值划分的误差</p><p>2.如果该误差小于最小误差，则更新当前的误差以及特征和对应的特征值，如果划分的样本数量不符合设置，则继续重新划分</p><p>3.最终，得到最佳划分的特征和特征值时，需要检查是否满足前提设置（tols表示允许的误差下降值，toln最小分割的样本数）</p><p>计算总的误差，当其与最优划分时的误差值不满足设置，则按照整体作为一棵树进行返回，，如果划分的样本数量不符合设置，则继续重新划分</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestSplit</span><span class="params">(dataSet,leaf=regLeaf,error=regError,ops=<span class="params">(<span class="number">1</span>,<span class="number">4</span>)</span>)</span>:</span></span><br><span class="line"><span class="keyword">if</span> len(set(dataSet[:,<span class="number">-1</span>].T.tolist()[<span class="number">0</span>]))==<span class="number">1</span>:</span><br><span class="line"><span class="keyword">return</span> <span class="literal">None</span>,leaf(dataSet)</span><br><span class="line">tole=ops[<span class="number">0</span>]</span><br><span class="line">totm=ops[<span class="number">1</span>]</span><br><span class="line">bestfeature=<span class="number">-1</span></span><br><span class="line">bestvalues=<span class="number">-1</span></span><br><span class="line">bestS=float(<span class="string">'inf'</span>)</span><br><span class="line">S=error(dataSet)</span><br><span class="line"></span><br><span class="line">m,n=np.shape(dataSet)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n<span class="number">-1</span>):</span><br><span class="line"><span class="keyword">for</span> v <span class="keyword">in</span> set(dataSet[:,i].T.tolist()[<span class="number">0</span>]):</span><br><span class="line">mat0,mat1=splitTwoData(dataSet,i,v)</span><br><span class="line"><span class="keyword">if</span>(np.shape(mat0)[<span class="number">0</span>]&lt;totm <span class="keyword">or</span> np.shape(mat1)[<span class="number">0</span>]&lt;totm):</span><br><span class="line"><span class="keyword">continue</span></span><br><span class="line">newS=error(mat0)+error(mat1)</span><br><span class="line"><span class="keyword">if</span>(bestS&gt;newS):</span><br><span class="line">bestS=newS</span><br><span class="line">bestfeature=i</span><br><span class="line">bestvalues=v</span><br><span class="line"><span class="keyword">if</span>((S-bestS)&lt;tole):</span><br><span class="line"><span class="keyword">return</span> <span class="literal">None</span>,leaf(dataSet)</span><br><span class="line">mat0,mat1=splitTwoData(dataSet,bestfeature,bestvalues)</span><br><span class="line"><span class="keyword">if</span>(np.shape(mat0)[<span class="number">0</span>]&lt;totm <span class="keyword">or</span> np.shape(mat1)[<span class="number">0</span>]&lt;totm):</span><br><span class="line"><span class="keyword">return</span> <span class="literal">None</span>,leaf(dataSet)</span><br><span class="line"><span class="keyword">return</span> bestfeature,bestvalues</span><br></pre></td></tr></table></figure><p>（3）构建树</p><p>1.当没有要构建的特征的时候</p><p>​    直接返回叶子节点的值</p><p>2.当有特征的时候</p><p>​    选择最佳切分时的特征和对应的节点的定值，并构建为当前节点</p><p>3.构建左右子树</p><p>​    根据当前节点的定值，分别将小于该值的作为左子树，大于该值的作为右子树。继续递归构建左右子树</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTree</span><span class="params">(dataSet,leaf=regLeaf,error=regError,ops=<span class="params">(<span class="number">1</span>,<span class="number">4</span>)</span>)</span>:</span></span><br><span class="line"><span class="comment">#将数据集分为两部分</span></span><br><span class="line">feature,value=chooseBestSplit(dataSet,leaf,error,ops)</span><br><span class="line"></span><br><span class="line"><span class="comment">#树构建到末尾的时候</span></span><br><span class="line"><span class="keyword">if</span>(feature==<span class="literal">None</span>):</span><br><span class="line"><span class="keyword">return</span> value</span><br><span class="line">tree=&#123;&#125;</span><br><span class="line">tree[<span class="string">'feature'</span>]=feature</span><br><span class="line">tree[<span class="string">'value'</span>]=value</span><br><span class="line"></span><br><span class="line"><span class="comment">#左子树，右子树</span></span><br><span class="line">left,right=splitTwoData(dataSet,feature,value)</span><br><span class="line">tree[<span class="string">'left'</span>]=createTree(left,leaf,error,ops)</span><br><span class="line">tree[<span class="string">'right'</span>]=createTree(right,leaf,error,ops)</span><br><span class="line"><span class="keyword">return</span> tree</span><br></pre></td></tr></table></figure><h3 id="回归树-1"><a href="#回归树-1" class="headerlink" title="回归树"></a>回归树</h3><p>（1）叶子节点</p><p>用当前子树下的所有数据的均值作为叶子节点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#返回叶子节点的值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regLeaf</span><span class="params">(dataSet)</span>:</span></span><br><span class="line"><span class="keyword">return</span> np.mean(dataSet[:,<span class="number">-1</span>])</span><br></pre></td></tr></table></figure><p>（2）误差</p><p>使用数据的总方差，即平方误差的总值来计算连续型数值的混乱度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#返回所有节点的均值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regError</span><span class="params">(dataSet)</span>:</span></span><br><span class="line"><span class="keyword">return</span> np.var(dataSet[:,<span class="number">-1</span>])*np.shape(dataSet)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><h3 id="模型树-1"><a href="#模型树-1" class="headerlink" title="模型树"></a>模型树</h3><p>（1）叶子节点</p><p>叶子节点使用线性回归方程：利用ws=(xTx)-1 * x.T * y公式求得回归方程的参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linearRes</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">m,n=np.shape(dataSet)</span><br><span class="line">x=np.mat(np.ones((m,n)))</span><br><span class="line">y=np.mat(np.ones((m,<span class="number">1</span>)))</span><br><span class="line">x[:,<span class="number">1</span>:n]=dataSet[:,<span class="number">0</span>:n<span class="number">-1</span>]</span><br><span class="line">y=dataSet[:,<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">xtx=x.T*x</span><br><span class="line"><span class="keyword">if</span>(np.linalg.det(xtx)==<span class="number">0</span>):</span><br><span class="line"><span class="keyword">raise</span> NameError(<span class="string">'Matrix is not invertible'</span>)</span><br><span class="line">ws=np.linalg.inv(xtx)*(x.T*y)</span><br><span class="line"><span class="keyword">return</span> ws,x,y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">modelLeaf</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">ws,x,y=linearRes(dataSet)</span><br><span class="line"><span class="keyword">return</span> ws</span><br></pre></td></tr></table></figure><p>（2）误差</p><p>使用数据的总方差，即平方误差的总值来计算连续型数值的混乱度：这里需要通过叶子节点的回归方程计算预测值和实际值的差值来计算方差</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">modelErr</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">ws,x,y=linearRes(dataSet)</span><br><span class="line">yhat=x*ws</span><br><span class="line"><span class="keyword">return</span> np.sum(np.power(y-yhat,<span class="number">2</span>))</span><br></pre></td></tr></table></figure><h2 id="树的预测"><a href="#树的预测" class="headerlink" title="树的预测"></a>树的预测</h2><p>预测的流程：自顶向下遍历整棵树，当到达叶子节点的时候调用对应叶子节点的模型进行预测</p><h3 id="遍历整棵树"><a href="#遍历整棵树" class="headerlink" title="遍历整棵树"></a>遍历整棵树</h3><p>（1）遍历结束的条件</p><p>当到达叶子节点，则调用对应叶子节点预测函数进行预测</p><p>（2）遍历左子树</p><p>当需要预测的特征的值小于构建模型的特征值，则当左子树有数据时继续遍历左子树，反之则利用构建模型的左子树进行预测</p><p>（3）遍历右子树</p><p>当需要预测的特征的值大于构建模型的特征值，则当右子树有数据时继续遍历右子树，反之则利用构建模型的右子树进行预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#遍历整棵树，在叶子节点上进行预测</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">treeEval</span><span class="params">(tree,indata,modelEval=regTreeEval)</span>:</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> isTree(tree):</span><br><span class="line"><span class="keyword">return</span> modelEval(tree,indata)</span><br><span class="line"><span class="keyword">if</span>(indata[tree[<span class="string">'feature'</span>]]&gt;tree[<span class="string">'value'</span>]):</span><br><span class="line"><span class="keyword">if</span>(isTree(tree[<span class="string">'left'</span>])):</span><br><span class="line"><span class="keyword">return</span> treeEval(tree[<span class="string">'left'</span>],indata,modelEval)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">return</span> modelEval(tree[<span class="string">'left'</span>],indata)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">if</span>(isTree(tree[<span class="string">'right'</span>])):</span><br><span class="line"><span class="keyword">return</span> treeEval(tree[<span class="string">'right'</span>],indata,modelEval)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">return</span> modelEval(tree[<span class="string">'right'</span>],indata)</span><br></pre></td></tr></table></figure><h3 id="在叶子节点上预测"><a href="#在叶子节点上预测" class="headerlink" title="在叶子节点上预测"></a>在叶子节点上预测</h3><p>（1）回归树</p><p>因为回归树的叶子节点就是一个分段数值，所以直接返回当前叶子节点的值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#对回归树叶子节点进行预测</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regTreeEval</span><span class="params">(model,indata)</span>:</span></span><br><span class="line"><span class="keyword">return</span> float(model)</span><br></pre></td></tr></table></figure><p>（2）模型树</p><p>因为模型树的叶子节点时一个回归方程模型的参数，所以使用预测的数值乘以该参数得到预测值进行返回</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#对模型树叶子节点进行预测</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">modelTreeEval</span><span class="params">(model,indata)</span>:</span></span><br><span class="line">m,n=np.shape(indata)</span><br><span class="line">x=np.mat(np.ones((<span class="number">1</span>,n+<span class="number">1</span>)))</span><br><span class="line">x[:,<span class="number">1</span>:n+<span class="number">1</span>]=indata</span><br><span class="line"><span class="keyword">return</span> float(x*model)</span><br></pre></td></tr></table></figure><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>对每一个输入的测试数据调用之前训练好的树的模型进行预测，并记录每个样本的预测值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#对预测数据返回预测值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTreeEval</span><span class="params">(tree,testData,modelEval=regTreeEval)</span>:</span></span><br><span class="line">m=len(testData)</span><br><span class="line">yhat=np.mat(np.zeros((m,<span class="number">1</span>)))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">yhat[i,<span class="number">0</span>]=treeEval(tree,np.mat(testData[i]),modelEval)</span><br><span class="line"><span class="keyword">return</span> yhat</span><br></pre></td></tr></table></figure><p>使用相关系数来衡量模型的好坏</p><p>调用r=np.corrcoef(yhat,testMat[:,1],rowvar=0)[0,1]，利用numpy下的corrcoef函数，输入预测值和实际值，得到相关系数为：</p><p>0.9640852318222141<br>0.9760412191380623<br>0.9434684235674758</p><p>依次为回归树、模型树、线性回归，可见使用模型树的效果最好</p><h2 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a>剪枝</h2><h3 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h3><p>通过被切分的最小样本数和允许下降的最小误差是对树进行了一个预剪枝，但是树的构建对这两个参数的设置非常敏感</p><h3 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h3><p>（1）将数据集分为训练集和测试集</p><p>（2）使用训练集来构建足够复杂的树</p><p>（3）利用测试集进行剪枝</p><p>对已有的树来切分测试数据</p><p>1.当没有测试数据时，返回树中节点的平均值</p><p>2.当存在左子树或者右子树时</p><p>切分数据：按照已有树的特征和特征值切分测试数据为左子树数据和右子树数据</p><p>3.构造左子树</p><p>输入当前已有树的左子树和切分的左子树数据递归构建树</p><p>4.构造右子树</p><p>输入当前已有树的右子树和切分的右子树数据递归构建树</p><p>5.当到达叶子节点的时候</p><p>切分数据：按照已有树的特征和特征值切分测试数据为左节点数据和右节点数据</p><p>计算不合并左右节点的误差：即使用划分的左节点的值减去已有树的左节点，用划分的右节点的值减去已有树的右节点，得到不合并节点的误差</p><p>计算合并左右节点的误差：用当前的测试数据值减去合并已有树的左右节点的均值得到误差</p><p>如果合并误差小于不合并误差则返回合并已有树节点的值，反之返回来的树</p><p>不断递归对树进行剪枝</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># #切分测试数据来剪枝</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prune</span><span class="params">(tree,testData)</span>:</span></span><br><span class="line"><span class="keyword">if</span> np.shape(testData)[<span class="number">0</span>]==<span class="number">0</span>:</span><br><span class="line"><span class="keyword">return</span> getMean(tree)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> isTree(tree[<span class="string">'left'</span>]) <span class="keyword">or</span> isTree(tree[<span class="string">'right'</span>]):</span><br><span class="line">lset,rset=splitTwoData(testData,tree[<span class="string">'feature'</span>],tree[<span class="string">'value'</span>])</span><br><span class="line"><span class="comment">#如果没有到叶子节点，则继续寻找</span></span><br><span class="line"><span class="keyword">if</span> isTree(tree[<span class="string">'left'</span>]):</span><br><span class="line">tree[<span class="string">'left'</span>]=prune(tree[<span class="string">'left'</span>],lset)</span><br><span class="line"><span class="keyword">if</span> isTree(tree[<span class="string">'right'</span>]):</span><br><span class="line">tree[<span class="string">'right'</span>]=prune(tree[<span class="string">'right'</span>],rset)</span><br><span class="line"></span><br><span class="line"><span class="comment">#当到达了叶子节点,计算合并和非合并的误差</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> isTree(tree[<span class="string">'left'</span>]) <span class="keyword">and</span> <span class="keyword">not</span> isTree(tree[<span class="string">'right'</span>]):</span><br><span class="line">lset,rset=splitTwoData(testData,tree[<span class="string">'feature'</span>],tree[<span class="string">'value'</span>])</span><br><span class="line">nomergeErr=np.sum(np.power(lset[:,<span class="number">-1</span>]-tree[<span class="string">'left'</span>],<span class="number">2</span>))+np.sum(np.power(rset[:,<span class="number">-1</span>]-tree[<span class="string">'right'</span>],<span class="number">2</span>))</span><br><span class="line">treeMean=(tree[<span class="string">'left'</span>]+tree[<span class="string">'right'</span>])/<span class="number">2</span></span><br><span class="line">mergeErr=np.sum(np.power(testData[:,<span class="number">-1</span>]-treeMean,<span class="number">2</span>))</span><br><span class="line"><span class="keyword">if</span> mergeErr&lt;nomergeErr:</span><br><span class="line">print(<span class="string">'merge'</span>)</span><br><span class="line"><span class="keyword">return</span> treeMean</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">return</span> tree</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">return</span> tree</span><br></pre></td></tr></table></figure><h2 id="matplotlib画图-tkinter构建GUI"><a href="#matplotlib画图-tkinter构建GUI" class="headerlink" title="matplotlib画图+tkinter构建GUI"></a>matplotlib画图+tkinter构建GUI</h2><h3 id="整体流程"><a href="#整体流程" class="headerlink" title="整体流程"></a>整体流程</h3><p>（1）构建窗口GUI</p><p>使用tkinter下的Tk()构建一个窗口，窗口中可以设置相关组件，最后加上窗口的消息循环mainloop()，这样就构建了一个tkinter窗口</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tkinter <span class="keyword">import</span> *</span><br><span class="line">root=Tk()</span><br><span class="line"><span class="comment">#填充组件</span></span><br><span class="line">...</span><br><span class="line">root.mainloop()</span><br></pre></td></tr></table></figure><p>（2）填充组件</p><p>Label组件：用于显示文本，传入需要填充的窗口root，需要显示的文本，最后利用grid()，输入需要填充的位置（行和列）</p><p>Button组件：按钮，传入需要填充的窗口root，需要显示的文本，点击按钮触发的事件（command），最后利用grid()，输入需要填充的位置（行和列）</p><p>Checkbutton组件：复选框，传入需要填充的窗口root，需要显示的文本，按钮的整数值（variable），最后利用grid()，输入需要填充的位置（行和列）</p><p>Entry组件：用于输入文本，传入需要填充的窗口root，最后利用grid()，输入需要填充的位置（行和列）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Label(root,text=<span class="string">'tolN'</span>).grid(row=<span class="number">1</span>,column=<span class="number">0</span>)</span><br><span class="line">entryN=Entry(root)</span><br><span class="line">entryN.grid(row=<span class="number">1</span>,column=<span class="number">1</span>)</span><br><span class="line">entryN.insert(<span class="number">0</span>,<span class="string">'10'</span>)</span><br><span class="line"></span><br><span class="line">Label(root,text=<span class="string">'tolS'</span>).grid(row=<span class="number">2</span>,column=<span class="number">0</span>)</span><br><span class="line">entryS=Entry(root)</span><br><span class="line">entryS.grid(row=<span class="number">2</span>,column=<span class="number">1</span>)</span><br><span class="line">entryS.insert(<span class="number">0</span>,<span class="string">'1.0'</span>)</span><br><span class="line"></span><br><span class="line">Button(root,text=<span class="string">'ReDraw'</span>,command=drawNewTree).grid(row=<span class="number">1</span>,column=<span class="number">2</span>,rowspan=<span class="number">3</span>)</span><br><span class="line">Button(root,text=<span class="string">'Quit'</span>,fg=<span class="string">'black'</span>,command=root.quit).grid(row=<span class="number">1</span>,column=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">var=IntVar()<span class="comment">#按钮整数值</span></span><br><span class="line">check=Checkbutton(root,text=<span class="string">'model tree'</span></span><br></pre></td></tr></table></figure><p>（3）结合matplotlib画图</p><p>为了让matplotlib在tkinter创建的GUI上画图，需要修改matplotlib的后端，并且在Tk上放一个画布用于画图</p><p>1.修改matplotlib的后端</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line">matplotlib.use(<span class="string">'TkAgg'</span>)</span><br><span class="line"><span class="keyword">from</span> matplotlib.backends.backend_tkagg <span class="keyword">import</span> FigureCanvasTkAgg</span><br><span class="line"><span class="keyword">from</span> matplotlib.figure <span class="keyword">import</span> Figure</span><br></pre></td></tr></table></figure><p>2.在Tk上放一个画布，使用grid来控制位置</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">reDraw.f=Figure(figsize=(<span class="number">5</span>,<span class="number">4</span>),dpi=<span class="number">100</span>)</span><br><span class="line">reDraw.canvas=FigureCanvasTkAgg(reDraw.f,master=root)</span><br><span class="line">reDraw.canvas.draw()</span><br><span class="line">reDraw.canvas.get_tk_widget().grid(row=<span class="number">0</span>,columnspan=<span class="number">3</span>)</span><br></pre></td></tr></table></figure><h3 id="可视化结果"><a href="#可视化结果" class="headerlink" title="可视化结果"></a>可视化结果</h3><p>（1）定义画图的函数</p><p>定义画图函数的全局变量：传入的原始数据集以及测试数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建与reDraw相关的全局变量</span></span><br><span class="line">reDraw.rawDat=np.mat(regTrees.createDataset(<span class="string">'./sine.txt'</span>))</span><br><span class="line">reDraw.testDat=np.arange(min(reDraw.rawDat[:,<span class="number">0</span>]),max(reDraw.rawDat[:,<span class="number">0</span>]),<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure><p>清除原始图，重新构建新的子图（防止画图过程中图的重叠）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">reDraw.f.clf()</span><br><span class="line">reDraw.a=reDraw.f.add_subplot(<span class="number">111</span>)</span><br></pre></td></tr></table></figure><p>利用scatter画散点图，plot画线，draw()用于图像的显示</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">reDraw.a.scatter(np.array(reDraw.rawDat[:,<span class="number">0</span>]),np.array(reDraw.rawDat[:,<span class="number">1</span>]),s=<span class="number">5</span>)</span><br><span class="line">reDraw.a.plot(reDraw.testDat,yhat,linewidth=<span class="number">2</span>)</span><br><span class="line">reDraw.canvas.draw()</span><br></pre></td></tr></table></figure><p>（2）结果显示</p><p>当勾选选择模型树的复选框的时候，调用模型树，反之调用回归树</p><p>输入最小分割的样本数为10，允许的误差下降值为1.0</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20201013160749.png" alt></p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20201013160807.png" alt></p><p>从结果可以看出当使用模型树拟合效果更好些</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;当遇到非线性的数据的时候可以使用树回归，树回归可用于分类和回归&lt;/p&gt;&lt;h2 id=&quot;回归树和模型树&quot;&gt;&lt;a href=&quot;#回归树和模型树&quot; class=&quot;headerlink&quot; title=&quot;回归树和模型树&quot;&gt;&lt;/a&gt;回归树和模型树&lt;/h2&gt;&lt;h3 id=&quot;回归树&quot;&gt;&lt;a href=&quot;#回归树&quot; class=&quot;headerlink&quot; title=&quot;回归树&quot;&gt;&lt;/a&gt;回归树&lt;/h3&gt;&lt;p&gt;回归树的叶节点的模型使用的是分段常数&lt;/p&gt;&lt;h3 id=&quot;模型树&quot;&gt;&lt;a href=&quot;#模型树&quot; class=&quot;headerlink&quot; title=&quot;模型树&quot;&gt;&lt;/a&gt;模型树&lt;/h3&gt;&lt;p&gt;模型树的叶节点的模型使用的是线性回归方程&lt;/p&gt;&lt;h3 id=&quot;总结&quot;&gt;&lt;a href=&quot;#总结&quot; class=&quot;headerlink&quot; title=&quot;总结&quot;&gt;&lt;/a&gt;总结&lt;/h3&gt;&lt;p&gt;两种树的构建方式都相同，只需要定义两种叶子节点的模型，根据情况进行修改即可。&lt;/p&gt;&lt;h2 id=&quot;树的构建（CART）&quot;&gt;&lt;a href=&quot;#树的构建（CART）&quot; class=&quot;headerlink&quot; title=&quot;树的构建（CART）&quot;&gt;&lt;/a&gt;树的构建（CART）&lt;/h2&gt;&lt;h3 id=&quot;算法流程&quot;&gt;&lt;a href=&quot;#算法流程&quot; class=&quot;headerlink&quot; title=&quot;算法流程&quot;&gt;&lt;/a&gt;算法流程&lt;/h3&gt;&lt;p&gt;决策树（ID3算法）每次用最佳特征来划分树，并且只能处理离散的特征，而树回归中（CART算法）可以处理连续的特征，当特征值大于定值就走左子树，反之走右子树&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="回归" scheme="https://www.xiapf.com/tags/%E5%9B%9E%E5%BD%92/"/>
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="分类" scheme="https://www.xiapf.com/tags/%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>决策树</title>
    <link href="https://www.xiapf.com/blogs/decisionTree/"/>
    <id>https://www.xiapf.com/blogs/decisionTree/</id>
    <published>2020-09-23T07:17:37.000Z</published>
    <updated>2020-09-23T07:19:35.133Z</updated>
    
    <content type="html"><![CDATA[<h2 id="分类器含义"><a href="#分类器含义" class="headerlink" title="分类器含义"></a>分类器含义</h2><p>决策树分类器就是一个带有终止块的流程图，终止块表示分类结果。</p><p>决策树的过程就是根据特征对应的标签中，选择最佳的特征来划分数据集，不停的划分，使得数据集中的数据属于同一个分类，在这过程中构造出的树，根据其分支可以进行决策。</p><h2 id="流程（ID3算法思想）"><a href="#流程（ID3算法思想）" class="headerlink" title="流程（ID3算法思想）"></a>流程（ID3算法思想）</h2><h3 id="测量给定数据的不一致性，即熵"><a href="#测量给定数据的不一致性，即熵" class="headerlink" title="测量给定数据的不一致性，即熵"></a>测量给定数据的不一致性，即熵</h3><p>计算熵的公式：计算所有类别出现的期望值</p><a id="more"></a><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200923140629.png" alt></p><p>（1）计算所有类别出现的次数</p><p>（2）类别出现的概率：当前类别出现的次数/数据总长度</p><p>（3）熵的计算：利用公式代入类别值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#计算给定数据的香农熵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calShanoEnt</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">numEntries=len(dataSet)</span><br><span class="line">labelCounts=&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> curVec <span class="keyword">in</span> dataSet:</span><br><span class="line">curlabel=curVec[<span class="number">-1</span>]</span><br><span class="line"><span class="keyword">if</span> curlabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys():</span><br><span class="line">labelCounts[curlabel]=<span class="number">0</span></span><br><span class="line">labelCounts[curlabel]+=<span class="number">1</span></span><br><span class="line">shanoEnt=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> labelCounts:</span><br><span class="line">value=labelCounts[key]</span><br><span class="line">prob=float(value)/numEntries</span><br><span class="line">shanoEnt-=prob*np.log2(prob)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> shanoEnt</span><br></pre></td></tr></table></figure><h3 id="根据给定方案划分数据集"><a href="#根据给定方案划分数据集" class="headerlink" title="根据给定方案划分数据集"></a>根据给定方案划分数据集</h3><p>对于按照给定特征，给定特征的取值来划分数据集：</p><p>对数据集的每一行，当特征对应值等于给定值的时候，将该列除了选定的那列特征值外保存在新的数据中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#按照给定特征划分数据集</span></span><br><span class="line"><span class="comment">#数据集，划分数据集的特征，特征的返回值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitDataSet</span><span class="params">(dataSet,axis,value)</span>:</span></span><br><span class="line">returnData=[]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> curVec <span class="keyword">in</span> dataSet:</span><br><span class="line"><span class="keyword">if</span>(curVec[axis]==value):</span><br><span class="line">extractData=curVec[:axis]</span><br><span class="line">extractData.extend(curVec[axis+<span class="number">1</span>:])</span><br><span class="line">returnData.append(extractData)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> returnData</span><br></pre></td></tr></table></figure><h3 id="选择最好的数据划分方式"><a href="#选择最好的数据划分方式" class="headerlink" title="选择最好的数据划分方式"></a>选择最好的数据划分方式</h3><p>（1）将最后一列的标签去除，得到所有的特征，并计算所有数据的信息熵（用于衡量数据的不一致性）</p><p>（2）对每个特征得到其特征下的取值，并求得该划分结果下的信息熵，并进行累加，得到每个特征划分下的信息增益，选择最大的信息增益对应的特征最为最好的数据划分。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">#选择最好的数据划分方式 通过度量数据集的信息熵</span><br><span class="line">def chooseBestFeatureToSplit(dataSet):</span><br><span class="line">#选择所有的特征</span><br><span class="line">numFeature=len(dataSet[<span class="number">0</span>])<span class="number">-1</span></span><br><span class="line">allShanoEnt=calShanoEnt(dataSet)</span><br><span class="line">bestEntropy=<span class="number">0</span></span><br><span class="line">bestFeature=<span class="number">-1</span></span><br><span class="line"></span><br><span class="line">#得到每个特征下的取值</span><br><span class="line"><span class="keyword">for</span> i in range(numFeature):</span><br><span class="line">feature=[example[i] <span class="keyword">for</span> example in dataSet]</span><br><span class="line">setFeature=<span class="built_in">set</span>(feature)</span><br><span class="line">entropy=<span class="number">0</span></span><br><span class="line">#每种划分方式下的信息熵</span><br><span class="line"><span class="keyword">for</span> value in setFeature:</span><br><span class="line">returnData=splitDataSet(dataSet,i,value)</span><br><span class="line">prob=<span class="keyword">float</span>(len(returnData)/len(dataSet))</span><br><span class="line">entropy+=prob*calShanoEnt(returnData)#信息熵</span><br><span class="line">entropyinfo=allShanoEnt-entropy</span><br><span class="line">#得到最好的信息增益</span><br><span class="line"><span class="keyword">if</span>(entropyinfo&gt;bestEntropy):</span><br><span class="line">bestEntropy=entropyinfo</span><br><span class="line">bestFeature=i</span><br><span class="line"><span class="keyword">return</span> bestFeature</span><br></pre></td></tr></table></figure><h3 id="构建决策树"><a href="#构建决策树" class="headerlink" title="构建决策树"></a>构建决策树</h3><p>输入需要分类的数据（包含数据的特征及最终的分类结果）和数据的特征对应的标签，通过递归的方法来构建树</p><p>（1）将每次分类的结果单独保存</p><p>（2）递归结束的条件：</p><p>当最终的分类的结果只有一个结果，即到达了叶子节点，此时返回分类的结果</p><p>当遍历完所有的特征，但是叶子节点的标签不是唯一的，则直接返回次数最多的类标签给他，作为其分类：采用多数投票的方式，计算所有标签的次数，利用sorted对标签内容排序，取第一个标签，即出现次数最多的返回。</p><p>（3）继续递归的条件：</p><p>1°选择最佳划分数据的方式，得到划分数据的特征的位置，得到特征对应的属性内容，同时在属性内容中删除该特征，下次不再分类</p><p>2°根据特征的属性内容建立树</p><p>3°得到当前划分数据集特征的所有数值，并去掉重复值</p><p>构造分支：根据特征的属性内容下的分支值（即数据集特征值），按照最佳分割的数据集，继续构造剩余的树</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#当处理完所有的特征，类标签还不是唯一的，此时采用多数表决的方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">majorityVote</span><span class="params">(classList)</span>:</span></span><br><span class="line">classCount=&#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> classList:</span><br><span class="line"><span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> classCount.keys():</span><br><span class="line">classCount[i]=<span class="number">0</span></span><br><span class="line">classCount[i]+=<span class="number">1</span></span><br><span class="line">sortedClassCount=sorted(classCount.iteritems(),key=operator.itemgetter(<span class="number">1</span>),reverse=<span class="literal">True</span>)<span class="comment">#降序</span></span><br><span class="line"><span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#构建树</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTree</span><span class="params">(dataSet,labels)</span>:</span></span><br><span class="line">classList=[example[<span class="number">-1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line"></span><br><span class="line"><span class="comment">#类别完全相同时停止分类</span></span><br><span class="line"><span class="comment">#if(classList.count(classList[0])==len(classList)):</span></span><br><span class="line"><span class="comment">#return classList[0]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#遍历完所有特征，返回出现次数最多的类标</span></span><br><span class="line"><span class="keyword">if</span>(len(dataSet[<span class="number">0</span>])==<span class="number">1</span>):</span><br><span class="line"><span class="keyword">return</span> majorityVote(classList)</span><br><span class="line"></span><br><span class="line">bestFeature=chooseBestFeatureToSplit(dataSet)</span><br><span class="line">bestLabel=labels[bestFeature]</span><br><span class="line"><span class="keyword">del</span>(labels[bestFeature])<span class="comment">#下次不再对着特征分类</span></span><br><span class="line"></span><br><span class="line">mytree=&#123;bestLabel:&#123;&#125;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#选择最佳特征对应的值</span></span><br><span class="line">feature=[example[bestFeature] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">setFeature=set(feature)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> value <span class="keyword">in</span> setFeature:</span><br><span class="line">sublabel=labels[:]</span><br><span class="line">mytree[bestLabel][value]=createTree(splitDataSet(dataSet,bestFeature,value),sublabel)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> mytree</span><br></pre></td></tr></table></figure><p>导入隐形眼镜数据集，数据集中包含眼部的状态和医生推荐的类型</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200923144626.jpg" alt></p><p>眼部的状态对应的标签为：[‘age’,’prescript’,’astigmatic’,’tearRate’]</p><p>输入数据集及特征对应的标签，最终创建的决策树为：（利用annotate绘制文本注解，来显示决策树）</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200923144601.png" alt></p><h2 id="保存与载入决策树"><a href="#保存与载入决策树" class="headerlink" title="保存与载入决策树"></a>保存与载入决策树</h2><p>1.保存决策树</p><p>使用pickle中的dump将决策树保存在对应文件中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stroeTree</span><span class="params">(tree,filename)</span>:</span></span><br><span class="line">fw=open(filename,<span class="string">'wb'</span>)</span><br><span class="line">pickle.dump(tree,fw)</span><br><span class="line">fw.close()</span><br></pre></td></tr></table></figure><p>2.载入决策树</p><p>使用pickle中的load导入之前保存的决策树</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">restoreTree</span><span class="params">(filename)</span>:</span></span><br><span class="line">fr=open(filename,<span class="string">'rb'</span>)</span><br><span class="line"><span class="keyword">return</span> pickle.load(fr)</span><br></pre></td></tr></table></figure><h2 id="利用决策树进行分类"><a href="#利用决策树进行分类" class="headerlink" title="利用决策树进行分类"></a>利用决策树进行分类</h2><p>输入对应的决策树，决策树特征对应的标签，数据特征的值，使用递归的方法找到对应树枝下的分类：</p><p>（1）利用next(iter)找到当前遍历的树的部分的根节点</p><p>（2）找到根节点下的分支，并定位根节点在特征标签中对应的值</p><p>（3）找到在该根节点下，测试数据对应的分支，当分支是字典类型，则需要继续按照该字典下对应的树查找标签，当该分支下就是标签则立即返回，该值就是测试数据的标签。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用决策树进行分类</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(input,featurelabel,test)</span>:</span></span><br><span class="line">first=next(iter(input))</span><br><span class="line">second=input[first]</span><br><span class="line"></span><br><span class="line">featureindex=featurelabel.index(first)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> second.keys():</span><br><span class="line"><span class="keyword">if</span>(test[featureindex]==key):</span><br><span class="line"><span class="keyword">if</span>(type(second[key]).__name__==<span class="string">"dict"</span>):</span><br><span class="line">classlabel=classify(second[key],featurelabel,test)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">classlabel=second[key]</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> classlabel</span><br></pre></td></tr></table></figure><p>输入：[‘presbyopic’,’hyper’,’no’,’normal’]进行测试，输出预测的结果为soft，查看原决策树，预测结果无误。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200923151617.jpg" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;分类器含义&quot;&gt;&lt;a href=&quot;#分类器含义&quot; class=&quot;headerlink&quot; title=&quot;分类器含义&quot;&gt;&lt;/a&gt;分类器含义&lt;/h2&gt;&lt;p&gt;决策树分类器就是一个带有终止块的流程图，终止块表示分类结果。&lt;/p&gt;&lt;p&gt;决策树的过程就是根据特征对应的标签中，选择最佳的特征来划分数据集，不停的划分，使得数据集中的数据属于同一个分类，在这过程中构造出的树，根据其分支可以进行决策。&lt;/p&gt;&lt;h2 id=&quot;流程（ID3算法思想）&quot;&gt;&lt;a href=&quot;#流程（ID3算法思想）&quot; class=&quot;headerlink&quot; title=&quot;流程（ID3算法思想）&quot;&gt;&lt;/a&gt;流程（ID3算法思想）&lt;/h2&gt;&lt;h3 id=&quot;测量给定数据的不一致性，即熵&quot;&gt;&lt;a href=&quot;#测量给定数据的不一致性，即熵&quot; class=&quot;headerlink&quot; title=&quot;测量给定数据的不一致性，即熵&quot;&gt;&lt;/a&gt;测量给定数据的不一致性，即熵&lt;/h3&gt;&lt;p&gt;计算熵的公式：计算所有类别出现的期望值&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="分类" scheme="https://www.xiapf.com/tags/%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>Leecode做题记录（三）</title>
    <link href="https://www.xiapf.com/blogs/LC3/"/>
    <id>https://www.xiapf.com/blogs/LC3/</id>
    <published>2020-09-23T07:17:29.000Z</published>
    <updated>2020-09-23T07:18:31.197Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目：287-寻找重复数"><a href="#题目：287-寻找重复数" class="headerlink" title="题目：287. 寻找重复数"></a>题目：287. 寻找重复数</h2><p>描述：给定一个包含 n + 1 个整数的数组 nums，其数字都在 1 到 n 之间（包括 1 和 n），可知至少存在一个重复的整数。假设只有一个重复的整数，找出这个重复的数</p><p>说明：</p><p>不能更改原数组（假设数组是只读的）。<br>只能使用额外的 O(1) 的空间。<br>时间复杂度小于 O(n2) 。<br>数组中只有一个重复的数字，但它可能不止重复出现一次。</p><a id="more"></a><p>思路：</p><p>不能更改原数组则不能使用排序算法，只能使用O（1）空间则不能使用哈希表。</p><p>因此想到在链表中找环：</p><p>（1）建立索引到数组的映射</p><p>（2）快指针走两步，慢指针走一步</p><p>（3）则到找到相遇的地方时，将一个指针指向一开始，两个指向首位和相遇位置的指针分别走一步，当值相等的地方则是环的入口，则是数组中重复的位置</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">findDuplicate</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//建立n-&gt;f(n)的映射</span></span><br><span class="line">        <span class="keyword">int</span> res;</span><br><span class="line">        <span class="keyword">int</span> slow=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> fast=<span class="number">0</span>;</span><br><span class="line">        slow=nums[slow];</span><br><span class="line">        fast=nums[nums[fast]];</span><br><span class="line">        <span class="comment">//fast比slow快两步</span></span><br><span class="line">        <span class="keyword">while</span>(slow!=fast)</span><br><span class="line">        &#123;</span><br><span class="line">            slow=nums[slow];</span><br><span class="line">            fast=nums[nums[fast]];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> pre1=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> pre2=slow;</span><br><span class="line">        <span class="keyword">while</span>(pre1!=pre2)</span><br><span class="line">        &#123;</span><br><span class="line">            pre1=nums[pre1];</span><br><span class="line">            pre2=nums[pre2];</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        res=pre1;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：617-合并二叉树"><a href="#题目：617-合并二叉树" class="headerlink" title="题目：617. 合并二叉树"></a>题目：617. 合并二叉树</h2><p>描述：给定两个二叉树，想象当你将它们中的一个覆盖到另一个上时，两个二叉树的一些节点便会重叠。你需要将他们合并为一个新的二叉树。合并的规则是如果两个节点重叠，那么将他们的值相加作为节点合并后的新值，否则不为 NULL 的节点将直接作为新二叉树的节点。</p><p>思路：</p><p>遇到二叉树主要是从左右子树考虑即可：</p><p>（1）当遇到的t1和t2其中一棵树不为空的时候，就将两者的节点的值相加（如果有一棵树的当前节点为空，则设置为0）</p><p>（2）接着，对新树的左子树处理，将t1的左子树和t2的左子树合并为新树的左子树（如果有一棵树的当前节点为空，则需要设置为NULL）</p><p>（3）接着，对新树的右子树处理，将t1的右子树和t2的右子树合并为新树的右子树（如果有一棵树的当前节点为空，则需要设置为NULL）</p><p>最后，</p><p>当t1和t2的当前节点都为空，说明两棵树已经合并完毕，此时递归结束。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">TreeNode* <span class="title">mergeTrees</span><span class="params">(TreeNode* t1, TreeNode* t2)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//合并两棵树</span></span><br><span class="line">       <span class="keyword">if</span>(t1==<span class="literal">NULL</span>&amp;&amp;t2==<span class="literal">NULL</span>)</span><br><span class="line">           <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">       </span><br><span class="line">       TreeNode* tmp=<span class="keyword">new</span> TreeNode(<span class="number">-1</span>);</span><br><span class="line">       </span><br><span class="line">       <span class="keyword">int</span> a=t1!=<span class="literal">NULL</span>?t1-&gt;val:<span class="number">0</span>;</span><br><span class="line">       <span class="keyword">int</span> b=t2!=<span class="literal">NULL</span>?t2-&gt;val:<span class="number">0</span>;</span><br><span class="line">       tmp-&gt;val=a+b;</span><br><span class="line">       tmp-&gt;left=mergeTrees(t1!=<span class="literal">NULL</span>?t1-&gt;left:<span class="literal">NULL</span>, t2!=<span class="literal">NULL</span>?t2-&gt;left:<span class="literal">NULL</span>);</span><br><span class="line">       tmp-&gt;right=mergeTrees(t1!=<span class="literal">NULL</span>?t1-&gt;right:<span class="literal">NULL</span>, t2!=<span class="literal">NULL</span>?t2-&gt;right:<span class="literal">NULL</span>);</span><br><span class="line">       </span><br><span class="line">       <span class="keyword">return</span> tmp;</span><br><span class="line"></span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：538-把二叉搜索树转换为累加树"><a href="#题目：538-把二叉搜索树转换为累加树" class="headerlink" title="题目：538. 把二叉搜索树转换为累加树"></a>题目：538. 把二叉搜索树转换为累加树</h2><p>描述：给定一个二叉搜索树（Binary Search Tree），把它转换成为累加树（Greater Tree)，使得每个节点的值是原来的节点值加上所有大于它的节点值之和。</p><p>思路：</p><p>当中序遍历一棵树的时候，节点时按照从小到大的顺序进行排列，当一个节点需要加上所有大于它节点的值，只需要反向中序遍历一棵树，将当前的累加值加给当前的节点，即为大于该节点的累加值。</p><p>（1）反向中序遍历，先遍历树的右子树，找到最右边的节点</p><p>（2）将当前累加值加给当前的节点，则累加值也需要更新为当前节点的值（因为之前的累加值+当前节点的值=累计值）</p><p>（3）遍历树的左子树，以此循环</p><p>最后，</p><p>当树的节点为空，则遍历结束，即递归结束。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">int</span> num=<span class="number">0</span>;<span class="comment">//累加值</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">TreeNode* <span class="title">convertBST</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//中序遍历逆序计算结果，并加给对应点</span></span><br><span class="line">        <span class="keyword">if</span>(root==<span class="literal">NULL</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">        convertBST(root-&gt;right);</span><br><span class="line">        root-&gt;val+=num;</span><br><span class="line">        num=root-&gt;val;</span><br><span class="line">        convertBST(root-&gt;left);</span><br><span class="line">        <span class="keyword">return</span> root;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="题目：37-解数独"><a href="#题目：37-解数独" class="headerlink" title="题目：37. 解数独"></a>题目：37. 解数独</h2><p>描述：编写一个程序，通过已填充的空格来解决数独问题。</p><p>一个数独的解法需遵循如下规则：</p><p>数字 1-9 在每一行只能出现一次。<br>数字 1-9 在每一列只能出现一次。<br>数字 1-9 在每一个以粗实线分隔的 3x3 宫内只能出现一次。<br>空白格用 ‘.’ 表示。</p><p>思路：</p><p>（1）在每个位置（每行每列）摆放1 ~ 9 的数字</p><p>（2）验证该数字能否摆放在棋盘格的当前位置上</p><p>验证方法：当前行里没有该数字；当前列里没有该数字；所属的小方格里没有该数字（按照行数对应方格，按照列数对应方格），则能摆放，有一个不满足，则不能摆放当前的位置</p><p>（3）不断递归摆放数字，如果最终能摆放完全，则返回该结果</p><p>（4）不能摆放完全，则当前摆放的数字设置为‘.’(空)，重新摆放新的数字（即为回溯）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">solveSudoku</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">char</span>&gt;&gt;&amp; board)</span> </span>&#123;</span><br><span class="line">      backTrack(board);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">bool</span> <span class="title">backTrack</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">char</span>&gt;&gt;&amp; board)</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">      <span class="comment">//按照每个摆放的位置进行判断</span></span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;board.<span class="built_in">size</span>();i++)</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;board[<span class="number">0</span>].<span class="built_in">size</span>();j++)</span><br><span class="line">          &#123;</span><br><span class="line">              <span class="keyword">if</span>(board[i][j]==<span class="string">'.'</span>)</span><br><span class="line">              &#123;</span><br><span class="line">                  <span class="keyword">for</span>(<span class="keyword">char</span> k=<span class="string">'1'</span>;k&lt;=<span class="string">'9'</span>;k++)</span><br><span class="line">                  &#123;</span><br><span class="line">                      <span class="keyword">if</span>(isSuitable(i,j,k,board))</span><br><span class="line">                      &#123;</span><br><span class="line">                          <span class="comment">//回溯</span></span><br><span class="line">                          board[i][j]=k;</span><br><span class="line">                          <span class="keyword">if</span>(backTrack(board))</span><br><span class="line">                              <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">                          board[i][j]=<span class="string">'.'</span>;</span><br><span class="line">                      &#125;</span><br><span class="line">                  &#125;</span><br><span class="line">                  <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">bool</span> <span class="title">isSuitable</span><span class="params">(<span class="keyword">int</span> row,<span class="keyword">int</span> col,<span class="keyword">char</span> k,<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">char</span>&gt;&gt;&amp; board)</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">      <span class="comment">//当前行</span></span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="number">9</span>;i++)</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="keyword">if</span>(board[row][i]==k)</span><br><span class="line">              <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">//当前列</span></span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="number">9</span>;i++)</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="keyword">if</span>(board[i][col]==k)</span><br><span class="line">              <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">//当前小方格</span></span><br><span class="line">      <span class="keyword">int</span> squareRow=(row/<span class="number">3</span>)*<span class="number">3</span>;</span><br><span class="line">      <span class="keyword">int</span> squareCol=(col/<span class="number">3</span>)*<span class="number">3</span>;</span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> i=squareRow;i&lt;<span class="number">3</span>+squareRow;i++)</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="keyword">for</span>(<span class="keyword">int</span> j=squareCol;j&lt;<span class="number">3</span>+squareCol;j++)</span><br><span class="line">              <span class="keyword">if</span>(board[i][j]==k)</span><br><span class="line">                  <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：739-每日温度"><a href="#题目：739-每日温度" class="headerlink" title="题目：739. 每日温度"></a>题目：739. 每日温度</h2><p>描述：请根据每日 气温 列表，重新生成一个列表。对应位置的输出为：要想观测到更高的气温，至少需要等待的天数。如果气温在这之后都不会升高，请在该位置用 0 来代替。</p><p>例如，给定一个列表 temperatures = [73, 74, 75, 71, 69, 72, 76, 73]，你的输出应该是 [1, 1, 4, 2, 1, 1, 0, 0]。提示：气温 列表长度的范围是 [1, 30000]。每个气温的值的均为华氏度，都是在 [30, 100] 范围内的整数。</p><p>思路：</p><p>每次遍历到数组中的元素后在后面的元素中找到比它大的数字，返回两者之间的索引位置差即为间隔的天数，没有找到比他大的则返回0。但是这样的两层循环，每次都要遍历一遍数组，时间复杂度高，可以采用递减栈的方式：</p><p>（1）遍历所有的数组中的元素</p><p>（2）将数组的索引作为值存入到递减栈中，当栈内元素不为空，并且当前元素大于栈顶的元素，则栈顶元素对应的数值（间隔的天数）就是当前元素的位置减去栈顶元素的位置</p><p>其中的原理为：</p><p>栈顶的元素是需要得到其后面比它大的元素的位置，当当前遇到比它大的，即保存为索引之差，当当前遇到的比他小，说明大的数字在后面，则当前数字也需要入栈，继续找后面的大的元素。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; dailyTemperatures(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; T) &#123;</span><br><span class="line">      <span class="comment">//依次向后找到比他小的</span></span><br><span class="line">      <span class="comment">//递减栈 入栈的是索引</span></span><br><span class="line">      <span class="keyword">int</span> len=T.<span class="built_in">size</span>();</span><br><span class="line">      <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res(len);</span><br><span class="line">      <span class="built_in">stack</span>&lt;<span class="keyword">int</span>&gt; p;</span><br><span class="line">      <span class="keyword">int</span> t;</span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;len;i++)</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="keyword">while</span>(!p.empty()&amp;&amp;T[i]&gt;T[p.top()])</span><br><span class="line">          &#123;</span><br><span class="line">              t=p.top();</span><br><span class="line">              res[t]=i-t;</span><br><span class="line">              p.pop();</span><br><span class="line">          &#125;</span><br><span class="line">          p.push(i);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> res;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：494-目标和"><a href="#题目：494-目标和" class="headerlink" title="题目：494. 目标和"></a>题目：494. 目标和</h2><p>描述：给定一个非负整数数组，a1, a2, …, an, 和一个目标数，S。现在你有两个符号 + 和 -。对于数组中的任意一个整数，你都可以从 + 或 -中选择一个符号添加在前面。返回可以使最终数组和为目标数 S 的所有添加符号的方法数。</p><p>思路：</p><p>1.递归法</p><p>递归列出所有的结果</p><p>（1）将目标值递归减去或者加上所有的元素，</p><p>（2）果满足最终目标数字等于0，则说明找到了一个添加符号的方法，反之则继续找下一个组合</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">findTargetSumWays</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> S)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//递归求解所有可能</span></span><br><span class="line">       dfs(nums,<span class="number">0</span>,S);</span><br><span class="line">       <span class="keyword">return</span> res;</span><br><span class="line"></span><br><span class="line">   &#125;</span><br><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums,<span class="keyword">int</span> index,<span class="keyword">long</span> target)</span></span></span><br><span class="line"><span class="function">   </span>&#123;</span><br><span class="line">       <span class="keyword">if</span>(index==nums.<span class="built_in">size</span>())</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">if</span>(target==<span class="number">0</span>)</span><br><span class="line">               res++;</span><br><span class="line">           <span class="keyword">return</span>;</span><br><span class="line">       &#125;</span><br><span class="line">       dfs(nums,index+<span class="number">1</span>,target+nums[index]);</span><br><span class="line">       dfs(nums,index+<span class="number">1</span>,target-nums[index]);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>2.动态规划</p><p>转换为子序和为某值的子集数量</p><p>x代表整数集合，y代表负数集合，sum代表所有数据之和，S表示目标和，则x+y=sum，x-y=S，得到x=(sum+S)/2，即需要找到子序和为(sum+S)/2的子集</p><p>（1）建立状态转移方程dp，其中dp [ i ] [ j ]表示前0 ~ i项中组成和为j的子集的数量</p><p>（2）遍历数组中所有的元素</p><p>（3）定义需要求的子序和为w=(sum+S)/2，求出0 ~ w之间的子序和的子集数量</p><p>当当前的元素值大于子序和时，不取该数，则dp [ i ] [ j ]=dp [ i-1 ] [ j ]，即前一个子序和的数量</p><p>当当前元素值小于子序和时，分为取该数和不取该数，dp [ i ] [ j ]=dp [ i-1 ] [ j-nums [ i-1] ]+dp [ i-1 ] [ j ]</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">findTargetSumWays</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> S)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//递归求解  x=(sum+S)/2</span></span><br><span class="line">       <span class="keyword">int</span> res;</span><br><span class="line">       <span class="keyword">int</span> sum=<span class="number">0</span>;</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">auto</span> a:nums)</span><br><span class="line">           sum+=a;</span><br><span class="line">       </span><br><span class="line">       <span class="comment">//数组中的元素不满足等于S的情形</span></span><br><span class="line">       <span class="keyword">if</span>(sum&lt;S||(sum+S)%<span class="number">2</span>==<span class="number">1</span>)</span><br><span class="line">           <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">       <span class="keyword">int</span> w=(sum+S)/<span class="number">2</span>;<span class="comment">//组成容量为w的方式有多少种</span></span><br><span class="line">       <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; dp(nums.<span class="built_in">size</span>()+<span class="number">1</span>,<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;(w+<span class="number">1</span>));</span><br><span class="line">       dp[<span class="number">0</span>][<span class="number">0</span>]=<span class="number">1</span>;</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;nums.<span class="built_in">size</span>()+<span class="number">1</span>;i++)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;=w;j++)</span><br><span class="line">           &#123;</span><br><span class="line">               <span class="keyword">if</span>(j&lt;nums[i<span class="number">-1</span>])<span class="comment">//不拿当前物品</span></span><br><span class="line">                   dp[i][j]=dp[i<span class="number">-1</span>][j];</span><br><span class="line">               <span class="keyword">else</span></span><br><span class="line">                   dp[i][j]=dp[i<span class="number">-1</span>][j]+dp[i<span class="number">-1</span>][j-nums[i<span class="number">-1</span>]];</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> dp[nums.<span class="built_in">size</span>()][w];</span><br><span class="line"></span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：560-和为K的子数组"><a href="#题目：560-和为K的子数组" class="headerlink" title="题目：560. 和为K的子数组"></a>题目：560. 和为K的子数组</h2><p>描述：给定一个整数数组和一个整数 k，你需要找到该数组中和为 k 的连续的子数组的个数。</p><p>思路：因为有负值，所以使用滑动窗口较复杂。</p><p>1.暴力法：固定左边界</p><p>（1）固定左边界，以此向右累加得到子序和</p><p>（2）判断当前累加和是否是等于目标值，等于的话则把计数值加一</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">subarraySum</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line">      <span class="comment">//和为k,固定左边界</span></span><br><span class="line">      <span class="keyword">int</span> res=<span class="number">0</span>;</span><br><span class="line">      </span><br><span class="line">      <span class="keyword">int</span> len=nums.<span class="built_in">size</span>();</span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> left=<span class="number">0</span>;left&lt;nums.<span class="built_in">size</span>();left++)</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="keyword">int</span> sum=<span class="number">0</span>;</span><br><span class="line">          <span class="keyword">for</span>(<span class="keyword">int</span> right=left;right&lt;nums.<span class="built_in">size</span>();right++)</span><br><span class="line">          &#123;</span><br><span class="line">              sum+=nums[right];</span><br><span class="line">              <span class="keyword">if</span>(sum==k)</span><br><span class="line">                  res++;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> res;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>2.前缀和</p><p>暴力法会重复计算子数组的和，因此采用优先计算出前缀和来加快运算速度</p><p>（1）先计算出在所有位置的数组之和（即索引为0至当前位置的元素之和）</p><p>（2）用差分法计算区间段内的和，left从0变化到nums.size，right从left变化到nums.size，计算right+1和left区间段的和</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">subarraySum</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//和为k</span></span><br><span class="line">       <span class="comment">//前缀和</span></span><br><span class="line">       <span class="keyword">int</span> res=<span class="number">0</span>;</span><br><span class="line">       <span class="keyword">int</span> len=nums.<span class="built_in">size</span>();</span><br><span class="line">       <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; prefixSum(len+<span class="number">1</span>);</span><br><span class="line">       prefixSum[<span class="number">0</span>]=<span class="number">0</span>;</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;len;i++)</span><br><span class="line">       &#123;</span><br><span class="line">           prefixSum[i+<span class="number">1</span>]=prefixSum[i]+nums[i];</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;len;i++)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">for</span>(<span class="keyword">int</span> j=i;j&lt;len;j++)</span><br><span class="line">           &#123;</span><br><span class="line">               <span class="keyword">if</span>(prefixSum[j+<span class="number">1</span>]-prefixSum[i]==k)<span class="comment">//用差分法得到区间段的和</span></span><br><span class="line">                   res++;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       </span><br><span class="line">       </span><br><span class="line">       <span class="keyword">return</span> res;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>3.利用哈希表</p><p>前缀和的方法时间复杂度仍为O(N2)，采用哈希表优化前缀和</p><p>（1）累加计算前缀和sum</p><p>（2）在和为sum的位置查找之前的前缀和是否有等于sum-k（目标值的），这样当前位置的前缀和sum-(sum-k)=k，即目标值，找到则计数加上前缀和等于sum-k的值，即为几个等于sum-k的前缀和的位置，这样就有几个和为k的子数组</p><p>（3）不断的向哈希表中增加前缀和的值</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">subarraySum</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//和为k</span></span><br><span class="line">       <span class="comment">//前缀和</span></span><br><span class="line">       <span class="comment">//使用哈希表</span></span><br><span class="line">       <span class="keyword">int</span> res=<span class="number">0</span>;</span><br><span class="line">       <span class="keyword">int</span> len=nums.<span class="built_in">size</span>();</span><br><span class="line">       <span class="built_in">unordered_map</span>&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; <span class="built_in">map</span>;</span><br><span class="line">       <span class="keyword">int</span> sum=<span class="number">0</span>;</span><br><span class="line">       <span class="built_in">map</span>[<span class="number">0</span>]=<span class="number">1</span>;<span class="comment">//前缀和为0的个数为1</span></span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;len;i++)</span><br><span class="line">       &#123;</span><br><span class="line">           sum+=nums[i];</span><br><span class="line">           <span class="keyword">if</span>(<span class="built_in">map</span>.<span class="built_in">find</span>(sum-k)!=<span class="built_in">map</span>.<span class="built_in">end</span>())</span><br><span class="line">               res+=<span class="built_in">map</span>[sum-k];</span><br><span class="line">           <span class="built_in">map</span>[sum]++;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> res;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：300-最长上升子序列"><a href="#题目：300-最长上升子序列" class="headerlink" title="题目：300. 最长上升子序列"></a>题目：300. 最长上升子序列</h2><p>描述：给定一个无序的整数数组，找到其中最长上升子序列的长度。</p><p>思路：</p><p>可以使用暴力法，使用回溯法把所有递增的上升子序列列出，复杂度是O(2^n)，因此此题可以使用动态规划。</p><p>动态规划重点是定义状态和状态转移方程：</p><p>（1）定义状态——问题问什么就把什么定义为状态：此题问的是上升子序列的长度，则将dp [ i ]定义为以nums [ i ]结尾的上升子序列的长度。</p><p>（2）初始化：所有以自身作为结尾的上升子序列的元素的状态初始化为1，因为自身一开始有长度1.</p><p>（3）状态转移方程：以nums [ i ]结尾的上升子序列的长度=目前以自己为结尾的序列长度，或者是当之前的元素小于自己，则可以用之前的元素的序列长度加上自己（+1），两者取较大的值。</p><p>最终的结果是遍历所有元素结尾的子序列，找到最长的一个作为结果返回。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">lengthOfLIS</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//问题问什么，就把什么定义成状态</span></span><br><span class="line">        <span class="comment">//将子序列的长度定义为状态</span></span><br><span class="line">        <span class="keyword">if</span>(nums.empty())</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> len=nums.<span class="built_in">size</span>();</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; dp(len);</span><br><span class="line">        <span class="comment">//初始化</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;len;i++)</span><br><span class="line">            dp[i]=<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">int</span> res=dp[<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line">        <span class="comment">//状态转移</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;len;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;i;j++)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span>(nums[j]&lt;nums[i])</span><br><span class="line">                    dp[i]=<span class="built_in">max</span>(dp[j]+<span class="number">1</span>,dp[i]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;len;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            res=<span class="built_in">max</span>(res,dp[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：581-最短无序连续子数组"><a href="#题目：581-最短无序连续子数组" class="headerlink" title="题目：581. 最短无序连续子数组"></a>题目：581. 最短无序连续子数组</h2><p>描述：给定一个整数数组，你需要寻找一个连续的子数组，如果对这个子数组进行升序排序，那么整个数组都会变为升序排序。你找到的子数组应是最短的，请输出它的长度。</p><p>例如：输入: [2, 6, 4, 8, 10, 9, 15]<br>输出: 5<br>解释: 你只需要对 [6, 4, 8, 10, 9] 进行升序排序，那么整个表都会变为升序排序。</p><p>思路：题目是找到从左边开始非递增和从右边开始非递减的数组的长度。</p><p>（1）对每个数组元素，向右开始搜索</p><p>（2）当搜索到的元素小于当前的元素，则这两个元素中间的数组无序</p><p>（3）更新左边界为当前元素（取更小的），右边界为搜索到的小的元素（取更大的），确定无序数组的边界</p><p>最后无序数组为左右边界之差+1</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">findUnsortedSubarray</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//从左到右找到破坏递增的数据</span></span><br><span class="line">        <span class="comment">//从右到左找到破坏递减的数据</span></span><br><span class="line">        <span class="keyword">int</span> len=nums.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">int</span> left=len;</span><br><span class="line">        <span class="keyword">int</span> right=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;len<span class="number">-1</span>;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=i+<span class="number">1</span>;j&lt;len;j++)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span>(nums[j]&lt;nums[i])</span><br><span class="line">                &#123;</span><br><span class="line">                    left=<span class="built_in">min</span>(left,i);</span><br><span class="line">                    right=<span class="built_in">max</span>(right,j);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(right&lt;left)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">return</span> right-left+<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>以上的算法时间复杂度为O(n2)，对算法改进：</p><p>（1）从左到右找到破坏递增的最小的数据</p><p>（2）从右到左找到破坏递减的最大的数据</p><p>则二者对应的数据的索引之差即为无序数组的长度</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">findUnsortedSubarray</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//从左到右找到破坏递增的数据</span></span><br><span class="line">        <span class="comment">//从右到左找到破坏递减的数据</span></span><br><span class="line">        <span class="keyword">int</span> len=nums.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">int</span> nmin=INT_MAX;</span><br><span class="line">        <span class="keyword">int</span> nmax=INT_MIN;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;nums.<span class="built_in">size</span>();i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(nums[i]&lt;nums[i<span class="number">-1</span>])</span><br><span class="line">                nmin=<span class="built_in">min</span>(nmin,nums[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=nums.<span class="built_in">size</span>()<span class="number">-1</span>;i&gt;<span class="number">0</span>;i--)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(nums[i]&lt;nums[i<span class="number">-1</span>])</span><br><span class="line">                nmax=<span class="built_in">max</span>(nmax,nums[i<span class="number">-1</span>]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> left=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(;left&lt;nums.<span class="built_in">size</span>();left++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(nums[left]&gt;nmin)</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> right=nums.<span class="built_in">size</span>()<span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">for</span>(;right&gt;=<span class="number">0</span>;right--)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(nums[right]&lt;nmax)</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span>(right&lt;left)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">return</span> right-left+<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>时间复杂度为O(n)</p><h2 id="题目：297-二叉树的序列化与反序列化"><a href="#题目：297-二叉树的序列化与反序列化" class="headerlink" title="题目：297. 二叉树的序列化与反序列化"></a>题目：297. 二叉树的序列化与反序列化</h2><p>描述：序列化是将一个数据结构或者对象转换为连续的比特位的操作，进而可以将转换后的数据存储在一个文件或者内存中，同时也可以通过网络传输到另一个计算机环境，采取相反方式重构得到原数据。请设计一个算法来实现二叉树的序列化与反序列化。这里不限定你的序列 / 反序列化算法执行逻辑，你只需要保证一个二叉树可以被序列化为一个字符串并且将这个字符串反序列化为原始的树结构。</p><p>思路：</p><p>使用层次遍历来进行二叉树的序列化和</p><p>1.序列化：将二叉树转换为字符串</p><p>（1）将树从根开始依次放入队列中</p><p>（2）每次取出一个节点，如果是空节点，直接将“null”加在序列字符串后面；如果不是空节点，则读入该节点的数值，并<u>通过to_string将int类型转换为string类型</u>，并加在序列字符串后面</p><p>（3）将该节点的左右节点入队，以供下一次读入处理（因为空节点也需要序列化，所以这里不需要对左右节点进行空值判定）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="built_in">string</span> <span class="title">serialize</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">string</span> ser;</span><br><span class="line">        <span class="built_in">queue</span>&lt;TreeNode*&gt; p;</span><br><span class="line">        p.push(root);</span><br><span class="line">        <span class="keyword">while</span>(!p.empty())</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">int</span> <span class="built_in">width</span>=p.<span class="built_in">size</span>();</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="built_in">width</span>;i++)</span><br><span class="line">            &#123;</span><br><span class="line">                TreeNode* r=p.front();</span><br><span class="line">                p.pop();</span><br><span class="line">                <span class="keyword">if</span>(r!=<span class="literal">NULL</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="built_in">string</span> val=to_string(r-&gt;val);</span><br><span class="line">                    ser+=val;</span><br><span class="line">                        </span><br><span class="line">                    p.push(r-&gt;left);</span><br><span class="line">                    p.push(r-&gt;right);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                    ser+=<span class="string">"null"</span>;</span><br><span class="line">                    </span><br><span class="line">                ser+=<span class="string">","</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ser;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>2.反序列化：将字符串转换为一棵二叉树</p><p>（1）将原来的字符串转换为利于分割的：利用stringstream+getline进行字符串分割，将原来的字符串转换为输入流，利用getline每次遇到‘，’停止读入，这样一次就读入一个数据</p><p>（2）将读入的第一个数据作为根，当根为空，整个树为空；当根不为空时，则将该根入队</p><p>（3）每次从队列中取出一个节点，当是非空节点的时候，利用getline读入该节点的左节点（左节点需要非空，才能构造新的节点），将当前取出的根节点的左孩子指向该节点，并把左节点入队，右节点同样如此。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">// Decodes your encoded data to tree.</span></span><br><span class="line"><span class="function">TreeNode* <span class="title">deserialize</span><span class="params">(<span class="built_in">string</span> data)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">stringstream</span> s;</span><br><span class="line">    s&lt;&lt;data;</span><br><span class="line">    <span class="built_in">string</span> str_n;</span><br><span class="line">    getline(s, str_n, <span class="string">','</span>);</span><br><span class="line">    <span class="keyword">if</span>(str_n==<span class="string">"null"</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    </span><br><span class="line">    TreeNode* root=<span class="keyword">new</span> TreeNode(atoi(str_n.c_str()));</span><br><span class="line">    <span class="built_in">queue</span>&lt;TreeNode*&gt; p;</span><br><span class="line">    p.push(root);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span>(!p.empty())</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> <span class="built_in">width</span>=p.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="built_in">width</span>;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            TreeNode* r=p.front();</span><br><span class="line">            p.pop();</span><br><span class="line">            <span class="keyword">if</span>(r!=<span class="literal">NULL</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="built_in">string</span> str_l;</span><br><span class="line">                <span class="built_in">string</span> str_r;</span><br><span class="line">                <span class="keyword">if</span>(getline(s, str_l,<span class="string">','</span>)&amp;&amp;str_l!=<span class="string">"null"</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    TreeNode* left=<span class="keyword">new</span> TreeNode(atoi(str_l.c_str()));</span><br><span class="line">                    p.push(left);</span><br><span class="line">                    r-&gt;left=left;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span>(getline(s, str_r, <span class="string">','</span>)&amp;&amp;str_r!=<span class="string">"null"</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    TreeNode* right=<span class="keyword">new</span> TreeNode(atoi(str_r.c_str()));</span><br><span class="line">                    p.push(right);</span><br><span class="line">                    r-&gt;right=right;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> root;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="题目：968-监控二叉树"><a href="#题目：968-监控二叉树" class="headerlink" title="题目：968. 监控二叉树"></a>题目：968. 监控二叉树</h2><p>描述：给定一个二叉树，我们在树的节点上安装摄像头。节点上的每个摄影头都可以监视其父对象、自身及其直接子对象。计算监控树的所有节点所需的最小摄像头数量。</p><p>思路：</p><p>本题重点是要搞清节点的三个状态：0:没有覆盖（摄像头照不到）；1:已经覆盖（摄像头照得到）；2：已经装了摄像头，遍历所有的节点，根据其左右子节点的状态，判断其是否需要安装摄像头，并返回当前节点的状态，以供其父节点使用。</p><p>（1）遍历树，当当前的节点为空节点的时候，即初始状态，为了使得摄像头最少，初始叶子节点应该是被覆盖的</p><p>（2）找到当前节点的左右节点的状态（递归），因为left和right分别有三种状态：0，1，2，组合起来一共有九种</p><p>0+0，0+1，0+2，1+0，1+1，1+2，2+0，2+1，2+2，分析可得：</p><p>当有一个为没有覆盖的时候（即左右子节点有一个为0），此时，需要在当前的节点加一个摄像头，保证其中没有覆盖的子节点能覆盖到，此时的节点状态为2；</p><p>当两个都被摄像头覆盖时（即左右子节点都为1），此时，当前节点不确定是否需要安装，返回其状态0，根据他的父节点来判断；</p><p>当两个摄像头有一个装了摄像头，一个被覆盖或者是两个都装摄像头，此时的当前节点没必要装摄像头，因为已经被覆盖，此时的节点状态为1。</p><p>（3）当遍历到最后的根节点的时候为0，说明根节点没有被覆盖，最终结果加1。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">int</span> res=<span class="number">0</span>;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">minCameraCover</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//根据左右节点的状态来判断当前的状态</span></span><br><span class="line">        <span class="comment">//0:没有覆盖（摄像头照不到）</span></span><br><span class="line">        <span class="comment">//1:已经覆盖（摄像头照得到）</span></span><br><span class="line">        <span class="comment">//2：已经装了摄像头</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">//遍历到最后的根节点为0，即没有覆盖到，则结果加1</span></span><br><span class="line">        <span class="keyword">if</span>(dfs(root)==<span class="number">0</span>)</span><br><span class="line">            res++;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">dfs</span><span class="params">(TreeNode* root)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(root==<span class="literal">NULL</span>)<span class="comment">//初始状态，设置为已覆盖</span></span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">int</span> left=dfs(root-&gt;left);</span><br><span class="line">        <span class="keyword">int</span> right=dfs(root-&gt;right);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//根据左右节点的状态，判断当前的状态</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">//有一个没被覆盖，当前节点要放摄像头</span></span><br><span class="line">        <span class="keyword">if</span>(left==<span class="number">0</span>||right==<span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            res++;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">2</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//两个都被覆盖，当前不确定要不要放，设置当前状态</span></span><br><span class="line">        <span class="keyword">if</span>(left==<span class="number">1</span>&amp;&amp;right==<span class="number">1</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//两个中有一个装了摄像头，一个被覆盖</span></span><br><span class="line">        <span class="keyword">if</span>(left+right&gt;=<span class="number">3</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;<span class="comment">//不会到这里，因为上面已经把所有情况罗列出来了，这里是随便设置的一个的数据</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：394-字符串解码"><a href="#题目：394-字符串解码" class="headerlink" title="题目：394. 字符串解码"></a>题目：394. 字符串解码</h2><p>描述：给定一个经过编码的字符串，返回它解码后的字符串。编码规则为: k[encoded_string]，表示其中方括号内部的 encoded_string 正好重复 k 次。注意 k 保证为正整数。你可以认为输入字符串总是有效的；输入字符串中没有额外的空格，且输入的方括号总是符合格式要求的。此外，你可以认为原始数据不包含数字，所有的数字只表示重复的次数 k ，例如不会出现像 3a 或 2[4] 的输入。</p><p>思路：</p><p>遇到括号问题一般都想到使用栈，由于这里有数字，数字有可能不止一位数，又有字母，考虑使用两个栈分别存储数字和字母。</p><p>（1）建立数字栈和字母栈</p><p>（2）遍历字符串，对遇到不同的字符分别处理：</p><p>当遇到数字的时候，将字符通过c-‘0’转换为int类型，并将原来没有入栈的数字乘以10加上现在的数据，以防数字不是一位数；</p><p>当遇到‘ [ ’左括号的时候，此时要把括号外面的数字和字母（此时的字母肯定是不需要重复的），将二者入栈，数字入栈是为了一会儿将括号里的字母重复使用，字母入栈是为了将其和需要重复的部分叠加。并将此时数字和字母存储部分清空，便于存储括号内的字母；</p><p>当遇到字母的时候，将此时遇到的所有字母叠加；</p><p>当遇到‘ ] ’右括号的时候，此时需要把数字栈中的数字读出，并将此时剩余的字母，即为括号内的字母重复该数字的次数，并累加到字母栈中，此时字母栈的栈顶就是叠加好的字母结果。</p><p>最后，</p><p>字母栈的栈顶就是一直累加的结果。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//使用栈 分两个栈处理，一个字母栈，一个数字栈</span></span><br><span class="line">    <span class="comment">//遇到‘['入栈，遇到’]'出栈</span></span><br><span class="line">    <span class="built_in">stack</span>&lt;<span class="built_in">string</span>&gt; stringStack;;</span><br><span class="line">    <span class="built_in">stack</span>&lt;<span class="keyword">int</span>&gt; numStack;</span><br><span class="line">    <span class="keyword">int</span> num=<span class="number">0</span>;</span><br><span class="line">    <span class="built_in">string</span> curStr=<span class="string">""</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;s.<span class="built_in">size</span>();i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(s[i]&gt;=<span class="string">'0'</span>&amp;&amp;s[i]&lt;=<span class="string">'9'</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            num=num*<span class="number">10</span>+s[i]-<span class="string">'0'</span>;<span class="comment">//转换为int类型</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(s[i]==<span class="string">'['</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            numStack.push(num);</span><br><span class="line">            num=<span class="number">0</span>;</span><br><span class="line">            stringStack.push(curStr);</span><br><span class="line">            curStr=<span class="string">""</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(s[i]&gt;=<span class="string">'a'</span>&amp;&amp;s[i]&lt;=<span class="string">'z'</span>||s[i]&gt;=<span class="string">'A'</span>&amp;&amp;s[i]&lt;=<span class="string">'Z'</span>)</span><br><span class="line">            curStr+=s[i];</span><br><span class="line">        <span class="keyword">if</span>(s[i]==<span class="string">']'</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">int</span> time=numStack.top();</span><br><span class="line">            numStack.pop();</span><br><span class="line">            <span class="comment">//把‘['前的加上现在需要重复的部分</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;time;i++)</span><br><span class="line">            &#123;</span><br><span class="line">                stringStack.top()+=curStr;</span><br><span class="line">            &#125;</span><br><span class="line">            curStr=stringStack.top();</span><br><span class="line">            stringStack.pop();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> curStr;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;题目：287-寻找重复数&quot;&gt;&lt;a href=&quot;#题目：287-寻找重复数&quot; class=&quot;headerlink&quot; title=&quot;题目：287. 寻找重复数&quot;&gt;&lt;/a&gt;题目：287. 寻找重复数&lt;/h2&gt;&lt;p&gt;描述：给定一个包含 n + 1 个整数的数组 nums，其数字都在 1 到 n 之间（包括 1 和 n），可知至少存在一个重复的整数。假设只有一个重复的整数，找出这个重复的数&lt;/p&gt;&lt;p&gt;说明：&lt;/p&gt;&lt;p&gt;不能更改原数组（假设数组是只读的）。&lt;br&gt;只能使用额外的 O(1) 的空间。&lt;br&gt;时间复杂度小于 O(n2) 。&lt;br&gt;数组中只有一个重复的数字，但它可能不止重复出现一次。&lt;/p&gt;
    
    </summary>
    
    
      <category term="算法" scheme="https://www.xiapf.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="https://www.xiapf.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>大规模机器学习——处理大数据的算法</title>
    <link href="https://www.xiapf.com/blogs/tfLargeData/"/>
    <id>https://www.xiapf.com/blogs/tfLargeData/</id>
    <published>2020-09-14T05:32:14.000Z</published>
    <updated>2020-09-14T05:33:07.183Z</updated>
    
    <content type="html"><![CDATA[<p>数据集规模增大，需要有合理的计算方法来处理大规模数据</p><p>（1）随机梯度下降</p><p>（2）减少映射</p><h2 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h2><h3 id="批量梯度下降：每次迭代用m个样本"><a href="#批量梯度下降：每次迭代用m个样本" class="headerlink" title="批量梯度下降：每次迭代用m个样本"></a>批量梯度下降：每次迭代用m个样本</h3><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200914124507.png" alt></p><p>代价函数中求出m个样本的误差，再求平均，计算梯度时对每个参数都需要计算m个样本的误差，但数据量很大的时候，代价高。</p><h3 id="随机梯度下降：每次迭代用1个样本"><a href="#随机梯度下降：每次迭代用1个样本" class="headerlink" title="随机梯度下降：每次迭代用1个样本"></a>随机梯度下降：每次迭代用1个样本</h3><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200914124858.png" alt></p><p>将原有的批量梯度下降的公式进行修正，定义单个样本的误差为cost，成本函数为m个样本的误差cost函数求和，再取平均。</p><a id="more"></a><p>流程：</p><p>（1）随机打乱所有的数据，将m个样本重新随机排列</p><p>（2）对每次遍历一个样本，就更新所有的参数：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200914125411.png" alt></p><p>此时不需要对m个样本求和得到梯度项，只对单个样本求梯度项来更新参数。</p><p>仅遍历一次m个样本，就能得到很好的假设，不像批量梯度下降，需要迭代n * m次，n是迭代次数，成本代价高。因此，随机梯度下降速度更快。能够连续不断朝着全局最小值的方向优化，得到很接近最小值的参数。</p><h3 id="mini-batch梯度下降：每次迭代用b个样本-1-lt-b-lt-m"><a href="#mini-batch梯度下降：每次迭代用b个样本-1-lt-b-lt-m" class="headerlink" title="mini-batch梯度下降：每次迭代用b个样本(1&lt;b&lt;m)"></a>mini-batch梯度下降：每次迭代用b个样本(1&lt;b&lt;m)</h3><p>每次需要选择b个样本来更新参数，下一次在原来的基础上再选择b个样本</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200914130107.png" alt></p><p>需要自行确定mini-batch的大小即b的大小</p><h3 id="如何确定收敛"><a href="#如何确定收敛" class="headerlink" title="如何确定收敛"></a>如何确定收敛</h3><p>记录每个cost的值，每1000个进行一次迭代，求前1000个的平均值，随着迭代次数的增加，代价函数曲线应该是呈波动上下减少的趋势。</p><p>（1）当噪声太大，老是上下震动，则增加求均值的样本的数量</p><p>（2）选择小的学习率alpha值，曲线也会更平缓</p><p>（3）当曲线是上升趋势，则算法发散，需要用更小的学习率alpha</p><p>学习率alpha的值可以设置为随时间的变化而逐渐减少：alpha=常数1/（迭代次数+常数2），常数1和常数2需要自行确定，常用的alpha值是常数</p><h3 id="在线学习：随机梯度下降的变种"><a href="#在线学习：随机梯度下降的变种" class="headerlink" title="在线学习：随机梯度下降的变种"></a>在线学习：随机梯度下降的变种</h3><p>不使用固定的数据集，当获取到一个新样本的时候，就利用该样本学习更新模型中的参数，并丢弃（后续不再使用），继续捕获新的样本。（每次只处理一个样本）</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200914132616.jpg" alt></p><h2 id="减少映射"><a href="#减少映射" class="headerlink" title="减少映射"></a>减少映射</h2><p>通过Map-reduce的方式进行数据并行操作。</p><p>假设有a台电脑，则可以将m个样本分为a份，第一个电脑运行1<del>m/a个样本，第二个电脑运行m/a+1</del>2m/a个样本，以此类推，更新参数的时候，将a台电脑运行的结果进行整合。</p><p>对参数的更新中，对偏导的求解可以分解如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200914132341.png" alt></p><p>可以选择不同的优化目标计算对应的偏导项，对temp(j)（第j部机器运算的梯度值）进行求和。</p><p>这是用MapReduce将算法表示成对训练集的一种求和。</p><p>当只有一台电脑的时候，用不同的内核并行计算。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;数据集规模增大，需要有合理的计算方法来处理大规模数据&lt;/p&gt;&lt;p&gt;（1）随机梯度下降&lt;/p&gt;&lt;p&gt;（2）减少映射&lt;/p&gt;&lt;h2 id=&quot;随机梯度下降&quot;&gt;&lt;a href=&quot;#随机梯度下降&quot; class=&quot;headerlink&quot; title=&quot;随机梯度下降&quot;&gt;&lt;/a&gt;随机梯度下降&lt;/h2&gt;&lt;h3 id=&quot;批量梯度下降：每次迭代用m个样本&quot;&gt;&lt;a href=&quot;#批量梯度下降：每次迭代用m个样本&quot; class=&quot;headerlink&quot; title=&quot;批量梯度下降：每次迭代用m个样本&quot;&gt;&lt;/a&gt;批量梯度下降：每次迭代用m个样本&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200914124507.png&quot; alt&gt;&lt;/p&gt;&lt;p&gt;代价函数中求出m个样本的误差，再求平均，计算梯度时对每个参数都需要计算m个样本的误差，但数据量很大的时候，代价高。&lt;/p&gt;&lt;h3 id=&quot;随机梯度下降：每次迭代用1个样本&quot;&gt;&lt;a href=&quot;#随机梯度下降：每次迭代用1个样本&quot; class=&quot;headerlink&quot; title=&quot;随机梯度下降：每次迭代用1个样本&quot;&gt;&lt;/a&gt;随机梯度下降：每次迭代用1个样本&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200914124858.png&quot; alt&gt;&lt;/p&gt;&lt;p&gt;将原有的批量梯度下降的公式进行修正，定义单个样本的误差为cost，成本函数为m个样本的误差cost函数求和，再取平均。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统</title>
    <link href="https://www.xiapf.com/blogs/tfRecommendSys/"/>
    <id>https://www.xiapf.com/blogs/tfRecommendSys/</id>
    <published>2020-09-11T08:37:10.000Z</published>
    <updated>2020-09-14T06:00:11.557Z</updated>
    
    <content type="html"><![CDATA[<h2 id="推荐算法的应用场景"><a href="#推荐算法的应用场景" class="headerlink" title="推荐算法的应用场景"></a>推荐算法的应用场景</h2><p>当已知部分user对一些product有评价，对该用户基于之前的评价推荐相近的product，这时候就需要用到推荐算法。</p><p>如给定一些用户对一些电影的评分，求对这些用户推荐什么电影。</p><h2 id="符号含义"><a href="#符号含义" class="headerlink" title="符号含义"></a>符号含义</h2><p>r（i，j）：表示用户j评价了电影i</p><p>y（i，j）：用户j对电影i的评价等级（如果用户评价过该电影）</p><a id="more"></a><p>θ（j）：用户j对电影评价的参数</p><p>x（i）：电影i的特征向量</p><h2 id="基于内容的推荐算法"><a href="#基于内容的推荐算法" class="headerlink" title="基于内容的推荐算法"></a>基于内容的推荐算法</h2><h3 id="应用背景"><a href="#应用背景" class="headerlink" title="应用背景"></a>应用背景</h3><p>当已知每个电影的特征向量x（i），求对用户推荐的电影，此时已知的是每个电影的特征即电影的各内容已知，对每个用户j，学习其参数向量θ，预测用户j评价电影i的值，则用参数向量θ * 特征向量x即可。</p><h3 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h3><p>总体思路是对每个用户应用一个不同的线性回归的方程，使用用户的参数向量乘以电影的特征向量就是当前的预测，将预测减去实际值，目标是缩小预测和实际之间的误差。</p><p>根据该思路得到的优化目标为：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200911145904.png" alt></p><p>该优化函数根据随机初始化的用户的参数向量，得到每个用户对电影评分的预测值，求出所有用户对所有电影的评价并求和，根据线性回归中误差最小的原理，最终得到用户的参数向量。</p><p>与线性回归不同的是，在优化目标中把1/m去掉了</p><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200911150621.png" alt></p><h2 id="基于协同过滤的推荐算法"><a href="#基于协同过滤的推荐算法" class="headerlink" title="基于协同过滤的推荐算法"></a>基于协同过滤的推荐算法</h2><h3 id="应用背景-1"><a href="#应用背景-1" class="headerlink" title="应用背景"></a>应用背景</h3><p>现实中当不知道电影的特征向量，也不知道用户的参数向量，就需要用到协同过滤的推荐算法。</p><h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><p>（1）数据预处理，导入数据的评分矩阵和用户评价矩阵，并将评分矩阵进行均值化</p><p>（2）随机初始化电影的特征向量和用户的参数向量</p><p>（3）建立优化目标函数</p><p>以预测值和实际值误差最小作为目标，同时加上是的特征向量和参数向量最小的正则项</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200911155633.png" alt></p><p>（4）计算函数梯度</p><p>分别更新特征向量和参数向量</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200911155655.png" alt></p><p>（5）使用优化算法，按照梯度下降的方向求得优化函数最小的参数值</p><p>（6）得到要求的用户的参数矩阵，乘以所有电影的特征向量，再加上预处理中减去的评分矩阵的均值，按照排序的方法，取前几个电影即为需要推荐的</p><h3 id="数据预处理：均值规范化"><a href="#数据预处理：均值规范化" class="headerlink" title="数据预处理：均值规范化"></a>数据预处理：均值规范化</h3><p>（1）预处理原因</p><p>由于有用户没有评分任何一个电影，则该用户的参数向量为0，则不管算法如何运行，最终的参数向量仍为0，最终预测也都是0，这样的预测没有意义。</p><p>因此，需要对电影评分Y矩阵进行均值归一化，将Y=Y-Y.mean()（Y的均值），这样保证了没有任何评分的用户最终预测也不是0。</p><p>（2）预处理后的数据在模型训练后如何进行预测</p><p>将预测值θ * x再加上Y的均值Y.mean()，因为一开始的预处理把所有评分都减去了均值，最后的预测需要加上</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200911153057.png" alt></p><p>读入数据：使用scipy下的io库中的loadmat读取mat文件中的用户评价表r(用户j评价过电影i则标记为1，反之标记为0)，以及读取电影评分表y，读取的r.shape=y.shape=(1682,943)，说明有1682个电影，943个评价用户。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.导入数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(path)</span>:</span></span><br><span class="line">data=sio.loadmat(path)</span><br><span class="line">y=data.get(<span class="string">'Y'</span>)</span><br><span class="line">r=data.get(<span class="string">'R'</span>)</span><br><span class="line"><span class="keyword">return</span> y,r</span><br><span class="line"></span><br><span class="line">y,r=load_data(<span class="string">'./data/ex8_movies.mat'</span>)</span><br></pre></td></tr></table></figure><p>评分均值化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_mena=y-y.mean()</span><br></pre></td></tr></table></figure><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><p>使用numpy下的standard_normal生成大小为（电影，特征）的正态分布向量x，大小为（用户，特征）的正态分布向量θ</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_origin=np.random.standard_normal((n_movies,n_features))</span><br><span class="line">theta_origin=np.random.standard_normal((n_users,n_features))</span><br></pre></td></tr></table></figure><p>为了减少参数数量，将x,θ利用ravel()属性连接</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#合并</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge</span><span class="params">(x,theta)</span>:</span></span><br><span class="line"><span class="keyword">return</span> np.concatenate((x.ravel(),theta.ravel()))</span><br></pre></td></tr></table></figure><p>根据连接的参数和电影的数量、用户的数量，特征数量得到x和θ</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#分离</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">separate</span><span class="params">(param,n_movies,n_users,n_features)</span>:</span></span><br><span class="line"><span class="keyword">return</span> param[:n_movies*n_features].reshape(n_movies,n_features),param[n_movies*n_features:].reshape(n_users,n_features)</span><br></pre></td></tr></table></figure><h3 id="求成本函数"><a href="#求成本函数" class="headerlink" title="求成本函数"></a>求成本函数</h3><p>按照公式计算成本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#2.计算损失</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(param,y,r,n_features)</span>:</span></span><br><span class="line">n_movies,n_users=y.shape <span class="comment">#y的大小是电影乘以用户</span></span><br><span class="line">x,theta=separate(param,n_movies,n_users,n_features)</span><br><span class="line"></span><br><span class="line">inner=np.multiply(np.dot(x,theta.T)-y,r) <span class="comment">#为什么要乘以r</span></span><br><span class="line">inner_cost=(<span class="number">1</span>/<span class="number">2</span>)*np.sum(np.power(inner,<span class="number">2</span>))</span><br><span class="line"><span class="keyword">return</span> inner_cost</span><br><span class="line"><span class="comment">#加上正则化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regular_cost</span><span class="params">(param,y,r,n_features,l=<span class="number">1</span>)</span>:</span></span><br><span class="line">n_movies,n_users=y.shape</span><br><span class="line">x,theta=separate(param,n_movies,n_users,n_features)</span><br><span class="line"></span><br><span class="line">inner_cost=cost(param,y,r,n_features)</span><br><span class="line"></span><br><span class="line">regular=(l/<span class="number">2</span>)*np.sum(np.power(param,<span class="number">2</span>)) </span><br><span class="line"><span class="keyword">return</span> inner_cost+regular</span><br></pre></td></tr></table></figure><h3 id="求梯度"><a href="#求梯度" class="headerlink" title="求梯度"></a>求梯度</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#3.计算梯度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad</span><span class="params">(param,y,r,n_features)</span>:</span></span><br><span class="line">n_movies,n_users=y.shape</span><br><span class="line">x,theta=separate(param,n_movies,n_users,n_features)</span><br><span class="line"></span><br><span class="line">inner=np.multiply(np.dot(x,theta.T)-y,r)</span><br><span class="line">x_grad=np.dot(inner,theta)</span><br><span class="line">theta_grad=np.dot(inner.T,x)</span><br><span class="line"><span class="keyword">return</span> merge(x_grad,theta_grad)</span><br><span class="line"></span><br><span class="line"><span class="comment">#加上正则化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regular_grad</span><span class="params">(param,y,r,n_features,l=<span class="number">1</span>)</span>:</span></span><br><span class="line">inner_grad=grad(param,y,r,n_features)</span><br><span class="line">regular=l*param  <span class="comment">#param是x和theta的连接</span></span><br><span class="line"><span class="keyword">return</span> inner_grad+regular</span><br></pre></td></tr></table></figure><h3 id="搭建模型求成本最小值"><a href="#搭建模型求成本最小值" class="headerlink" title="搭建模型求成本最小值"></a>搭建模型求成本最小值</h3><p>利用scipy下的optimize函数的最小化minimize，输入需要优化的成本函数fun，初始化的特征向量和参数向量x0，其他的输入成本函数中的参数args，梯度jac参数，最小化成本的方法method</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">res=opt.minimize(fun=regular_cost,x0=param,args=(y_mena,r,n_features,l),jac=regular_grad,method=<span class="string">'TNC'</span>)</span><br></pre></td></tr></table></figure><h3 id="进行预测"><a href="#进行预测" class="headerlink" title="进行预测"></a>进行预测</h3><p>（1）加入新用户</p><p>在原有的943个用户中，在第0个位置插入0用户，设定该用户的评分（对1682个电影的评分）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#插入第0个用户</span></span><br><span class="line">ratings=np.zeros(y.shape[<span class="number">0</span>])</span><br><span class="line">ratings[<span class="number">0</span>]=<span class="number">4</span></span><br><span class="line">ratings[<span class="number">6</span>] = <span class="number">3</span></span><br><span class="line">ratings[<span class="number">11</span>] = <span class="number">5</span></span><br><span class="line">ratings[<span class="number">53</span>] = <span class="number">4</span></span><br><span class="line">ratings[<span class="number">63</span>] = <span class="number">5</span></span><br><span class="line">ratings[<span class="number">65</span>] = <span class="number">3</span></span><br><span class="line">ratings[<span class="number">68</span>] = <span class="number">5</span></span><br><span class="line">ratings[<span class="number">97</span>] = <span class="number">2</span></span><br><span class="line">ratings[<span class="number">182</span>] = <span class="number">4</span></span><br><span class="line">ratings[<span class="number">225</span>] = <span class="number">5</span></span><br><span class="line">ratings[<span class="number">354</span>] = <span class="number">5</span></span><br></pre></td></tr></table></figure><p>对评分矩阵y和用户评价矩阵r进行修正，对0用户评价了的电影在r矩阵对应的位置设置为1，其余为0，在第0列中（第j列代表第j个用户）评价了的电影的位置插入对应的评分</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">r=np.insert(r,<span class="number">0</span>,ratings!=<span class="number">0</span>,axis=<span class="number">1</span>)</span><br><span class="line">y=np.insert(y,<span class="number">0</span>,ratings,axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>得到现在新的r,y的大小为(1682, 944)</p><p>（2）求得最优的电影特征向量和用户参数向量进行预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">res=opt.minimize(fun=regular_cost,x0=param,args=(y_mena,r,n_features,l),jac=regular_grad,method=<span class="string">'TNC'</span>)</span><br><span class="line">param_train=res.x</span><br></pre></td></tr></table></figure><p>运行模型res，得到</p><blockquote><p>fun: 64721.49781506245<br>     jac: array([-1.08020837e-07, -1.29065977e-07, -1.57187711e-07, …,<br>        7.27238394e-08,  5.33163536e-08,  3.41754532e-07])<br> message: ‘Converged (|f_n-f_(n-1)| ~= 0)’<br>    nfev: 2272<br>     nit: 73<br>  status: 1<br> success: True<br>       x: array([-0.23955828,  0.16407661,  0.29655461, …, -0.25011702,<br>       -0.05913302, -0.37323785])</p></blockquote><p>得到的res.x即为得到电影特征向量和用户参数向量，即x,θ（4）使用x * θ得到所有用户的预测值，取第0列代表第0个用户，再加上评价矩阵y的均值，最终得到实际的预测值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">predict=np.dot(x_train,theta_train.T)</span><br><span class="line"><span class="comment">#找到第0个用户</span></span><br><span class="line">real_predict=predict[:,<span class="number">0</span>]+y.mean()</span><br></pre></td></tr></table></figure><blockquote><p>[[3.43489966 4.31346413 3.62320829 … 4.0002449  4.17788315 3.62404608]<br> [2.29870271 2.862435   2.55152555 … 2.51846228 3.51772075 3.55448165]<br> [2.07116718 3.23831192 1.93001552 … 2.21423696 2.8404734  2.3667349 ]<br> …<br> [0.28474721 0.35122272 0.27118557 … 0.30820796 0.29570489 0.33098931]<br> [0.35362524 0.44858761 0.46314461 … 0.3805095  0.56209768 0.51186381]<br> [0.43396907 0.87167246 0.60553899 … 0.55149309 0.64343062 0.61699602]]</p></blockquote><p>大小为（1682，944），得到用户j对电影i的预测评分，最后需要需要的第0用户即第0列，加上评价矩阵y的均值</p><p>（3）将得到的预测值使用numpy下的argsort，利用[::-1]按照降序排列得到前几个的索引值，利用实际预测值中取索引前几个作为预测的电影</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#以降序形式排列</span></span><br><span class="line">idx=np.argsort(real_predict)[::<span class="number">-1</span>]</span><br><span class="line">print(real_predict[idx][:<span class="number">10</span>])<span class="comment">#输出前10个索引对应的行的评分</span></span><br></pre></td></tr></table></figure><p>取预测值的前十个预测的评分，其对应的电影即为最可能推荐给该用户的</p><blockquote><p>[4.12535237 4.04413113 3.99324234 3.9190281  3.81690478 3.8155606<br> 3.76602376 3.76322594 3.75904683 3.75077914]</p></blockquote><p>（4）读入电影列表，将y评分对应的电影读入，并按照得到的预测值取前几个的索引作为推荐的电影</p><p>读入电影名称文件，去除每行中的空格分割，并将每行的电影名字以空格连接（使用’  ‘.join(str)，用空格练剑字符串str）</p><p>注：latin-1编码能读难解码的文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#读入电影的名称</span></span><br><span class="line">movie_txt=<span class="string">'./data/movie_ids.txt'</span></span><br><span class="line">movie_list=[]</span><br><span class="line">file=open(movie_txt,encoding=<span class="string">'latin-1'</span>)</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> file.readlines():</span><br><span class="line">cur=line.strip().split(<span class="string">' '</span>)</span><br><span class="line">movie_list.append(<span class="string">' '</span>.join(cur[<span class="number">1</span>:])) </span><br><span class="line">movie_list=np.array(movie_list)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> movie_list[idx][:<span class="number">10</span>]:</span><br><span class="line">print(i)</span><br></pre></td></tr></table></figure><p>最终输出的最有可能推荐给0用户的c电影为：</p><blockquote><p>Titanic (1997)<br>Star Wars (1977)<br>Shawshank Redemption, The (1994)<br>Forrest Gump (1994)<br>Raiders of the Lost Ark (1981)<br>Braveheart (1995)<br>Return of the Jedi (1983)<br>Usual Suspects, The (1995)<br>Godfather, The (1972)<br>Schindler’s List (1993)<br>[Finished in 152.6s]</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;推荐算法的应用场景&quot;&gt;&lt;a href=&quot;#推荐算法的应用场景&quot; class=&quot;headerlink&quot; title=&quot;推荐算法的应用场景&quot;&gt;&lt;/a&gt;推荐算法的应用场景&lt;/h2&gt;&lt;p&gt;当已知部分user对一些product有评价，对该用户基于之前的评价推荐相近的product，这时候就需要用到推荐算法。&lt;/p&gt;&lt;p&gt;如给定一些用户对一些电影的评分，求对这些用户推荐什么电影。&lt;/p&gt;&lt;h2 id=&quot;符号含义&quot;&gt;&lt;a href=&quot;#符号含义&quot; class=&quot;headerlink&quot; title=&quot;符号含义&quot;&gt;&lt;/a&gt;符号含义&lt;/h2&gt;&lt;p&gt;r（i，j）：表示用户j评价了电影i&lt;/p&gt;&lt;p&gt;y（i，j）：用户j对电影i的评价等级（如果用户评价过该电影）&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Leecode做题记录（二）</title>
    <link href="https://www.xiapf.com/blogs/LC2/"/>
    <id>https://www.xiapf.com/blogs/LC2/</id>
    <published>2020-09-08T04:47:07.000Z</published>
    <updated>2020-09-08T04:49:29.279Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目：面试题-08-03-魔术索引"><a href="#题目：面试题-08-03-魔术索引" class="headerlink" title="题目：面试题 08.03. 魔术索引"></a>题目：面试题 08.03. 魔术索引</h2><p>描述：魔术索引。 在数组A[0…n-1]中，有所谓的魔术索引，满足条件A[i] = i。给定一个有序整数数组，编写一种方法找出魔术索引，若有的话，在数组A中找出一个魔术索引，如果没有，则返回-1。若有多个魔术索引，返回索引值最小的一个。</p><p>思路：</p><p>（1）数组中没有重复数字</p><a id="more"></a><p>有序递增数组自然想到二分查找，将数组中的数字减去自身索引，则以元素和索引相等的数字为界限，前面的数字必然小于0，后面的数字大于0，只需要找元素为0的数字。将数组一分为二，如果中间值正好等于0就返回，反之，如果中间值大于0，则在前半部分找，小于0在后半部分找。</p><p>当有多个重复数字时，则不能使用上述二分法。</p><p>使用分治法：当中间值等于索引，则根据当前res值进行赋值，如果res没有赋值过则之间赋值，如果mid&lt;res，也进行赋值，此时找到一个索引，为了找到更小的，则在左边继续找。</p><p>当中间值不等于索引值，则先在左边找，再在右边找。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">search</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums,<span class="keyword">int</span> left,<span class="keyword">int</span> right,<span class="keyword">int</span>&amp; res)</span></span></span><br><span class="line"><span class="function"> </span>&#123;</span><br><span class="line">     <span class="keyword">if</span>(left&gt;right)</span><br><span class="line">         <span class="keyword">return</span>;</span><br><span class="line">     <span class="keyword">int</span> mid=left+(right-left)/<span class="number">2</span>;</span><br><span class="line">     <span class="comment">//找到了一个解，再在左边找,看有无更小的</span></span><br><span class="line">     <span class="keyword">if</span>(nums[mid]==mid)</span><br><span class="line">     &#123;</span><br><span class="line">         <span class="keyword">if</span>(res==<span class="number">-1</span>)</span><br><span class="line">             res=mid;</span><br><span class="line">         <span class="keyword">else</span> <span class="keyword">if</span>(mid&lt;res)</span><br><span class="line">             res=mid;</span><br><span class="line">         search(nums, left, mid<span class="number">-1</span>,res);</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="comment">//没有找到解，在左边找,坐标找不到再去右边</span></span><br><span class="line">     <span class="keyword">else</span></span><br><span class="line">     &#123;</span><br><span class="line">         search(nums, left, mid<span class="number">-1</span>,res);</span><br><span class="line">         search(nums, mid+<span class="number">1</span>, right,res);</span><br><span class="line">     &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>（2）数组中有重复数字</p><p>对遍历法的优化，当索引和当前数字不相等的时候，在当前索引+1和当前数字中取大的数。如果当前数字大，说明索引为当前数字之前都不可能匹配大，如果当前数字小，就继续下一个索引进行匹配。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;len;)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span>(nums[i]==i)</span><br><span class="line">        <span class="keyword">return</span> i;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        i=<span class="built_in">max</span>(i+<span class="number">1</span>,nums[i]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="题目：104-二叉树的最大深度"><a href="#题目：104-二叉树的最大深度" class="headerlink" title="题目：104. 二叉树的最大深度"></a>题目：104. 二叉树的最大深度</h2><p>描述：给定一个二叉树，找出其最大深度。二叉树的深度为根节点到最远叶子节点的最长路径上的节点数。说明: 叶子节点是指没有子节点的节点。</p><p>思路：求深度可以分解为求左子树的深度，右子树深度的较大值加一，则是根节点的深度。或者可以利用广度遍历，每层节点进入队列后对层数加一</p><p>（1）广度遍历</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">queue</span>&lt;TreeNode*&gt; p;</span><br><span class="line">p.push(root);</span><br><span class="line"><span class="keyword">while</span>(!p.empty())</span><br><span class="line">&#123;</span><br><span class="line">    layer=layer+<span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> <span class="built_in">width</span>=p.<span class="built_in">size</span>();</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="built_in">width</span>;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        TreeNode* r=p.front();</span><br><span class="line">        p.pop();</span><br><span class="line">        <span class="keyword">if</span>(r-&gt;left) p.push(r-&gt;left);</span><br><span class="line">        <span class="keyword">if</span>(r-&gt;right) p.push(r-&gt;right);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>（2）深度遍历（递归）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(root==<span class="literal">NULL</span>)</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="built_in">max</span>(maxDepth(root-&gt;left),maxDepth(root-&gt;right))+<span class="number">1</span>;</span><br></pre></td></tr></table></figure><h2 id="题目：21-合并两个有序链表"><a href="#题目：21-合并两个有序链表" class="headerlink" title="题目：21. 合并两个有序链表"></a>题目：21. 合并两个有序链表</h2><p>描述：将两个升序链表合并为一个新的 升序 链表并返回。新链表是通过拼接给定的两个链表的所有节点组成的。 </p><p>思路：</p><p>（1）迭代法</p><p>将L1,L2中小的值的节点赋给新的链表，当两个链表中短的那个遍历结束，就把长的链表剩余还有剩的节点附在新链表后面。</p><p>小技巧：设置一个dummyHead作为新链表，最后返回dummyHead-&gt;next</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">ListNode* l3=<span class="keyword">new</span> ListNode(<span class="number">0</span>);</span><br><span class="line">    ListNode* result=l3;<span class="comment">//dummyHead</span></span><br><span class="line">    <span class="keyword">while</span>(l1!=<span class="literal">NULL</span>&amp;&amp;l2!=<span class="literal">NULL</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(l1-&gt;val&lt;l2-&gt;val)</span><br><span class="line">        &#123;</span><br><span class="line">            l3-&gt;next=l1;</span><br><span class="line">            l1=l1-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            l3-&gt;next=l2;</span><br><span class="line">            l2=l2-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        l3=l3-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    l3-&gt;next=l1==<span class="literal">NULL</span>?l2:l1;</span><br></pre></td></tr></table></figure><p>（2）递归法</p><p>当L1,L2都是空链表直接合并，当不为空，就将小的那个作为下次合并的节点，当有一个为空，则返回另一个不为空的链表，直接放在结果后面。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//结束条件</span></span><br><span class="line"><span class="keyword">if</span>(l1==<span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span> l2;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(l2==<span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span> l1;</span><br><span class="line"><span class="comment">//递归条件</span></span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(l1-&gt;val&lt;l2-&gt;val)</span><br><span class="line">    &#123;</span><br><span class="line">        l1-&gt;next=mergeTwoLists(l1-&gt;next,l2);</span><br><span class="line">        <span class="keyword">return</span> l1;<span class="comment">//最终返回的结果</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        l2-&gt;next=mergeTwoLists(l1,l2-&gt;next);</span><br><span class="line">        <span class="keyword">return</span> l2;<span class="comment">//最终返回的结果</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：415-字符串相加"><a href="#题目：415-字符串相加" class="headerlink" title="题目：415. 字符串相加"></a>题目：415. 字符串相加</h2><p>描述：给定两个字符串形式的非负整数 num1 和num2 ，计算它们的和。注意：num1 和num2 的长度都小于 5100.num1 和num2 都只包含数字 0-9.num1 和num2 都不包含任何前导零。你不能使用任何內建 BigInteger 库， 也不能直接将输入的字符串转换为整数形式。</p><p>思路：从两个字符的最后逐位向前取字符转换为数字，将两个数字相加，末尾数字作为当前新字符串的最后一位，高位数字作为进位，供下一次相加。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="built_in">string</span> <span class="title">res</span><span class="params">(num1.<span class="built_in">size</span>()+num2.<span class="built_in">size</span>(),<span class="string">'0'</span>)</span></span>;</span><br><span class="line">    <span class="comment">//从最后一位开始加</span></span><br><span class="line">    <span class="keyword">while</span>(i&gt;=<span class="number">0</span>||j&gt;=<span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> a=i&gt;=<span class="number">0</span>?num1[i]-<span class="string">'0'</span>:<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> b=j&gt;=<span class="number">0</span>?num2[j]-<span class="string">'0'</span>:<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> temp=(res[r]-<span class="string">'0'</span>)+a+b;<span class="comment">//数字相加</span></span><br><span class="line">        res[r]=temp%<span class="number">10</span>+<span class="string">'0'</span>;<span class="comment">//当前位</span></span><br><span class="line">        res[r<span class="number">-1</span>]=res[r<span class="number">-1</span>]+temp/<span class="number">10</span>;<span class="comment">//进位</span></span><br><span class="line">        i--;</span><br><span class="line">        j--;</span><br><span class="line">        r--;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> k=<span class="number">0</span>;k&lt;res.<span class="built_in">size</span>();k++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(res[k]!=<span class="string">'0'</span>)</span><br><span class="line">            <span class="keyword">return</span> res.substr(k);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>技巧：当两个字符串长度不相等时，但又需要对字符串中每个字符处理时，采用三目运算符进行赋值</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> a=i&gt;=<span class="number">0</span>?num1[i]-<span class="string">'0'</span>:<span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> b=j&gt;=<span class="number">0</span>?num2[j]-<span class="string">'0'</span>:<span class="number">0</span>;</span><br></pre></td></tr></table></figure><p>（2）字符转换为数字：a[i]-‘0’；数字转换为字符：temp+’0’</p><h2 id="题目：43-字符串相乘"><a href="#题目：43-字符串相乘" class="headerlink" title="题目：43. 字符串相乘"></a>题目：43. 字符串相乘</h2><p>描述：给定两个以字符串形式表示的非负整数 num1 和 num2，返回 num1 和 num2 的乘积，它们的乘积也表示为字符串形式。</p><p>思路：字符串相乘和字符串相加思路类似，两个l1,l2字符串相乘可以看成l1和l2的依次每一位相乘再相加，相乘得到的数字的尾数作为当前新字符串的数字，进位放在前一位便于后面计算使用，即利用乘法的竖向错位相加思想。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="built_in">string</span> <span class="title">res</span><span class="params">(len1+len2,<span class="string">'0'</span>)</span></span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j=len2<span class="number">-1</span>;j&gt;=<span class="number">0</span>;j--)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=len1<span class="number">-1</span>;i&gt;=<span class="number">0</span>;i--)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">int</span> temp=(res[i+j+<span class="number">1</span>]-<span class="string">'0'</span>)+(num1[i]-<span class="string">'0'</span>)*(num2[j]-<span class="string">'0'</span>);</span><br><span class="line">            res[i+j+<span class="number">1</span>]=temp%<span class="number">10</span>+<span class="string">'0'</span>;<span class="comment">//当前位 转换为字符</span></span><br><span class="line">            res[i+j]=res[i+j]+temp/<span class="number">10</span>;<span class="comment">//上一位加上进位</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;len1+len2;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(res[i]!=<span class="string">'0'</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">return</span> res.substr(i);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：207-课程表"><a href="#题目：207-课程表" class="headerlink" title="题目：207. 课程表"></a>题目：207. 课程表</h2><p>描述：你这个学期必须选修 numCourse 门课程，记为 0 到 numCourse-1 。在选修某些课程之前需要一些先修课程。 例如，想要学习课程 0 ，你需要先完成课程 1 ，我们用一个匹配来表示他们：[0,1]，给定课程总量以及它们的先决条件，请你判断是否可能完成所有课程的学习？</p><p>思路：拓扑排序：广度优先不理+贪心算法用于有向图（所有前驱活动都排在该活动的前面，并且可以完成所有活动）。每次都从图中删除没有前驱的节点，设置入度数组，每次对入度为0的节点移除，并修改其后驱的入度-1，依次得到的节点就是拓扑排序的序列。</p><p>模板：构建入度数组和邻接表——&gt;入度为0的节点入队列——&gt;广度遍历（每次对入度为0的节点移除，并修改其后驱的入度-1，对后面节点入度为0的入队）</p><p>如果不能遍历所有节点，说明存在环，无法进行排序，即活动无法进行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">vector&lt;int&gt; indu(numCourses,<span class="number">0</span>);</span><br><span class="line">vector&lt;vector&lt;int&gt;&gt; graph(numCourses,vector&lt;int&gt;());</span><br><span class="line">//<span class="number">1.</span>构建每个点的入度表和邻接表</span><br><span class="line"><span class="keyword">for</span>(int i=<span class="number">0</span>;i&lt;prerequisites.size();i++)</span><br><span class="line">&#123;</span><br><span class="line">    indu[prerequisites[i][<span class="number">0</span>]]++;//第<span class="number">0</span>列的点有第<span class="number">1</span>列指向它，所以入度加一</span><br><span class="line">    graph[prerequisites[i][<span class="number">1</span>]].push_back(prerequisites[i][<span class="number">0</span>]);//第<span class="number">1</span>列的点指向第<span class="number">0</span>列，所以保存为邻接表形式</span><br><span class="line">&#125;</span><br><span class="line">queue&lt;int&gt; p;</span><br><span class="line">//<span class="number">2.</span>队列首节点，入度为<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span>(int i=<span class="number">0</span>;i&lt;indu.size();i++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span>(indu[i]==<span class="number">0</span>)</span><br><span class="line">        p.push(i);</span><br><span class="line">&#125;</span><br><span class="line">int count=<span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span>(!p.empty())</span><br><span class="line">&#123;</span><br><span class="line">    int tmp=p.front();</span><br><span class="line">    p.pop();</span><br><span class="line">    count++;</span><br><span class="line">    //把出队的节点的后续节点的入度<span class="number">-1</span></span><br><span class="line">    <span class="keyword">for</span>(int i=<span class="number">0</span>;i&lt;graph[tmp].size();i++)</span><br><span class="line">    &#123;</span><br><span class="line">        indu[graph[tmp][i]]--;</span><br><span class="line">        <span class="keyword">if</span>(indu[graph[tmp][i]]==<span class="number">0</span>)</span><br><span class="line">            p.push(graph[tmp][i]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="题目：210-课程表-II"><a href="#题目：210-课程表-II" class="headerlink" title="题目：210. 课程表 II"></a>题目：210. 课程表 II</h2><p>描述：现在你总共有 n 门课需要选，记为 0 到 n-1。在选修某些课程之前需要一些先修课程。 例如，想要学习课程 0 ，你需要先完成课程 1 ，我们用一个匹配来表示他们: [0,1]，给定课程总量以及它们的先决条件，返回你为了学完所有课程所安排的学习顺序。可能会有多个正确的顺序，你只要返回一种就可以了。如果不可能完成所有课程，返回一个空数组。</p><p>思路：思路同上，在每次广度遍历时，记录每次移除的节点，即为排序的结果。</p><h2 id="题目：130-被围绕的区域"><a href="#题目：130-被围绕的区域" class="headerlink" title="题目：130. 被围绕的区域"></a>题目：130. 被围绕的区域</h2><p>描述：给定一个二维的矩阵，包含 ‘X’ 和 ‘O’（字母 O）。找到所有被 ‘X’ 围绕的区域，并将这些区域里所有的 ‘O’ 用 ‘X’ 填充。</p><p>被围绕的区间不会存在于边界上，换句话说，任何边界上的 ‘O’ 都不会被填充为 ‘X’。 任何不在边界上，或不与边界上的 ‘O’ 相连的 ‘O’ 最终都会被填充为 ‘X’。如果两个元素在水平或垂直方向相邻，则称它们是“相连”的。</p><p>思路：找被围绕的区域略复杂，因此从另一个角度思考，找非围绕的区域，即边届上的’O‘及与边界上的’O’相连的’O’。</p><p>（1）当在边界上（第一行和最后一行、第一列和最后一列）找到’O’的时候将’O’用’P’（或者其他不同于’O’和‘X’的符号代替）</p><p>（2）并在该位置的上下左右查找是否存在’O’（用递归）。</p><p>（3）到最后显示结果时，将“边界”的‘P’设置为’O’，因为没有被围绕，其他的地方设置为‘X’</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//第一行和最后一行</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;col;j++)</span><br><span class="line">    &#123;</span><br><span class="line">        dfs(board,<span class="number">0</span>,j);</span><br><span class="line">        dfs(board,row<span class="number">-1</span>,j);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//第一列和最后一列 同上 dfs(board,i,0);  dfs(board,i,col-1);</span></span><br><span class="line">    <span class="comment">//图像转换</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;row;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;col;j++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(board[i][j]==<span class="string">'O'</span>)</span><br><span class="line">                board[i][j]=<span class="string">'X'</span>;</span><br><span class="line">            <span class="keyword">if</span>(board[i][j]==<span class="string">'P'</span>)</span><br><span class="line">                board[i][j]=<span class="string">'O'</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">char</span>&gt;&gt;&amp; board,<span class="keyword">int</span> row,<span class="keyword">int</span> col)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(row&lt;<span class="number">0</span>||row&gt;=board.<span class="built_in">size</span>()||col&lt;<span class="number">0</span>||col&gt;=board[<span class="number">0</span>].<span class="built_in">size</span>())</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    <span class="keyword">if</span>(board[row][col]==<span class="string">'O'</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        board[row][col]=<span class="string">'P'</span>;</span><br><span class="line">        <span class="comment">//上下左右找有无连接的‘o'</span></span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; dir&#123;&#123;<span class="number">0</span>,<span class="number">1</span>&#125;,&#123;<span class="number">0</span>,<span class="number">-1</span>&#125;,&#123;<span class="number">1</span>,<span class="number">0</span>&#125;,&#123;<span class="number">-1</span>,<span class="number">0</span>&#125;&#125;;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;dir.<span class="built_in">size</span>();i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">int</span> a=row+dir[i][<span class="number">0</span>];</span><br><span class="line">            <span class="keyword">int</span> b=col+dir[i][<span class="number">1</span>];</span><br><span class="line">            dfs(board,a,b);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="题目：696-计数二进制子串"><a href="#题目：696-计数二进制子串" class="headerlink" title="题目：696. 计数二进制子串"></a>题目：696. 计数二进制子串</h2><p>描述：给定一个字符串 s，计算具有相同数量0和1的非空(连续)子字符串的数量，并且这些子字符串中的所有0和所有1都是组合在一起的。重复出现的子串要计算它们出现的次数。</p><p>思路：要找到连续0，1的子串，可以采用中心扩展法。</p><p>（1）当遇到0，1不一样的位置，则找到了当前位置的最小二进制子串</p><p>（2）为了把当前位置所有子串都计数，则可以以当前位置为中心向两边扩展，当向前扩展的字符和左边字符相同，向后扩展的字符和右边字符相同，则得到当前位置扩展后的字符串。</p><p>（3）对不同的位置进行扩展，得到最终的子串个数。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//找到前后位置不同的数字，并前后扩展</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;s.length();i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> start=i<span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">int</span> <span class="built_in">end</span>=i;</span><br><span class="line">        <span class="keyword">char</span> l=s[start];</span><br><span class="line">        <span class="keyword">char</span> r=s[<span class="built_in">end</span>];</span><br><span class="line">        <span class="keyword">if</span>(l==r) <span class="comment">//找到0，1不同的位置</span></span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        <span class="keyword">while</span>(start&gt;=<span class="number">0</span>&amp;&amp;<span class="built_in">end</span>&lt;s.length()&amp;&amp;l==s[start]&amp;&amp;r==s[<span class="built_in">end</span>])</span><br><span class="line">        &#123;</span><br><span class="line">            start--;</span><br><span class="line">            <span class="built_in">end</span>++;</span><br><span class="line">            count++;</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：647-回文子串"><a href="#题目：647-回文子串" class="headerlink" title="题目：647. 回文子串"></a>题目：647. 回文子串</h2><p>描述：给定一个字符串，你的任务是计算这个字符串中有多少个回文子串。具有不同开始位置或结束位置的子串，即使是由相同的字符组成，也会被计为是不同的子串。</p><p>思路：思路同上，使用中心扩展法，找到每个位置的回文串，因为回文串的长度可以为偶数或者奇数。</p><p>（1）对字符串中每个字符使用中心扩展法找回文子串</p><p>（2）回文子串长度为偶数，则前一个指针指向当前位置，后一个指针指向后一个位置，对称查找</p><p>（3）回文子串长度为奇数，则两个指针都指向当前位置，然后再对称查找</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">countSubstrings</span><span class="params">(<span class="built_in">string</span> s)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//中心扩展法</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;s.length();i++)</span><br><span class="line">        &#123;</span><br><span class="line">            countExpandString(s, i, i+<span class="number">1</span>);<span class="comment">//偶数回文串</span></span><br><span class="line">            countExpandString(s, i, i);<span class="comment">//奇数回文串</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> count;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">countExpandString</span><span class="params">(<span class="built_in">string</span> s,<span class="keyword">int</span> start,<span class="keyword">int</span> <span class="built_in">end</span>)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">while</span>(start&gt;=<span class="number">0</span>&amp;&amp;<span class="built_in">end</span>&lt;s.length()&amp;&amp;s[start]==s[<span class="built_in">end</span>])</span><br><span class="line">        &#123;</span><br><span class="line">            start--;</span><br><span class="line">            <span class="built_in">end</span>++;</span><br><span class="line">            count++;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：109-有序链表转换二叉搜索树"><a href="#题目：109-有序链表转换二叉搜索树" class="headerlink" title="题目：109. 有序链表转换二叉搜索树"></a>题目：109. 有序链表转换二叉搜索树</h2><p>描述：给定一个单链表，其中的元素按升序排序，将其转换为高度平衡的二叉搜索树。本题中，一个高度平衡二叉树是指一个二叉树每个节点 的左右两个子树的高度差的绝对值不超过 1。</p><p>思路：由于是有序链表，所以按照中点构造左右子树即可</p><p>（1）找中点：使用快慢指针找到当前链表的中点（快指针走2步，慢指针走2步，当快指针到末尾的时候，慢指针正好在中间），找到中点后，下面构造树</p><p>（2）确定根节点：慢指针的指向的值即为中点，赋值给当前的根节点</p><p>（3）递归构造左子树：从链表开始到慢指针的位置（即中点的位置）用作构造左子树</p><p>（4）递归构造右子树：从慢指针下一个的位置（即中点下一个的位置）到链表最后用作构造左子树</p><p>不断递归，构造了二叉平衡树，保证了根节点大于左边节点，小于右边节点，当链表头尾相同，说明只剩下最后一个叶子节点，当前树枝构造完成。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">root=merge(head, <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line"><span class="function">TreeNode* <span class="title">merge</span><span class="params">(ListNode* head,ListNode* tail)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(head==tail)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    ListNode* fast=head;</span><br><span class="line">    ListNode* slow=head;</span><br><span class="line">    <span class="keyword">while</span> (fast!=tail&amp;&amp;fast-&gt;next!=tail) &#123;</span><br><span class="line">        slow=slow-&gt;next;</span><br><span class="line">        fast=fast-&gt;next-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    TreeNode* root=<span class="keyword">new</span> TreeNode(slow-&gt;val);</span><br><span class="line">    root-&gt;left=merge(head, slow);</span><br><span class="line">    root-&gt;right=merge(slow-&gt;next, tail);</span><br><span class="line">    <span class="keyword">return</span> root;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="题目：448-找到所有数组中消失的数字"><a href="#题目：448-找到所有数组中消失的数字" class="headerlink" title="题目：448. 找到所有数组中消失的数字"></a>题目：448. 找到所有数组中消失的数字</h2><p>描述：给定一个范围在  1 ≤ a[i] ≤ n ( n = 数组大小 ) 的 整型数组，数组中的元素一些出现了两次，另一些只出现一次。找到所有在 [1, n] 范围之间没有出现在数组中的数字。</p><p>您能在不使用额外空间且时间复杂度为O(n)的情况下完成这个任务吗? 你可以假定返回的数组不算在额外空间内。</p><p>思路：题目可以转换为索引为1 ~ n的数字中哪些没有出现在数组的元素中</p><p>（1）将数组中出现的数字对应的索引位置标记为负数，则表示该索引出现过：为了不使得前面数字对后面数字加负号而产生影响，则将原始需要赋赋值的位置取正数（nums[abs(nums[i])-1]=-abs(nums[abs(nums[i])-1])）</p><p>（2）遍历原始数组，出现正数的位置则说明当前索引没有出现（注意结果要用索引+1）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(int i=<span class="number">0</span>;i&lt;nums.size();i++)</span><br><span class="line">&#123;</span><br><span class="line">    nums[abs(nums[i])<span class="number">-1</span>]=-abs(nums[abs(nums[i])<span class="number">-1</span>]);//用绝对值不让原来的数受影响</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(int i=<span class="number">0</span>;i&lt;nums.size();i++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span>(nums[i]&gt;<span class="number">0</span>)</span><br><span class="line">        res.push_back(i+<span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;题目：面试题-08-03-魔术索引&quot;&gt;&lt;a href=&quot;#题目：面试题-08-03-魔术索引&quot; class=&quot;headerlink&quot; title=&quot;题目：面试题 08.03. 魔术索引&quot;&gt;&lt;/a&gt;题目：面试题 08.03. 魔术索引&lt;/h2&gt;&lt;p&gt;描述：魔术索引。 在数组A[0…n-1]中，有所谓的魔术索引，满足条件A[i] = i。给定一个有序整数数组，编写一种方法找出魔术索引，若有的话，在数组A中找出一个魔术索引，如果没有，则返回-1。若有多个魔术索引，返回索引值最小的一个。&lt;/p&gt;&lt;p&gt;思路：&lt;/p&gt;&lt;p&gt;（1）数组中没有重复数字&lt;/p&gt;
    
    </summary>
    
    
      <category term="算法" scheme="https://www.xiapf.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="https://www.xiapf.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>异常检测</title>
    <link href="https://www.xiapf.com/blogs/tfAnomalyDet/"/>
    <id>https://www.xiapf.com/blogs/tfAnomalyDet/</id>
    <published>2020-09-07T08:51:38.000Z</published>
    <updated>2020-09-14T06:04:30.157Z</updated>
    
    <content type="html"><![CDATA[<p>异常检测是指给定一个数据，放入原有模型中判断该点是否异常。</p><h2 id="算法简介"><a href="#算法简介" class="headerlink" title="算法简介"></a>算法简介</h2><p>（1）根据数据集{x1,x2,…,xm}计算数据集的特征值的均值和方差</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200914135110.png" alt></p><p>（2）根据数据集{x1,x2,…,xm}建立模型p(x)，将所有特征的模型相乘，模型p(x)定义如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200914135143.jpg" alt></p><p>根据之前得到的所有特征的均值和方差计算得到p(x)，即得到了概率估计</p><a id="more"></a><p>（3）确定概率估计函数后，给定一个新的值xtest，当p(xtest)&lt;epsilon，则代表异常</p><h2 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h2><h3 id="构造数据集：选择与异常数据相关的特征"><a href="#构造数据集：选择与异常数据相关的特征" class="headerlink" title="构造数据集：选择与异常数据相关的特征"></a>构造数据集：选择与异常数据相关的特征</h3><p>选择的特征经历是高斯分布，可以使用一下方法使得分高斯分布的函数分布得接近高斯分布：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200908135855.png" alt></p><p>训练集和验证集由原始数据集通过scipy.io下的loadmat导入</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.导入数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(path)</span>:</span></span><br><span class="line">data=sio.loadmat(path)</span><br><span class="line">print(data.keys())</span><br><span class="line">x=data.get(<span class="string">'X'</span>)</span><br><span class="line">xval=data.get(<span class="string">'Xval'</span>)</span><br><span class="line">yval=data.get(<span class="string">'yval'</span>)</span><br><span class="line"><span class="keyword">return</span> x,xval,yval</span><br><span class="line">x,xval,yval=load_data(<span class="string">'./data/ex8data2.mat'</span>)</span><br><span class="line"><span class="comment">#划分验证集和测试集数据</span></span><br><span class="line">xval,xtest,yval,ytest=model_selection.train_test_split(xval,yval,test_size=<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure><p>训练集：此时导入的x数据为（307，2）即307个样本，每个样本的特征为2个，训练集没有标签y，仅用于训练模型</p><p>验证集和测试集：将导入的验证集通过sklearn下的model_selection库的train_test_split函数，输入验证集和划分比列test_size，得到验证集和测试集，其中验证集的x数据为（153，2），标签y为（153，1）对应153个样本是否存在异常，y=0说明正常，y=1说明异常。测试集的x数据为（154，2），标签y为（154，1）对应154个样本是否存在异常。</p><p>训练集数据如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200907162738.png" alt></p><h3 id="使用测试集训练模型：使用训练集数据拟合模型的参数，根据参数得到概率密度函数p，即需要求得的模型"><a href="#使用测试集训练模型：使用训练集数据拟合模型的参数，根据参数得到概率密度函数p，即需要求得的模型" class="headerlink" title="使用测试集训练模型：使用训练集数据拟合模型的参数，根据参数得到概率密度函数p，即需要求得的模型"></a><strong><u>使用测试集训练模型</u></strong>：使用训练集数据拟合模型的参数，根据参数得到概率密度函数p，即需要求得的模型</h3><p>1°模型由均值和方差确定，根据如下公式求出模型</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200907155412.png" alt></p><p>利用numpy数组对象的mean()，val()方法求出训练数据的均值和方差</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#2.参数估计</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parameter_estimate</span><span class="params">(x)</span>:</span></span><br><span class="line">mean=x.mean(axis=<span class="number">0</span>)</span><br><span class="line">var=x.var(axis=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># cov=np.cov(x.T)</span></span><br><span class="line"><span class="keyword">return</span> mean,var</span><br><span class="line">mean,var=parameter_estimate(x)</span><br></pre></td></tr></table></figure><p>得到的均值和方差如下，即得到了每个特征的均值和方差：</p><blockquote><p>[14.11222578 14.99771051]<br>[1.83263141 1.70974533]</p></blockquote><p>2°根据训练集数据和求得的均值、方差得出概率密度函数</p><p>根据scipy库下的stats统计函数中的norm.pdf方法得到对应数据、均值、方差的概率密度函数，其中scipy.stats表示生产指定分布，norm表示正态分布，pdf表示求正态分布在x处的概率密度函数的函数值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#3.应用参数得到概率密度函数(使用训练集)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">norm_data</span><span class="params">(x,mean,var)</span>:</span></span><br><span class="line">m,n=x.shape</span><br><span class="line">p=np.zeros((m,n))</span><br><span class="line"></span><br><span class="line"><span class="comment">#第i个特征的概率密度p(x)</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">tmp=stats.norm(mean[i],var[i])</span><br><span class="line">p[:,i]=tmp.pdf(x[:,i])</span><br><span class="line"><span class="keyword">return</span> p</span><br><span class="line">p=norm_data(x,mean,var)</span><br></pre></td></tr></table></figure><p>得到每个样本的每个特征的概率密度函数值，大小为（307，2）</p><h3 id="使用验证集确定模型的阈值：当概率密度函数值大于阈值则认定为正常，当小于阈值则是异常值"><a href="#使用验证集确定模型的阈值：当概率密度函数值大于阈值则认定为正常，当小于阈值则是异常值" class="headerlink" title="使用验证集确定模型的阈值：当概率密度函数值大于阈值则认定为正常，当小于阈值则是异常值"></a><u><strong>使用验证集确定模型的阈值</strong></u>：当概率密度函数值大于阈值则认定为正常，当小于阈值则是异常值</h3><p>1°计算当前模型下验证集经过概率密度函数后的函数值</p><p>2°确定阈值的区间，根据当前概率密度函数值最小和最大值之间，利用np.arange取1000个步长</p><p>3°循环读取不同的阈值，利用概率密度值小于阈值得到所有样本的预测值，从而利用numpy下的判断表达式真的函数logical_and判断预测值和实际标签是否一致</p><p>4°计算当前阈值下的召回率和准确率，从而求出F1分数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#4.选择合适的阈值(使用验证集)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_threshold</span><span class="params">(pval,yval)</span>:</span></span><br><span class="line">best_f1score=<span class="number">0</span></span><br><span class="line">best_threshold=<span class="number">0</span></span><br><span class="line">step=(pval.max()-pval.min())/<span class="number">1000</span></span><br><span class="line"></span><br><span class="line">thresholds=np.arange(pval.min(),pval.max(),step)</span><br><span class="line"><span class="keyword">for</span> threshold <span class="keyword">in</span> thresholds:</span><br><span class="line">tp=np.sum(np.logical_and(phat==<span class="number">1</span>,yval==<span class="number">1</span>))</span><br><span class="line">fp=np.sum(np.logical_and(phat==<span class="number">1</span>,yval==<span class="number">0</span>))</span><br><span class="line">fn=np.sum(np.logical_and(phat==<span class="number">0</span>,yval==<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">precision=tp/(tp+fp)</span><br><span class="line">recall=tp/(tp+fn)</span><br><span class="line">f1score=(<span class="number">2</span>*precision*recall)/(precision+recall)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(f1score&gt;best_f1score): <span class="comment">#f1分数越大越好</span></span><br><span class="line">best_f1score=f1score</span><br><span class="line">best_threshold=threshold</span><br><span class="line"><span class="keyword">return</span> best_threshold,best_f1score</span><br><span class="line"></span><br><span class="line">pval=norm_data(xval,mean,var)</span><br><span class="line">best_threshold,best_f1score=select_threshold(pval,yval)</span><br><span class="line">print(best_threshold,best_f1score)</span><br></pre></td></tr></table></figure><p>最终选择的阈值为：0.009566706005956842</p><p>根据得到的最优阈值和训练集的概率值，利用np.where找到满足（p&lt;阈值）的样本点的索引，将异常值用红色的点标注出来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">output=np.where(p&lt;best_threshold)</span><br><span class="line">print(output)</span><br><span class="line">fig,ax=plt.subplots(figsize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">ax.scatter(x[:,<span class="number">0</span>],x[:,<span class="number">1</span>])</span><br><span class="line">ax.scatter(x[output[<span class="number">0</span>],<span class="number">0</span>],x[output[<span class="number">0</span>],<span class="number">1</span>],c=<span class="string">'r'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200907163035.png" alt></p><h3 id="使用测试集测试得到的异常检测的模型"><a href="#使用测试集测试得到的异常检测的模型" class="headerlink" title="使用测试集测试得到的异常检测的模型"></a>使用测试集测试得到的异常检测的模型</h3><p>根据训练集得到的概率密度函数计算概率值，同时利用得到的最优阈值，得到每个样本每个特征的预测值，与实际值比对，并将异常点用红色标注出来，计算模型的F1分数为0.6666666666666666</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#用新的测试数据(使用测试集)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(xtest,ytest,ptest,threshold)</span>:</span></span><br><span class="line">output=np.where(ptest&lt;threshold)</span><br><span class="line"></span><br><span class="line">phat=ptest&lt;threshold</span><br><span class="line">tp=np.sum(np.logical_and(phat==<span class="number">1</span>,ytest==<span class="number">1</span>))</span><br><span class="line">fp=np.sum(np.logical_and(phat==<span class="number">1</span>,ytest==<span class="number">0</span>))</span><br><span class="line">fn=np.sum(np.logical_and(phat==<span class="number">0</span>,ytest==<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">precision=tp/(tp+fp)</span><br><span class="line">recall=tp/(tp+fn)</span><br><span class="line">f1score=(<span class="number">2</span>*precision*recall)/(precision+recall)</span><br><span class="line">fig,ax=plt.subplots(figsize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">ax.scatter(xtest[:,<span class="number">0</span>],xtest[:,<span class="number">1</span>])</span><br><span class="line">ax.scatter(xtest[output[<span class="number">0</span>],<span class="number">0</span>],xtest[output[<span class="number">0</span>],<span class="number">1</span>],c=<span class="string">'r'</span>)</span><br><span class="line"><span class="keyword">return</span> f1score</span><br><span class="line"></span><br><span class="line">ptest=norm_data(xtest,mean,var)</span><br><span class="line">f1score=predict(xtest,ytest,ptest,best_threshold)</span><br><span class="line">print(f1score)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200907163041.png" alt></p><h3 id="（附）对高维度的数据"><a href="#（附）对高维度的数据" class="headerlink" title="（附）对高维度的数据"></a>（附）对高维度的数据</h3><p>应用场景：</p><p>1°描述2个特征变量之间的正相关负相关的情况，也可以给高度相关的变量建立模型</p><p>2°x属于n * n的向量（n&gt;2），对所有p(x)建立统一的模型</p><p>3°能自动捕捉不同特征之间的关系（正负样本之间的特征联系）</p><p>4°协方差矩阵需要是奇异的，样本数量要大于特征数据（m&gt;n）</p><p>（1）模型中的均值和方差计算公式</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200908140059.png" alt></p><p>利用numpy数组的mean属性得到均值，利用numpy.var得到协方差矩阵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#参数估计</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mul_parameter_estimate</span><span class="params">(x)</span>:</span></span><br><span class="line">mul_mean=x.mean(axis=<span class="number">0</span>)</span><br><span class="line">mul_cov=np.cov(x.T) <span class="comment">#(n,n)</span></span><br><span class="line"><span class="keyword">return</span> mul_mean,mul_cov</span><br><span class="line">mul_mean,mul_cov=mul_parameter_estimate(x)</span><br></pre></td></tr></table></figure><p>得到11个特征的均值，协方差矩阵为11 * 11（特征 * 特征）</p><p>（2）概率密度函数的定义</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200908140124.png" alt></p><p>在x处的概率密度函数的函数值，可以利用scipy库下的统计函数stats的求多元正态随机变量multivariate_normal，根据输入对应的均值和协方差得到随机变量，利用pdf得到数据的概率密度函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 得到概率密度函数</span></span><br><span class="line">mul_norm=stats.multivariate_normal(mul_mean,mul_cov)</span><br><span class="line">mul_p=mul_norm.pdf(x)</span><br></pre></td></tr></table></figure><p>（3）利用验证集选择合理的阈值</p><p>根据验证集的概率值和实际值，利用sklearn.metrics下的f1_score得到预测值的F1分数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score,classification_report</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mul_select_threshold</span><span class="params">(mul_pval,yval)</span>:</span></span><br><span class="line">step=(mul_pval.max()-mul_pval.min())/<span class="number">10000</span></span><br><span class="line">thresholds=np.arange(mul_pval.min(),mul_pval.max(),step)</span><br><span class="line"></span><br><span class="line">f1scores=[]</span><br><span class="line"><span class="keyword">for</span> threshold <span class="keyword">in</span> thresholds:</span><br><span class="line">yhat=mul_pval&lt;threshold</span><br><span class="line">f1scores.append(f1_score(yhat,yval))</span><br><span class="line">best_index=np.argmax(f1scores)</span><br><span class="line">best_f1score=f1scores[best_index]</span><br><span class="line">best_threshold=thresholds[best_index]</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> best_threshold,best_f1score</span><br><span class="line"></span><br><span class="line">best_threshold,best_f1score=mul_select_threshold(mul_pval,yval)</span><br></pre></td></tr></table></figure><p>得到的最优阈值为：5.23949932112034e-19</p><p>（4）使用测试集验证模型</p><p>根据模型得到的概率值和上面求出的最优阈值，得到，再利用sklearn.metrics下的classifiction_report得出准确率、召回率、预测的精度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用测试集验证</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mul_predict</span><span class="params">(xtest,ytest,ptest,threshold)</span>:</span></span><br><span class="line">output=np.where(ptest&lt;threshold)</span><br><span class="line">print(output)</span><br><span class="line"></span><br><span class="line">phat=ptest&lt;threshold</span><br><span class="line">print(classification_report(phat,ytest))</span><br><span class="line"><span class="keyword">return</span> phat</span><br><span class="line"></span><br><span class="line">mul_ptest=mul_norm.pdf(xtest)</span><br><span class="line">phat=mul_predict(xtest,ytest,mul_ptest,best_threshold)</span><br></pre></td></tr></table></figure><p>对模型的评价为：</p><blockquote><pre><code>          precision    recall  f1-score   support   False       0.93      0.98      0.95        42    True       0.83      0.62      0.71         8accuracy                           0.92        50</code></pre></blockquote><p>模型的精度为92%</p><p>预测异常的样本的索引为：(array([ 5, 10, 18, 21, 23, 32, 36, 47]),)</p><p>实际异常的样本的所有为：array([ 5, 10, 18, 19, 32, 36])</p><p>可见模型正确预测了5个异常点，错误预测了3个异常点，少预测了1个异常点。</p><h2 id="和监督学习的区别"><a href="#和监督学习的区别" class="headerlink" title="和监督学习的区别"></a>和监督学习的区别</h2><p>（1）监督学习</p><p>有大量的正例和反例</p><p>用已知的样本可以充分学习预测未来的情形（即算法可以适应未来数据的变化）</p><p>如：垃圾邮件检测，天气预报，疾病分类</p><p>（2）异常检测</p><p>有大量的反例（即y=0的正常点）和少量的正例（即y=1的异常点）</p><p>未来的异常不确定（很多未见过的异常样本），有不同类型的异常，用反例建模更方便</p><p>如：欺诈检测，数据中心检测，引擎制造</p><p>当y=1很多，即反例很多，将可以转换为监督需学习</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;异常检测是指给定一个数据，放入原有模型中判断该点是否异常。&lt;/p&gt;&lt;h2 id=&quot;算法简介&quot;&gt;&lt;a href=&quot;#算法简介&quot; class=&quot;headerlink&quot; title=&quot;算法简介&quot;&gt;&lt;/a&gt;算法简介&lt;/h2&gt;&lt;p&gt;（1）根据数据集{x1,x2,…,xm}计算数据集的特征值的均值和方差&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200914135110.png&quot; alt&gt;&lt;/p&gt;&lt;p&gt;（2）根据数据集{x1,x2,…,xm}建立模型p(x)，将所有特征的模型相乘，模型p(x)定义如下：&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200914135143.jpg&quot; alt&gt;&lt;/p&gt;&lt;p&gt;根据之前得到的所有特征的均值和方差计算得到p(x)，即得到了概率估计&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>降维算法PCA算法</title>
    <link href="https://www.xiapf.com/blogs/tfPCA/"/>
    <id>https://www.xiapf.com/blogs/tfPCA/</id>
    <published>2020-08-14T05:48:35.000Z</published>
    <updated>2020-10-21T06:52:40.821Z</updated>
    
    <content type="html"><![CDATA[<h2 id="无监督学习算法——降维"><a href="#无监督学习算法——降维" class="headerlink" title="无监督学习算法——降维"></a>无监督学习算法——降维</h2><p>降维的两种作用：数据压缩和可视化数据</p><h3 id="数据压缩"><a href="#数据压缩" class="headerlink" title="数据压缩"></a>数据压缩</h3><p>使用降维可以进行数据压缩，减少数据内存，加快算法速度。</p><p>原因：当数据来源于多方的时候，此时很容易出现高冗余的特征，此时可以对数据降维。</p><p>做法：将原始数据找到一个低维度的空间，将数据投影到该空间上，得到的新数据即为压缩后的数据</p><h3 id="可视化数据"><a href="#可视化数据" class="headerlink" title="可视化数据"></a>可视化数据</h3><p>原因：当数据集很大时，其中的特征数量很大，对原始数据无法直接画图看出数据分布，此时可以使用降维算法，将多维度的数据降维至2维，这样就能很容易的将数据可视化。</p><a id="more"></a><p>方法：选择k=2或者k=3来压缩数据，通过将原数据变为2维或者3维来可视化</p><h2 id="降维问题常用算法——PCA算法"><a href="#降维问题常用算法——PCA算法" class="headerlink" title="降维问题常用算法——PCA算法"></a>降维问题常用算法——PCA算法</h2><h3 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h3><p>原始数据为n维</p><p>（1）找到一个向量u属于Rn空间的向量，找该向量的原则就是数据投影的误差最小化的方向</p><p>（2）将数据从n维-&gt;k维，找到k个方向的向量u(i),…u(k)，此时这些向量定义了一个低维度的平面</p><p>（3）将数据投影到这k个向量展开的线性子空间上，即k维平面上</p><p>总的来说，<u>pca是找到一个低维度的空间，通过最小化投影距离的方法来对数据投影，以此实现数据的降维</u>。投影就是两个向量的乘积</p><h3 id="pca中二维数据降维和线性回归的区别"><a href="#pca中二维数据降维和线性回归的区别" class="headerlink" title="pca中二维数据降维和线性回归的区别"></a>pca中二维数据降维和线性回归的区别</h3><p>（1）<u>pca</u>中将数据从2维-&gt;1维，是通过找到一个直线，<u>最小化点和直线之间的最短距离</u>，此时的直线为低维度的空间，然后将数据投影到直线上。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200814133252.png" alt></p><p>（2）<u>线性回归</u>是通过找到拟合数据的直线，也是找到一个直线，但是是<u>最小化的点与假设直线得到的预测值之间的距离</u>。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200814133849.png" alt></p><p>上述二者虽然都是找直线，但是最小化的目标不同，一个是最小化最短距离（即垂直距离），一个是最小化预测值和真实值距离（即竖直距离）</p><h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><p>总体过程：<u>数据预处理-&gt;找到协方差矩阵-&gt;求解协方差矩阵的特征向量-&gt;提取前k个特征向量定义低维度平面-&gt;数据投影</u></p><p>（1）数据预处理</p><p>利用scipy.io库下的loadmat函数导入mat格式的数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(path)</span>:</span></span><br><span class="line">data=sio.loadmat(path)</span><br><span class="line">x=data.get(<span class="string">'X'</span>)</span><br><span class="line"><span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200814104721.png" alt></p><p>导入的数据为无标签的数据x，x的维度为（50，2），即每个样本有两个数据特征，并且每行代表一个样本点。</p><p>对数据进行均值标准化+特征缩放，减少不同量纲之间的影响（对每个特征进行处理即每一列进行预处理）：</p><p>均值标准化：使用原始点-平均值</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200814105146.png" alt></p><p>特征缩放：用均值标准化的结果除以（最大值-最小值）或者是特征的标准差</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200814105211.png" alt></p><p>对每一个特征进行标准化，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#归一化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize</span><span class="params">(x)</span>:</span></span><br><span class="line">m,n=x.shape</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">x[:,i]=(x[:,i]-x[:,i].mean())/x[:,i].std()</span><br><span class="line"><span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>（2）pca处理</p><p>pca的总体过程就是找到一个低维的空间，对数据进行投影，目标是在找到向量u(i)指定空间方向，同时要最小化投影距离。</p><p>则为了找到投影的空间向量u(i)，则是通过计算原始数据的协方差矩阵，得到协方差矩阵的特征向量即为投影的低维的空间的向量指向。</p><p>（3）协方差矩阵</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200814110351.png" alt></p><p>其中m为样本总数，n为样本的特征数量，得到的协方差矩阵的维度为（n,n）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">covariance_matrix</span><span class="params">(x)</span>:</span></span><br><span class="line">m,n=x.shape</span><br><span class="line">sigma=(<span class="number">1</span>/m)*np.dot(x.T,x)</span><br><span class="line"><span class="keyword">return</span> sigma <span class="comment">#(n,n)</span></span><br></pre></td></tr></table></figure><p>（4）协方差矩阵的特征向量</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200814121906.jpg" alt></p><p>利用奇异值分解svd得到矩阵的特征向量U，U的每一列代表一个特征向量，当需要提取k个主成分时，保留U矩阵的前K个向量来指向低维度子空间方向即可，需要注意的是提取的k数量不能大于U的列数。由于协方差矩阵的维度为（n,n），协方差矩阵的特征向量U的维度为（n,n）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#取前k个特征向量，进行投影</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">project_data</span><span class="params">(x,U,k)</span>:</span></span><br><span class="line">m,n=U.shape</span><br><span class="line"><span class="keyword">if</span>(k&gt;n):</span><br><span class="line">print(<span class="string">'k should be less than the number of columns in U'</span>)</span><br><span class="line">exit()</span><br><span class="line">z=np.dot(x,U[:,:k])</span><br><span class="line"><span class="keyword">return</span> z</span><br></pre></td></tr></table></figure><p>（5）提取前k个特征向量投影</p><p>当提取特征向量U的前k个向量后得到的低维度空间的方向向量Ureduce，其维度为（n,k），使用数据x在空间上进行投影，即x * Ureduce，则投影后的数据z=x * Ureduce，其维度为（m,k）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#取前k个特征向量，进行投影</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">project_data</span><span class="params">(x,U,k)</span>:</span></span><br><span class="line">m,n=U.shape</span><br><span class="line"><span class="keyword">if</span>(k&gt;n):</span><br><span class="line">print(<span class="string">'k should be less than the number of columns in U'</span>)</span><br><span class="line">exit()</span><br><span class="line">z=np.dot(x,U[:,:k])</span><br><span class="line"><span class="keyword">return</span> z</span><br></pre></td></tr></table></figure><p>这样就得到投影后的数据z，即将原始数据x进行降维后得到的新数据z，使用seaborn下的regplot显示原始数据x，这里将x转换为表格数据传给regplot，使用seaborn下的rugplot显示降维数据z，因为降维后的数据为一维数据，所以在图上用线条长度表示大小。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200814122315.jpg" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">data=pd.DataFrame(x_norm,columns=[<span class="string">'X1'</span>,<span class="string">'X2'</span>])</span><br><span class="line">fig,(ax1,ax2)=plt.subplots(ncols=<span class="number">2</span>,figsize=(<span class="number">12</span>,<span class="number">4</span>))</span><br><span class="line">sns.regplot(<span class="string">'X1'</span>,<span class="string">'X2'</span>,data=data,fit_reg=<span class="literal">False</span>,ax=ax1)</span><br><span class="line">ax1.set_title(<span class="string">'raw_data'</span>)</span><br><span class="line"></span><br><span class="line">sns.rugplot(z,ax=ax2)<span class="comment">#用于绘制出一维数组中数据点实际的分布位置情况，即不添加任何数学意义上的拟合，单纯的将记录值在坐标轴上表现出来</span></span><br><span class="line">ax2.set_title(<span class="string">'product_data'</span>)</span><br><span class="line">ax2.set_xlabel(<span class="string">'z'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="压缩后的数据还原"><a href="#压缩后的数据还原" class="headerlink" title="压缩后的数据还原"></a>压缩后的数据还原</h3><p>由于投影后的数据z=x * Ureduce，则可以使用压缩后还原的数据可以表示为x_approx=z * Ureduce.T，用x_approx作为原始数据的近似，z的维度为（m,k），Ureduce.T维度为（k,n），则近似后的数据维度为（m,n）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将投影数据还原</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduct_data</span><span class="params">(z,U)</span>:</span></span><br><span class="line">m,n=z.shape</span><br><span class="line"><span class="keyword">if</span>(n&gt;=U.shape[<span class="number">0</span>]):</span><br><span class="line">print(<span class="string">'The dimension of z should be smaller than the dimension of U'</span>)</span><br><span class="line">exit()</span><br><span class="line">x_approx=np.dot(z,U[:,:n].T)</span><br><span class="line"><span class="keyword">return</span> x_approx</span><br></pre></td></tr></table></figure><p>这样就得到近似的原始数据x_approx，使用seaborn下的regplot显示原始数据x，这里将x转换为表格数据传给regplot，使用seaborn下的rugplot显示降维数据z，因为降维后的数据为一维数据，所以在图上用线条长度表示大小，使用seaborn下的regplot显示近似原始数据x_approx，这里将x_approx转换为表格数据传给regplot。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200814122402.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">data=pd.DataFrame(x_norm,columns=[<span class="string">'X1'</span>,<span class="string">'X2'</span>])</span><br><span class="line">data_approx=pd.DataFrame(x_approx,columns=[<span class="string">'X1'</span>,<span class="string">'X2'</span>])</span><br><span class="line"></span><br><span class="line">fig,(ax1,ax2,ax3)=plt.subplots(ncols=<span class="number">3</span>,figsize=(<span class="number">12</span>,<span class="number">6</span>))</span><br><span class="line">sns.regplot(<span class="string">'X1'</span>,<span class="string">'X2'</span>,data=data,fit_reg=<span class="literal">False</span>,ax=ax1)</span><br><span class="line">ax1.set_title(<span class="string">'raw_data'</span>)</span><br><span class="line"></span><br><span class="line">sns.rugplot(z,ax=ax2)<span class="comment">#用于绘制出一维数组中数据点实际的分布位置情况，即不添加任何数学意义上的拟合，单纯的将记录值在坐标轴上表现出来</span></span><br><span class="line">ax2.set_title(<span class="string">'product_data'</span>)</span><br><span class="line">ax2.set_xlabel(<span class="string">'z'</span>)</span><br><span class="line"></span><br><span class="line">sns.regplot(<span class="string">'X1'</span>,<span class="string">'X2'</span>,data=data_approx,fit_reg=<span class="literal">False</span>,ax=ax3)</span><br><span class="line">ax3.set_title(<span class="string">'approx_data'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="k——主成分数量的选择"><a href="#k——主成分数量的选择" class="headerlink" title="k——主成分数量的选择"></a>k——主成分数量的选择</h3><p>总体思路：</p><p>（1）使用不同的k值（从小到大），运行pca算法，得到不同的低维子空间的向量Ureduce，投影后的数据z，进而得到投影后的近似原始数据x_approx</p><p>（2）根据以下式子计算原始数据和近似数据的误差 / 原始数据的方差即数据的波动情况，找到使得式子小于等于某个数（0.01/0.1等数据），则称选择的k有99%的方差性保留，即通过近似数据得到的误差对整体数据波动的影响，得到通过pca投影后的数据保留了多少原始数据。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200814124032.png" alt></p><p>但是通过以上思路每次都要运性pca，效率慢，因此可以利用奇异值分解后得到的奇异值矩阵S进行主成分的选择，奇异值矩阵告诉了方差被保留的比例：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200814132003.png" alt></p><p>首先计算所有奇异值矩阵中的和，即主对角线元素相加，随着i的增加，分别取对应行的主对角元素，当满足上述式子后得到的i值就是能够保留99%的方差的k的取值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#k的选择</span></span><br><span class="line">sum_s=np.sum(S)</span><br><span class="line">tmp_s=<span class="number">0</span></span><br><span class="line">print(sum_s)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(S)):</span><br><span class="line">tmp_s+=S[i]</span><br><span class="line"><span class="keyword">if</span>(tmp_s/sum_s&gt;=<span class="number">0.99</span>):</span><br><span class="line">k=i</span><br><span class="line"><span class="keyword">break</span></span><br><span class="line">print(k)</span><br></pre></td></tr></table></figure><p>最终k的取值为1</p><h3 id="对监督学习算法加速"><a href="#对监督学习算法加速" class="headerlink" title="对监督学习算法加速"></a>对监督学习算法加速</h3><p>数据集{ (x1,y1）…,（xm,ym）}</p><p>（1）数据抽取：将监督学习中的数据集中，提出标签后保留无标签的数据</p><p>（2）pca降维：对无标签数据进行降维，得到新数据z，数据集变为{ (z1,y1）…,（zm,ym）}</p><p>（3）使用新数据运行算法，得到假设h(θ)中参数值，则模型训练结束</p><p>（4）对测试样本，将xtest通过pca得到ztest，使用新数据ztest传入假设函数，得到模型的测试数据预测值。</p><p>注：</p><p>（1）pca只在训练数据集上使用，低维度的数据能让算法运行更快；</p><p>（2）不能用于减少过拟合，虽然能减少数据特征，但是因为丢弃标签进行pca，所有会导致一部分数据丢失，过拟合采用加入正则化项；</p><p>（3）当使用原始数据建立算法系统后，运行慢或者占用内存多才考虑加入第二步的pca降维，其余情况不用先考虑加入pca。</p><h2 id="用sklearn库实现PCA算法"><a href="#用sklearn库实现PCA算法" class="headerlink" title="用sklearn库实现PCA算法"></a>用sklearn库实现PCA算法</h2><p>（1）搭建模型：利用sklearn库下的decomposition库下的PCA，指定成分n_components的大小，如果大小设置为整数，则n_components=k，即主成分保留的数量。如果将其设置为小数，则说明降维后的数据能保留的信息，即保留原始数据的方差性的比例。如n_components=0.99表示99%的方差性保留。</p><p>（2）拟合数据：利用fit_transform函数输入训练数据，并将原始数据降维</p><p>（3）恢复数据：利用inverse_transform函数得到投影后的数据恢复得到的近似原始数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#用sklearn库的pca</span></span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="comment">#搭建模型</span></span><br><span class="line">sk_pca=PCA(n_components=<span class="number">100</span>) <span class="comment">#PCA方法参数n_components，如果设置为整数，则n_components=k。如果将其设置为小数，则说明降维后的数据能保留的信息  </span></span><br><span class="line"></span><br><span class="line"><span class="comment">#降维</span></span><br><span class="line">z=sk_pca.fit_transform(x)</span><br><span class="line"></span><br><span class="line"><span class="comment">#恢复数据</span></span><br><span class="line">x_approx=sk_pca.inverse_transform(z)</span><br></pre></td></tr></table></figure><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><h3 id="注意点"><a href="#注意点" class="headerlink" title="注意点"></a>注意点</h3><p>当导入图片数据时，需要将数据旋转只统一方向，即需要对原始数据进行reshape。</p><p>将每个照片转换为（32，32）,再转换为一维数据的1024数据，每个样本横向堆叠，得到的x维度为（5000，1024）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x=load_data(<span class="string">'./data/ex7faces.mat'</span>)</span><br><span class="line">x=np.array([x_each.reshape(<span class="number">32</span>,<span class="number">32</span>).T.reshape(<span class="number">1024</span>) <span class="keyword">for</span> x_each <span class="keyword">in</span> x]) <span class="comment">#每个照片转换为（32，32）,再转换为一维数据</span></span><br></pre></td></tr></table></figure><p>显示前64个图片</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200814135305.png" alt></p><p>利用plt的subplots得到各个子图区域，通过ax的matshow将数据转换为图片大小显示像素数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#画出前n个图片</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_n_img</span><span class="params">(x,n)</span>:</span></span><br><span class="line">m=x.shape[<span class="number">1</span>]</span><br><span class="line">img_size=int(np.sqrt(m))</span><br><span class="line">gird_size=int(np.sqrt(n))</span><br><span class="line">first_n_img=x[:n,:]</span><br><span class="line"></span><br><span class="line">fig,ax=plt.subplots(gird_size,gird_size,sharex=<span class="literal">True</span>,sharey=<span class="literal">True</span>,figsize=(gird_size,gird_size))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> r <span class="keyword">in</span> range(gird_size):</span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> range(gird_size):</span><br><span class="line">ax[r,c].matshow(first_n_img[gird_size*r+c].reshape(img_size,img_size))</span><br><span class="line">plt.xticks(np.array([]))</span><br><span class="line">plt.yticks(np.array([]))</span><br></pre></td></tr></table></figure><p>如果没有上面reshape的预处理，得到的图像如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200814135608.png" alt></p><p>此时的图像没有摆正，不利于后续观察</p><h3 id="用pca进行人脸识别"><a href="#用pca进行人脸识别" class="headerlink" title="用pca进行人脸识别"></a>用pca进行人脸识别</h3><p>待续</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;无监督学习算法——降维&quot;&gt;&lt;a href=&quot;#无监督学习算法——降维&quot; class=&quot;headerlink&quot; title=&quot;无监督学习算法——降维&quot;&gt;&lt;/a&gt;无监督学习算法——降维&lt;/h2&gt;&lt;p&gt;降维的两种作用：数据压缩和可视化数据&lt;/p&gt;&lt;h3 id=&quot;数据压缩&quot;&gt;&lt;a href=&quot;#数据压缩&quot; class=&quot;headerlink&quot; title=&quot;数据压缩&quot;&gt;&lt;/a&gt;数据压缩&lt;/h3&gt;&lt;p&gt;使用降维可以进行数据压缩，减少数据内存，加快算法速度。&lt;/p&gt;&lt;p&gt;原因：当数据来源于多方的时候，此时很容易出现高冗余的特征，此时可以对数据降维。&lt;/p&gt;&lt;p&gt;做法：将原始数据找到一个低维度的空间，将数据投影到该空间上，得到的新数据即为压缩后的数据&lt;/p&gt;&lt;h3 id=&quot;可视化数据&quot;&gt;&lt;a href=&quot;#可视化数据&quot; class=&quot;headerlink&quot; title=&quot;可视化数据&quot;&gt;&lt;/a&gt;可视化数据&lt;/h3&gt;&lt;p&gt;原因：当数据集很大时，其中的特征数量很大，对原始数据无法直接画图看出数据分布，此时可以使用降维算法，将多维度的数据降维至2维，这样就能很容易的将数据可视化。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="降维" scheme="https://www.xiapf.com/tags/%E9%99%8D%E7%BB%B4/"/>
    
  </entry>
  
  <entry>
    <title>聚类算法KMeans算法</title>
    <link href="https://www.xiapf.com/blogs/tfKMeans/"/>
    <id>https://www.xiapf.com/blogs/tfKMeans/</id>
    <published>2020-08-10T10:32:54.000Z</published>
    <updated>2020-08-10T10:37:07.141Z</updated>
    
    <content type="html"><![CDATA[<h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><h3 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h3><p>聚类算法用于无监督学习，KMeans算法是聚类算法的一种，当给定训练集{x1,x2,….,xm }，找到这些无标签数据中的结构。聚类问题就是给定一组未加标签的数据集，将数据分为有紧密关系的子集成簇。</p><h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><p>输入：</p><p>（1）从数据中聚类的簇的个数K</p><p>（2）训练数据集（无标签数据）</p><p>利用scioy.io库下的函数loadmat导入mat文件，数据中有两个特征，没有标签；</p><a id="more"></a><p>将数据利用pandas库下的DataFrame转换为表格形式，方便调用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(path)</span>:</span></span><br><span class="line">data=sio.loadmat(path)</span><br><span class="line">df=pd.DataFrame(data.get(<span class="string">'X'</span>),columns=[<span class="string">'x1'</span>,<span class="string">'x2'</span>])</span><br><span class="line"><span class="keyword">return</span> df</span><br></pre></td></tr></table></figure><p>数据展示可以看出是三个簇的样子，但是数据都没有标签，每个数据点目前都是一样的</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200810155815.png" alt></p><hr><p>算法：</p><p>（0）随机初始化K个簇的中心：μ1，μ2，…，μk</p><p>利用pandas库下的DataFrame数据的sample来进行随机取几个样本点作为初始的簇中心</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#随机初始化族中心</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_init</span><span class="params">(data,k)</span>:</span></span><br><span class="line"><span class="keyword">return</span> np.array(data.sample(k))<span class="comment">#表格中的采样方法</span></span><br></pre></td></tr></table></figure><hr><p>不断重复下面（1）（2）步骤，直至簇的中心不再变化：</p><p>（1）簇分配：遍历样本，计算每个样本距离最近的簇的中心，进行点的分配</p><p>根据公式||x(i)-μ||2，找到某个中心点μ，使得样本点x(i)到它的距离的平方最小，即取距离最小的簇。</p><p>分配点：利用numpy库下的apply_along_axis，传入所有数据，根据最小距离函数：计算每个样本点距离簇中心的最小距离。</p><p>求距离：利用numpy库下的apply_along_axis。传入每个样本点和簇中心的差，根据第二范数公式得到距离，利用argmin求的最小距离。</p><p>附：numpy库下的apply_along_axis可以沿着某一个计算轴，根据函数计算传入数据的输出值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#计算点和中心的距离</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_find_your_cluster</span><span class="params">(x,centroids)</span>:</span></span><br><span class="line">distance_index=np.apply_along_axis(np.linalg.norm,<span class="number">1</span>,x-centroids)<span class="comment">#方法，数据的轴，数据</span></span><br><span class="line"><span class="keyword">return</span> np.argmin(distance_index) <span class="comment">#选择最小的距离的族的索引</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将点分配到簇中</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">assign_cluster</span><span class="params">(data,centroids)</span>:</span><span class="comment">#每次传一个样本计算距离</span></span><br><span class="line"><span class="keyword">return</span> np.apply_along_axis(<span class="keyword">lambda</span> x:_find_your_cluster(x,centroids),<span class="number">1</span>,np.array(data)) <span class="comment">#lambda x:_find_your_cluster中的x对应函数中的x,x对应np.array(data)</span></span><br></pre></td></tr></table></figure><p>将距离点最小的簇的索引分配给该点，便于后续分组求簇的均值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#把每个数据对应的中心点加到表格数据中</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_cluster_centroids</span><span class="params">(data,C)</span>:</span></span><br><span class="line">data_new=data.copy()</span><br><span class="line">data_new[<span class="string">'C'</span>]=C</span><br><span class="line"><span class="keyword">return</span> data_new</span><br></pre></td></tr></table></figure><hr><p>（2）移动聚类中心：将之前分配的簇，计算簇内点的均值，记作新的簇的中心</p><p>根据每个点对应分的簇，利用pandas库下的DataFrame数据下的分组函数groupby，按照某一列进行分组，同时利用mean()求数据均值，将数据利用sort_values按照某一列排序（可选），因为之前加入了每个点对应的簇，所以为了保持数据一致，需要利用drop函数把数据中最后一列簇索引沿着列方向删除。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##重新计算新的簇的中心 利用分组函数按照簇的中心分组,取平均得到新的质心</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">new_centroids</span><span class="params">(data,C)</span>:</span></span><br><span class="line">data=add_cluster_centroids(data,C) <span class="comment">#用被C更新过的表格</span></span><br><span class="line"><span class="keyword">return</span> np.array(data.groupby(<span class="string">'C'</span>,as_index=<span class="literal">False</span>).mean().sort_values(by=<span class="string">'C'</span>).drop(<span class="string">'C'</span>,axis=<span class="number">1</span>)) <span class="comment">#把排序那列的名称保留</span></span><br></pre></td></tr></table></figure><hr><p>总体算法：循环以上两个步骤，每次迭代分配点到相应的簇中，并重新计算簇的中心：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#kmeans算法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kmeans_for_iters</span><span class="params">(data,k,iters=<span class="number">100</span>,tol=<span class="number">0.0001</span>)</span>:</span><span class="comment">#tol为结束条件</span></span><br><span class="line"><span class="comment">#随机初始化簇类中心</span></span><br><span class="line">centroids=random_init(data,k)</span><br><span class="line">costs=[]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(iters):</span><br><span class="line">print(<span class="string">'run iter:&#123;&#125;'</span>.format(i))</span><br><span class="line">C=assign_cluster(data,centroids)<span class="comment">#将点分配到簇中</span></span><br><span class="line">centroids=new_centroids(data,C)<span class="comment">#重新计算新的簇的中心</span></span><br><span class="line">costs.append(cost(data,centroids,C))<span class="comment">#计算代价函数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(len(costs)&gt;<span class="number">1</span>):</span><br><span class="line"><span class="keyword">if</span> (np.abs(costs[<span class="number">-1</span>]-costs[<span class="number">-2</span>])/costs[<span class="number">-1</span>])&lt;tol:<span class="comment">#当簇的中心不再变化</span></span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"><span class="keyword">return</span> C,centroids,costs[<span class="number">-1</span>]<span class="comment">#返回最终的簇的索引，簇，成本</span></span><br></pre></td></tr></table></figure><h3 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h3><p>和其他算法相同，KMeans算法在运行过程中也是通过分配数据到簇中，不断改变簇的中心，来最小化代价函数的。</p><p>（1）定义</p><p>代价函数：每个样本点到其所属的簇中心距离之和是最小的，其中c(i)表示样本点所属簇的类别/索引，μ表示簇中心，μc(I)表示所属c(I)类别的簇的中心。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200810164450.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#代价函数 计算每个样本和其对应的簇中心的距离</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(data,centroids,C)</span>:</span></span><br><span class="line">m=data.shape[<span class="number">0</span>]</span><br><span class="line">cost_term=(<span class="number">1</span>/m)*np.sum(np.apply_along_axis(np.linalg.norm,<span class="number">1</span>,np.array(data)-centroids[C]))</span><br><span class="line"><span class="keyword">return</span> cost_term</span><br></pre></td></tr></table></figure><p>（2）算法中如何体现</p><p>由公式可以看出，要保证样本到所属类别的中心的簇距离最小，即可以分为两部分：一是保证样本正确分到所属类别，即在分配簇时，要找到最小距离的簇；二是簇内每个点到中心距离要最小，即簇的中心需要选好。</p><p>在分配簇中：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200810155839.jpg" alt></p><p>通过找到与样本点最小的中心，来最小化c(i)，即找到样本所属的类别。</p><p>在调整簇中心中：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200810164918.jpg" alt></p><p>通过将簇内的点求均值找到更合理的簇中心，即要最小化μ。</p><h3 id="如何解决局部最优"><a href="#如何解决局部最优" class="headerlink" title="如何解决局部最优"></a>如何解决局部最优</h3><p>因为在初始选择簇中心的时候，是随机选择几个样本点作为簇中心，如果初始选择不佳，则分类就会出现如下问题：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200810170305.png" alt></p><p>图像中可以看出簇也是分为3类，但是明显可以看出分类有问题，初始化的时候在上面的圆圈中找了两个点，在下面的圆圈中找了一个点，所以导致下面两个簇分为一个类，上面的一个簇反而拆分成了两个簇。</p><p>这种分类也是遵循了最小化代价函数，保证点到簇中心最小，但是由于初始化不当，陷入了这种局部最优。为了避免这种局部最优问题，可以多次随机初始化训练KMeans算法，每次计算代价函数，最终选择代价函数最小的模型作为最终的结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#多次随机初始化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kmeans</span><span class="params">(data,k,iters=<span class="number">100</span>,n_nums=<span class="number">10</span>)</span>:</span></span><br><span class="line"><span class="comment">#选择成本最小的</span></span><br><span class="line">nums_initial=np.array([kmeans_for_iters(data,k,iters) <span class="keyword">for</span> _ <span class="keyword">in</span> range(n_nums)]) <span class="comment">#保存每次结果</span></span><br><span class="line">best_res=np.argmin(nums_initial[:,<span class="number">-1</span>])</span><br><span class="line"><span class="keyword">return</span> nums_initial[best_res]</span><br></pre></td></tr></table></figure><p>这样保证了初始的随机取点，又得到了最小代价的模型。</p><h3 id="如何选择聚类数量K"><a href="#如何选择聚类数量K" class="headerlink" title="如何选择聚类数量K"></a>如何选择聚类数量K</h3><p>（1）肘部法则</p><p>改变K计算不同K下的代价，进行画图，找到代价下降很快后突然变慢的点，这种位置是肘部，则是最佳的K，如图：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200810170705.png" alt></p><p>但是实际应用中代价可能没有明显变化，所以这种方法不能适用所有情况。</p><p>选择不同的K的值得到不同的模型，计算代价：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">klist=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>]</span><br><span class="line">kcosts=[]</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> klist:</span><br><span class="line">best_res=kmeans(data,k)</span><br><span class="line">kcost=best_res[<span class="number">-1</span>]</span><br><span class="line">kcosts.append(kcost)</span><br><span class="line">plt.plot(klist,kcosts)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>得到分别对应的代价值为：[2.4599108484075822, 1.5470043751823426, 0.794176363371587, 0.7065872753726378, 0.6211872806110786, 0.5793526629824044, 0.5062612369155735, 0.4789280784350547]</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200810171424.png" alt></p><p>可以看出该数据集进行K选择之后，能符合肘部法则，选择K=3是最佳的分类簇</p><p>按照K=3，分类的结果为：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200810171734.png" alt></p><p>分类结果也合理，按照不同的颜色标识了不同的簇。</p><p>（2）根据算法的应用目的进行K的选择</p><p>当K增大，则分类会细致，当K变小，则分类少应用方便，需要根据具体目的选择K的值。</p><h2 id="用sklearn库实现KMeans算法"><a href="#用sklearn库实现KMeans算法" class="headerlink" title="用sklearn库实现KMeans算法"></a>用sklearn库实现KMeans算法</h2><p>（1）搭建模型：利用sklearn库下的cluster聚类库下的KMeans，指定分类簇n_clusters的大小</p><p>（2）拟合数据：利用fit函数输入训练数据</p><p>（3）模型结果：利用predict函数得到每个数据点对应的簇的类别</p><p>cluster_centers_属性得到每个簇的中心</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用sklearn中的kmeans</span></span><br><span class="line"><span class="keyword">import</span> sklearn.cluster</span><br><span class="line">km=sklearn.cluster.KMeans(n_clusters=<span class="number">3</span>)</span><br><span class="line">km.fit(data)</span><br><span class="line">C=km.predict(data)</span><br><span class="line">centroids=km.cluster_centers_</span><br></pre></td></tr></table></figure><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><h3 id="描述-1"><a href="#描述-1" class="headerlink" title="描述"></a>描述</h3><p>将一幅图像128 * 128的彩色图像利用KMeans算法进行压缩。</p><h3 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h3><p>将图像中每个像素点进行分类，分为K个簇，使用簇中心来代替簇内的像素点，以此压缩图片。</p><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p>1、导入数据并标准化</p><p>利用skimage库下的io库下的imread读入图像，并将每个像素点除以255，通过归一化减少量纲的影响，否则大小不一的像素会使得结果完全受像素值影响。并将彩色图像像素值转换为二维数据形式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入数据</span></span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> io</span><br><span class="line">img=io.imread(<span class="string">'./data/bird_small.png'</span>)/<span class="number">255</span></span><br><span class="line">m,_,channel=img.shape</span><br><span class="line">data=img.reshape(m*m,channel)</span><br></pre></td></tr></table></figure><p>2、多次随机一次KMeans</p><p>（1）随机初始化簇类中心</p><p>（2）每次迭代中，将数据按照到各个簇中心的最小距离分配点，分配后重新计算数据的均值即簇的中心</p><p>1°把所有数据分配到不同的簇中：按照样本点到不同簇的距离取最小值则为该样本所在的簇</p><p>2°得到每个簇所在的索引位置，并将每个数据后面加上对应中心的位置</p><p>3°分配完一次所有数据后，按照簇内的数据的平均值重新计算簇的中心</p><p>模型训练：n_clusters指明簇的个数，max_iter指明每个迭代次数，n_init指明随机初始化的次数，n_jobs指明并行进程数（-1表示和CPU内核相同）。</p><p>通过predict得到每个样本点对应的簇的类别，通过cluster_centers_得到簇的中心，使用得到的簇的中心来代替簇内的点，得到的像素值利用skimage.io下的imsave进行保存</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用sklearn中的kmeans</span></span><br><span class="line"><span class="keyword">import</span> sklearn.cluster</span><br><span class="line">km=sklearn.cluster.KMeans(n_clusters=test,max_iter=<span class="number">10</span>,n_init=<span class="number">3</span>,n_jobs=<span class="number">-1</span>)</span><br><span class="line">km.fit(data)</span><br><span class="line">centroids=km.cluster_centers_</span><br><span class="line">C=km.predict(data)</span><br><span class="line"> <span class="comment"># 用每个簇的中心来代替簇内的点</span></span><br><span class="line">pic=centroids[C].reshape(m,m,channel)</span><br><span class="line">io.imsave(<span class="string">'5.png'</span>,pic)</span><br></pre></td></tr></table></figure><p>3、分析不同K的取值的影响</p><p>（1）分析不同K取值下的代价函数的变化</p><p>选择不同的簇类的K，得到不同模型下的簇的类别和簇中心点，根据代价函数定义保存每次的代价值，并将对应压缩的图像保存在compress文件夹下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sklearn.cluster</span><br><span class="line">clusters=list(range(<span class="number">1</span>,<span class="number">21</span>))</span><br><span class="line">kcosts=[]</span><br><span class="line"><span class="keyword">for</span> cluster <span class="keyword">in</span> clusters:</span><br><span class="line">km=sklearn.cluster.KMeans(n_clusters=cluster,max_iter=<span class="number">10</span>,n_init=<span class="number">3</span>,n_jobs=<span class="number">-1</span>)</span><br><span class="line">km.fit(data)</span><br><span class="line">centroids=km.cluster_centers_</span><br><span class="line">C=km.predict(data)</span><br><span class="line">kcost=cost(data,centroids,C)</span><br><span class="line">kcosts.append(kcost)</span><br><span class="line">pic=centroids[C].reshape(m,m,channel)</span><br><span class="line">io.imsave(<span class="string">'./compress/&#123;&#125;.png'</span>.format(cluster),pic)</span><br><span class="line">plt.plot(clusters,kcosts)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>最终随着K的变化代价值变化为：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200810181145.png" alt></p><p>从图中可以看出没有明显的肘部，则肘部法则无法对判断K的取值起作用。</p><p>（2）从应用目的分析不同的K的取值的影响</p><p>查看不同K的取值下的压缩的结果：（将图像以K的值命名）</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200810181531.png" alt></p><p>可以看出随着K的增大，图像大小在增大，越小的K得到的压缩图像色彩就越少，越大的K得到的压缩图像就越丰富，当K取1 ~ 4时，图像为单一色彩，其中K取1时不能显示出原始图像信息，所以可以根据后续的应用来选择K，当只是为了识别出原始图像，不要求过多颜色，可以将K取小的值，当需要保留原始图像中丰富的色彩，则需要取大的K的值。</p><p>总体来看，原始图像33kb，压缩之后的图像最大的（K=20）也压缩了原始图像的三分之一，并且基本和原始图像没有变化，可见KMeans算法用于压缩图像的有效性。</p><p>使用subplots显示多幅图像，并且通过ax[0]，ax[1]设置每个子图的标题和具体显示的内容，通过plt.rcParams[‘font.sans-serif’]=[‘Arial Unicode MS’]显示中文标题。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">img=io.imread(<span class="string">'./data/bird_small.png'</span>)/<span class="number">255</span></span><br><span class="line">img2=io.imread(<span class="string">'./compress/20.png'</span>)/<span class="number">255</span></span><br><span class="line">fig,ax=plt.subplots(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">plt.rcParams[<span class="string">'font.sans-serif'</span>]=[<span class="string">'Arial Unicode MS'</span>] <span class="comment">#显示中文</span></span><br><span class="line">ax[<span class="number">0</span>].imshow(img)</span><br><span class="line">ax[<span class="number">0</span>].set_title(<span class="string">'原始图像'</span>)</span><br><span class="line">ax[<span class="number">1</span>].imshow(img2)</span><br><span class="line">ax[<span class="number">1</span>].set_title(<span class="string">'压缩之后'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>原始图像和压缩之后：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200810182931.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;描述&quot;&gt;&lt;a href=&quot;#描述&quot; class=&quot;headerlink&quot; title=&quot;描述&quot;&gt;&lt;/a&gt;描述&lt;/h2&gt;&lt;h3 id=&quot;使用场景&quot;&gt;&lt;a href=&quot;#使用场景&quot; class=&quot;headerlink&quot; title=&quot;使用场景&quot;&gt;&lt;/a&gt;使用场景&lt;/h3&gt;&lt;p&gt;聚类算法用于无监督学习，KMeans算法是聚类算法的一种，当给定训练集{x1,x2,….,xm }，找到这些无标签数据中的结构。聚类问题就是给定一组未加标签的数据集，将数据分为有紧密关系的子集成簇。&lt;/p&gt;&lt;h3 id=&quot;算法流程&quot;&gt;&lt;a href=&quot;#算法流程&quot; class=&quot;headerlink&quot; title=&quot;算法流程&quot;&gt;&lt;/a&gt;算法流程&lt;/h3&gt;&lt;p&gt;输入：&lt;/p&gt;&lt;p&gt;（1）从数据中聚类的簇的个数K&lt;/p&gt;&lt;p&gt;（2）训练数据集（无标签数据）&lt;/p&gt;&lt;p&gt;利用scioy.io库下的函数loadmat导入mat文件，数据中有两个特征，没有标签；&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="无监督学习" scheme="https://www.xiapf.com/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>SVM算法</title>
    <link href="https://www.xiapf.com/blogs/tfSVM/"/>
    <id>https://www.xiapf.com/blogs/tfSVM/</id>
    <published>2020-08-06T14:58:58.000Z</published>
    <updated>2020-08-06T15:06:08.719Z</updated>
    
    <content type="html"><![CDATA[<h2 id="算法简介"><a href="#算法简介" class="headerlink" title="算法简介"></a>算法简介</h2><h3 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h3><p>代价函数和logistic回归类似，但是为了保证最大间隔，<strong><u>当y=1时，需要θT * X&gt;=1，而不是θT * X&gt;=0，当y=0时，需要θT * X&lt;=-1，而不是θT * X&lt;0，</u></strong>因此将假设函数部分替换为如下的函数：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200806205412.png" alt></p><p>因此代价函数定义如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200806205909.jpg" alt></p><p>其中C的作用和logistic回归中的1/λ作用相同，平衡数据拟合程度和模型中参数的大小,SVM的代价函数不是像logistic中一样输出概率而是直接预测分类（是0还是1）</p><a id="more"></a><p>当C很大的时候，此时需要让前一个求和尽量为0，即需要更多的拟合训练数据，产生过拟合所以会导致低偏差，高方差，对异常点也会很敏感，可以看成是λ很小。</p><p>当C很小的时候，此时需要不需要尽量拟合数据，前一项影响降低，后一项影响增加，即需要让后一项数据尽量为0，即参数要小，所以会导致低方差，高偏差，对异常点不敏感，可以看成是λ很大。</p><h3 id="最大间隔"><a href="#最大间隔" class="headerlink" title="最大间隔"></a>最大间隔</h3><p>（1）最大间隔的好处</p><p>与其他分类算法对比，如logistic分类算法仅是对样本数据进行拟合即可，但是SVM算法在拟合数据时，尽量把正负样本以最大的间隔分开，所以拟合的直线和数据之间拥有更大的距离/间距，过程中是不断让训练样本的最小距离更大，所以和其他算法比SVM算法更具有鲁棒性。</p><p>（2）最大间隔如何得出</p><p>简化代价函数，可以把C放到很大，此时代价函数只剩下后一项，同时要满足间隔的要求，则代价函数可以变为：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200806211426.png" alt></p><p>代价函数变形：</p><p>其中代价函数的后一项可以看成是各个参数平方相加，加上根号和平方，即可以表示成向量的长度的平方，即总的向量θ=[θ1,θ2…,θn]的列向量，代价函数可以写成1/2 * (θ12+θ22+…)=1/2(√θ12+θ22+…)2=1/2 * ||θ||2，即向量θ的长度的平方</p><p>约束条件变形：</p><p>约束条件中的θ和 X(i)都看成向量，画图如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200806212113.png" alt></p><p>θT * X(i)可以看出向量的内积，其值相当于X(i)在θ上的投影的长度乘以θ的长度，记p=投影长度，||θ||是向量θ的长度，则原来的约束条件变为：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200806212528.png" alt></p><p>则综合代价函数变形和约束条件变形，可得代价函数变形为：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200806212646.png" alt></p><p>分析：</p><p>从约束条件可以看出，当X(I)到参数θ，即直线的距离太小，按照等式p * ||θ||&gt;=1，则θ就需要很大，这样代价函数就无法取得最小值，所以SVM函数的代价函数决定了。要找X(I)到参数θ，即直线的距离大的，所以保证了以尽量大的间隔划分正负样本。</p><h2 id="加入核函数"><a href="#加入核函数" class="headerlink" title="加入核函数"></a>加入核函数</h2><h3 id="原因及核函数定义"><a href="#原因及核函数定义" class="headerlink" title="原因及核函数定义"></a>原因及核函数定义</h3><p>当面对线性可分的数据，和logistic回归类似，用训练数据拟合一条直线即可，即假设函数用指向方程形式。但是当遇到非线性可分的数据时，这时候需要选择更复杂的特征来构架假设函数。这时候就需要一个能自动选择更好的特征：</p><p>（1）构造标记点</p><p>通过当前样本点和所有标记点相似度度量，将当前样本点映射为新的由相似度度量组成的特征向量，此时的特征变量变为当前样本点集中所有标记点之间的距离，这时候就完成了选择更好的特征，比原来特征更复杂也更能体现数据特点。</p><p>如何选择标记点：一般将所有训练数据作为标记点，则映射之后的特征向量表示了当前样本和其他样本之间的距离。</p><p>（2）定义核函数/相似度度量函数</p><p>常用的是高斯函数：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200806214111.png" alt></p><h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><p>（1）给定（x(1),y(1)）….（x(m),y(m)）一共m个样本</p><p>（2）根据核函数得到新的特征向量</p><p>当前样本为x(i)，则根据核函数公式计算它与所有训练样本之间的相似度数值，得到m个数字，则x(i)映射为含有m维的特征向量fm</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200806214450.png" alt></p><p>（3）根据代价函数最小化得到参数θ，当θT * X&gt;0，则预测y=1</p><p>代价函数中部分细节修正，θ2可以看成是θT * θ，算法会给式子中间加上矩阵M（M取决于核函数），为了提高运算效率，因为当参数维度很高时，则计算θT * θ代价会很高。</p><h3 id="核函数类型"><a href="#核函数类型" class="headerlink" title="核函数类型"></a>核函数类型</h3><p>（1）线性核函数：即无内核参数，适用于线性可分数据</p><p>此时的SVM是一个线性分类器，利用sklearn中的svm库函数包，通过linearSVC调用无内核的SVM算法，输入参数C和使用的损失函数，用fit拟合训练数据，通过score计算准确度，使用decision_function返回各点到决策边界的距离</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用线性svm算法</span></span><br><span class="line">svml=sklearn.svm.LinearSVC(C=<span class="number">1</span>,loss=<span class="string">'hinge'</span>)<span class="comment">#设置损失函数（标准损失函数）以及C的值</span></span><br><span class="line"><span class="comment">#用训练数据拟合模型</span></span><br><span class="line">svml.fit(data[[<span class="string">'X1'</span>,<span class="string">'X2'</span>]],data[<span class="string">'y'</span>]) <span class="comment">#X1,X2作为一个整体</span></span><br><span class="line"><span class="comment">#返回平均准确度</span></span><br><span class="line">svml_score=svml.score(data[[<span class="string">'X1'</span>,<span class="string">'X2'</span>]],data[<span class="string">'y'</span>])</span><br><span class="line">data[<span class="string">'decision_function'</span>]=svml.decision_function(data[[<span class="string">'X1'</span>,<span class="string">'X2'</span>]])</span><br></pre></td></tr></table></figure><p>通过颜色反映距离大小，画图：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200806220044.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#按照点到分割平面的距离画图</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_sdf</span><span class="params">(data)</span>:</span></span><br><span class="line">fig,ax=plt.subplots(figsize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">ax.scatter(data[<span class="string">'X1'</span>],data[<span class="string">'X2'</span>],c=data[<span class="string">'decision_function'</span>],cmap=<span class="string">'RdBu'</span>)<span class="comment">#颜色用点的标签来标记</span></span><br><span class="line">ax.set_title(<span class="string">'svm:C=1'</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'X1'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'X2'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>（2）高斯核函数</p><p>此时需要自己构造高斯内核函数：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200806214111.png" alt></p><p>需要选择高斯核中gamma参数，当gamma很小，此时的特征变化剧烈，会导致低偏差，高方差，反之，当gamma很大，则特征变化缓慢，会导致低方差，高偏差。</p><p>利用sklearn中的svm库函数包，通过SVC调用有内核的SVM算法，输入参数C、调用的核函数（rbf是高斯核函数）、核函数中的gamma值（其中probility是返回对数组标签的概率故居），用fit拟合训练数据，通过score计算准确度，使用decision_function返回各点到决策边界的距离，通过predict_proba返回数据的标签，predict_proba(x)[:,0]返回第0列类别的对应数据的概率估计。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用svm算法</span></span><br><span class="line">svmg=sklearn.svm.SVC(C=<span class="number">100</span>,kernel=<span class="string">'rbf'</span>,gamma=<span class="number">10</span>,probability=<span class="literal">True</span>)<span class="comment">#高斯核函数</span></span><br><span class="line"><span class="comment">#用训练数据拟合模型</span></span><br><span class="line">svmg.fit(data[[<span class="string">'X1'</span>,<span class="string">'X2'</span>]],data[<span class="string">'y'</span>]) <span class="comment">#X1,X2作为一个整体</span></span><br><span class="line"><span class="comment">#返回平均准确度</span></span><br><span class="line">svmg_score=svmg.score(data[[<span class="string">'X1'</span>,<span class="string">'X2'</span>]],data[<span class="string">'y'</span>])</span><br><span class="line">data[<span class="string">'decision_function'</span>]=svmg.decision_function(data[[<span class="string">'X1'</span>,<span class="string">'X2'</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment">#用概率估计,估计类别</span></span><br><span class="line">data[<span class="string">'svmg_proba'</span>]=svmg.predict_proba(data[[<span class="string">'X1'</span>,<span class="string">'X2'</span>]])[:,<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>按照概率估计，属于当前类别的用红色标注出来</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200806220753.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_proba</span><span class="params">(data)</span>:</span></span><br><span class="line">fig,ax=plt.subplots(figsize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">ax.scatter(data[<span class="string">'X1'</span>],data[<span class="string">'X2'</span>],c=data[<span class="string">'svmg_proba'</span>],cmap=<span class="string">'Reds'</span>)<span class="comment">#颜色用点的标签来标记,第一个类是红色</span></span><br><span class="line">ax.set_title(<span class="string">'svm:C=1'</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'X1'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'X2'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>注意点：当使用高斯核函数之前，需要将x进行归一化，不然核函数的数字很容易受到大的数字的影响。</p><p>附：多分类问题可以和logistic回归的多分类一样，采用one-vs-all方法，训练k个svm分类器，最终选择最大的θ(i)T * X的类别i。</p><h2 id="模型调参"><a href="#模型调参" class="headerlink" title="模型调参"></a>模型调参</h2><p>可以利用sklearn中的svm库函数包，但是需要自己定义代价函数中参数C和使用的核函数，以及核函数中的gamma参数，如何正确的选择参数C和ganmma</p><p>数据导入：</p><p>使用scipy.io下的loadmat导入mat格式的数据，并利用pandas库将数据转换为表格形式，导入训练数据及验证集数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(path)</span>:</span></span><br><span class="line">data=sio.loadmat(path)</span><br><span class="line">train=pd.DataFrame(data.get(<span class="string">'X'</span>),columns=[<span class="string">'X1'</span>,<span class="string">'X2'</span>])</span><br><span class="line">train[<span class="string">'y'</span>]=data.get(<span class="string">'y'</span>)</span><br><span class="line">cv=pd.DataFrame(data.get(<span class="string">'Xval'</span>),columns=[<span class="string">'Xval1'</span>,<span class="string">'Xval2'</span>])</span><br><span class="line">cv[<span class="string">'yval'</span>]=data.get(<span class="string">'yval'</span>)</span><br><span class="line"><span class="keyword">return</span> train,cv</span><br></pre></td></tr></table></figure><p>按照数据类别显示数据的颜色，一个类别显示红色，一个类别颜色很淡</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200806222024.png" alt></p><h3 id="手动选择参数"><a href="#手动选择参数" class="headerlink" title="手动选择参数"></a>手动选择参数</h3><p>选择不同的C和gamm值传入模型，使用fit将训练数据拟合模型，将得到的模型在验证集上验证准确度。最终选择准确度最高的分数对应的C和gamma值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_parameter</span><span class="params">(train,cv)</span>:</span></span><br><span class="line">candidate=[<span class="number">0.01</span>, <span class="number">0.03</span>, <span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">10</span>, <span class="number">30</span>, <span class="number">100</span>]</span><br><span class="line">parameters=[(C,gamma) <span class="keyword">for</span> C <span class="keyword">in</span> candidate <span class="keyword">for</span> gamma <span class="keyword">in</span> candidate]</span><br><span class="line"></span><br><span class="line">scores=[]</span><br><span class="line"><span class="keyword">for</span> C,gamma <span class="keyword">in</span> parameters:</span><br><span class="line">svmg=sklearn.svm.SVC(C=C,kernel=<span class="string">'rbf'</span>,gamma=gamma)</span><br><span class="line">svmg.fit(train[[<span class="string">'X1'</span>,<span class="string">'X2'</span>]],train[<span class="string">'y'</span>])</span><br><span class="line">score=svmg.score(cv[[<span class="string">'Xval1'</span>,<span class="string">'Xval2'</span>]],cv[<span class="string">'yval'</span>])</span><br><span class="line">scores.append(score)</span><br><span class="line">best_score=scores[np.argmax(scores)]</span><br><span class="line">best_parm=parameters[np.argmax(scores)] <span class="comment">#np.argmax(scores)返回最大的索引</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#找到最好的C和gamma进行模型训练</span></span><br><span class="line">svmg=sklearn.svm.SVC(C=best_parm[<span class="number">0</span>],kernel=<span class="string">'rbf'</span>,gamma=best_parm[<span class="number">1</span>])</span><br><span class="line">svmg.fit(train[[<span class="string">'X1'</span>,<span class="string">'X2'</span>]],train[<span class="string">'y'</span>])</span><br><span class="line">yhat=svmg.predict(cv[[<span class="string">'Xval1'</span>,<span class="string">'Xval2'</span>]])</span><br><span class="line">print(classification_report(cv[<span class="string">'yval'</span>],yhat))</span><br></pre></td></tr></table></figure><p>最佳准确率和最佳参数为：</p><p>0.965<br>(0.3, 100)</p><p>使用最好的参数训练模型，在验证集上通过sklearn.metrics库下的classification_report评估模型结果：</p><blockquote><pre><code>          precision    recall  f1-score   support       0       0.97      0.96      0.97       113       1       0.95      0.97      0.96        87accuracy                           0.96       200</code></pre></blockquote><p>总体准确度为96%</p><h3 id="利用库函数中的模型调参"><a href="#利用库函数中的模型调参" class="headerlink" title="利用库函数中的模型调参"></a>利用库函数中的模型调参</h3><p>也可以直接利用sklearn.selection库下的网格网格搜索GridSearchCV，输入模型和参数，自动得到最佳精度和最佳参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#输入模型和参数，自动输出最佳的参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">autoselect_parameter</span><span class="params">(train,cv)</span>:</span></span><br><span class="line">candidate=[<span class="number">0.01</span>, <span class="number">0.03</span>, <span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">10</span>, <span class="number">30</span>, <span class="number">100</span>]</span><br><span class="line">parameters=&#123;<span class="string">'C'</span>:candidate,<span class="string">'gamma'</span>:candidate&#125;</span><br><span class="line">svmg=sklearn.svm.SVC()</span><br><span class="line">gscv=GridSearchCV(svmg,parameters,n_jobs=<span class="number">-1</span>)<span class="comment">#并行数和cpu核一致</span></span><br><span class="line">gscv.fit(train[[<span class="string">'X1'</span>,<span class="string">'X2'</span>]],train[<span class="string">'y'</span>])</span><br><span class="line">best_score=gscv.best_score_</span><br><span class="line">best_parm=gscv.best_params_</span><br><span class="line"></span><br><span class="line"><span class="comment">#找到最好的C和gamma进行模型训练</span></span><br><span class="line">svmg=sklearn.svm.SVC(C=best_parm[<span class="string">'C'</span>],kernel=<span class="string">'rbf'</span>,gamma=best_parm[<span class="string">'gamma'</span>])</span><br><span class="line">svmg.fit(train[[<span class="string">'X1'</span>,<span class="string">'X2'</span>]],train[<span class="string">'y'</span>])</span><br><span class="line">yhat=svmg.predict(cv[[<span class="string">'Xval1'</span>,<span class="string">'Xval2'</span>]])</span><br><span class="line">print(classification_report(cv[<span class="string">'yval'</span>],yhat))</span><br></pre></td></tr></table></figure><p>最佳准确率和最佳参数为：</p><p>0.9194905869324475<br>{‘C’: 30, ‘gamma’: 3}</p><p>使用最好的参数训练模型，在验证集上通过sklearn.metrics库下的classification_report评估模型结果：</p><blockquote><pre><code>          precision    recall  f1-score   support       0       0.95      0.96      0.96       113       1       0.95      0.93      0.94        87accuracy                           0.95       200</code></pre></blockquote><p>总体准确度为95%，结果和手动选择参数不同，原因是：网格搜索会自动把部分训练集数据划分一小部分作为验证集，所以搭建模型的训练数据量和手动选择参数中训练数据量不同，模型上有些微的差异，所以得到准确度会有一点不同。</p><h2 id="模型对比"><a href="#模型对比" class="headerlink" title="模型对比"></a>模型对比</h2><p>使用垃圾分类邮件数据集来对比SVM分类和logistic回归分类的效果，其中数据集已经经过处理，x按照词汇表中的单词得到了每个邮件对应的向量。</p><p>使用scipy.io下的loadmat导入mat格式的数据，分别导入训练数据及测试集数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(path1,path2)</span>:</span></span><br><span class="line">data1=sio.loadmat(path1)</span><br><span class="line">data2=sio.loadmat(path2)</span><br><span class="line"></span><br><span class="line">x=data1.get(<span class="string">'X'</span>)</span><br><span class="line">y=data1.get(<span class="string">'y'</span>)</span><br><span class="line">xtest=data2.get(<span class="string">'Xtest'</span>)</span><br><span class="line">ytest=data2.get(<span class="string">'ytest'</span>)</span><br><span class="line">y=np.ravel(y)</span><br><span class="line">ytest=np.ravel(ytest)</span><br><span class="line"><span class="keyword">return</span> x,y,xtest,ytest</span><br></pre></td></tr></table></figure><h3 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h3><p>利用sklearn中的svm库函数包，通过SVC调用有内核的SVM算法，使用默认参数，用fit将训练集数据拟合模型，通过predict预测测试集的输出，通过sklearn.metrics库下的classification_report评估模型结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用svm</span></span><br><span class="line">svm=sklearn.svm.SVC()</span><br><span class="line">svm.fit(x,y)</span><br><span class="line">yhat=svm.predict(xtest)</span><br><span class="line">print(classification_report(ytest,yhat))</span><br></pre></td></tr></table></figure><blockquote><pre><code>          precision    recall  f1-score   support       0       0.99      1.00      0.99       692       1       0.99      0.97      0.98       308accuracy                           0.99      1000</code></pre></blockquote><p>总体精确度达99%，但是运行时间较长，因为比logtistic回归多用了核函数。</p><h3 id="logostic回归"><a href="#logostic回归" class="headerlink" title="logostic回归"></a>logostic回归</h3><p>利用sklearn中的linear_model库函数包，通过LogisticRegression调用logistic回归算法，使用默认参数，用fit将训练集数据拟合模型，通过predict预测测试集的输出，通过sklearn.metrics库下的classification_report评估模型结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用logotic回归</span></span><br><span class="line">logitic=sklearn.linear_model.LogisticRegression()</span><br><span class="line">logitic.fit(x,y)</span><br><span class="line">yhat=logitic.predict(xtest)</span><br><span class="line">print(classification_report(ytest,yhat))</span><br></pre></td></tr></table></figure><blockquote><pre><code>          precision    recall  f1-score   support       0       1.00      0.99      1.00       692       1       0.99      0.99      0.99       308accuracy                           0.99      1000</code></pre></blockquote><p>总体精确度达99%，运行时间较短。</p><h3 id="两种分类算法如何选择"><a href="#两种分类算法如何选择" class="headerlink" title="两种分类算法如何选择"></a>两种分类算法如何选择</h3><p>n:代表特征数量，m:代表训练样本数量</p><p>（1）当n&gt;m</p><p>选择：用logistic回归/不带核函数的svm</p><p>原因：没有足够多的数据去拟合复杂的非线性函数</p><p>（2）当n很小，m很大（n=1 ~1000,m&gt;=5万）</p><p>选择：用logistic回归/不带核函数的svm</p><p>原因：m太大，特征变量多，用核函数的svm成本代价高</p><p>（3）当n很小，m大小适中（n=1 ~1000,m&lt;5万）</p><p>选择：用带核函数的svm</p><p>原因：m适中，特征变量不多，用核函数的svm成本代价不高，同时有足够多的数据去拟合复杂的非线性函数</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;算法简介&quot;&gt;&lt;a href=&quot;#算法简介&quot; class=&quot;headerlink&quot; title=&quot;算法简介&quot;&gt;&lt;/a&gt;算法简介&lt;/h2&gt;&lt;h3 id=&quot;代价函数&quot;&gt;&lt;a href=&quot;#代价函数&quot; class=&quot;headerlink&quot; title=&quot;代价函数&quot;&gt;&lt;/a&gt;代价函数&lt;/h3&gt;&lt;p&gt;代价函数和logistic回归类似，但是为了保证最大间隔，&lt;strong&gt;&lt;u&gt;当y=1时，需要θT * X&amp;gt;=1，而不是θT * X&amp;gt;=0，当y=0时，需要θT * X&amp;lt;=-1，而不是θT * X&amp;lt;0，&lt;/u&gt;&lt;/strong&gt;因此将假设函数部分替换为如下的函数：&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200806205412.png&quot; alt&gt;&lt;/p&gt;&lt;p&gt;因此代价函数定义如下：&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200806205909.jpg&quot; alt&gt;&lt;/p&gt;&lt;p&gt;其中C的作用和logistic回归中的1/λ作用相同，平衡数据拟合程度和模型中参数的大小,SVM的代价函数不是像logistic中一样输出概率而是直接预测分类（是0还是1）&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="分类" scheme="https://www.xiapf.com/tags/%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>神经网络算法</title>
    <link href="https://www.xiapf.com/blogs/tfNN/"/>
    <id>https://www.xiapf.com/blogs/tfNN/</id>
    <published>2020-08-04T14:41:03.000Z</published>
    <updated>2020-08-04T14:44:03.235Z</updated>
    
    <content type="html"><![CDATA[<h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><h3 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h3><p>当特征很多，学习非线性假设成本很好，这时候可以考虑使用神经网络算法。</p><h3 id="网络设计"><a href="#网络设计" class="headerlink" title="网络设计"></a>网络设计</h3><p>神经网络可以看成是神经元组合而成，每一层有若干个神经元接收上一层的输入，经过神经元的计算之后将值传给下一层，最后一层可以接上一个sigmoid函数或者softmax函数输出分类值。</p><p>一个简单的神经网络如下图：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200804205816.jpg" alt></p><h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><a id="more"></a><p>不同的特征传入网络中，其中红色圆框部分是一个神经元的结构：根据传入特征利用假设函数公式z=wT * x计算出线性值z，然后经过激活函数sigmoid函数输出当前神经元的激活输出值。</p><p>根据不同神经元输出叠加最终传递到最后一层得到当前预测输出，与logistic回归类似，神经网络也是根据每次的输出计算代价函数来反向传播修正参数。代价函数与logistic一样。</p><h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>总体流程：将最终一层的神经元输出传入代价函数，如上图，得到每次的损失为L=-yloga-(1-y)log(1-a)，即得到预测的输出a和实际值y之间的误差，从后向前将误差传递给前一层，利用梯度下降法，通过误差对每层值求导将误差变化传递给每层参数并进行修正：</p><p>1°从最后一层L向前一层输出值a：即误差L对输出的激活值求导，即dL/da，简写为da，根据代价公式可得求导值为da=-y/a+(1-y)/(1-a)</p><p>2°从最后一层L向前一层线性值z：即误差L对线性值z求导，即dL/dz，简写为dz，dz可以写成dL/da * da/dz，其中dL/da上一层求出为da=-y/a+(1-y)/(1-a)，而da/dz根据激活函数公式等于a(1-a)，代入化简得dz=a-y，即可以看做是神经元内的激活值a将自己得到的误差值传递给线性值z</p><p>3°从最后一层L向前一层输入值：即误差L对线性值w求导，即dL/dw，简写为dw，dw可以写成dL/da * da/dz * dz/dw，其中dL/da上一层求出为da=-y/a+(1-y)/(1-a)， da/dz根据激活函数公式等于a(1-a)，即dL/da * da/dz=dz，而dz/dw根据假设函数公式得到等于x，代入化简得dw=da * x，即可以看做是神经元内的线性值z将自己得到的误差值传递给参数w</p><p>最终，不断反复前向传播，根据代价函数从后向前反向传播，通过梯度下降公式利用代价中得到的误差修正参数。</p><h2 id="搭建一个简单的神经网络"><a href="#搭建一个简单的神经网络" class="headerlink" title="搭建一个简单的神经网络"></a>搭建一个简单的神经网络</h2><h3 id="描述-1"><a href="#描述-1" class="headerlink" title="描述"></a>描述</h3><p>使用神经网络进行手写数字识别</p><h3 id="数据集处理"><a href="#数据集处理" class="headerlink" title="数据集处理"></a>数据集处理</h3><p>利用scipy.io中loadmat读入数据集中图像像素（20 * 20），在特征x中加上偏置项，同时将y转换为向量化标签，由于原始的标签10-0（10代表0），1-1，2-2，…，则建立一个10维列向量，将i-1索引上设置为1，即当标签为10时，第9个位置为1，其余位置为0的列向量表示数字0，当标签为1，则索引为1-1=0的位置上为1，其余位置为0的列向量表示数字1，以此类推，得到y的标签如下：（仅展示数字0和1）</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200804214032.jpg" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.处理数据 利用scipy</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(path,transpose=True)</span>:</span></span><br><span class="line">data=sio.loadmat(path)</span><br><span class="line">X=data.get(<span class="string">'X'</span>)</span><br><span class="line">y=data.get(<span class="string">'y'</span>)</span><br><span class="line">y=y.reshape(y.shape[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">return</span> X,y</span><br><span class="line"><span class="comment">#2.预处理数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(raw_x,raw_y)</span>:</span></span><br><span class="line"><span class="comment">#在x的第一列插入元素1</span></span><br><span class="line">x=np.insert(raw_x,<span class="number">0</span>,values=np.ones(raw_x.shape[<span class="number">0</span>]),axis=<span class="number">1</span>) </span><br><span class="line">res=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> raw_y:</span><br><span class="line">y_array=np.zeros(<span class="number">10</span>)</span><br><span class="line">y_array[i<span class="number">-1</span>]=<span class="number">1</span></span><br><span class="line">print(y_array)</span><br><span class="line">res.append(y_array)</span><br><span class="line"><span class="keyword">return</span> x,np.array(res)</span><br></pre></td></tr></table></figure><h3 id="初始参数"><a href="#初始参数" class="headerlink" title="初始参数"></a>初始参数</h3><p>使用预训练的参数作为初始参数或者随机初始化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#预训练模型参数导入</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_weight</span><span class="params">(path)</span>:</span></span><br><span class="line">data=sio.loadmat(path)</span><br><span class="line">theta1=data.get(<span class="string">'Theta1'</span>)</span><br><span class="line">theta2=data.get(<span class="string">'Theta2'</span>)</span><br><span class="line"><span class="keyword">return</span> theta1,theta2</span><br><span class="line"><span class="comment">#随机初始化</span></span><br><span class="line"><span class="comment">#随机初始化参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initial</span><span class="params">(size)</span>:</span></span><br><span class="line"><span class="keyword">return</span> np.random.uniform(<span class="number">-0.12</span>,<span class="number">0.12</span>,size)</span><br></pre></td></tr></table></figure><h3 id="前向传播-1"><a href="#前向传播-1" class="headerlink" title="前向传播"></a>前向传播</h3><p>根据公式计算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#2.搭建前向传播网络</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(theta,x)</span>:</span></span><br><span class="line">theta1,theta2=demerge(theta)</span><br><span class="line">m=x.shape[<span class="number">0</span>]</span><br><span class="line">a1=x</span><br><span class="line">z2=np.dot(a1,theta1.T)</span><br><span class="line">a2=np.insert(sigmoid(z2),<span class="number">0</span>,np.ones(m),axis=<span class="number">1</span>)</span><br><span class="line">z3=np.dot(a2,theta2.T)</span><br><span class="line">a3=sigmoid(z3)</span><br><span class="line"><span class="keyword">return</span> a1,z2,a2,z3,a3</span><br></pre></td></tr></table></figure><h3 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h3><p>根据前向传播得到的输出值，根据交叉熵公式计算成本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#3.成本函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(theta,x,y)</span>:</span></span><br><span class="line">m=x.shape[<span class="number">0</span>]</span><br><span class="line">_,_,_,_,h=forward(theta,x)</span><br><span class="line">cost_term=(<span class="number">-1</span>/m)*np.sum(np.multiply(y,np.log(h))+np.multiply((<span class="number">1</span>-y),np.log(<span class="number">1</span>-h)))</span><br><span class="line"><span class="keyword">return</span> cost_term</span><br><span class="line"><span class="comment">#加上正则化项</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regular_cost</span><span class="params">(theta,x,y,l=<span class="number">1</span>)</span>:</span></span><br><span class="line">theta1,theta2=demerge(theta)</span><br><span class="line">m=len(x)</span><br><span class="line">regular1=(l/(<span class="number">2</span>*m))*np.sum(np.power(theta1[:,<span class="number">1</span>:],<span class="number">2</span>))</span><br><span class="line">regular2=(l/(<span class="number">2</span>*m))*np.sum(np.power(theta2[:,<span class="number">1</span>:],<span class="number">2</span>))</span><br><span class="line">regular_term=cost(theta,x,y)+regular1+regular2</span><br><span class="line"><span class="keyword">return</span> regular_term</span><br></pre></td></tr></table></figure><h3 id="反向传播-1"><a href="#反向传播-1" class="headerlink" title="反向传播"></a>反向传播</h3><p>计算前向传播中每层的线性值和激活值，对每个样本得到当前样本的线性值和激活值</p><p>最后一层向前一层：根据代价函数将误差传到线性值dz=a-y，则对参数的梯度需要累加dw+=dz3 * da2：</p><p>d3i=a3i-yi<br>delta2+=np.mat(d3i).T * np.mat(a2i)</p><p>中间层传递：dL/dz3 * dz3/da2 * da2/dz2=dz3 * w3 * 对激活函数求导，则对参数的梯度需要累加dw+=dz2 * da1：</p><p>d2i=np.multiply(np.dot(theta2.T,d3i),sigmoid_grad(z2i))<br>delta1+=np.mat(d2i[1:]).T * np.mat(a1i) </p><p>最终将累加的参数的梯度除以样本总数，则得到进行一次反向传播参数需要修正的量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_grad</span><span class="params">(a)</span>:</span></span><br><span class="line"><span class="keyword">return</span> np.multiply(sigmoid(a),<span class="number">1</span>-sigmoid(a))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(theta,x,y)</span>:</span></span><br><span class="line">theta1,theta2=demerge(theta)</span><br><span class="line">m=len(x)</span><br><span class="line">delta1=np.zeros(theta1.shape)</span><br><span class="line">delta2=np.zeros(theta2.shape)</span><br><span class="line">a1,z2,a2,z3,a3=forward(theta,x)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">a1i=a1[i,:]<span class="comment">#1,401</span></span><br><span class="line">z2i=z2[i,:]<span class="comment">#1,25</span></span><br><span class="line">a2i=a2[i,:]<span class="comment">#1,26</span></span><br><span class="line">z3i=z3[i,:]<span class="comment">#1,10</span></span><br><span class="line">yi=y[i,:]<span class="comment">#1,10</span></span><br><span class="line">a3i=a3[i,:]<span class="comment">#1,10</span></span><br><span class="line"></span><br><span class="line">d3i=a3i-yi  <span class="comment">#1*10</span></span><br><span class="line">delta2+=np.mat(d3i).T*np.mat(a2i) <span class="comment">#10,26</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#误差乘以参数</span></span><br><span class="line">z2i=np.insert(z2i,<span class="number">0</span>,np.ones(<span class="number">1</span>))</span><br><span class="line">d2i=np.multiply(np.dot(theta2.T,d3i),sigmoid_grad(z2i)) </span><br><span class="line">delta1+=np.mat(d2i[<span class="number">1</span>:]).T*np.mat(a1i) <span class="comment">#25,401</span></span><br><span class="line"></span><br><span class="line">delta1=delta1/m</span><br><span class="line">delta2=delta2/m</span><br><span class="line"><span class="keyword">return</span> merge(delta1,delta2)</span><br><span class="line"><span class="comment">#加入正则化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regular_grad</span><span class="params">(theta,x,y,l=<span class="number">1</span>)</span>:</span></span><br><span class="line">delta1,delta2=demerge(gradient(theta,x,y))</span><br><span class="line">t1,t2=demerge(theta)</span><br><span class="line">m=len(x)</span><br><span class="line">theta1[:,<span class="number">0</span>]=<span class="number">0</span></span><br><span class="line">regular_grad1=(l/m)*theta1+delta1</span><br><span class="line">theta2[:,<span class="number">0</span>]=<span class="number">0</span></span><br><span class="line">regular_grad2=(l/m)*theta2+delta2</span><br><span class="line"><span class="keyword">return</span> merge(regular_grad1,regular_grad2)</span><br></pre></td></tr></table></figure><h3 id="搭建模型"><a href="#搭建模型" class="headerlink" title="搭建模型"></a>搭建模型</h3><p>根据牛顿截断法，求出神经网络代价最小的参数，即拟合数据的参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#搭建模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_traing</span><span class="params">(x,y,l=<span class="number">1</span>,size=<span class="number">10285</span>)</span>:</span></span><br><span class="line">theta=initial(size)</span><br><span class="line">res=opt.minimize(fun=regular_cost,x0=theta,args=(x,y,l),jac=regular_grad,method=<span class="string">'TNC'</span>)</span><br><span class="line"><span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><p>用高级优化算法运行比较慢。</p><p>最终得到的训练的参数为x: array([-0.55359235, -0.11360808,  0.08538822, …, -1.99119256,<br>        0.07256982,  0.85786155])</p><p>根据得出的模型，将数据经过一次前向传播得到的输出和实际比对，得出预测的准确率：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#测试准确度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_accuracy</span><span class="params">(theta,x,y)</span>:</span></span><br><span class="line">_,_,_,_,h=forward(theta,x)</span><br><span class="line">yhat=np.argmax(h,axis=<span class="number">1</span>)</span><br><span class="line">print(classification_report(y,yhat))</span><br><span class="line">m=len(yhat)</span><br><span class="line">count=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line"><span class="keyword">if</span>(yhat[i]==np.argmax(y[i,:])):</span><br><span class="line">count=count+<span class="number">1</span></span><br><span class="line">print(<span class="string">'show_accuracy:&#123;&#125;'</span>.format(count/m))</span><br></pre></td></tr></table></figure><p>预测精度为：show_accuracy:0.9764，可见神经网络的准确度很高</p><h3 id="附：观察神经网络隐藏层学习的内容"><a href="#附：观察神经网络隐藏层学习的内容" class="headerlink" title="附：观察神经网络隐藏层学习的内容"></a>附：观察神经网络隐藏层学习的内容</h3><p>最终得到神经网络每层的参数即最终学习到的模型，这里两层神经网络（输入层不算在层数内），利用画图中的matshow显示第一个输入层到隐藏层学习到参数内容。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200804223607.jpg" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#显示隐藏层</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_hidden_layer</span><span class="params">(theta)</span>:</span></span><br><span class="line">final_theat,_=demerge(theta)</span><br><span class="line">hidden_theta=final_theat[:,<span class="number">1</span>:]</span><br><span class="line">size=<span class="number">20</span></span><br><span class="line"></span><br><span class="line">fig,ax=plt.subplots(<span class="number">5</span>,<span class="number">5</span>,sharex=<span class="literal">True</span>,sharey=<span class="literal">True</span>,figsize=((<span class="number">5</span>,<span class="number">5</span>)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">ax[i,j].matshow(hidden_theta[<span class="number">5</span>*i+j].reshape((size,size)),cmap=matplotlib.cm.binary) <span class="comment">#cmap控制颜色显示</span></span><br><span class="line">plt.xticks(np.array([]))</span><br><span class="line">plt.yticks(np.array([]))</span><br></pre></td></tr></table></figure><p>可以看出隐藏层学习了图片的轮廓信息。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;描述&quot;&gt;&lt;a href=&quot;#描述&quot; class=&quot;headerlink&quot; title=&quot;描述&quot;&gt;&lt;/a&gt;描述&lt;/h2&gt;&lt;h3 id=&quot;使用场景&quot;&gt;&lt;a href=&quot;#使用场景&quot; class=&quot;headerlink&quot; title=&quot;使用场景&quot;&gt;&lt;/a&gt;使用场景&lt;/h3&gt;&lt;p&gt;当特征很多，学习非线性假设成本很好，这时候可以考虑使用神经网络算法。&lt;/p&gt;&lt;h3 id=&quot;网络设计&quot;&gt;&lt;a href=&quot;#网络设计&quot; class=&quot;headerlink&quot; title=&quot;网络设计&quot;&gt;&lt;/a&gt;网络设计&lt;/h3&gt;&lt;p&gt;神经网络可以看成是神经元组合而成，每一层有若干个神经元接收上一层的输入，经过神经元的计算之后将值传给下一层，最后一层可以接上一个sigmoid函数或者softmax函数输出分类值。&lt;/p&gt;&lt;p&gt;一个简单的神经网络如下图：&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200804205816.jpg&quot; alt&gt;&lt;/p&gt;&lt;h3 id=&quot;前向传播&quot;&gt;&lt;a href=&quot;#前向传播&quot; class=&quot;headerlink&quot; title=&quot;前向传播&quot;&gt;&lt;/a&gt;前向传播&lt;/h3&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习系统设计</title>
    <link href="https://www.xiapf.com/blogs/tfRegDg/"/>
    <id>https://www.xiapf.com/blogs/tfRegDg/</id>
    <published>2020-08-03T14:21:53.000Z</published>
    <updated>2020-08-06T15:01:05.076Z</updated>
    
    <content type="html"><![CDATA[<h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><h3 id="数据描述"><a href="#数据描述" class="headerlink" title="数据描述"></a>数据描述</h3><p>给定数据集中包含训练集、验证集、测试集，常用比例6：2：2</p><p>其中训练集用于快速搭建算法模型，验证集用于选择最佳模型（如根据不同的正则化参数或者网络设置等超参设置，得到不同的模型），测试局用于评估得到的最佳模型。</p><h3 id="模型搭建描述——训练集"><a href="#模型搭建描述——训练集" class="headerlink" title="模型搭建描述——训练集"></a>模型搭建描述——训练集</h3><p>使用线性回归对数据集中的样本进行分类，通过搭建算法优化目标：平方差代价函数，每一步优化的梯度计算，导入训练集数据使用优化算法优化目标，快速搭建一个算法模型。</p><a id="more"></a><p><u>即通过训练集的全部或部分数据得到假设函数h(x)，根据最小化代价函数得到模型参数θ</u></p><h3 id="模型改进描述——验证集"><a href="#模型改进描述——验证集" class="headerlink" title="模型改进描述——验证集"></a>模型改进描述——验证集</h3><p>当快速搭建一个简单系统后，搭建的模型对数据不一定很好拟合，此时使用验证集对模型优化。采用画学习曲线或者误差分析的方法。</p><p><u>即通过不同的超参或者数据得到的模型参数θ，得到在验证集上的代价函数的值Jcv(θ)，选择Jcv(θ)最小的模型</u></p><p>模型容易出现高偏差和高方差：</p><p>高偏差：模型不能很好拟合训练集，欠拟合状态，此时Jtrain(θ)很大，Jcv(θ)和Jtrain(θ)差不多大。</p><p>措施：特征（增加特征数量，增加多项式特征），正则化（减少正则化系数（因为不够拟合数据））</p><p>高方差：得到的模型在训练集上很好拟合（精度很高），在验证集上不能很好拟合数据，说明模型无法泛化到新的样本，过拟合状态，此时Jtrain(θ)很小，Jcv(θ)&gt;&gt;Jtrain(θ)</p><p>措施：数据（收集更多的训练集数据），正则化（增大正则化系数），特征（减少特征数量）</p><p>（1）画学习曲线</p><p>人为的选择一些训练样本来搭建模型，并通过得到的模型计算在这些训练样本的误差Jtrain(θ)，并计算所有验证集在这个模型上的误差Jcv(θ)，画出训练样本和误差的曲线变化。学习曲线大致趋势如图</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200803210856.png" alt></p><p>当图像中Jcv(θ)变得水平并不再变化，并且和Jtrain(θ)很接近时，说明是存在高偏差</p><p>当图像中Jtrain(θ)和Jcv(θ)存在很大间距，说明存在高方差</p><p>（2）误差分析</p><p>手动列出所有出错的样本，对出错的样本分类，找到影响错误占比最多的类，对其进行改进。可以列出表格：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200803214007.jpg" alt></p><p>（3）误差评估：需要用一个数值结果评估改进后的算法的执行效果，使用单一实数评价，能快速判断结果</p><p>1°对称性分类：使用正确率或错误率</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200803213857.jpg" alt></p><p>2°不对称性分类（即数据集内0，1类分类不平衡，存在偏斜类，一个类的数据比另一个类数据多的多，无法使用分类精确度来衡量）：此时需要把少的那类作为1类</p><p>查准率precision：所有预测为真的数量中，实际确实为真。预测为真且实际为真的数量占所有预测为真的数量</p><p>召回率recall：实际为真的数量中，正确预测为真预测为真且实际为真的数量占所有实际为真的数量</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200803214028.jpg" alt></p><p>F1分数（平衡查准率和召回率）：<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200803213949.jpg" alt></p><h3 id="模型评估描述——测试集"><a href="#模型评估描述——测试集" class="headerlink" title="模型评估描述——测试集"></a>模型评估描述——测试集</h3><p><u>通过验证集上的代价函数的值Jtest(θ)，通过该值衡量模型的泛化误差，即选择的模型好坏</u></p><p>或者使用上述的单一实数评价指标。</p><h2 id="训练集搭建模型——原始特征、无正则化"><a href="#训练集搭建模型——原始特征、无正则化" class="headerlink" title="训练集搭建模型——原始特征、无正则化"></a>训练集搭建模型——原始特征、无正则化</h2><p>（1）导入数据</p><p>使用scipy.io下的loadmat导入mat格式的数据如下</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200803213846.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data=sio.loadmat(path)</span><br><span class="line">x=data.get(<span class="string">'X'</span>)</span><br><span class="line">y=data.get(<span class="string">'y'</span>)</span><br><span class="line">y=y.reshape(y.shape[<span class="number">0</span>])</span><br><span class="line"> x,xval,xtest=[np.insert(a,<span class="number">0</span>,np.ones(a.shape[<span class="number">0</span>]),axis=<span class="number">1</span>) <span class="keyword">for</span> a <span class="keyword">in</span> (x,xval,xtest)]<span class="comment">#处理数据，加入偏置项</span></span><br></pre></td></tr></table></figure><p>（2）定义代价函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#代价函数 平方差误差</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(theta,x,y)</span>:</span></span><br><span class="line">h=np.dot(x,theta.T)</span><br><span class="line">m=x.shape[<span class="number">0</span>]</span><br><span class="line">cost_term=(<span class="number">1</span>/(<span class="number">2</span>*m))*np.sum(np.power((h-y),<span class="number">2</span>))</span><br><span class="line"><span class="keyword">return</span> cost_term</span><br><span class="line"><span class="comment">#正则化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regular_cost</span><span class="params">(theta,x,y,l=<span class="number">1</span>)</span>:</span></span><br><span class="line">cost_term=cost(theta,x,y)</span><br><span class="line">m=x.shape[<span class="number">0</span>]</span><br><span class="line">theta_no0=theta[<span class="number">1</span>:]</span><br><span class="line">regular_term=cost_term+(l/(<span class="number">2</span>*m))*np.sum(np.power(theta_no0,<span class="number">2</span>))</span><br><span class="line"><span class="keyword">return</span> regular_term</span><br></pre></td></tr></table></figure><p>（3）定义每一步的梯度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#梯度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(theta,x,y)</span>:</span></span><br><span class="line">m,n=x.shape</span><br><span class="line">h=np.dot(x,theta.T)</span><br><span class="line">grad=np.zeros(len(theta))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">grad[i:]=(<span class="number">1</span>/m)*np.sum((h-y)*x[:,i])</span><br><span class="line"><span class="keyword">return</span> grad</span><br><span class="line"><span class="comment">#正则化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regular_grad</span><span class="params">(theta,x,y,l=<span class="number">1</span>)</span>:</span></span><br><span class="line">theta_no0=theta.copy()</span><br><span class="line">theta_no0[<span class="number">0</span>]=<span class="number">0</span></span><br><span class="line">m=x.shape[<span class="number">0</span>]</span><br><span class="line">grad=gradient(theta,x,y)</span><br><span class="line">regular_grad=grad+(l/m)*theta_no0</span><br><span class="line"><span class="keyword">return</span> regular_grad</span><br></pre></td></tr></table></figure><p>（4）搭建模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#拟合数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">liner_regression</span><span class="params">(x,y,l)</span>:</span></span><br><span class="line">theta=np.ones(x.shape[<span class="number">1</span>])</span><br><span class="line">res=opt.minimize(fun=regular_cost,x0=theta,jac=regular_grad,args=(x,y,l),method=<span class="string">'TNC'</span>)</span><br><span class="line"><span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><p>（5）结果</p><p>最终得到的模型参数：[13.08790376  0.36777923]</p><p>得到的拟合直线：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200803214458.png" alt></p><p>从拟合结果可以看出模型欠拟合，得到的拟合直线没有很好的贴合训练数据。使用验证集进一步分析改进。</p><h2 id="验证集改进模型"><a href="#验证集改进模型" class="headerlink" title="验证集改进模型"></a>验证集改进模型</h2><h3 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h3><p>每次选取部分训练集得出模型，算出误差Jtrain(θ)，再将所有验证集放在得出的模型中得到误差Jcv(θ)如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200803214746.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#画学习曲线，随着样本数量的增加，训练集和验证集的误差</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_learingline</span><span class="params">(x,y,xval,yval,l=<span class="number">0</span>)</span>:</span></span><br><span class="line">traing_cost=[]</span><br><span class="line">cv_cost=[]</span><br><span class="line">m=x.shape[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,m+<span class="number">1</span>):</span><br><span class="line">res_tmp=liner_regression(x[:i,:],y[:i],l)</span><br><span class="line">theta_tmp=res_tmp.x</span><br><span class="line">traing=regular_cost(theta_tmp,x[:i,:],y[:i],l)</span><br><span class="line">cv=regular_cost(theta_tmp,xval,yval,l)</span><br><span class="line">traing_cost.append(traing)</span><br><span class="line">cv_cost.append(cv)</span><br><span class="line">plt.plot(list(range(<span class="number">1</span>,m+<span class="number">1</span>)),traing_cost,label=<span class="string">'traing_cost'</span>)</span><br><span class="line">plt.plot(list(range(<span class="number">1</span>,m+<span class="number">1</span>)),cv_cost,label=<span class="string">'cv_cost'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(<span class="string">'m of train'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'error'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>从训练集样本-误差图像可以看出，Jcv(θ)变得水平并且和Jtrain(θ)很近，说明出现了高偏差，需要从增加特征和减少正则化入手。</p><h3 id="多项式特征"><a href="#多项式特征" class="headerlink" title="多项式特征"></a>多项式特征</h3><p>将原始特征进行1 ~8 次方构造新的特征，并将特征进行归一化，加快算法寻优速度。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200803215502.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#重新导入数据</span></span><br><span class="line">x,y,xval,yval,xtest,ytest=load_data(<span class="string">'ex5data1.mat'</span>)</span><br><span class="line">x=np.ravel(x)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare_data</span><span class="params">(x,power)</span>:</span></span><br><span class="line">data=&#123;<span class="string">'f&#123;&#125;'</span>.format(i):np.power(x,i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,power+<span class="number">1</span>)&#125;</span><br><span class="line">df=pd.DataFrame(data)</span><br><span class="line"><span class="keyword">return</span> df</span><br><span class="line"><span class="comment">#归一化特征</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize_feature</span><span class="params">(df)</span>:</span></span><br><span class="line"><span class="keyword">return</span> df.apply(<span class="keyword">lambda</span> column:(column-column.mean())/column.std())</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare</span><span class="params">(x,power)</span>:</span></span><br><span class="line"><span class="comment">#准备数据</span></span><br><span class="line">df=prepare_data(x,power)</span><br><span class="line"><span class="comment">#归一化</span></span><br><span class="line">df=normalize_feature(df)</span><br><span class="line"><span class="comment">#转换</span></span><br><span class="line">ndf=np.mat(df)</span><br><span class="line">x=np.insert(ndf,<span class="number">0</span>,np.ones(ndf.shape[<span class="number">0</span>]),axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">return</span> x</span><br><span class="line">x_new=prepare(x,<span class="number">8</span>)</span><br></pre></td></tr></table></figure><h3 id="找到最佳正则化系数"><a href="#找到最佳正则化系数" class="headerlink" title="找到最佳正则化系数"></a>找到最佳正则化系数</h3><p>当通过多项式特征减少欠拟合，接着需要确定最佳的正则化系数。</p><p>步骤：（这里的模型搭建需要用到正则化，但是训练集和测试集上的误差函数不加入正则化）</p><p>（1）按照不同的步长设置正则化系数列表</p><p>（2）根据不同的正则化系数和训练集得到模型参数</p><p>（3）在验证集上通过得到的模型计算Jcv(θ)，最终选择Jcv(θ)最小的模型</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200803215927.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#找到最佳的正则化系数,成本函数不加入正则化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_lambda</span><span class="params">(x_new,y,xval_new,yval,lambda_values)</span>:</span></span><br><span class="line">traing_cost=[]</span><br><span class="line">cv_cost=[]</span><br><span class="line">m=x_new.shape[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> lambda_values:</span><br><span class="line">res_tmp=liner_regression(x_new,y,l)</span><br><span class="line">theta_tmp=res_tmp.x</span><br><span class="line">traing=cost(theta_tmp,x_new,y)</span><br><span class="line">cv=cost(theta_tmp,xval_new,yval)</span><br><span class="line">traing_cost.append(traing)</span><br><span class="line">cv_cost.append(cv)</span><br><span class="line">plt.plot(lambda_values,traing_cost,label=<span class="string">'traing_cost'</span>)</span><br><span class="line">plt.plot(lambda_values,cv_cost,label=<span class="string">'cv_cost'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(<span class="string">'lambda values'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="keyword">return</span> cv_cost</span><br><span class="line">lambda_values=[<span class="number">0</span>,<span class="number">0.001</span>,<span class="number">0.003</span>,<span class="number">0.01</span>,<span class="number">0.03</span>,<span class="number">0.1</span>,<span class="number">0.3</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">10</span>]</span><br><span class="line">cv_cost=find_lambda(x_new,y,xval_new,yval,lambda_values)</span><br></pre></td></tr></table></figure><p>最终选择验证集上误差最小的正则化系数：final_l=lambda_values[np.argmin(cv_cost)]，选择lambda=1</p><h2 id="测试集评估模型"><a href="#测试集评估模型" class="headerlink" title="测试集评估模型"></a>测试集评估模型</h2><p>根据加入多项式特征和正则化系数后，选择的最优模型，使用测试集进行评估：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">res_tmp=liner_regression(x_new,y,final_l)</span><br><span class="line">theta_tmp=res_tmp.x</span><br><span class="line">test_cost=cost(theta_tmp,xtest_new,ytest)</span><br><span class="line">print(test_cost)</span><br></pre></td></tr></table></figure><p>得到在测试集上的泛化误差Jtest(θ)=7.466270203814922</p><p>使用分类准确率来评估模型：根据得到模型，计算其预测值，当预测值和真实值的平方误差大于0，则说明拟合失败</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">yhat=np.dot(xtest_new,theta_tmp.T)</span><br><span class="line">m,n=yhat.shape</span><br><span class="line">ytest=ytest.reshape((m,n))</span><br><span class="line">error=<span class="number">0</span></span><br><span class="line">cost_tmp=np.zeros((m,n))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">cost_tmp[<span class="number">0</span>,i]=np.power((yhat[<span class="number">0</span>,i]-ytest[<span class="number">0</span>,i]),<span class="number">2</span>)</span><br><span class="line"><span class="keyword">if</span>(cost_tmp[<span class="number">0</span>,i]&gt;<span class="number">0</span>):</span><br><span class="line">error+=<span class="number">1</span></span><br><span class="line">print(<span class="string">'accuarcy:'</span>,<span class="number">1</span>-error/n)</span><br></pre></td></tr></table></figure><p>得到模型精度accuarcy: 0.9523809523809523，可见优化的模型泛化能力很强。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;描述&quot;&gt;&lt;a href=&quot;#描述&quot; class=&quot;headerlink&quot; title=&quot;描述&quot;&gt;&lt;/a&gt;描述&lt;/h2&gt;&lt;h3 id=&quot;数据描述&quot;&gt;&lt;a href=&quot;#数据描述&quot; class=&quot;headerlink&quot; title=&quot;数据描述&quot;&gt;&lt;/a&gt;数据描述&lt;/h3&gt;&lt;p&gt;给定数据集中包含训练集、验证集、测试集，常用比例6：2：2&lt;/p&gt;&lt;p&gt;其中训练集用于快速搭建算法模型，验证集用于选择最佳模型（如根据不同的正则化参数或者网络设置等超参设置，得到不同的模型），测试局用于评估得到的最佳模型。&lt;/p&gt;&lt;h3 id=&quot;模型搭建描述——训练集&quot;&gt;&lt;a href=&quot;#模型搭建描述——训练集&quot; class=&quot;headerlink&quot; title=&quot;模型搭建描述——训练集&quot;&gt;&lt;/a&gt;模型搭建描述——训练集&lt;/h3&gt;&lt;p&gt;使用线性回归对数据集中的样本进行分类，通过搭建算法优化目标：平方差代价函数，每一步优化的梯度计算，导入训练集数据使用优化算法优化目标，快速搭建一个算法模型。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Leecode做题记录（一）</title>
    <link href="https://www.xiapf.com/blogs/LC1/"/>
    <id>https://www.xiapf.com/blogs/LC1/</id>
    <published>2020-07-27T13:50:24.000Z</published>
    <updated>2020-09-08T04:51:17.275Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Leecode做题记录（一）"><a href="#Leecode做题记录（一）" class="headerlink" title="Leecode做题记录（一）"></a>Leecode做题记录（一）</h1><h2 id="题目：378-有序矩阵中第K小的元素"><a href="#题目：378-有序矩阵中第K小的元素" class="headerlink" title="题目：378. 有序矩阵中第K小的元素"></a>题目：378. 有序矩阵中第K小的元素</h2><p>描述：给定一个 n * n 矩阵，其中每行和每列元素均按升序排序，找到矩阵中第k小的元素。请注意，它是排序后的第k小元素，而不是第 <code>k</code> 个不同的元素。</p><p>思路：</p><p>（1）优先队列</p><p>构造一个大小为k的小顶堆，不断插入数组元素，自动进行排序</p><p>代码</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">priority_queue&lt;<span class="keyword">int</span>&gt; p;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;matrix.<span class="built_in">size</span>();i++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;matrix[<span class="number">0</span>].<span class="built_in">size</span>();j++)</span><br><span class="line">    &#123;</span><br><span class="line">        p.push(matrix[i][j]);</span><br><span class="line">        <span class="keyword">if</span>(p.<span class="built_in">size</span>()&gt;k)</span><br><span class="line">            p.pop();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><a id="more"></a><p>（2）二分查找</p><p>左上角到右下角构成一个递增序列,中间的mid的值将数列分成了两部分，可以画图看，是两个板块</p><p>当小于等于mid的数的数量小于k时，说明第k小的元素要在前半部分板块查找；反之当当小于等于mid的数的数量大于等于k时，要在后半部分板块查找</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">        <span class="keyword">int</span> left=matrix[<span class="number">0</span>][<span class="number">0</span>];</span><br><span class="line"><span class="keyword">int</span> right=matrix.back().back();</span><br><span class="line"><span class="keyword">while</span>(left&lt;right)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">int</span> mid=(left+right)/<span class="number">2</span>;</span><br><span class="line">    <span class="keyword">if</span>(check(matrix,mid,k))</span><br><span class="line">        right=mid;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        left=mid+<span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="题目：108-将有序数组转换为二叉搜索树"><a href="#题目：108-将有序数组转换为二叉搜索树" class="headerlink" title="题目：108. 将有序数组转换为二叉搜索树"></a>题目：108. 将有序数组转换为二叉搜索树</h2><p> 描述：将一个按照升序排列的有序数组，转换为一棵高度平衡二叉搜索树。本题中，一个高度平衡二叉树是指一个二叉树每个节点 的左右两个子树的高度差的绝对值不超过 1</p><p>思路：树的构造要找准根节点，给出的数组是递增数组，为了构造二叉平衡树，根节点选择中间节点，选择了根节点之后，分别按照同样的方法在左右子树中划分根节点、左子树、右子树；当指向数组的左指针等于右指针，此时数组中没有树，即需要返回。</p><p>注：根据中点构造根节点时，区间是左闭右开</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">    root=dfs(<span class="number">0</span>,nums.<span class="built_in">size</span>(),nums);</span><br><span class="line"></span><br><span class="line"><span class="function">TreeNode* <span class="title">dfs</span><span class="params">(<span class="keyword">int</span> left,<span class="keyword">int</span> right,<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; nums)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(left==right)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">int</span> mid=(left+right)/<span class="number">2</span>;</span><br><span class="line">    TreeNode* root=<span class="keyword">new</span> TreeNode(nums[mid]);</span><br><span class="line">    root-&gt;left=dfs(left,mid,nums);</span><br><span class="line">    root-&gt;right=dfs(mid+<span class="number">1</span>, right,nums);</span><br><span class="line">    <span class="keyword">return</span> root;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="题目：62-不同路径-63-不同路径-II"><a href="#题目：62-不同路径-63-不同路径-II" class="headerlink" title="题目：62. 不同路径    63. 不同路径 II"></a>题目：62. 不同路径    63. 不同路径 II</h2><p>描述1：一个机器人位于一个 m x n 网格的左上角 （起始点在下图中标记为“Start” ）。机器人每次只能向下或者向右移动一步。机器人试图达到网格的右下角（在下图中标记为“Finish”）。问总共有多少条不同的路径？</p><p>描述2：一个机器人位于一个 m x n 网格的左上角 （起始点在下图中标记为“Start” ）。机器人每次只能向下或者向右移动一步。机器人试图达到网格的右下角（在下图中标记为“Finish”）。现在考虑网格中有障碍物。那么从左上角到右下角将会有多少条不同的路径？</p><p>思路：动态规划，（i,j）表述到达位置i,j的路径数，由于机器人智能向右向下走，所以该数目等于当前位置的左边和上面路径之和。</p><p>需要注意对第一行和第一列的单独处理，当有路障的时候，判断无路障再计算路径数目，有路障此路不通，路径数量为0</p><p>代码1：没有障碍的简单情况</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;m;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;n;j++)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">//1.处理第一行和第一列</span></span><br><span class="line">                <span class="keyword">if</span>(i==<span class="number">0</span>||j==<span class="number">0</span>)</span><br><span class="line">                    dp[i][j]=<span class="number">0</span>;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                    <span class="comment">//2.i，j位置的路径总数等于左边和上面的路径之和</span></span><br><span class="line">                    dp[i][j]=dp[i][j<span class="number">-1</span>]+dp[i<span class="number">-1</span>][j];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><p>代码2：有障碍的情况，此时第一行和第一列需要单独处理（根据有无路障进行赋值）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//3.处理剩余的</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;m;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">1</span>;j&lt;n;j++)</span><br><span class="line">                <span class="comment">//当没有障碍的时候</span></span><br><span class="line">                <span class="keyword">if</span>(obstacleGrid[i][j]==<span class="number">0</span>)</span><br><span class="line">                    dp[i][j]=dp[i<span class="number">-1</span>][j]+dp[i][j<span class="number">-1</span>];</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><h2 id="题目：349-两个数组的交集，350-两个数组的交集-II"><a href="#题目：349-两个数组的交集，350-两个数组的交集-II" class="headerlink" title="题目：349. 两个数组的交集，350. 两个数组的交集 II"></a>题目：349. 两个数组的交集，350. 两个数组的交集 II</h2><p>描述1：给定两个数组，编写一个函数来计算它们的交集。输出结果中的每个元素一定是唯一的。</p><p>描述2：给定两个数组，编写一个函数来计算它们的交集。输出结果中每个元素出现的次数，应与元素在两个数组中出现的次数一致。</p><p>思路：排序+双指针，将两个数组排序后使用双指针，当数组a中元素小于数组b中元素，a数组直至后移，反之b数组指针后移，当相等时则找到了交集中的元素。</p><p>当交集中的元素需要唯一的时候，可以使用set去重。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//1.排序</span></span><br><span class="line">sort(nums1.<span class="built_in">begin</span>(),nums1.<span class="built_in">end</span>());</span><br><span class="line">sort(nums2.<span class="built_in">begin</span>(),nums2.<span class="built_in">end</span>());</span><br><span class="line"></span><br><span class="line"><span class="comment">//2.双指针</span></span><br><span class="line"><span class="keyword">int</span> i=<span class="number">0</span>,j=<span class="number">0</span>;</span><br><span class="line"><span class="built_in">set</span>&lt;<span class="keyword">int</span>&gt; s;</span><br><span class="line"><span class="keyword">while</span>((i&lt;nums1.<span class="built_in">size</span>())&amp;&amp;(j&lt;nums2.<span class="built_in">size</span>()))</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span>(nums1[i]&lt;nums2[j])</span><br><span class="line">        i++;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (nums2[j]&lt;nums1[i])</span><br><span class="line">        j++;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        s.insert(nums1[i]);</span><br><span class="line">        i++;</span><br><span class="line">        j++;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>set去重</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> k=<span class="number">0</span>;</span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res(s.<span class="built_in">size</span>());</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">auto</span> num:s)</span><br><span class="line">&#123;</span><br><span class="line">    res[k]=num;</span><br><span class="line">    k++;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>附：1.当a数组比b数组小很多的时候，使用哈希表，a数组用于计数，在b数组中进行哈希查找，时间复杂度O(max(a,b)），空间复杂度O(min(a,b)）</p><p>2.磁盘内存有限不能一次加载所有元素到内存，不能使用内部排序算法，使用归并排序，将分割的子数组写成小文件，归并后合并为大文件</p><h2 id="题目：112-路径总和"><a href="#题目：112-路径总和" class="headerlink" title="题目：112. 路径总和"></a>题目：112. 路径总和</h2><p>描述：给定一个二叉树和一个目标和，判断该树中是否存在根节点到叶子节点的路径，这条路径上所有节点值相加等于目标和。说明: 叶子节点是指没有子节点的节点。</p><p>思路：递归，使用目标值减去当前节点的值，不断在左右子树找一条路径，当找到一条路径（即遇到叶子节点），剩余的值等于叶子节点说明找到路径，此时返回TRUE，如果左右子树都找不到路径，则返回FALSE。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">     <span class="comment">//1.递归结束，找到一个叶子节点即路径</span></span><br><span class="line"><span class="keyword">if</span>((root-&gt;left==<span class="literal">NULL</span>)&amp;&amp;(root-&gt;right==<span class="literal">NULL</span>))</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">//判断剩余的值是否和叶子的值相等</span></span><br><span class="line">    <span class="keyword">if</span>(sum==root-&gt;val)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//2.递归，减去当前节点的值</span></span><br><span class="line"><span class="keyword">int</span> val=root-&gt;val;</span><br><span class="line">sum=sum-val;        </span><br><span class="line"><span class="comment">//3.继续往左右子树找路径</span></span><br><span class="line"><span class="keyword">return</span> hasPathSum(root-&gt;left, sum)||hasPathSum(root-&gt;right, sum);</span><br></pre></td></tr></table></figure><h2 id="题目：面试题-16-11-跳水板"><a href="#题目：面试题-16-11-跳水板" class="headerlink" title="题目：面试题 16.11. 跳水板"></a>题目：面试题 16.11. 跳水板</h2><p>描述：你正在使用一堆木板建造跳水板。有两种类型的木板，其中长度较短的木板长度为shorter，长度较长的木板长度为longer。你必须正好使用k块木板。编写一个方法，生成跳水板所有可能的长度。返回的长度需要从小到大排列。</p><p>思路：这是两个木板的排列组合问题，只需要考虑每次选取短木板和长木板的个数，短木板个数可以取0 ~ k，长木板个数可以取0 ~ k，按照不同的情况相加得到不同的结果。</p><p>因为需要从小到大排列，所以一开始取最多的短木板，慢慢减少短木板的数量，同时增加长木板的数量。最终由k+1中结果（木板数量在0 ~ k之间变化，共k+1种）。</p><p>注：当短木板的数量=长木板的数量，此时题目就退化为取k次短木板/长木板。当k=0的特殊情况也需要考虑。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">        <span class="comment">//1.特殊情况</span></span><br><span class="line"><span class="keyword">if</span>(k==<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> &#123;&#125;;</span><br><span class="line"><span class="comment">//2.两块板长度相同</span></span><br><span class="line"><span class="keyword">if</span>(shorter==longer)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res(<span class="number">1</span>);</span><br><span class="line">    res[<span class="number">0</span>]=k*shorter;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//3.对两块板子取不同的次数,最终有k+1种结果</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res(k+<span class="number">1</span>);</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;=k;i++)</span><br><span class="line">&#123;</span><br><span class="line">    res[i]=(k-i)*shorter+i*longer;<span class="comment">//结果从小到大排列</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="题目：7-整数反转"><a href="#题目：7-整数反转" class="headerlink" title="题目：7. 整数反转"></a>题目：7. 整数反转</h2><p>描述：给出一个 32 位的有符号整数，你需要将这个整数中每位上的数字进行反转。</p><p>思路：每次取数字的最后一位，作为新数字的最高位。</p><p>重点是需要判断加上当前最后一位的转换的结果是否溢出：（转换为字符串也可以解决溢出，但使用库函数效率低）</p><p>（1）大于32位最大值：当前转换结果比最大值/10大，或者当前转换结果=最大值/10且当前最后一位数大于7（2^31-1最后一位是7），此时相加得到的结果溢出，返回0；</p><p>（2）小于32位最小值同理：当前转换结果比最小值/10小，或者当前转换结果=最小值/10且当前最后一位数大于-8（2^31最后一位是-8），此时相加得到的结果溢出。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">    <span class="keyword">while</span> (x!=<span class="number">0</span>) &#123;</span><br><span class="line">    temp=x%<span class="number">10</span>;<span class="comment">//每次的个位数</span></span><br><span class="line">    <span class="comment">//考虑溢出 2^31-1尾数为7，-2^31尾数为8</span></span><br><span class="line">    <span class="keyword">if</span>((result&gt;INT_MAX/<span class="number">10</span>)||(result==INT_MAX/<span class="number">10</span>&amp;&amp;temp&gt;<span class="number">7</span>))<span class="comment">//考虑大于最大值 2^31-1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span>((result&lt;INT_MIN/<span class="number">10</span>)||(result==INT_MIN/<span class="number">10</span>&amp;&amp;temp&lt;<span class="number">-8</span>))<span class="comment">//考虑大于最小值-2^31</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    result=result*<span class="number">10</span>+temp;</span><br><span class="line">    x=x/<span class="number">10</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="题目：面试题-17-13-恢复空格"><a href="#题目：面试题-17-13-恢复空格" class="headerlink" title="题目：面试题 17.13. 恢复空格"></a>题目：面试题 17.13. 恢复空格</h2><p>描述：哦，不！你不小心把一个长篇文章中的空格、标点都删掉了，并且大写也弄成了小写。像句子”I reset the computer. It still didn’t boot!”已经变成了”iresetthecomputeritstilldidntboot”。在处理标点符号和大小写之前，你得先把它断成词语。当然了，你有一本厚厚的词典dictionary，不过，有些词没在词典里。假设文章用sentence表示，设计一个算法，把文章断开，要求未识别的字符最少，返回未识别的字符数。注意：本题相对原题稍作改动，只需返回未识别的字符数</p><p>思路：动态规划：dp[i]表示第i个位置前未识别的字符数（这里设置dp[0]=0，从句子的第一个字符开始标注dp数组）</p><p>当在字典中找不到对应单词，则dp[i]=dp[i-1]+1;</p><p>为了判断句子中是否出现字典中的单词，遍历字典中所有单词，在当前位置的前i-len（len是当前遍历的字典的单词长度，此时len需要小于等于i，即句子中的单词能容纳一个字典中单词的大小）中如果得到的字符正好在字典中，则dp[i]需要在自身dp[i]和dp[i-len]中取最小值。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">        dp[<span class="number">0</span>]=<span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;dp.<span class="built_in">size</span>();i++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">//找不到单词默认情况</span></span><br><span class="line">    dp[i]=dp[i<span class="number">-1</span>]+<span class="number">1</span>;</span><br><span class="line">    <span class="comment">//前len个长度</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;dictionary.<span class="built_in">size</span>();j++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> len=dictionary[j].<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span>(len&lt;=i)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(sentence.substr(i-len,len)==dictionary[j])</span><br><span class="line">            dp[i]=<span class="built_in">min</span>(dp[i],dp[i-len]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="题目：9-回文数"><a href="#题目：9-回文数" class="headerlink" title="题目：9. 回文数"></a>题目：9. 回文数</h2><p>描述：判断一个整数是否是回文数。回文数是指正序（从左向右）和倒序（从右向左）读都是一样的整数。</p><p>思路：数学方法：负数翻转肯定不是回文，对正数处理，从末尾取值给新的数作为首位，得到新的数和原来的数对比即可。</p><p>这里需要判断是否溢出，使用新数在加数据之前，判断是否比最大值/10还大。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">        <span class="keyword">if</span>(x&lt;<span class="number">0</span>)<span class="comment">//负数</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line"><span class="keyword">int</span> y=<span class="number">0</span>;<span class="comment">//正数</span></span><br><span class="line"><span class="keyword">int</span> old=x;</span><br><span class="line"><span class="keyword">while</span>(old!=<span class="number">0</span>)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">int</span> tmp=old%<span class="number">10</span>;</span><br><span class="line">    <span class="comment">//判断是否溢出</span></span><br><span class="line">    <span class="keyword">if</span>(y&gt;INT_MAX/<span class="number">10</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    y=y*<span class="number">10</span>+tmp;</span><br><span class="line">    old=old/<span class="number">10</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> y==x;</span><br></pre></td></tr></table></figure><h2 id="题目：13-罗马数字转整数"><a href="#题目：13-罗马数字转整数" class="headerlink" title="题目：13. 罗马数字转整数"></a>题目：13. 罗马数字转整数</h2><p>描述：罗马数字包含以下七种字符: I， V， X， L，C，D 和 M。</p><p>思路：哈希表+罗马数字串的规律：</p><p>使用哈希表存储每个罗马字符代表的数字，形成字典。</p><p>当当前字符比后面字符代表的数小，就减去该字符代表的值，反之，则加上该值，最后得到的结果就是罗马字符对应的数字。如IV，I代表1，V代表5，遇到I时，当前res=-1，遇到V正好到末尾，末尾‘\0’在字典中为0，则加上V代表的5，最终得到数字为4。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">        <span class="built_in">unordered_map</span>&lt;<span class="keyword">char</span>, <span class="keyword">int</span>&gt; <span class="built_in">map</span>=&#123;&#123;<span class="string">'I'</span>,<span class="number">1</span>&#125;,&#123;<span class="string">'V'</span>,<span class="number">5</span>&#125;,&#123;<span class="string">'X'</span>,<span class="number">10</span>&#125;,&#123;<span class="string">'L'</span>,<span class="number">50</span>&#125;,&#123;<span class="string">'C'</span>,<span class="number">100</span>&#125;,&#123;<span class="string">'D'</span>,<span class="number">500</span>&#125;,&#123;<span class="string">'M'</span>,<span class="number">1000</span>&#125;&#125;;</span><br><span class="line"><span class="keyword">int</span> add=<span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> res=<span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;s.length();i++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">map</span>[s[i]]&lt;<span class="built_in">map</span>[s[i+<span class="number">1</span>]])</span><br><span class="line">        res-=<span class="built_in">map</span>[s[i]];</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        res+=<span class="built_in">map</span>[s[i]];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="题目：120-三角形最小路径和"><a href="#题目：120-三角形最小路径和" class="headerlink" title="题目：120. 三角形最小路径和"></a>题目：120. 三角形最小路径和</h2><p>描述：给定一个三角形，找出自顶向下的最小路径和。每一步只能移动到下一行中相邻的结点上。相邻的结点 在这里指的是 下标 与 上一层结点下标 相同或者等于 上一层结点下标 + 1 的两个结点。</p><p>思路：自底部向上使用动态规划，dp[i] [j]表示从第i行j列到最底部的路径，每次都向上选自己上方或者右边较小的数。所以每个位置最小的路径都是自身加上下一行同列、右边列的较小数作为当前路径。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">        <span class="comment">//增加第n行的虚拟行</span></span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; dp(len+<span class="number">1</span>,<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;(len+<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=len<span class="number">-1</span>;i&gt;=<span class="number">0</span>;i--)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;=i;j++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//3.将本行当前元素和下一行中较小的相加</span></span><br><span class="line">        dp[i][j]=<span class="built_in">min</span>(dp[i+<span class="number">1</span>][j],dp[i+<span class="number">1</span>][j+<span class="number">1</span>])+triangle[i][j];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//每次都选最小的x往上加，最后得到的[0][0]节点就是最小路径</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="题目：14-最长公共前缀"><a href="#题目：14-最长公共前缀" class="headerlink" title="题目：14. 最长公共前缀"></a>题目：14. 最长公共前缀</h2><p>描述：编写一个函数来查找字符串数组中的最长公共前缀。如果不存在公共前缀，返回空字符串 “”。</p><p>思路：将第一个字符串作为比较字符串，遍历剩余的字符串。</p><p>对每个字符串，当字符一样，则继续遍历；</p><p>当字符不一样，删除不一样的字符。遍历所有的剩余字符串得到的就是最长的公共前缀，如果没有前缀，则最后字符串会被全部删完，得到空串。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">        <span class="built_in">string</span> result=strs[<span class="number">0</span>];</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;strs.<span class="built_in">size</span>();i++)<span class="comment">//strs中的字符串z从第一个开始比较，第0个已经做了参照</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;result.length();j++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(result[j]==strs[i][j])</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            result.erase(j);<span class="comment">//不相同的字符删除</span></span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="题目：96-不同的二叉搜索树"><a href="#题目：96-不同的二叉搜索树" class="headerlink" title="题目：96. 不同的二叉搜索树"></a>题目：96. 不同的二叉搜索树</h2><p>描述：给定一个整数 n，求以 1 … n 为节点组成的二叉搜索树有多少种？</p><p>思路：使用动态规划，dp[i]表示长度为i的树的个数，长度为i的树的个数可以分解为左子树个数 * 右子树个数（因为是同时满足才能得到长度为i的个数，所以是相乘）。左子树个数等于长度为i-1的树的个数，右子树的个数等于长度为i-当前值的树的个数。</p><p>又因为是二叉平衡树，考虑不同根节点当前长度的树的个数就是dp[i]的值</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">        dp[<span class="number">0</span>]=<span class="number">1</span>;</span><br><span class="line"><span class="comment">//不同长度1~n</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> len=<span class="number">1</span>;len&lt;=n;len++)</span><br><span class="line">&#123;</span><br><span class="line">     <span class="comment">//用不同的数字作为根节点</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> root=<span class="number">1</span>;root&lt;=len;root++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> left=root<span class="number">-1</span>;<span class="comment">//左子树长度</span></span><br><span class="line">        <span class="keyword">int</span> right=len-root;<span class="comment">//右子树长度</span></span><br><span class="line">        <span class="comment">//左右子树两两结合</span></span><br><span class="line">        dp[len]+=dp[left]*dp[right];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="题目：20-有效的括号"><a href="#题目：20-有效的括号" class="headerlink" title="题目：20. 有效的括号"></a>题目：20. 有效的括号</h2><p>描述：给定一个只包括 ‘(‘，’)’，’{‘，’}’，’[‘，’]’ 的字符串，判断字符串是否有效。有效字符串需满足：左括号必须用相同类型的右括号闭合。左括号必须以正确的顺序闭合。注意空字符串可被认为是有效字符串</p><p>思路：使用哈希表存储括号对应关系，右括号在前，左括号在后。使用栈存储读到的字符。</p><p>当字符串长度为奇数肯定不是有效括号，可提前进行判断。</p><p>（1）依次读取字符串中字符，当遇到左括号直接入栈。</p><p>（2）遇到右括号时，当栈内为空，则不是有效括号；</p><p>当栈顶的括号等于哈希表中当前读入字符对应的左括号，则栈顶括号得到匹配，出栈，继续读下一个字符；</p><p>当不等于哈希表中对应扩招，则不是有效括号。</p><p>（3）当读完所有字符，栈内没有内容，说明都匹配上了，反之有剩余，说明不是有效括号。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">        <span class="built_in">unordered_map</span>&lt;<span class="keyword">char</span>, <span class="keyword">char</span>&gt; <span class="built_in">map</span>&#123;&#123;<span class="string">'&#125;'</span>,<span class="string">'&#123;'</span>&#125;,&#123;<span class="string">']'</span>,<span class="string">'['</span>&#125;,&#123;<span class="string">')'</span>,<span class="string">'('</span>&#125;&#125;;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;s.length();i++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span>(s[i]==<span class="string">'&#123;'</span>||s[i]==<span class="string">'['</span>||s[i]==<span class="string">'('</span>)<span class="comment">//左括号，入栈</span></span><br><span class="line">        <span class="built_in">stack</span>.push(s[i]);</span><br><span class="line">    <span class="keyword">else</span><span class="comment">//遇到非左括号</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(<span class="built_in">stack</span>.empty())</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">char</span> temp=<span class="built_in">map</span>[s[i]];</span><br><span class="line">            <span class="keyword">if</span>(<span class="built_in">map</span>[s[i]]==<span class="built_in">stack</span>.top())<span class="comment">//top返回栈顶引用  想要提取栈顶元素，直接用s.top()！！！</span></span><br><span class="line">            &#123;</span><br><span class="line">                <span class="built_in">stack</span>.pop();<span class="comment">//出栈</span></span><br><span class="line">                <span class="keyword">continue</span>;<span class="comment">//继续检测下一个</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                <span class="keyword">return</span>  <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="题目：392-判断子序列"><a href="#题目：392-判断子序列" class="headerlink" title="题目：392. 判断子序列"></a>题目：392. 判断子序列</h2><p>描述：给定字符串 s 和 t ，判断 s 是否为 t 的子序列。你可以认为 s 和 t 中仅包含英文小写字母。字符串 t 可能会很长（长度 ~= 500,000），而 s 是个短字符串（长度 &lt;=100）。字符串的一个子序列是原始字符串删除一些（也可以不删除）字符而不改变剩余字符相对位置形成的新字符串。（例如，”ace”是”abcde”的一个子序列，而”aec”不是）。</p><p>后续挑战 :如果有大量输入的 S，称作S1, S2, … , Sk 其中 k &gt;= 10亿，你需要依次检查它们是否为 T 的子序列。在这种情况下，你会怎样改变代码？</p><p>思路：</p><p>（1）当输入的比对子序列s数量少时：使用双指针</p><p>一个指针指向s，一个指针指向t，当t中遇到和s相同的字符时，移动s的指针，继续比较，一直到t的结尾，当相同的字符数量和s的长度一致，说明是子序列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">        <span class="keyword">while</span>(i&lt;s.size()&amp;&amp;j&lt;t.size())</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span>(s[i]==t[j])</span><br><span class="line">    &#123;</span><br><span class="line">        i++;</span><br><span class="line">        j++;</span><br><span class="line">        count++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        j++;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>（2）当输入的比对子序列s数量多时：使用动态规划（自动机思想）</p><p>对t记录每个位置后面的字符中，在26个字母出现的对应在字符串中位置，则每次只需要对si字符串遍历一次，进行跳跃遍历，当遍历到当前字符时，找下一个字符的位置，找不到则不是子序列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">      //让字符从第一个开始</span><br><span class="line">    t=<span class="string">" "</span>+t;</span><br><span class="line">int len=t.size();</span><br><span class="line">vector&lt;vector&lt;int&gt;&gt; dp(len,vector&lt;int&gt;(<span class="number">26</span>));</span><br><span class="line">int pos;</span><br><span class="line"><span class="keyword">for</span>(int c=<span class="number">0</span>;c&lt;<span class="number">26</span>;c++)</span><br><span class="line">&#123;</span><br><span class="line">    pos=<span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">for</span>(int i=t.size()<span class="number">-1</span>;i&gt;=<span class="number">0</span>;i--)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[i][c]=pos;</span><br><span class="line">        //下一个字符出现的位置 dp[<span class="number">1</span>][<span class="string">'b'</span> - <span class="string">'a'</span>]记录的是index = <span class="number">1</span>后面下一次出现 b 的 index，即 <span class="number">9</span>。</span><br><span class="line">        <span class="keyword">if</span>(t[i]==c+<span class="string">'a'</span>)</span><br><span class="line">            pos=i;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//一次遍历</span><br><span class="line">int i=<span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span>(char c:s)</span><br><span class="line">&#123;</span><br><span class="line">    i=dp[i][c-<span class="string">'a'</span>];</span><br><span class="line">    <span class="keyword">if</span>(i==<span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">return</span> false;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> true;</span><br></pre></td></tr></table></figure><h2 id="题目：1025-除数博弈"><a href="#题目：1025-除数博弈" class="headerlink" title="题目：1025. 除数博弈"></a>题目：1025. 除数博弈</h2><p>描述：爱丽丝和鲍勃一起玩游戏，他们轮流行动。爱丽丝先手开局。最初，黑板上有一个数字 N 。在每个玩家的回合，玩家需要执行以下操作：选出任一 x，满足 0 &lt; x &lt; N 且 N % x == 0 。用 N - x 替换黑板上的数字 N 。如果玩家无法执行这些操作，就会输掉游戏。只有在爱丽丝在游戏中取得胜利时才返回 True，否则返回 False。假设两个玩家都以最佳状态参与游戏。</p><p>思路：博弈类题目，按照题目规律写出几个符合要求的数字就能看出其中规律</p><p>当数字是偶数的时候，爱丽丝必赢。</p><p>原因：</p><p>（1）当N是奇数时，N进行一次N-x是偶数，那么后手一定拿的是偶数，此时后手只需要拿一个奇数，保证后面给的都是奇数，则先手必输</p><p>（2）当N是偶数时，先手只需要拿一个奇数1，保证后面给的都是奇数，则先手必赢</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">        //列出几种情况，归纳总结</span><br><span class="line"><span class="keyword">if</span>(N%<span class="number">2</span>==<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> true;</span><br><span class="line"><span class="keyword">return</span> false;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Leecode做题记录（一）&quot;&gt;&lt;a href=&quot;#Leecode做题记录（一）&quot; class=&quot;headerlink&quot; title=&quot;Leecode做题记录（一）&quot;&gt;&lt;/a&gt;Leecode做题记录（一）&lt;/h1&gt;&lt;h2 id=&quot;题目：378-有序矩阵中第K小的元素&quot;&gt;&lt;a href=&quot;#题目：378-有序矩阵中第K小的元素&quot; class=&quot;headerlink&quot; title=&quot;题目：378. 有序矩阵中第K小的元素&quot;&gt;&lt;/a&gt;题目：378. 有序矩阵中第K小的元素&lt;/h2&gt;&lt;p&gt;描述：给定一个 n * n 矩阵，其中每行和每列元素均按升序排序，找到矩阵中第k小的元素。请注意，它是排序后的第k小元素，而不是第 &lt;code&gt;k&lt;/code&gt; 个不同的元素。&lt;/p&gt;&lt;p&gt;思路：&lt;/p&gt;&lt;p&gt;（1）优先队列&lt;/p&gt;&lt;p&gt;构造一个大小为k的小顶堆，不断插入数组元素，自动进行排序&lt;/p&gt;&lt;p&gt;代码&lt;/p&gt;&lt;figure class=&quot;highlight c++&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;priority_queue&amp;lt;&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt;&amp;gt; p;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt;(&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; i=&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;;i&amp;lt;matrix.&lt;span class=&quot;built_in&quot;&gt;size&lt;/span&gt;();i++)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt;(&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; j=&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;;j&amp;lt;matrix[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;].&lt;span class=&quot;built_in&quot;&gt;size&lt;/span&gt;();j++)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        p.push(matrix[i][j]);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt;(p.&lt;span class=&quot;built_in&quot;&gt;size&lt;/span&gt;()&amp;gt;k)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            p.pop();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="算法" scheme="https://www.xiapf.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="https://www.xiapf.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>使用logistic回归实现多分类</title>
    <link href="https://www.xiapf.com/blogs/tfLog2/"/>
    <id>https://www.xiapf.com/blogs/tfLog2/</id>
    <published>2020-07-23T03:31:37.000Z</published>
    <updated>2020-08-06T14:59:50.653Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>使用logistic回归识别0~9的手写数字，并测试模型精度。数据集来源于斯坦福大学机器学习课程手势识别数据集： <a href="https://www.coursera.org/course/ml" target="_blank" rel="external nofollow noopener noreferrer">https://www.coursera.org/course/ml</a> 。</p><h2 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h2><p>（1）算法使用场景：logistic回归用于解决分类问题，即用于预测离散数值的输出</p><p>（2）假设函数</p><p>和线性回归类似，logistic回归是为了将几类数据划分开，沿用线性回归的思想，线性回归是找出一条直线能够拟合出数据集中的数据，logistic回归可以看做通过给出的所有数据点拟合出分类边界的直线，这里直线的函数就称为假设函数，假设函数定义为：h(x)=θ0+θ1 *x1+θ2 * x2+…+θn * x2，其中θ0是偏置参数，θ的个数取决于数据集的特征数量n，算法的目标就是要求出θ0 ~ θn这些参数。将这些参数竖向堆叠，可以将参数表示为一个新向量θ，假设函数就可以表示为：</p><a id="more"></a><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200723103016.png" alt></p><p>logistic回归是解决分类问题，最终结果可以理解为是为了得出数据属于其中一个类的概率，所以假设函数的值需要在0 ~1之间，所以最终的假设函数需要加上g(z)函数即sigmoid函数，能将输出控制在0 ~ 1之间：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200723103355.png" alt></p><p>（3）损失函数</p><p>logistic回归可以看做是参数寻优，即找到的参数需要让预测的输出和时间输出之间误差最小。为了避免陷入局部最优，代价函数需要是凸函数，由于假设函数是非线性函数，所以不能使用平方差函数，这里采用交叉熵函数：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200720151306.png" alt></p><p>（4）梯度下降</p><p>在优化参数过程中，采用梯度下降算法，为了找到的参数使得损失函数值最小（即误差最小），则每次参数都向着损失梯度下降的方向变化：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200723113520.png" alt></p><p>（5）正则化</p><p>当数据集较小时，训练的模型容易出现过拟合，即数据能很好的拟合训练集，但是在不能很好拟合测试集，出现高方差，无法将训练的模型泛化到新样本，此时就要加入正则化项。</p><p>在损失函数和梯度下降中增加对参数的惩罚系数，修正模型中的每个参数，其中正则化参数λ用来一边控制使得训练的模型拟合训练数据，一边使得模型中每个参数不要占比太大，减少过拟合的问题。</p><p>由于θ0是偏置常数项，不用进行梯度下降，加入正则化项后的损失函数</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200720164015.png" alt></p><p>由于θ0是偏置常数项，不用进行梯度下降，加入正则化项后的梯度</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200720164029.png" alt></p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><h3 id="数据集处理"><a href="#数据集处理" class="headerlink" title="数据集处理"></a>数据集处理</h3><p>（1）导入原始数据集</p><p>数据集中包含5000个手写数字，每个数字共有500个手写图片数据，每个图片以像素为20 * 20的灰度图像存储；标签值存储对应的图片代表的数字，其中数字0以标签10进行存储。</p><p>随机展示一个手写图片：使用matshow显示像素值所代表的图片，并查看对应的标签值</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200723101314.png" alt></p><p>the img should be:8</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_image</span><span class="params">(x)</span>:</span></span><br><span class="line">size=int(np.sqrt(len(x)))</span><br><span class="line">fig,ax=plt.subplots(figsize=(<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">ax.matshow(x.reshape((size,size)),cmap=matplotlib.cm.binary)</span><br><span class="line">plt.xticks(np.array([]))<span class="comment">#x轴，y轴不显示数字</span></span><br><span class="line">plt.yticks(np.array([]))</span><br><span class="line">m=raw_x.shape[<span class="number">0</span>]</span><br><span class="line">pick=np.random.randint(<span class="number">0</span>,m)</span><br><span class="line">plot_image(raw_x[pick,:])</span><br><span class="line">plt.show()</span><br><span class="line">print(<span class="string">'the img should be:&#123;&#125;'</span>.format(raw_y[pick]))</span><br></pre></td></tr></table></figure><p>使用scipy库函数读取原始数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(path,transpose=True)</span>:</span></span><br><span class="line">data=sio.loadmat(path)</span><br><span class="line">X=data.get(<span class="string">'X'</span>)</span><br><span class="line">y=data.get(<span class="string">'y'</span>)</span><br><span class="line">y=y.reshape(y.shape[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">return</span> X,y</span><br></pre></td></tr></table></figure><p>（2）预处理原始数据</p><p>在数据特征x的第一列插入元素1，用于模型参数中的偏置项，同时将标签y转换为向量化标签，即当一个样本x代表的图片为1，即标签y为1时，将y向量化转换为[0,1,0,…]，此时的标签y为10 * 1的列向量，则5000个样本的标签最终向量化转换为（10，5000）的标签值y，向量化后能根据结果快速判断预测值属于哪一个类别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(raw_x,raw_y)</span>:</span></span><br><span class="line"><span class="comment">#在x的第一列插入元素1</span></span><br><span class="line">x=np.insert(raw_x,<span class="number">0</span>,values=np.ones(raw_x.shape[<span class="number">0</span>]),axis=<span class="number">1</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment">#将y转换为向量化标签  raw_y代表0~9数字，标签为10,1,2...9 转换为10个标签</span></span><br><span class="line">y_matrix=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">11</span>):</span><br><span class="line">y_matrix.append((raw_y==i).astype(int))</span><br><span class="line"></span><br><span class="line"><span class="comment">#10代表的0放置最前面</span></span><br><span class="line">y_matrix=[y_matrix[<span class="number">-1</span>]]+y_matrix[:<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">y=np.array(y_matrix)</span><br><span class="line"><span class="keyword">return</span> x,y</span><br></pre></td></tr></table></figure><p>（3）划分训练集和测试集</p><p>将处理后的数据集打乱，按照8：2的比例划分训练集和测试集，其中训练集用于模型训练，测试集用于测试模型精度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shuffle_data</span><span class="params">(raw_x,raw_y)</span>:</span></span><br><span class="line">indices=list(range(len(raw_x)))</span><br><span class="line">np.random.shuffle(indices)</span><br><span class="line">m,n=raw_x.shape</span><br><span class="line">train_x=np.zeros((<span class="number">4000</span>,n))</span><br><span class="line">train_y=np.zeros((<span class="number">4000</span>))</span><br><span class="line">test_x=np.zeros((<span class="number">1000</span>,n))</span><br><span class="line">test_y=np.zeros((<span class="number">1000</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4000</span>):</span><br><span class="line">train_x[i]=raw_x[indices[i]]</span><br><span class="line">train_y[i]=raw_y[indices[i]]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">test_x[i]=raw_x[indices[i+<span class="number">4000</span>]]</span><br><span class="line">test_y[i]=raw_y[indices[i+<span class="number">4000</span>]]</span><br><span class="line"><span class="keyword">return</span> train_x,train_y,test_x,test_y</span><br></pre></td></tr></table></figure><h3 id="模型搭建"><a href="#模型搭建" class="headerlink" title="模型搭建"></a>模型搭建</h3><p>（1）带正则项的损失函数</p><p>根据交叉熵损失函数公式得出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#损失函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(theta,x,y)</span>:</span></span><br><span class="line">yhat=sigmoid(np.dot(x,theta))</span><br><span class="line">cost_term=-np.mean((y*np.log(yhat)+(<span class="number">1</span>-y)*np.log(<span class="number">1</span>-yhat)))</span><br><span class="line"><span class="keyword">return</span> cost_term</span><br><span class="line"></span><br><span class="line"><span class="comment">#加上正则项</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regular_cost</span><span class="params">(theta,x,y,l=<span class="number">1</span>)</span>:</span></span><br><span class="line">theta_no1=theta[<span class="number">1</span>:]</span><br><span class="line">regular=(l/(<span class="number">2</span>*len(x)))*np.sum(np.power(theta_no1,<span class="number">2</span>))</span><br><span class="line">regular_term=cost(theta,x,y)+regular</span><br><span class="line"><span class="keyword">return</span> regular_term</span><br></pre></td></tr></table></figure><p>（2）带正则项的梯度计算</p><p>计算一次梯度下降的变化为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#梯度下降算法</span></span><br><span class="line"><span class="comment">#一次梯度变化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(theta,x,y)</span>:</span></span><br><span class="line">yhat=sigmoid(np.dot(x,theta))</span><br><span class="line">grad=(<span class="number">1</span>/len(x))*np.dot(x.T,(yhat-y))</span><br><span class="line"><span class="keyword">return</span> grad</span><br><span class="line"><span class="comment">#加入正则项</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regular_gradient</span><span class="params">(theta,x,y,l=<span class="number">1</span>)</span>:</span></span><br><span class="line">theta_no1=theta[<span class="number">1</span>:]</span><br><span class="line">regular=(l/len(x))*theta_no1</span><br><span class="line">regular=np.concatenate((np.array([<span class="number">0</span>]),regular))</span><br><span class="line">regular_grad=gradient(theta,x,y)+regular</span><br><span class="line"><span class="keyword">return</span> regular_grad</span><br></pre></td></tr></table></figure><p>（3）logistic回归模型搭建</p><p>使用牛顿截断法，求出使得损失函数的最小的参数值，其中传递给损失函数的元祖为训练数据集，辅助梯度计算采用梯度下降算法中梯度计算法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logistic_regression</span><span class="params">(x,y,l=<span class="number">1</span>)</span>:</span></span><br><span class="line">theta=np.zeros(x.shape[<span class="number">1</span>])</span><br><span class="line">res=opt.minimize(fun=regular_cost,x0=theta,args=(x,y,l),jac=regular_gradient,method=<span class="string">'TNC'</span>)<span class="comment">#使用截断牛顿算法求最小值</span></span><br><span class="line"><span class="keyword">return</span> res.x</span><br></pre></td></tr></table></figure><p>以上搭建了logistic回归的二元分类问题，多分类问题是独立分为多个二分类问题，即第一个二分类问题是将标签为数字0的设置为1，其余标签设置为0，第二个二分类问题是将标签为数字1的设置为1，其余标签设置为0，以此类推，构建10个独立二分类器，最终得到的参数进行组合。当x输入10个分类器中时，选择假设函数最大的类别作为自己的预测值。</p><p>循环输入y中的10个标签值，搭建手写数字10分类模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">theta=[logistic_regression(train_x,train_y[k]) <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">10</span>)]</span><br><span class="line">theta=np.array(theta)</span><br></pre></td></tr></table></figure><p>得到的模型最优参数维度为（10，401），每一行参数代表预测为当前数字的二分类器模型，因为有401个特征，列数为401，组合起来就是10分类模型</p><blockquote><p>[[-5.40410646e+00  0.00000000e+00  0.00000000e+00 … -1.16566197e-04<br>   7.87772670e-06  0.00000000e+00]<br> [-2.38354102e+00  0.00000000e+00  0.00000000e+00 …  1.30441649e-03<br>  -7.51759934e-10  0.00000000e+00]<br> [-3.18402772e+00  0.00000000e+00  0.00000000e+00 …  4.45897167e-03<br>  -5.08363274e-04  0.00000000e+00]<br> …<br> [-1.90573390e+00  0.00000000e+00  0.00000000e+00 … -5.27541665e-04<br>   6.62163734e-05  0.00000000e+00]<br> [-7.98711225e+00  0.00000000e+00  0.00000000e+00 … -8.94541915e-05<br>   7.21498244e-06  0.00000000e+00]<br> [-4.57362378e+00  0.00000000e+00  0.00000000e+00 … -1.33527099e-03<br>   9.98511880e-05  0.00000000e+00]]</p></blockquote><h3 id="模型预测"><a href="#模型预测" class="headerlink" title="模型预测"></a>模型预测</h3><p>（1）将测试集输入训练好的模型，得到模型预测的精度</p><p>将得到的模型求出其假设函数值，并求出最大值作为预测值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">inner=sigmoid(np.dot(test_x,theta.T))</span><br><span class="line">yhat=np.argmax(inner,axis=<span class="number">1</span>)</span><br><span class="line">y_raw=test_raw_y.copy()</span><br><span class="line">y_raw[y_raw==<span class="number">10</span>]=<span class="number">0</span></span><br><span class="line">print(classification_report(y_raw,yhat))</span><br></pre></td></tr></table></figure><p>使用sklearn库的评价包，输入真实输出和预测输出得出预测准确率、召回率、F1分数、每个数字的测试集大小如下</p><pre><code>          precision    recall  f1-score   support     0.0       0.94      0.99      0.97       103     1.0       0.88      0.96      0.92        93     2.0       0.90      0.86      0.88       102     3.0       0.94      0.84      0.89       103     4.0       0.87      0.90      0.89       108     5.0       0.83      0.89      0.86        90     6.0       0.96      0.97      0.97       101     7.0       0.90      0.93      0.92       100     8.0       0.90      0.80      0.85       100     9.0       0.84      0.83      0.83       100accuracy                           0.90      1000</code></pre><p>测试集总体预测精度为90%</p><p>（2）正则化是否使用对比</p><p>当不使用正则化时，训练集预测准确率为98%，测试集预测准确率为87%；当使用正则化时，训练集预测准确率为95%，测试集预测准确率为90%</p><p>可以看出，当未使用正则化时，训练集达98%，说明模型“很好的”拟合了数据，但是测试集准确率只有87%，说明模型没有能够泛化至新的样本，出现了过拟合。当加入了正则化项后，测试集准确率提高了达到90%，说明正则化项的加入有助于减少模型的过拟合现象。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;问题描述&lt;/h2&gt;&lt;p&gt;使用logistic回归识别0~9的手写数字，并测试模型精度。数据集来源于斯坦福大学机器学习课程手势识别数据集： &lt;a href=&quot;https://www.coursera.org/course/ml&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;https://www.coursera.org/course/ml&lt;/a&gt; 。&lt;/p&gt;&lt;h2 id=&quot;算法原理&quot;&gt;&lt;a href=&quot;#算法原理&quot; class=&quot;headerlink&quot; title=&quot;算法原理&quot;&gt;&lt;/a&gt;算法原理&lt;/h2&gt;&lt;p&gt;（1）算法使用场景：logistic回归用于解决分类问题，即用于预测离散数值的输出&lt;/p&gt;&lt;p&gt;（2）假设函数&lt;/p&gt;&lt;p&gt;和线性回归类似，logistic回归是为了将几类数据划分开，沿用线性回归的思想，线性回归是找出一条直线能够拟合出数据集中的数据，logistic回归可以看做通过给出的所有数据点拟合出分类边界的直线，这里直线的函数就称为假设函数，假设函数定义为：h(x)=θ0+θ1 *x1+θ2 * x2+…+θn * x2，其中θ0是偏置参数，θ的个数取决于数据集的特征数量n，算法的目标就是要求出θ0 ~ θn这些参数。将这些参数竖向堆叠，可以将参数表示为一个新向量θ，假设函数就可以表示为：&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="分类" scheme="https://www.xiapf.com/tags/%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>使用tensorflow搭建诗句生成器（二）</title>
    <link href="https://www.xiapf.com/blogs/tfPoem2/"/>
    <id>https://www.xiapf.com/blogs/tfPoem2/</id>
    <published>2020-07-21T01:50:13.000Z</published>
    <updated>2020-10-27T05:59:47.631Z</updated>
    
    <content type="html"><![CDATA[<h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p>从字符角度出发，生成一个模型能够自动生成文本，即能够通过已有的字符，预测下一个字符出现的概率。</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><h3 id="字符级模型"><a href="#字符级模型" class="headerlink" title="字符级模型"></a>字符级模型</h3><p>采用字符级模型，参考Andrej Karpathy写的The Unreasonable Effectiveness of Recurrent Neural Networks，这里采用lstm作为基本单元</p><a id="more"></a><p>（1）基本lstm单元</p><p>定义基本的lstm单元，使用MultiRNNCell连接多层lstm，并加上dropout</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cell_fn = tf.nn.rnn_cell.BasicLSTMCell</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> is_training <span class="keyword">and</span> self.dropout &gt; <span class="number">0</span>:</span><br><span class="line">        cells = [tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=<span class="number">1.0</span>-self.dropout) <span class="keyword">for</span> cell <span class="keyword">in</span> cells]<span class="comment">#输出部分作为下一层的输入</span></span><br><span class="line"></span><br><span class="line">    multi_cell = tf.nn.rnn_cell.MultiRNNCell(cells)</span><br></pre></td></tr></table></figure><p>（2）定义网络结构</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200720194245.png" alt></p><p>每个句子拆分成字符输入网络，每次输入的诗的大小通过num_unrollings进行设置，seq_length用于设置每次输入的单个句子的长度，经过embedding层得到字符的词嵌入向量表示，输入lstm单元，经过全连接层，最终得到预测的输出，其中经过lstm输出lstm组合单元的输出激活值值，传入到下一层。</p><p>利用static_rnn使用指定的RNN生成循环神经网络：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用指定的RNN神经元创建循环神经网络</span></span><br><span class="line">    outputs, final_state = tf.nn.static_rnn(</span><br><span class="line">            cell = multi_cell,</span><br><span class="line">            inputs = sliced_inputs,</span><br><span class="line">            initial_state=self.initial_state)</span><br><span class="line">    self.final_state = final_state</span><br></pre></td></tr></table></figure><p>网络中其他设置：</p><p>1°损失函数：稀疏softmax交叉熵函数sparse_softmax_cross_entropy_with_logits；</p><p>2°训练时的设置：防止梯度爆照，进行梯度修剪，当梯度大于最大梯度时，直接使用最大梯度作为当前梯度值；</p><blockquote><p>tvars = tf.trainable_variables()<br>grads, _ = tf.clip_by_global_norm(tf.gradients(self.mean_loss, tvars), self.max_grad_norm)</p></blockquote><p>3°梯度下降算法采用adam算法</p><h3 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h3><p>（1）训练集数据转换为词嵌入</p><p>使用Word2Vec算法训练好的词向量模型，以&lt; / s &gt;开头，按词频排列，去除低频词。</p><p>词向量模型存储在vectors_poem.bin中，使用skim-gram模型，选择一个词作为输入，在选定词的前后距离内选择一个词作为目标词来作为训练集，在当前输入的上下文中得到下一个字的概率值为</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200623162639.png" alt></p><p>（2）训练数据集构建</p><p>当前诗句为“锄禾日当午”，则输入文本为“锄禾日当午”，输出文本为“禾日当午锄”，使用前一个字推出上下文中后一个字来构建训练集。</p><p>1°根据训练的词向量模型，将所有的诗句转换为词向量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poem_ids = DataLoader.get_text_idx(poems, self.w2v.vocab_hash, self.seq_max_length)</span><br></pre></td></tr></table></figure><p>以seq_max_length作为一句诗的最大长度，当超过最大长度，文本加上结束符]，按照转换之后的文本转换为词向量，不在词向量中的字符使用&lt; unknown &gt;标识。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_text_idx</span><span class="params">(text, vocab, max_document_length)</span>:</span></span><br><span class="line">        max_document_length_without_end = max_document_length - <span class="number">1</span></span><br><span class="line">        text_array = []</span><br><span class="line">        <span class="keyword">for</span> i, x <span class="keyword">in</span> enumerate(text):</span><br><span class="line">            line = []</span><br><span class="line">            <span class="keyword">if</span> len(x) &gt; max_document_length:</span><br><span class="line">                x_parts = x[:max_document_length_without_end]</span><br><span class="line">                idx = x_parts.rfind(<span class="string">'。'</span>)</span><br><span class="line">                <span class="keyword">if</span> idx &gt; <span class="number">-1</span>:</span><br><span class="line">                    x_parts = x_parts[<span class="number">0</span>:idx + <span class="number">1</span>] + <span class="string">']'</span></span><br><span class="line">                x = x_parts</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> j, w <span class="keyword">in</span> enumerate(x):</span><br><span class="line">                <span class="keyword">if</span> (w <span class="keyword">not</span> <span class="keyword">in</span> vocab):</span><br><span class="line">                    w = <span class="string">'&lt;unknown&gt;'</span></span><br><span class="line">                line.append(vocab[w])</span><br><span class="line">            text_array.append(line)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> text_array</span><br></pre></td></tr></table></figure><p>2°按照训练集、开发集、测试集进行划分，并将数据保存在poem_id.txt中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">n_samples = len(poem_ids)</span><br><span class="line">    train_size = int(self.train_frac * n_samples)</span><br><span class="line">    valid_size = int(self.valid_frac * n_samples)</span><br><span class="line">    test_size = n_samples - train_size - valid_size</span><br><span class="line"></span><br><span class="line">    self.testingSamples = poem_ids[-test_size:]</span><br><span class="line">    self.validationSamples = poem_ids[-valid_size - test_size: -test_size]</span><br><span class="line">    self.trainingSamples = poem_ids[:train_size]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存处理过的训练数据集</span></span><br><span class="line">    poem_ids_file = os.path.join(self.data_dir, <span class="string">'poem_ids.txt'</span>)</span><br><span class="line">    self.save_dataset(poem_ids_file)</span><br></pre></td></tr></table></figure><p>3°每次训练按照batch_size读取数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">x_seqs = np.ndarray((new_sample_size, self.seq_max_length), dtype=np.int32)</span><br><span class="line">    y_seqs = np.ndarray((new_sample_size, self.seq_max_length), dtype=np.int32)</span><br><span class="line">    self.x_lengths = []</span><br><span class="line">    <span class="keyword">for</span> i, sample <span class="keyword">in</span> enumerate(samples):</span><br><span class="line">        x_lengths.append(len(sample))</span><br><span class="line">        x_seqs[i] = sample + [self.padToken] * (self.seq_max_length - len(sample))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(sample_size, new_sample_size):</span><br><span class="line">        copyi = i - sample_size</span><br><span class="line">        x_seqs[i] = x_seqs[copyi]</span><br><span class="line">        x_lengths.append(x_lengths[copyi])</span><br><span class="line"></span><br><span class="line">    y_seqs[:, :<span class="number">-1</span>] = x_seqs[:, <span class="number">1</span>:]  <span class="comment"># x的1-最后给，y的0-倒数第二个</span></span><br><span class="line">    y_seqs[:, <span class="number">-1</span>] = x_seqs[:, <span class="number">0</span>]  <span class="comment"># x的第0个给y的最后一个</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;描述&quot;&gt;&lt;a href=&quot;#描述&quot; class=&quot;headerlink&quot; title=&quot;描述&quot;&gt;&lt;/a&gt;描述&lt;/h2&gt;&lt;p&gt;从字符角度出发，生成一个模型能够自动生成文本，即能够通过已有的字符，预测下一个字符出现的概率。&lt;/p&gt;&lt;h2 id=&quot;实现&quot;&gt;&lt;a href=&quot;#实现&quot; class=&quot;headerlink&quot; title=&quot;实现&quot;&gt;&lt;/a&gt;实现&lt;/h2&gt;&lt;h3 id=&quot;字符级模型&quot;&gt;&lt;a href=&quot;#字符级模型&quot; class=&quot;headerlink&quot; title=&quot;字符级模型&quot;&gt;&lt;/a&gt;字符级模型&lt;/h3&gt;&lt;p&gt;采用字符级模型，参考Andrej Karpathy写的The Unreasonable Effectiveness of Recurrent Neural Networks，这里采用lstm作为基本单元&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="自然语言处理" scheme="https://www.xiapf.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>用tensorflow实现机器学习算法——logistic回归</title>
    <link href="https://www.xiapf.com/blogs/tfLog/"/>
    <id>https://www.xiapf.com/blogs/tfLog/</id>
    <published>2020-07-20T08:48:38.000Z</published>
    <updated>2020-08-06T14:59:52.398Z</updated>
    
    <content type="html"><![CDATA[<h2 id="与线性回归的区别"><a href="#与线性回归的区别" class="headerlink" title="与线性回归的区别"></a>与线性回归的区别</h2><p>1.解决问题的不同</p><p>线性回归是用于解决回归问题，即用于预测连续输出</p><p>logistic回归用于解决分类问题，即用于预测离散数值的输出</p><p>2.假设函数不同</p><p>线性回归的假设函数为h=θ^T * x</p><p>logistic回归的假设函数为h=1/(1+e^(-θ^T * x))：由于logistic回归预测的是分类问题，分类标签为0或者1，所以加上了sigmoid函数，将输出转换为0或者1，当得出的线性值θ^T * x&gt;=0，即假设函数h的值&gt;=0.5，此时输出标签为1，当得出的线性值θ^T * x&lt;0，即假设函数h的值&lt;0.5，此时输出标签为0.</p><a id="more"></a><p>3.损失函数不同</p><p>线性回归损失函数为：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200720151414.png" alt></p><p>logitstic回归损失函数为：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200720151306.png" alt></p><p>不沿用线性回归的损失函数是因为logistic回归中的假设函数为非线性函数，代入误差的平方中不是凸函数，所以重新选择了新的凸函数作为logistic回归的损失函数。</p><h2 id="二分类问题"><a href="#二分类问题" class="headerlink" title="二分类问题"></a>二分类问题</h2><h3 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h3><p>（1）利用pandas中的read_csv读入（以逗号分隔才能这么读入），在第一列插入1作为偏置的特征</p><p>当使用梯度下降就需要做归一化：加快收敛速度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">data=pd.read_csv(<span class="string">'ex2data1.txt'</span>,names=[<span class="string">'test1'</span>,<span class="string">'test2'</span>,<span class="string">'score'</span>])</span><br><span class="line"></span><br><span class="line">col=data.shape[<span class="number">1</span>]</span><br><span class="line">x=data.iloc[:,:col<span class="number">-1</span>]</span><br><span class="line">x=(x-x.mean())/x.std()</span><br><span class="line">x.insert(<span class="number">0</span>,<span class="string">'ones'</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">y=data.iloc[:,col<span class="number">-1</span>:col]</span><br></pre></td></tr></table></figure><p>（2）定义sigmoid函数，将输出转换在0 ~ 1之间</p><p>def sigmoid(z):<br>    return 1/(1+np.exp(-z))</p><p>（3）每次迭代计算误差值，并利用梯度下降公式对每个参数进行修正；根据损失函数计算当前损失</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#2.计算成本函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">computeCost</span><span class="params">(theta,x,y)</span>:</span></span><br><span class="line">theta=np.mat(theta)</span><br><span class="line">m=len(x)</span><br><span class="line">inner=sigmoid(x*theta.T)</span><br><span class="line">cost=(<span class="number">-1</span>/m)*np.sum(np.multiply(y,np.log(inner))+np.multiply((<span class="number">1</span>-y),np.log(<span class="number">1</span>-inner)))<span class="comment">#对应位置相乘</span></span><br><span class="line"><span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line"><span class="comment">#3.梯度下降</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradentDescent</span><span class="params">(x,theta,y,alpha,iters)</span>:</span></span><br><span class="line">x=np.mat(x)</span><br><span class="line">y=np.mat(y)</span><br><span class="line">theta=np.mat(theta)</span><br><span class="line">m,n=x.shape</span><br><span class="line">costs=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(iters):</span><br><span class="line">error=sigmoid(x*theta.T)-y</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">theta[:,j]=theta[:,j]-alpha*(<span class="number">1</span>/m)*np.sum(np.multiply(error,x[:,j]))</span><br><span class="line">cost=computeCost(theta,x,y)</span><br><span class="line">costs.append(cost)</span><br><span class="line"><span class="keyword">return</span> theta,costs</span><br></pre></td></tr></table></figure><p>（4）根据得到的参数进行预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#4.预测</span></span><br><span class="line"><span class="comment">#准确率</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(theta,x)</span>:</span></span><br><span class="line">yhat=sigmoid(x*theta.T)</span><br><span class="line">m,n=yhat.shape</span><br><span class="line">p=np.zeros((m,n))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line"><span class="keyword">if</span>(yhat[i]&gt;=<span class="number">0.5</span>):</span><br><span class="line">p[i]=<span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">p[i]=<span class="number">0</span></span><br><span class="line"><span class="keyword">return</span> p</span><br></pre></td></tr></table></figure><p>得到模型的参数为：[[1.26836232 3.05675087 2.82011823]]<br>准确率为：0.89</p><h3 id="使用其他优化算法——scipy-optimize-自带的fmin-tnc"><a href="#使用其他优化算法——scipy-optimize-自带的fmin-tnc" class="headerlink" title="使用其他优化算法——scipy.optimize 自带的fmin_tnc"></a>使用其他优化算法——scipy.optimize 自带的fmin_tnc</h3><p>根据给出的损失函数和导数能够直接求得最小值</p><blockquote><p>res=op.fmin_tnc(func=computeCost,x0=theta,args=(x,y),fprime=gradent)</p></blockquote><p>其中导数使用梯度下降部分的导数求解方式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradent</span><span class="params">(theta,x,y)</span>:</span></span><br><span class="line">x=np.mat(x)</span><br><span class="line">y=np.mat(y)</span><br><span class="line">theta=np.mat(theta)</span><br><span class="line">m,n=x.shape</span><br><span class="line">error=sigmoid(x*theta.T)-y</span><br><span class="line">grad=np.zeros((n))</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">grad[j]=(<span class="number">1</span>/m)*np.sum(np.multiply(error,x[:,j]))</span><br><span class="line"><span class="keyword">return</span> grad</span><br></pre></td></tr></table></figure><p>得到的第一个参数就是模型的参数(array([1.7179636 , 4.01149168, 3.7426532 ]), 13, 1)<br>准确率为：0.89</p><h3 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h3><p>从结果看，梯度下降算法和其他的高级优化算法结果相近，但是梯度下降算法需要给出迭代次数和学习率alpha。</p><h2 id="使用tensorflow"><a href="#使用tensorflow" class="headerlink" title="使用tensorflow"></a>使用tensorflow</h2><p>（1）定义占位符用于输入数据</p><p>（2）定义当前作用域下的参数，预测值的计算，损失的计算，即正向传播得到的值</p><p>（3）定义梯度下降算法</p><p>（4）运行会话，将输入数据、下降算法、损失、参数输入，最终得到损失和参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用TensorFlow实现逻辑回归</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logisticRegression</span><span class="params">(x_data,y_data,alpha,iters)</span>:</span></span><br><span class="line">x=tf.placeholder(tf.float32,x_data.shape)</span><br><span class="line">y=tf.placeholder(tf.float32,y_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'logisticRegression'</span>):</span><br><span class="line">w=tf.get_variable(<span class="string">'weight'</span>,(x.shape[<span class="number">1</span>],<span class="number">1</span>),initializer=tf.constant_initializer())</span><br><span class="line">inner=tf.matmul(x,w)</span><br><span class="line">yhat=tf.nn.softmax(inner)</span><br><span class="line"><span class="comment"># loss=-tf.reduce_mean(tf.reduce_sum(y*tf.log(yhat)+(1-y)*tf.log(1-yhat),axis=1))#reduction_indices表示按行相加</span></span><br><span class="line">loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=yhat))</span><br><span class="line">opt=tf.train.GradientDescentOptimizer(learning_rate=alpha)</span><br><span class="line">opt_gennerate=opt.minimize(loss)</span><br><span class="line">loss_all=[]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(iters):</span><br><span class="line">_,loss_val,w_val=sess.run([opt_gennerate,loss,w],feed_dict=&#123;x:x_data,y:y_data&#125;)</span><br><span class="line">loss_all.append(loss_val)</span><br><span class="line"><span class="comment"># correct_predict=tf.equal(tf.argmax(yhat,1),tf.argmax(y,1))</span></span><br><span class="line"><span class="comment"># accuarcy=tf.reduce_mean(tf.cast(correct_predict,tf.float32))</span></span><br><span class="line"><span class="comment"># print('accuarcy:',accuarcy.eval(&#123;x:,y:&#125;)</span></span><br><span class="line">parameters=&#123;<span class="string">'theta'</span>:w_val,<span class="string">'loss'</span>:loss_all&#125;</span><br></pre></td></tr></table></figure><p>使用tensorflow的准确率为0.6</p><p>当需要得到测试集的准确率，可以增加以下代码：</p><blockquote><p>correct_predict=tf.equal(tf.argmax(yhat,1),tf.argmax(y,1))<br>accuarcy=tf.reduce_mean(tf.cast(correct_predict,tf.float32))<br>print(‘accuarcy:’,accuarcy.eval({x:,y:})</p></blockquote><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><h3 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h3><p>当出现过拟合现象：即数据能很好的拟合训练集，但是在不能很好拟合测试集，出现高方差，无法将训练的模型泛化到新样本，此时就要进行正则化。</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>在损失函数和梯度下降中增加对参数的惩罚系数，修正每个模型参数，其中正则化参数λ用来一边控制使得训练的模型拟合参数，一边使得每个模型参数不要太大，减少过拟合的问题。</p><p>线性回归和logistic回归中的梯度下降正则化的式子类似，但是其中的假设函数不同。</p><p>（1）线性回归的正则化</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200720164624.png" alt></p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200720164029.png" alt></p><p>（2）正规方程的正则化</p><p>正规方程可以直接求得最小的参数θ，但是当特征很多的时候，求逆矩阵运算量大，当特征少的时候可以考虑使用。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200720163840.png" alt></p><p>（3）logistic的正则化</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200720164015.png" alt></p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200720164029.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#2.计算成本函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">computeCostReg</span><span class="params">(theta,x,y,learning_rate)</span>:</span></span><br><span class="line">theta=np.mat(theta)</span><br><span class="line">m=len(x)</span><br><span class="line">inner=sigmoid(x*theta.T)</span><br><span class="line">reg=(learning_rate/(<span class="number">2</span>*m))*np.sum(np.power(theta[:,<span class="number">1</span>:theta.shape[<span class="number">1</span>]],<span class="number">2</span>))</span><br><span class="line">cost=(<span class="number">-1</span>/m)*np.sum(np.multiply(y,np.log(inner))+np.multiply((<span class="number">1</span>-y),np.log(<span class="number">1</span>-inner)))+reg<span class="comment">#对应位置相乘</span></span><br><span class="line"><span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line"><span class="comment">#3.梯度下降</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradientDescentReg</span><span class="params">(x,theta,y,alpha,iters,learning_rate)</span>:</span></span><br><span class="line">x=np.mat(x)</span><br><span class="line">y=np.mat(y)</span><br><span class="line">theta=np.mat(theta)</span><br><span class="line">m,n=x.shape</span><br><span class="line">costs=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(iters):</span><br><span class="line">error=sigmoid(x*theta.T)-y</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line"><span class="keyword">if</span> j==<span class="number">0</span>:</span><br><span class="line">theta[:,j]=theta[:,j]-alpha*(<span class="number">1</span>/m)*np.sum(np.multiply(error,x[:,j]))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">theta[:,j]=theta[:,j]-alpha*(<span class="number">1</span>/m)*np.sum(np.multiply(error,x[:,j]))+(learning_rate/m)*theta[:,j]</span><br><span class="line">cost=computeCostReg(theta,x,y,learning_rate)</span><br><span class="line">costs.append(cost)</span><br><span class="line"><span class="keyword">return</span> theta,costs</span><br></pre></td></tr></table></figure><p>就是在无正则化的损失函数计算和梯度下降中增加了正则化项，其中梯度下降中，需要对参数分类处理（第0个参数和其他参数）。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;与线性回归的区别&quot;&gt;&lt;a href=&quot;#与线性回归的区别&quot; class=&quot;headerlink&quot; title=&quot;与线性回归的区别&quot;&gt;&lt;/a&gt;与线性回归的区别&lt;/h2&gt;&lt;p&gt;1.解决问题的不同&lt;/p&gt;&lt;p&gt;线性回归是用于解决回归问题，即用于预测连续输出&lt;/p&gt;&lt;p&gt;logistic回归用于解决分类问题，即用于预测离散数值的输出&lt;/p&gt;&lt;p&gt;2.假设函数不同&lt;/p&gt;&lt;p&gt;线性回归的假设函数为h=θ^T * x&lt;/p&gt;&lt;p&gt;logistic回归的假设函数为h=1/(1+e^(-θ^T * x))：由于logistic回归预测的是分类问题，分类标签为0或者1，所以加上了sigmoid函数，将输出转换为0或者1，当得出的线性值θ^T * x&amp;gt;=0，即假设函数h的值&amp;gt;=0.5，此时输出标签为1，当得出的线性值θ^T * x&amp;lt;0，即假设函数h的值&amp;lt;0.5，此时输出标签为0.&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="分类" scheme="https://www.xiapf.com/tags/%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>自定义注解</title>
    <link href="https://www.xiapf.com/blogs/annotation/"/>
    <id>https://www.xiapf.com/blogs/annotation/</id>
    <published>2020-07-16T02:00:05.000Z</published>
    <updated>2020-07-20T09:15:08.156Z</updated>
    
    <content type="html"><![CDATA[<p>自定义注解说明：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Target</span>(&#123;ElementType.METHOD, ElementType.TYPE&#125;)<span class="comment">//可以作用在类上和方法上  </span></span><br><span class="line"><span class="meta">@Retention</span>(RetentionPolicy.RUNTIME)<span class="comment">//可以通过反射读取注解  </span></span><br><span class="line"><span class="meta">@Inherited</span><span class="comment">//可以被子类继承  </span></span><br><span class="line"><span class="meta">@Documented</span><span class="comment">//javadoc生成文件档时，包含本注解信息  </span></span><br><span class="line"><span class="keyword">public</span> <span class="meta">@interface</span> MyAnnotation &#123;  </span><br><span class="line">    <span class="function">String <span class="title">value</span><span class="params">()</span> <span class="keyword">default</span> ""</span>;<span class="comment">//使用时没有指定key,值默认赋给value,如：MyAnnotation("abc")  </span></span><br><span class="line">&#125;  </span><br><span class="line"></span><br><span class="line"><span class="meta">@Retention</span>(RetentionPolicy.SOURCE)  </span><br><span class="line"><span class="comment">//作用是不将注解保存在class文件中，也就是说象“//”一样在编译时被过滤掉了。  </span></span><br><span class="line">  </span><br><span class="line"><span class="meta">@Retention</span>(RetentionPolicy.CLASS)  </span><br><span class="line"><span class="comment">//作用是只将注解保存在class文件中，而使用反射读取注解时忽略这些注解。  </span></span><br><span class="line">  </span><br><span class="line"><span class="meta">@Retention</span>(RetentionPolicy.RUNTIME)  </span><br><span class="line"><span class="comment">//作用是即将注解保存在class文件中，也可以通过反射读取注解。这也是最常用的值</span></span><br></pre></td></tr></table></figure><a id="more"></a><p>获取注解：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//取得方法上的指定的注解  </span></span><br><span class="line">Method m=?  </span><br><span class="line">Annotation annotation = m.getAnnotation(MyAnnotation<span class="class">.<span class="keyword">class</span>)</span>;  </span><br><span class="line">  </span><br><span class="line"><span class="comment">//取得方法上的所有注解，包括继承的注解。  </span></span><br><span class="line">Annotation[] annotations = m.getAnnotations();  </span><br><span class="line">  </span><br><span class="line"><span class="comment">//取当前方法上的所有的注解，不包括继承的  </span></span><br><span class="line">Annotation[] annotations = m.getDeclaredAnnotations();  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//取得类上的指定的注解  </span></span><br><span class="line">Annotation annotation = 类<span class="class">.<span class="keyword">class</span>.<span class="title">getAnnotation</span>(<span class="title">MyAnnotation</span>.<span class="title">class</span>)</span>;  </span><br><span class="line">  </span><br><span class="line"><span class="comment">//取得类上的所有注解，包括继承的注解。  </span></span><br><span class="line">Annotation[] annotations = 类<span class="class">.<span class="keyword">class</span>.<span class="title">getAnnotations</span>()</span>;  </span><br><span class="line">  </span><br><span class="line"><span class="comment">//取当前类上的所有的注解，不包括继承的  </span></span><br><span class="line">Annotation[] annotations = 类<span class="class">.<span class="keyword">class</span>.<span class="title">getDeclaredAnnotations</span>()</span>;</span><br></pre></td></tr></table></figure><p>spring中获取注解</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Service</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RegistryRules</span> <span class="keyword">implements</span> <span class="title">ApplicationContextAware</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> ApplicationContext context;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setApplicationContext</span><span class="params">(ApplicationContext applicationContext)</span> <span class="keyword">throws</span> BeansException </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.context = applicationContext;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@PostConstruct</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">registry</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Map&lt;String, Object&gt; beansWithRuleType = context.getBeansWithAnnotation(RuleType<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        beansWithRuleType.entrySet().stream().forEach(e -&gt; &#123;</span><br><span class="line">            MyAnnotation annotation = e.getValue().getClass().getAnnotation(MyAnnotation<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意：如果自定义注解上还有@Transactional注解，或获取不到，因为@Transactional注解会使用代理的方式，重新生成新的代理类来调用方法，新的代理类上并没有自定义注解，解决方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//@Transactional会使自定义注解失效</span></span><br><span class="line">            <span class="comment">//MyAnnotation annotation = e.getValue().getClass().getAnnotation(RuleType.class);</span></span><br><span class="line">            MyAnnotation annotation = AnnotationUtils.findAnnotation(e.getValue().getClass(), MyAnnotation<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure><p>通过AOP方式使用注解：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="meta">@Aspect</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogRecord</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Pointcut</span>(<span class="string">"@within(com.xxx.SelLog) || @annotation(com.xxx.SelLog)"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doLog</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Around</span>(<span class="string">"doLog()"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">doAround</span><span class="params">(ProceedingJoinPoint joinPoint)</span> <span class="keyword">throws</span> throwable </span>&#123;</span><br><span class="line">        log.debug(<span class="string">"class:&#123;&#125;,method:&#123;&#125; start,params:&#123;&#125;"</span>, joinPoint.getTarget().getClass(),</span><br><span class="line">            joinPoint.getSignature().getName(), JSON.toJSONString(joinPoint.getArgs()));</span><br><span class="line">        Object result = joinPoint.proceed();</span><br><span class="line">        log.debug(<span class="string">"class:&#123;&#125;,method:&#123;&#125; end,result:&#123;&#125; "</span>, joinPoint.getTarget().getClass(),</span><br><span class="line">            joinPoint.getSignature().getName(), result);</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@AfterThrowing</span>(value = <span class="string">"doLog()"</span>, throwing = <span class="string">"e"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">doAfterThrowing</span><span class="params">(JoinPoint joinPoint, Exception e)</span> </span>&#123;</span><br><span class="line">        log.debug(<span class="string">"class:&#123;&#125;,method:&#123;&#125;,exception:&#123;&#125;"</span>, joinPoint.getTarget().getClass(),</span><br><span class="line">            joinPoint.getSignature().getName(), e);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;自定义注解说明：&lt;/p&gt;&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;@Target&lt;/span&gt;(&amp;#123;ElementType.METHOD, ElementType.TYPE&amp;#125;)&lt;span class=&quot;comment&quot;&gt;//可以作用在类上和方法上  &lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;@Retention&lt;/span&gt;(RetentionPolicy.RUNTIME)&lt;span class=&quot;comment&quot;&gt;//可以通过反射读取注解  &lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;@Inherited&lt;/span&gt;&lt;span class=&quot;comment&quot;&gt;//可以被子类继承  &lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;@Documented&lt;/span&gt;&lt;span class=&quot;comment&quot;&gt;//javadoc生成文件档时，包含本注解信息  &lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;meta&quot;&gt;@interface&lt;/span&gt; MyAnnotation &amp;#123;  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;String &lt;span class=&quot;title&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;default&lt;/span&gt; &quot;&quot;&lt;/span&gt;;&lt;span class=&quot;comment&quot;&gt;//使用时没有指定key,值默认赋给value,如：MyAnnotation(&quot;abc&quot;)  &lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;@Retention&lt;/span&gt;(RetentionPolicy.SOURCE)  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;//作用是不将注解保存在class文件中，也就是说象“//”一样在编译时被过滤掉了。  &lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;@Retention&lt;/span&gt;(RetentionPolicy.CLASS)  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;//作用是只将注解保存在class文件中，而使用反射读取注解时忽略这些注解。  &lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;@Retention&lt;/span&gt;(RetentionPolicy.RUNTIME)  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;//作用是即将注解保存在class文件中，也可以通过反射读取注解。这也是最常用的值&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="java" scheme="https://www.xiapf.com/categories/java/"/>
    
    
      <category term="java" scheme="https://www.xiapf.com/tags/java/"/>
    
      <category term="annotation" scheme="https://www.xiapf.com/tags/annotation/"/>
    
  </entry>
  
  <entry>
    <title>使用tensorflow搭建诗句生成器（一）</title>
    <link href="https://www.xiapf.com/blogs/tfPoem1/"/>
    <id>https://www.xiapf.com/blogs/tfPoem1/</id>
    <published>2020-07-14T12:09:54.000Z</published>
    <updated>2020-10-27T06:00:59.712Z</updated>
    
    <content type="html"><![CDATA[<h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p>根据生成的最佳字符级模型，使用Flask框架结合html搭建web应用显示生成的诗句。根据用户输入诗句数量，随机n句诗句；或者根据用户输入的文字，根据最佳模型采样生成对应诗句。</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><h3 id="前台显示"><a href="#前台显示" class="headerlink" title="前台显示"></a>前台显示</h3><p>1、python端使用Flask框架搭建web应用</p><p>（1）导入Flask类，并创建一个app实例</p><p>（2）通过路由route()装饰器告诉Flask触发函数的url</p><a id="more"></a><p>（3）接收数据：利用html中的post方法接收参数</p><p>（4）发送数据：利用render_template将模板名称，需要参数传递的变量名称传递给模板中的变量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1</span></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"><span class="comment">#2</span></span><br><span class="line"><span class="meta">@app.route('/',methods=['POST','GET'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_poem</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">#3</span></span><br><span class="line">    startwith=request.form.get(<span class="string">'start_with'</span>)</span><br><span class="line">    text=[]</span><br><span class="line">    start_with=startwith</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> start_with:</span><br><span class="line">      <span class="comment">#调用方法生成需要回传的变量值</span></span><br><span class="line">      text=writer.cangtou(start_with)</span><br><span class="line">    <span class="comment">#4</span></span><br><span class="line">    <span class="keyword">return</span> render_template(<span class="string">'index.html'</span>,choosetext=text)</span><br></pre></td></tr></table></figure><p>2、html中采用表单提交方式</p><p>html文件需要放在template文件夹下，方便write_poem函数查找并渲染模板</p><p>（1）将各个输入框绑定名称，可用request.form.get获取输入的数据</p><p>（2）中用于显示传回的数据</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">form</span> <span class="attr">action</span>=<span class="string">""</span> <span class="attr">method</span>=<span class="string">'POST'</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">p</span>&gt;</span>想要哪种类型的诗词<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">p</span>&gt;</span>1:自由诗(需要输入生成几句诗句）<span class="tag">&lt;<span class="name">br</span>&gt;</span>2:藏头诗<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">        请输入对应序号：<span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">class</span>=<span class="string">"txt_input"</span> <span class="attr">name</span>=<span class="string">"poem_style"</span>  <span class="attr">value</span>=<span class="string">''</span> <span class="attr">style</span>=<span class="string">"margin-top:10px;"</span>/&gt;</span><span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line">        如果选择1，请给出需要生成几句诗：<span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">class</span>=<span class="string">"txt_input"</span> <span class="attr">name</span>=<span class="string">"num_sentence"</span>  <span class="attr">value</span>=<span class="string">''</span> <span class="attr">style</span>=<span class="string">"margin-top:10px;"</span>/&gt;</span><span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line">        如果选择3，请给出若干文字用于诗句生成：<span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">class</span>=<span class="string">"txt_input"</span> <span class="attr">name</span>=<span class="string">"start_with"</span>  <span class="attr">value</span>=<span class="string">''</span> <span class="attr">style</span>=<span class="string">"margin-top:10px;"</span>/&gt;</span><span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"submit"</span> <span class="attr">value</span>=<span class="string">"完成"</span> <span class="attr">class</span>=<span class="string">"button-new"</span> <span class="attr">onclick</span>=<span class="string">"myFunction()"</span> <span class="attr">style</span>=<span class="string">"margin-top:15px;"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">p</span>&gt;</span>&#123;&#123;choosetext&#125;&#125;<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="后台采样"><a href="#后台采样" class="headerlink" title="后台采样"></a>后台采样</h3><p>（1）按照字符级模型搭建网络模型，这里设置batch_size=1</p><p>（2）根据得到的最佳模型，创建saver对象，加载参数至网络中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">self.model = CharRNNLM(is_training=<span class="literal">False</span>,w2v_model = self.w2v.model,vocab_size=w2v_vocab_size, infer=<span class="literal">True</span>, **params)</span><br><span class="line">            saver = tf.train.Saver(name=<span class="string">'model_saver'</span>)</span><br><span class="line">            saver.restore(self.sess, best_model)</span><br></pre></td></tr></table></figure><p>（3）初始化隐藏层输入为0，同时如果是藏头诗，则将输入的文本按照字典中位置得出当前字的向量表示，作为初始输入；如果是随机生成诗句，则随机在字典中选择一个位置形成向量作为初始输入</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">        <span class="keyword">if</span> start_text <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> len(start_text) &gt; <span class="number">0</span>:</span><br><span class="line">    seq = list(start_text)</span><br><span class="line">    <span class="keyword">for</span> char <span class="keyword">in</span> start_text[:<span class="number">-1</span>]:</span><br><span class="line">        x = np.array([[self.w2v_model.vocab_hash[char]]])</span><br><span class="line">    x = np.array([[self.w2v_model.vocab_hash[start_text[<span class="number">-1</span>]]]])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    x = np.array([[np.random.randint(<span class="number">0</span>, self.vocab_size)]])</span><br><span class="line">    seq = []</span><br></pre></td></tr></table></figure><p>（4）按照需要生成的诗句的长度，根据skip-gram模型，得出在当前输入的上下文中得到下一个字的概率值</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200623162639.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">state, logits = session.run([self.final_state, self.logits],</span><br><span class="line">                                        &#123;self.input_data: x, self.initial_state: state&#125;)</span><br><span class="line">unnormalized_probs = np.exp(logits[<span class="number">0</span>] - np.max(logits[<span class="number">0</span>]))</span><br><span class="line">probs = unnormalized_probs / np.sum(unnormalized_probs)</span><br></pre></td></tr></table></figure><p>按照出现的词的概率进行随机采样</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sample = np.random.choice(self.vocab_size, <span class="number">1</span>, p=probs)[<span class="number">0</span>] <span class="comment">#随机采样</span></span><br></pre></td></tr></table></figure><p>并将当前采样值作为下一次的输入</p><p>（5）两种诗句形成方式（以[作为开头是因为数据集中诗句都是以此开头）</p><p>随机生成诗句：以[作为开头生成长度60的诗句（60可调节），根据生成句子中的句号个数选择用户需要的进行返回。</p><p>藏头诗：将[加在输入的文本前面，每次读入一个文本，并以此作为开头进行采样，每次只保留第一个逗号和第一个句号两个诗句。</p><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>（1）用户输入需要生成的诗句类型，如选择自由生成诗句，需要输入诗句数量；如选择藏头诗句，需要输入藏头文字。</p><p>（2）前台将用户输入传回，后台根据输入的要求调用模型进行采样生成诗句。</p><p>（3）后台将生成的诗句传回需要渲染的模板中进行显示。</p><p>例如：输入2，选择生成藏头诗，藏头的文字为：八戒我爱你，则生成的诗句显示在页面如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200714201829.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;描述&quot;&gt;&lt;a href=&quot;#描述&quot; class=&quot;headerlink&quot; title=&quot;描述&quot;&gt;&lt;/a&gt;描述&lt;/h2&gt;&lt;p&gt;根据生成的最佳字符级模型，使用Flask框架结合html搭建web应用显示生成的诗句。根据用户输入诗句数量，随机n句诗句；或者根据用户输入的文字，根据最佳模型采样生成对应诗句。&lt;/p&gt;&lt;h2 id=&quot;实现&quot;&gt;&lt;a href=&quot;#实现&quot; class=&quot;headerlink&quot; title=&quot;实现&quot;&gt;&lt;/a&gt;实现&lt;/h2&gt;&lt;h3 id=&quot;前台显示&quot;&gt;&lt;a href=&quot;#前台显示&quot; class=&quot;headerlink&quot; title=&quot;前台显示&quot;&gt;&lt;/a&gt;前台显示&lt;/h3&gt;&lt;p&gt;1、python端使用Flask框架搭建web应用&lt;/p&gt;&lt;p&gt;（1）导入Flask类，并创建一个app实例&lt;/p&gt;&lt;p&gt;（2）通过路由route()装饰器告诉Flask触发函数的url&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="自然语言处理" scheme="https://www.xiapf.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>用tensorflow实现机器学习算法——线性回归</title>
    <link href="https://www.xiapf.com/blogs/tfReg/"/>
    <id>https://www.xiapf.com/blogs/tfReg/</id>
    <published>2020-07-13T09:50:46.000Z</published>
    <updated>2020-08-06T15:00:45.249Z</updated>
    
    <content type="html"><![CDATA[<h2 id="读取数据——使用pandas"><a href="#读取数据——使用pandas" class="headerlink" title="读取数据——使用pandas"></a>读取数据——使用pandas</h2><p>（1）利用pandas中的read_csv读入</p><p>raw_data=pd.read_csv(‘ex1data2.txt’,names=[‘population’,’person’,’price’])</p><p>（2）使用均值mean，方程std归一化数据</p><p>data=(raw_data-raw_data.mean())/raw_data.std()</p><a id="more"></a><p>（3）每次迭代计算误差值，并对每个参数进行修正，并计算当前损失</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#2.计算成本</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(x,y,theta)</span>:</span></span><br><span class="line">m=x.shape[<span class="number">0</span>]</span><br><span class="line">inner=np.power(((x*theta.T)-y),<span class="number">2</span>)</span><br><span class="line">cost=(<span class="number">1</span>/(<span class="number">2</span>*m))*np.sum(inner)</span><br><span class="line"><span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line"><span class="comment">#3.梯度下降</span></span><br><span class="line"><span class="comment">#theta 1*2</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_gradientdescent</span><span class="params">(x,y,theta,alpha,iters)</span>:</span></span><br><span class="line">costs=[]</span><br><span class="line">m=x.shape[<span class="number">0</span>]</span><br><span class="line">n=theta.shape[<span class="number">1</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(iters):</span><br><span class="line">error=x*theta.T-y</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">theta[:,j]=theta[:,j]-alpha*(<span class="number">1</span>/m)*np.sum(np.multiply(error,x[:,j]))</span><br><span class="line">cost=compute_cost(x,y,theta)</span><br><span class="line">costs.append(cost)</span><br><span class="line"><span class="keyword">return</span> theta,costs</span><br></pre></td></tr></table></figure><h2 id="使用tensorflow"><a href="#使用tensorflow" class="headerlink" title="使用tensorflow"></a>使用tensorflow</h2><p>（1）定义占位符用于输入数据</p><p>（2）定义当前作用域下的参数，预测值的计算，损失的计算，即正向传播得到的值</p><p>（3）定义梯度下降算法</p><p>（4）运行会话，将输入数据、下降算法、损失、参数输入，最终得到损失和参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment">#用tensorflow实现线性回归</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_regression</span><span class="params">(x_data,y_data,alpha,iters)</span>:</span></span><br><span class="line"><span class="comment">#1.占位符</span></span><br><span class="line">x=tf.placeholder(tf.float32,x_data.shape)</span><br><span class="line">y=tf.placeholder(tf.float32,y_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.定义参数和损失</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'linear_regression'</span>):</span><br><span class="line">w=tf.get_variable(<span class="string">'weight'</span>,shape=(x.shape[<span class="number">1</span>],<span class="number">1</span>),initializer=tf.constant_initializer())</span><br><span class="line">y_hat=tf.matmul(x,w)</span><br><span class="line">loss=(<span class="number">1</span>/(<span class="number">2</span>*x_data.shape[<span class="number">0</span>]))*tf.matmul((y_hat-y),(y_hat-y),transpose_a=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#3.定义梯度下降算法</span></span><br><span class="line">opt=tf.train.GradientDescentOptimizer(learning_rate=alpha)</span><br><span class="line">opt_generator=opt.minimize(loss)</span><br><span class="line"></span><br><span class="line">loss_data=[]</span><br><span class="line"><span class="comment">#3.运行会话</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(iters):</span><br><span class="line">_,loss_val,w_val=sess.run([opt_generator,loss,w],feed_dict=&#123;x:x_data,y:y_data&#125;)</span><br><span class="line">loss_data.append(loss_val[<span class="number">0</span>]) <span class="comment">#loss_val是1*1数组</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">res=&#123;<span class="string">'loss'</span>:loss_data,<span class="string">'theta'</span>:w_val&#125;</span><br><span class="line"><span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><p>得到的参数：</p><p>[[-3.2414012]<br> [ 1.1272942]]</p><p>损失图为：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200713174946.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;读取数据——使用pandas&quot;&gt;&lt;a href=&quot;#读取数据——使用pandas&quot; class=&quot;headerlink&quot; title=&quot;读取数据——使用pandas&quot;&gt;&lt;/a&gt;读取数据——使用pandas&lt;/h2&gt;&lt;p&gt;（1）利用pandas中的read_csv读入&lt;/p&gt;&lt;p&gt;raw_data=pd.read_csv(‘ex1data2.txt’,names=[‘population’,’person’,’price’])&lt;/p&gt;&lt;p&gt;（2）使用均值mean，方程std归一化数据&lt;/p&gt;&lt;p&gt;data=(raw_data-raw_data.mean())/raw_data.std()&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="回归" scheme="https://www.xiapf.com/tags/%E5%9B%9E%E5%BD%92/"/>
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>人脸识别中不同方法对比分析</title>
    <link href="https://www.xiapf.com/blogs/diversityFace/"/>
    <id>https://www.xiapf.com/blogs/diversityFace/</id>
    <published>2020-07-06T09:10:05.000Z</published>
    <updated>2020-07-06T09:11:36.427Z</updated>
    
    <content type="html"><![CDATA[<h2 id="项目设置"><a href="#项目设置" class="headerlink" title="项目设置"></a>项目设置</h2><p>（1）问题描述</p><p>对于待分类的图像，根据模式识别算法判断它和哪一个图像最相似</p><p>（2）数据集来源</p><p>实验数据集来自Biometric Ideal Test官网<a href="http://biometrics.idealtest.org/dbDetailForUser.do?id=9" target="_blank" rel="external nofollow noopener noreferrer">http://biometrics.idealtest.org/dbDetailForUser.do?id=9</a> ，保存在FaceV5文件夹下。</p><p>选择其中100个人的不同的4张图片作为数据集，选择每个人的一张图片作为测试集共100张图像，保存在used文件夹下，将剩余的图像作为训练集，共300张图像，保存在unused文件夹下。每张图片的第0 ~ 2位用数字表示，用来标识每个人的身份，最终测试样本预测的类别也是需要根据图片的名称来进行判断是否识别准确。</p><a id="more"></a><p>（3）评价标准</p><p>不同方法的评价指标采用单一实数指标：正确率accuracy=预测正确的样本数/预测样本总数。</p><p>knn分类器中的超参数K取值区间设置为[1,5]，分类的距离采用L2二阶范数、相关性系数、闵可夫斯基距离</p><h2 id="不同的算法比对"><a href="#不同的算法比对" class="headerlink" title="不同的算法比对"></a>不同的算法比对</h2><p>模式识别算法中的人脸识别分为两部分：特征提取和数据比对两个阶段</p><h3 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h3><p>（1）基于颜色像素的RGB模型</p><p>特征：将训练样本和输入样本分别计算出每个图像的颜色直方图hist，对直方图归一化后保存的数据作为图像的特征。</p><p>（2）基于统计特征脸的PCA方法</p><p>PCA主成分分析原理总体来说是通过K-L变换将高维向量变成低维向量，形成特征子空间，每个图像就通过投影到该子空间上作为自身特征向量进行后续识别。其中主成分分析法是指找到一个空间，即形成的特征子空间，在该空间上消除了数据的相关性，每个类别数据能够很好的分离。通过K-L变换求出了特征空间，将训练样本和测试样本中所有图片投影在特征空间中就能求得每个图片的特征向量。</p><p>（3）基于神经网络的模型</p><p>从官网下载VGG16的预训练模型，保留全连接层，分别将训练样本和测试样本输入网络，经过卷积层、池化层、全连接层，最终输出一个4096维列向量作为每个图片的特征向量。</p><h3 id="数据比对"><a href="#数据比对" class="headerlink" title="数据比对"></a>数据比对</h3><p>采用最近邻分类knn分类器，根据已保存的训练样本和测试样本的特征向量，找到距离测试样本最近的类别作为预测的人脸，具体计算步骤：<br>（1）定义分类器中的超参数K，表示进行投票决策的样本的数目。<br>（2）遍历所有测试样本，计算样本的特征向量和其他每个训练样本的距离，按照由近至远进行排序。<br>（3）计算离测试样本最近的K个样本，统计各个分类，将最多数量的分类代表的人脸作为当前测试样本的预测值。</p><h3 id="特征提取-数据比对"><a href="#特征提取-数据比对" class="headerlink" title="特征提取+数据比对"></a>特征提取+数据比对</h3><p>基于迁移学习的模型</p><p>基于迁移学习的识别方法将提取特征和数据比都交给网络自己完成，最终输出人脸的识别结果。基于迁移学习的方法VGG16Fc采用基于模型的迁移学习的方法，保留VGG16至全连接层的预训练模型，在模型最后增加两层新的全连接层，作为需要微调的神经元部分。</p><p>预训练部分的网络用来识别图片的轮廓、线段、人脸位置、表情等图像信息，微调部分的网络进行分类识别。根据训练样本中每个图像的特征向量和图像所代表的类别训练后两层全连接层，为防止过拟合，增加Dropout层，随机丢弃一些神经元节点，最终将测试样本的特征向量输入训练好的网络中得到预测的人脸结果。</p><h3 id="总体过程"><a href="#总体过程" class="headerlink" title="总体过程"></a>总体过程</h3><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200706151929.png" alt></p><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>（1）不同距离的度量特征向量</p><p>RGB+KNN方法中，使用相关性系数度量距离的平均识别率为55%，使用闵可夫斯基距离平均识别率为50.4%，使用L2范数平均识别率为55.4%。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200706152534.png" alt></p><p>PCA+KNN方法中，使用相关性系数度量距离的平均识别率为75.2%，使用闵可夫斯基距离平均识别率为69.8%，使用L2范数平均识别率为72.6%。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200706153119.png" alt></p><p>VGG+KNN方法中，使用相关性系数度量距离的平均识别率为89.4%，使用闵可夫斯基距离平均识别率为85.6%，使用L2范数平均识别率为89.4%。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200706153149.png" alt></p><p>分析1：从结果可以看出：不同的度量向量的方法对算法具有一定影响。当选择传统的方法基于像素和统计的识别方法时，使用L2范数和相关性系数识别准确率接近，并且识别的结果优于闵可夫斯基距离，使用闵可夫斯基距离识别最差，因为它将各个分量同等看待，没有考虑到向量之间的相关性，所以识别效果差。</p><p>当选择深度学习的方法，使用神经网络时，三种度量方式识别都很准确，因为神经网络比传统只基于颜色像素、统计的特征脸学习了更多图片的信息，因此表示每个图像特征向量更准确，同一个人的特征向量距离很近，不同的人的特征向量距离非常远，所以选择不同的距离计算方式对识别效果基本无太大影响。</p><p>分析2：针对最近邻分类器K的取值，从结果可以看出，随着K的增加，预测的准确率先升高再降低，说明增加K会提高预测准确率，但是K太大，会导致最近的样本中其他样本数量过多，而导致分类错误。由于训练集中每个人的图像数量为3张，因此当K取值为2的时候分类效果最好，即K略小于训练集中同一个人的图片数量时分类效果最好，所以需要合理选择K的值。由于这里训练集图片数量较少，K=1时也能取得较好的分类效果。</p><p>（2）不同的人脸识别的方法</p><p>按照（1）中选择L2范数作为KNN分类器中度量距离的方式。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200706153408.png" alt></p><p>分析1：从结果可以看出，基于颜色像素的方法仅考虑了人脸中的颜色，没有考虑人脸的轮廓的相似度等，所以该方法识别率最低，平均识别为55.4%，基于统计特征脸的方法考虑到了人脸的轮廓，利用特征脸，将图像投影到特征空间中，更多的考虑到了人脸的特点，所以识别效果较好，平均识别率达72.6%，但是这两种方法和使用神经网络的深度学习方法相比，识别效果都不佳，神经网络识别的方法从图像中提取CNN特征，能更好的对图像分类，平均识别率为89.4%。</p><p>分析2：将传统分类方法和神经网络分类的对比，基于神经网络的识别方法在提取出图像的特征后外接knn分类器进行识别，通过距离对图像进行分类，而基于迁移学习的方法是让网络学习图像的向量和对应人脸之间的对应关系，从而对测试图像进行识别分类，识别结果准确率86%，低于传统分类方法的识别结果，但并不完全因为最近邻分类方法由于网络学习分类的方法，基于迁移学习的方法在训练集上的精度最终能达97.78%，但是在测试集上的识别精度不佳，可能是由于训练样本集太小，只有300张图像，导致了过拟合。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;项目设置&quot;&gt;&lt;a href=&quot;#项目设置&quot; class=&quot;headerlink&quot; title=&quot;项目设置&quot;&gt;&lt;/a&gt;项目设置&lt;/h2&gt;&lt;p&gt;（1）问题描述&lt;/p&gt;&lt;p&gt;对于待分类的图像，根据模式识别算法判断它和哪一个图像最相似&lt;/p&gt;&lt;p&gt;（2）数据集来源&lt;/p&gt;&lt;p&gt;实验数据集来自Biometric Ideal Test官网&lt;a href=&quot;http://biometrics.idealtest.org/dbDetailForUser.do?id=9&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;http://biometrics.idealtest.org/dbDetailForUser.do?id=9&lt;/a&gt; ，保存在FaceV5文件夹下。&lt;/p&gt;&lt;p&gt;选择其中100个人的不同的4张图片作为数据集，选择每个人的一张图片作为测试集共100张图像，保存在used文件夹下，将剩余的图像作为训练集，共300张图像，保存在unused文件夹下。每张图片的第0 ~ 2位用数字表示，用来标识每个人的身份，最终测试样本预测的类别也是需要根据图片的名称来进行判断是否识别准确。&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="模式识别" scheme="https://www.xiapf.com/tags/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>序列模型RNN——语音识别与触发字检测</title>
    <link href="https://www.xiapf.com/blogs/rnn3TD/"/>
    <id>https://www.xiapf.com/blogs/rnn3TD/</id>
    <published>2020-06-30T08:14:33.000Z</published>
    <updated>2020-10-27T06:02:48.902Z</updated>
    
    <content type="html"><![CDATA[<h2 id="语言识别"><a href="#语言识别" class="headerlink" title="语言识别"></a>语言识别</h2><h3 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h3><p>输入一段语音，输出语音中文字。</p><p>（1）预处理：将输入的语音转换为频谱图，横轴显示时间，纵轴表示当前时间步的频率</p><p>（2）搭建模型：将预处理后的频谱图传入网络得到预测文字</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>（1）使用注意力模型</p><p>根据上下文向量和隐藏层的值每次输出一个文本</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630152005.png" alt></p><p>（2）使用CTC损失函数</p><p>主要是使用RNN中多对多模型，Tx=Ty，允许输出空格和重复的字符，保证输入输出相同。</p><a id="more"></a><p>对于预测的输出，可以将空格之间的字符进行折叠得到最终的结果</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630152045.png" alt></p><h2 id="触发字检测"><a href="#触发字检测" class="headerlink" title="触发字检测"></a>触发字检测</h2><p>对输入的一段音频，当未出现触发字时标签为0，出现触发字时标签为1。</p><p>防止训练集不平衡（防止0过多）：在出现触发字的时间t后，设置连续的一段音频标签为1。</p><p>网络结构采用RNN模型</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630152559.png" alt></p><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>当音频中出现“active”触发词时，在其后面加上一段蜂鸣声</p><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p>（1）构建训练集（单一训练样本的构建）</p><p>由于语音数据较为复杂，直接获取在嘈杂背景音下的说话声音较为困难，因此选择录制没有背景音的声音（正例和反例）和背景音，再进行合成，就很容易的有了在嘈杂背景下的说话声音。所以需要随机再背景音中插入正例（说active）和反例（不说active）的声音。</p><p>使用pydub合成音频大小为10000，原始音频大小为10秒，10/10000=0.001s=1ms，10秒离散成1ms，即10秒音频有10000个时间步</p><p>原始音频大小44100，频谱图得到的时间步Tx=5511，每个时间步的频率为101</p><p>1°随机选择插入的位置</p><p>根据片段的大小，在1~10000-片段大小内随机选择位置</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#随机选取片段时间</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_random_time_segment</span><span class="params">(segment_time_ms)</span>:</span></span><br><span class="line">segment_start=np.random.randint(low=<span class="number">0</span>,high=<span class="number">10000</span>-segment_time_ms)</span><br><span class="line">segment_end=segment_start+segment_time_ms<span class="number">-1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> (segment_start,segment_end)</span><br></pre></td></tr></table></figure><p>2°判断是否和之前插入位置重叠</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#判断有无和之前时间重叠</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_overlapping</span><span class="params">(segment_time,previous_time)</span>:</span></span><br><span class="line">segment_start,segment_end=segment_time</span><br><span class="line">overlap=<span class="literal">False</span></span><br><span class="line"><span class="keyword">for</span> previous_start,previous_end <span class="keyword">in</span> previous_time:</span><br><span class="line"><span class="keyword">if</span> segment_start&lt;=previous_end <span class="keyword">and</span> segment_end&gt;=previous_start:</span><br><span class="line">overlap=<span class="literal">True</span></span><br><span class="line"><span class="keyword">return</span> overlap</span><br></pre></td></tr></table></figure><p>3°根据选择的背景音插入正例或反例</p><p>随机选择插入位置，当不重复时，将插入时间归为已插入，同时将音频按照起始位置插入到背景音乐中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#选取片段</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert_audio_clip</span><span class="params">(background,audio_clip,previous_time)</span>:</span></span><br><span class="line"><span class="comment">#随机选一个时间</span></span><br><span class="line">segment_time_ms=len(audio_clip)</span><br><span class="line">segment_time=get_random_time_segment(segment_time_ms)</span><br><span class="line"></span><br><span class="line"><span class="comment">#判断是否重叠</span></span><br><span class="line"><span class="keyword">while</span> is_overlapping(segment_time,previous_time):</span><br><span class="line">segment_time=get_random_time_segment(segment_time_ms)</span><br><span class="line"></span><br><span class="line"><span class="comment">#不重叠的时间段</span></span><br><span class="line">previous_time.append(segment_time)</span><br><span class="line"></span><br><span class="line">new_background=background.overlay(audio_clip,position=segment_time[<span class="number">0</span>])<span class="comment">#要插入的音频和位置</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> new_background,segment_time</span><br></pre></td></tr></table></figure><p>4°将插入位置的标签设置为1</p><p>当插入位置没有到最后时，将插入位置后面的49个位置全部设置为1（平衡训练集中的0）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#设置标签项 在t位置为1，后面49个时间步设置为1</span></span><br><span class="line"><span class="comment">#如果 “activate” 在时间 t步结束 , 则设置 y⟨t+1⟩=1以及后面额外的连续49个的值为 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#以ms为单位的段的结束时间</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert_ones</span><span class="params">(y,segment_time_ms)</span>:</span></span><br><span class="line">segment_end_y=int(segment_time_ms*Ty/<span class="number">10000</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(segment_end_y+<span class="number">1</span>,segment_end_y+<span class="number">51</span>):</span><br><span class="line"><span class="keyword">if</span> i&lt;Ty:</span><br><span class="line">y[<span class="number">0</span>,i]=<span class="number">1</span></span><br><span class="line"><span class="keyword">return</span> y</span><br></pre></td></tr></table></figure><p>5°生成单个训练样本</p><p>从正例或者反例中随机选择几个片段，并按照选择片段的个数，随机选择插入的片段位置。</p><p>将选择的片段随机插入，并设置好标签项。</p><p>最终将标准化后的音频导出，这是就构造了一个训练样本。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#生成训练集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_training_example</span><span class="params">(background,activates,negtives)</span>:</span></span><br><span class="line">np.random.seed(<span class="number">18</span>)</span><br><span class="line"></span><br><span class="line">background=background<span class="number">-20</span> <span class="comment">#使噪音安静些</span></span><br><span class="line"></span><br><span class="line">y=np.zeros((<span class="number">1</span>,Ty))</span><br><span class="line">previous_time=[]</span><br><span class="line"></span><br><span class="line"><span class="comment">#正例 从列表中选择0~4随机的正例</span></span><br><span class="line">num_of_activites=np.random.randint(<span class="number">0</span>,<span class="number">5</span>)</span><br><span class="line">print(num_of_activites)</span><br><span class="line">random_indies=np.random.randint(len(activates),size=num_of_activites)</span><br><span class="line">print(random_indies)</span><br><span class="line">random_activtes=[activates[i] <span class="keyword">for</span> i <span class="keyword">in</span> random_indies]</span><br><span class="line"></span><br><span class="line"><span class="comment">#对每个选取的正例片段插入背景中，并更新标签</span></span><br><span class="line"><span class="keyword">for</span> random_activte <span class="keyword">in</span> random_activtes:</span><br><span class="line">background,segment_time=insert_audio_clip(background,random_activte,previous_time)</span><br><span class="line">segment_start,segment_end=segment_time</span><br><span class="line">y=insert_ones(y,segment_end)</span><br><span class="line"></span><br><span class="line"><span class="comment">#反例</span></span><br><span class="line">num_of_negatives=np.random.randint(<span class="number">0</span>,<span class="number">3</span>)</span><br><span class="line">random_indies=np.random.randint(len(negatives),size=num_of_negatives)</span><br><span class="line">random_negatives=[negatives[i] <span class="keyword">for</span> i <span class="keyword">in</span> random_indies]</span><br><span class="line"></span><br><span class="line"><span class="comment">#对每个选取的正例片段插入背景中，并更新标签</span></span><br><span class="line"><span class="keyword">for</span> random_negative <span class="keyword">in</span> random_negatives:</span><br><span class="line">background,_=insert_audio_clip(background,random_negative,previous_time)</span><br><span class="line"></span><br><span class="line"><span class="comment">#标准化剪辑的音量</span></span><br><span class="line">background=match_target_amplitude(background,<span class="number">-20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#保存</span></span><br><span class="line">file_handle=background.export(<span class="string">'train1'</span>+<span class="string">'.wav'</span>,format=<span class="string">'wav'</span>)</span><br><span class="line">print(<span class="string">'train1.wav已保存'</span>)</span><br><span class="line"><span class="comment"># print(file_handle)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#声谱图</span></span><br><span class="line">x=graph_spectrogram(<span class="string">'train1.wav'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> x,y</span><br></pre></td></tr></table></figure><p>（2）构建模型</p><p>为节省时间，这里训练集使用已处理好的样本，开发集使用真实音频并手动标记，因为系统需要在真实音频下使用。</p><p>使用np.load导入训练集和开发集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入已处理好的训练集和开发集</span></span><br><span class="line">X=np.load(<span class="string">'./XY_train/X.npy'</span>)</span><br><span class="line">Y=np.load(<span class="string">'./XY_train/Y.npy'</span>)</span><br><span class="line">X_dev=np.load(<span class="string">'./XY_dev/X_dev.npy'</span>)</span><br><span class="line">Y_dev=np.load(<span class="string">'./XY_dev/Y_dev.npy'</span>)</span><br></pre></td></tr></table></figure><p>按照如图结构构建模型，采用GRU单元</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630155058.jpg" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用两个GRU建立模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(input_shape)</span>:</span></span><br><span class="line">X_input=Input(shape=input_shape)</span><br><span class="line">X=Conv1D(<span class="number">196</span>,<span class="number">15</span>,strides=<span class="number">4</span>)(X_input)</span><br><span class="line">X=BatchNormalization()(X)</span><br><span class="line">X=Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line">X=Dropout(<span class="number">0.8</span>)(X)</span><br><span class="line"></span><br><span class="line">X=GRU(<span class="number">128</span>,return_sequences=<span class="literal">True</span>)(X)</span><br><span class="line">X=Dropout(<span class="number">0.8</span>)(X)</span><br><span class="line">X=BatchNormalization()(X)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X=GRU(<span class="number">128</span>,return_sequences=<span class="literal">True</span>)(X)</span><br><span class="line">X=Dropout(<span class="number">0.8</span>)(X)</span><br><span class="line">X=BatchNormalization()(X)</span><br><span class="line">X=Dropout(<span class="number">0.8</span>)(X)</span><br><span class="line"></span><br><span class="line">X=TimeDistributed(Dense(<span class="number">1</span>,activation=<span class="string">'sigmoid'</span>))(X)</span><br><span class="line"></span><br><span class="line">model=Model(inputs=X_input,outputs=X)</span><br><span class="line"><span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><p>（3）训练模型</p><p>为了快速建立可用系统，可以先导入预训练的模型，再编译模型（可以自定义梯度下降函数及参数）、训练模型，这时仅训练一次即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#编译及训练模型(导入下载的模型，再编译，训练一次)</span></span><br><span class="line">input_shape=(Tx,n_freq)</span><br><span class="line">model=model(input_shape)</span><br><span class="line">model=load_model(<span class="string">'./models/tr_model.h5'</span>)</span><br><span class="line">opt=Adam(lr=<span class="number">0.0001</span>,beta_1=<span class="number">0.9</span>,beta_2=<span class="number">0.999</span>,decay=<span class="number">0.01</span>)</span><br><span class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>,optimizer=opt,metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">model.fit(X,Y,epochs=<span class="number">1</span>,batch_size=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用开发集预测</span></span><br><span class="line">loss,acc=model.evaluate(X_dev,Y_dev)</span><br></pre></td></tr></table></figure><blockquote><p>loss: 0.3650805354118347<br>acc: 0.9451636075973511</p></blockquote><p>正确率达94.5%，此时模型根据输入的音频能很好的确定active触发词的位置</p><p>（4）在active触发词后加上峰鸣声</p><p>1°对输入数据预测</p><p>将输入的音频转换为频谱图，由于频谱图输出的是（频率，时间步），需要调换坐标轴，接着使用模型预测，得到输出值，输出值打印如图：（输出值中间代表每个时间步的输出）</p><p>上面是输入音频的频谱图，下面是预测输出</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630164935.png" alt></p><p>可以看出网络预测2个地方有触发词（预测的可能值很大）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#进行预测</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">detect_triggerword</span><span class="params">(filename)</span>:</span></span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">x=graph_spectrogram(filename)</span><br><span class="line"><span class="comment">#输出n_freq,Tx</span></span><br><span class="line"></span><br><span class="line">x=x.swapaxes(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">x=np.expand_dims(x,axis=<span class="number">0</span>)</span><br><span class="line">predictions=model.predict(x) <span class="comment">#中间对应每个时间步的输出</span></span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">plt.plot(predictions[<span class="number">0</span>,:,<span class="number">0</span>])</span><br><span class="line">plt.ylabel(<span class="string">'probability'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> predictions</span><br></pre></td></tr></table></figure><p>2°找到大于阈值的预测值位置加入峰鸣声</p><p>由于active触发词后49个位置都有1，为了不重复加入峰鸣声，设置每75步加入一次峰鸣，插入的位置在当前时间步占总体时间步再乘以音频持续时间</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 当出现一次触发词就插入峰鸣声音，为了不一直有声音，设置连续步</span></span><br><span class="line">chime_file=<span class="string">'audio_examples/chime.wav'</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chime_on_activate</span><span class="params">(filename,predictions,thresold)</span>:</span></span><br><span class="line">audio_clip=AudioSegment.from_wav(filename)</span><br><span class="line">chime=AudioSegment.from_wav(chime_file)</span><br><span class="line">Ty=predictions.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">countSteps=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(Ty):</span><br><span class="line">countSteps=countSteps+<span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> predictions[<span class="number">0</span>,i,<span class="number">0</span>]&gt;thresold <span class="keyword">and</span> countSteps&gt;<span class="number">75</span>:</span><br><span class="line">audio_clip=audio_clip.overlay(chime,position=((i/Ty)*audio_clip.duration_seconds)*<span class="number">1000</span>)  <span class="comment">#1000是转换为s ms-&gt;s</span></span><br><span class="line">      countSteps=<span class="number">0</span></span><br><span class="line">audio_clip.export(<span class="string">'clime_test1111.wav'</span>,format=<span class="string">'wav'</span>)</span><br></pre></td></tr></table></figure><p>3°预处理音频为10秒</p><p>设置静音大小10000，将输入音频前10000个插入，按照训练集的音频大小设置帧速率为44100，再进行保存</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#需要修剪音频在10秒之内</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_audio</span><span class="params">(filename)</span>:</span></span><br><span class="line"><span class="comment">#设置静音10000</span></span><br><span class="line">silentaudio=AudioSegment.silent(duration=<span class="number">10000</span>)</span><br><span class="line"><span class="comment">#保留文件前10秒</span></span><br><span class="line">preaudio=AudioSegment.from_wav(filename)[:<span class="number">10000</span>]</span><br><span class="line">audio_clip=silentaudio.overlay(preaudio)</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置帧速率44100</span></span><br><span class="line">audio_clip=audio_clip.set_frame_rate(<span class="number">44100</span>)</span><br><span class="line"></span><br><span class="line">myfile=<span class="string">'myaudio111.wav'</span></span><br><span class="line">audio_clip.export(myfile,format=<span class="string">'wav'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> myfile</span><br></pre></td></tr></table></figure><p>（5）结果</p><blockquote><p>myfile=preprocess_audio(‘./test1.wav’)<br>thresold=0.5 #峰鸣声没有添加，可以根据输出图调整阈值<br>predictions=detect_triggerword(myfile)<br>chime_on_activate(myfile,predictions,thresold)</p></blockquote><p>自己录制的音频13秒：<a href="https://pan.baidu.com/s/1cLyOHCMh-osYnJ18DZWsHw" target="_blank" rel="external nofollow noopener noreferrer">https://pan.baidu.com/s/1cLyOHCMh-osYnJ18DZWsHw</a></p><p>修剪之后10秒：<a href="https://pan.baidu.com/s/109EVMpY-dZ2tCkdZZzB4qg" target="_blank" rel="external nofollow noopener noreferrer">https://pan.baidu.com/s/109EVMpY-dZ2tCkdZZzB4qg</a></p><p>加入峰鸣之后：<a href="https://pan.baidu.com/s/1L_fMtBr-bBWLgnUDp9hDfQ" target="_blank" rel="external nofollow noopener noreferrer">https://pan.baidu.com/s/1L_fMtBr-bBWLgnUDp9hDfQ</a></p><p>由网络预测的两个地方的触发词和峰鸣加入的地方比对，发现音频添加正确。（由于发音问题，录制的音频有时预测不准确）</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;语言识别&quot;&gt;&lt;a href=&quot;#语言识别&quot; class=&quot;headerlink&quot; title=&quot;语言识别&quot;&gt;&lt;/a&gt;语言识别&lt;/h2&gt;&lt;h3 id=&quot;基本原理&quot;&gt;&lt;a href=&quot;#基本原理&quot; class=&quot;headerlink&quot; title=&quot;基本原理&quot;&gt;&lt;/a&gt;基本原理&lt;/h3&gt;&lt;p&gt;输入一段语音，输出语音中文字。&lt;/p&gt;&lt;p&gt;（1）预处理：将输入的语音转换为频谱图，横轴显示时间，纵轴表示当前时间步的频率&lt;/p&gt;&lt;p&gt;（2）搭建模型：将预处理后的频谱图传入网络得到预测文字&lt;/p&gt;&lt;h3 id=&quot;方法&quot;&gt;&lt;a href=&quot;#方法&quot; class=&quot;headerlink&quot; title=&quot;方法&quot;&gt;&lt;/a&gt;方法&lt;/h3&gt;&lt;p&gt;（1）使用注意力模型&lt;/p&gt;&lt;p&gt;根据上下文向量和隐藏层的值每次输出一个文本&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630152005.png&quot; alt&gt;&lt;/p&gt;&lt;p&gt;（2）使用CTC损失函数&lt;/p&gt;&lt;p&gt;主要是使用RNN中多对多模型，Tx=Ty，允许输出空格和重复的字符，保证输入输出相同。&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="循环神经网络" scheme="https://www.xiapf.com/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>序列模型RNN——有条件的语言模型</title>
    <link href="https://www.xiapf.com/blogs/rnn3MT/"/>
    <id>https://www.xiapf.com/blogs/rnn3MT/</id>
    <published>2020-06-30T07:03:14.000Z</published>
    <updated>2020-10-27T06:02:55.451Z</updated>
    
    <content type="html"><![CDATA[<h2 id="模型简介"><a href="#模型简介" class="headerlink" title="模型简介"></a>模型简介</h2><p>（1）sequence to sequence模型：根据给定的句子得到翻译的结果</p><p>模型由编码器和解码器构成：</p><p>编码器部分读入输入的句子中每个单词，每步不输出结果，仅在最后一步输出一个编码向量。</p><p>解码器部分接受编码部分传入的编码向量，每一步输出一个翻译单词即结果。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630110532.png" alt></p><p>（2）image sequence模型：根据给出的图片输出一个描述</p><a id="more"></a><p>模型由卷积网络和RNN模型构成：</p><p>卷积网络部分去掉最后的预测值，保留至全连接层，输入的图片经过卷积网络最终得到一个描述图片的向量。</p><p>RNN模型部分仅保留解码部分，接收卷积网络传入的描述向量，每一步输出一个翻译单词即结果。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630110603.png" alt></p><h2 id="有条件的语言模型——机器翻译模型"><a href="#有条件的语言模型——机器翻译模型" class="headerlink" title="有条件的语言模型——机器翻译模型"></a>有条件的语言模型——机器翻译模型</h2><h3 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h3><p>基本的语言模型是输入x，直接输出y，结构模型同RNN模型中国的多对多模型</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630110905.png" alt></p><h3 id="机器翻译模型"><a href="#机器翻译模型" class="headerlink" title="机器翻译模型"></a>机器翻译模型</h3><p>结构同sequence to sequence模型，由编码器和解码器构成，机器翻译会得到很多可能的翻译句子，解码器每步输出的结构都是为了让p(y|x)最大，即第一步时p(y1|x)，当输入单词是的得到y1的概率最大，第二步时由于接受第一步结果的输入p(y2|x,y1)，在输入单词和第一步输出结果的情况下概率最大，…以此类推，因此是有条件的模型。</p><p>为了找到最准确的翻译，因此，目标为：要找到一个翻译句子使得条件概率最大，使得p(y^1,y^2….y^ty|x)最大：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630111740.jpg" alt></p><p>即<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630111404.png" alt></p><h3 id="两种达到准确翻译的方法"><a href="#两种达到准确翻译的方法" class="headerlink" title="两种达到准确翻译的方法"></a>两种达到准确翻译的方法</h3><p>（1）贪心搜索</p><p>在解码器部分，每一步的输出都选择当前最大概率输出的一个单词。</p><p>但是根据概率公式看，贪心搜索得到的单词仅考虑当前最佳的一个结果，可能会因为前面的最佳而达不到后面最佳的单词，因此效果不是特别好。</p><p>（2）Beam Search（束搜索）</p><p>思路和贪心搜索类似，但是增加了束宽b，每一步输出都选概率最大的前b个单词。</p><p>每一步是复制了三个同样的网络来评估得到的单词概率，当b=1时就退化成了贪心搜索。</p><p>以b=3为例，在第一步选出的三个单词的基础上选择下一个单词时，就是复制了三个同样的网络，找到三个网络得到的单词中最大概率的前三个：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630112804.png" alt></p><h3 id="改进Beam-Search（束搜索）"><a href="#改进Beam-Search（束搜索）" class="headerlink" title="改进Beam Search（束搜索）"></a>改进Beam Search（束搜索）</h3><p>由于束搜索最终需要让概率最大，是将每步概率相乘得到，概率在0~1之间属于很小的数，很多很小的数相乘会出现数值下溢，同时当出现短句子的时候往往概率大，原概率容易出现翻译成短句子的现象，因此需要进行长度归一化。</p><p>（1）防止数值下溢：在概率之前加上log，因为log函数单调递增，当log函数增加，原函数也增加，所以无影响</p><p>（2）防止出现翻译出来大量的短句子：在最终求和的概率前面加上归一化的系数Ty^alpha，这里alpha可以进行尝试，取值为1，是进行长度归一化，取值为0，则没有归一化，这里可以取折中的0.7或者尝试其他值。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630113356.png" alt></p><p>束宽的选择：b越大，结果更好，但运行慢，b越小，结果会差，但运行快。需要合理选择</p><h3 id="误差分析"><a href="#误差分析" class="headerlink" title="误差分析"></a>误差分析</h3><p>如何判断机器翻译模型的好坏：由于机器翻译模型有RNN模型和束搜索模型同时作用，需要合理判别。机器翻译模型最终目标为：argmax   p（y|x），两者的作用：</p><p>（1）RNN模型：主要是为了能得到概率 p（y|x）</p><p>（2）束搜索模型：找到最大的概率，argmax</p><p>因此，当人工翻译概率为y * ，机器翻译为y^时:（同样可以列出表格，遍历开发集找到出错的样本进行分析）</p><p>当p(y *|x) &gt;p(y^|x)，说明束搜索没有找到更大的概率，需要改进束搜索，选择更大的束宽b；</p><p>当p(y *|x) &lt;=p(y^|x)，说明束搜索找到了大的概率，但给予人工翻译小的概率值，说明RNN模型计算概率存在问题，需要改RNN模型，增加正则化、增加更多训练数据或者尝试不同网络模型等方法。</p><h3 id="Bleu得分"><a href="#Bleu得分" class="headerlink" title="Bleu得分"></a>Bleu得分</h3><p>一个单实数评估指标，能够加速算法性能。</p><p>将机器翻译中的所有单词元组出现的概率相加，归一化后乘以惩罚项（防止过短的翻译）：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630114422.png" alt></p><p>（1）BP</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630114434.png" alt></p><p>当机器翻译长度比参考翻译长度长，则惩罚项不起作用，若短于参考翻译，则惩罚项起作用。</p><p>（2）Pn</p><p>Pn表示n元组单词的概率，n元组单词表示n个相邻的单词，概率为每个元组的单词的得分上限之和 / 所有元组单词在机器翻译中出现的次数：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630114454.png" alt></p><p>其中得分上限是指每个元组的单词在几个人工翻译句子中出现的最大次数</p><p>（3）总结</p><p>Bleu得分可以用来评价机翻的好坏，代替人工方法进行自动评估。</p><h2 id="Attention模型——更符合人的思维的机翻模型"><a href="#Attention模型——更符合人的思维的机翻模型" class="headerlink" title="Attention模型——更符合人的思维的机翻模型"></a>Attention模型——更符合人的思维的机翻模型</h2><h3 id="提出背景"><a href="#提出背景" class="headerlink" title="提出背景"></a>提出背景</h3><p>上述的机翻模型是需要编码器将所有句子读入，才进行翻译，当句子过长时，需要等的时间长，会出现考虑不到后面的句子Bleu得分低的情况，而且不符合人的思维。</p><p>人的思维是读入一部分句子就进行翻译，因此提出了注意力模型。</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>整体结构</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630121532.png" alt></p><p>说明：为了考虑前后单词的意思影响，需要采用双向RNN</p><p>1°每个节点有正向激活值和反向激活值</p><p>2°当输出第一个预测单词时，每个输入按照权重和激活值得到上下文向量</p><p>3°根据上下文向量和前一个隐藏层的值得到当前预测值</p><p>综上，实现了输入一段文字就实时输出预测的翻译单词。</p><p>（1）一个输出神经元结构</p><p>以时间t输出的y^t为例：</p><p>首先按照权重乘以激活值求得上下文向量context，然后利用前一层的隐藏层的值s^t-1得到预测值</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630121517.png" alt></p><p>（2）权重的得出</p><p>权重取值如图所示，所有节点的权重加起来需要满足为1</p><p>alpha&lt;t,t^`&gt;表示预测y^t需要放在a&lt;t^&gt;上的注意力数量，即权重，当前的权重和上一层的值和当前激活值有关，因此将二者传入网络。</p><p>求权重的方法：</p><p>构造小型的神经元进行计算，根据权重和上一个隐藏层值经过全连接层得到e的值，再连接一个全连接层求得式子的值，最后连接一个softmax激活函数，保证权重之和为1.</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630121548.png" alt></p><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>将人类描述的日期转换为机器能识别的日期，即YYYY-MM-DD 10位表达法</p><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p>（1）构建一个神经元的上下文：利用全局变量</p><p>1°定义重复层：由于需要根据上一层的隐藏层值和当前激活值求得权重，所以需要按照Tx个时间步复制上一层的值</p><p>2°定义连接层：将上一层隐藏层值和当前激活值连接</p><p>3°定义全连接层层：将连接值传入小型网络求得中间值</p><p>4°定义全连接层层：将中间值传入小型网络求得结果</p><p>5°定义softmax层：为了保证权重求和为1，最后输出层采用softmax，输入权重</p><p>6°定义点乘层：为了得到上下文context，将得到的权重和激活值相乘得到上下文向量</p><p>总结：根据前一层的隐藏层的值，重复Tx次和激活值连接，经过两个全连接层得到权重，最后和激活值相乘得到上下文向量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#对一个神经元求出其上下文</span></span><br><span class="line"><span class="comment">#设置全局变量 </span></span><br><span class="line">repeator=RepeatVector(Tx) <span class="comment">#重复</span></span><br><span class="line">concatenator=Concatenate(axis=<span class="number">-1</span>) <span class="comment">#连接</span></span><br><span class="line">densor1=Dense(<span class="number">10</span>,activation=<span class="string">'tanh'</span>)</span><br><span class="line">densor2=Dense(<span class="number">1</span>,activation=<span class="string">'relu'</span>)</span><br><span class="line">activator=Activation(softmax,name=<span class="string">"attention_weights"</span>)</span><br><span class="line">dotor=Dot(axes=<span class="number">1</span>) <span class="comment">#进行点积的轴的个数</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#上下文用到前一个隐藏层的值和当前的激活值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">one_step_attention</span><span class="params">(a,s_prev)</span>:</span></span><br><span class="line"><span class="comment">#要和Tx个a值相乘，需要重复</span></span><br><span class="line">s_prev=repeator(s_prev)</span><br><span class="line">concat=concatenator([a,s_prev])</span><br><span class="line"></span><br><span class="line"><span class="comment">#利用两个全连接层求出权重</span></span><br><span class="line">e=densor1(concat)</span><br><span class="line">energies=densor2(e)</span><br><span class="line"></span><br><span class="line"><span class="comment">#保证权重求和为1</span></span><br><span class="line">alpha=activator(energies)</span><br><span class="line"></span><br><span class="line"><span class="comment">#权重乘以激活值得到上下文</span></span><br><span class="line">context=dotor([alpha,a])</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> context</span><br></pre></td></tr></table></figure><p>（2）搭建模型：利用全局变量</p><p>这里使用LSTM单元</p><p>1°初始化网络：定义输入的X，LSTM单元的隐藏层的值s0，记忆细胞c0</p><p>2°定义双向RNN——Bidiretional：根据双向RNN网络计算出激活值</p><p>3°对于每个输出的时间步Ty，计算上下文的值</p><p>4°定义LSTM：将上下文向量、上一层的隐藏层值、记忆细胞传入LSTM单元中</p><p>5°定义Dense：将lstm的值传入全连接层得到当前时间步的输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#建立模型</span></span><br><span class="line"><span class="comment">#设置全局变量</span></span><br><span class="line">n_a=<span class="number">32</span></span><br><span class="line">n_s=<span class="number">64</span></span><br><span class="line">post_activation_LSTM_cell=LSTM(n_s,return_state=<span class="literal">True</span>)</span><br><span class="line">output_layer=Dense(len(machine_vocab),activation=softmax)</span><br><span class="line"><span class="comment">#return_sequences: 布尔值。是返回输出序列中的最后一个输出，还是全部序列。</span></span><br><span class="line"><span class="comment">#return_state: 布尔值。除了输出之外是否返回最后一个状态。</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(Tx,Ty,n_a,n_s,human_vocab_size,machine_vocab_size)</span>:</span></span><br><span class="line">X=Input(shape=(Tx,human_vocab_size))</span><br><span class="line">s0=Input(shape=(n_s,),name=<span class="string">'s0'</span>)</span><br><span class="line">c0=Input(shape=(n_s,),name=<span class="string">'c0'</span>)</span><br><span class="line">s=s0</span><br><span class="line">c=c0 <span class="comment">#c为记忆细胞</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#n_a为输出维度</span></span><br><span class="line">a=Bidirectional(LSTM(n_a,return_sequences=<span class="literal">True</span>),input_shape=(m,Tx,n_a*<span class="number">2</span>))(X)<span class="comment">#双向RNN</span></span><br><span class="line">outputs=[]</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出序列长度Ty</span></span><br><span class="line"><span class="comment">#每一步输出一个结果</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(Ty):</span><br><span class="line">context=one_step_attention(a,s)</span><br><span class="line"></span><br><span class="line"><span class="comment">#initial_state = [hidden state, cell state]</span></span><br><span class="line">s,_,c=post_activation_LSTM_cell(context,initial_state=[s,c])</span><br><span class="line">output=output_layer(s)</span><br><span class="line">outputs.append(output)</span><br><span class="line">model=Model(inputs=[X,s0,c0],outputs=outputs)</span><br><span class="line"><span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><p>（3）结果</p><p>这里设定输入的日期长度为30，即Tx=30，输入日期长度固定为10，即Ty=10</p><p>为了快速建立可用系统，可以先导入预训练的权重，再编译模型（可以自定义梯度下降函数及参数）、训练模型，最终根据输入的数据得出结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">model=model(Tx,Ty,n_a,n_s,len(human_vocab),len(machine_vocab))</span><br><span class="line"><span class="comment"># model.summary()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#编译模型</span></span><br><span class="line">model.load_weights(<span class="string">'models/model.h5'</span>)</span><br><span class="line">opt=Adam(lr=<span class="number">0.005</span>,beta_1=<span class="number">0.9</span>,beta_2=<span class="number">0.999</span>,decay=<span class="number">0.01</span>)</span><br><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,optimizer=opt,metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"><span class="comment">#输入训练集训练模型</span></span><br><span class="line">s0=np.zeros((m,n_s))</span><br><span class="line">c0=np.zeros((m,n_s))</span><br><span class="line">outputs=list(Yoh.swapaxes(<span class="number">0</span>,<span class="number">1</span>)) <span class="comment">#轴交换</span></span><br><span class="line">model.fit([Xoh,s0,c0],outputs,epochs=<span class="number">1</span>,batch_size=<span class="number">100</span>) <span class="comment">#迭代一次就能给出结果</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#输入数据进行预测</span></span><br><span class="line">Examples=[<span class="string">'3 May 1979'</span>, <span class="string">'5 April 09'</span>, <span class="string">'21th of August 2016'</span>, <span class="string">'Tue 10 Jul 2007'</span>, <span class="string">'Saturday May 9 2018'</span>, <span class="string">'March 3 2001'</span>, <span class="string">'March 3rd 2001'</span>, <span class="string">'1 March 2001'</span>]</span><br><span class="line"><span class="keyword">for</span> examples <span class="keyword">in</span> Examples:</span><br><span class="line">source=string_to_int(examples,Tx,human_vocab)</span><br><span class="line">source=np.array(list(map(<span class="keyword">lambda</span> x:to_categorical(x,num_classes=len(human_vocab)),source)))</span><br><span class="line">source=np.expand_dims(source,axis=<span class="number">0</span>) <span class="comment">#增加一个维度</span></span><br><span class="line">prediction=model.predict([source,s0,c0])</span><br><span class="line"><span class="comment"># print(prediction)</span></span><br><span class="line">prediction=np.argmax(prediction,axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">predictional=[inv_machine_vocab[int(i)] <span class="keyword">for</span> i <span class="keyword">in</span> prediction]</span><br><span class="line"></span><br><span class="line">print(<span class="string">'source:'</span>,examples)</span><br><span class="line">print(<span class="string">'predict'</span>,<span class="string">''</span>.join(predictional))</span><br></pre></td></tr></table></figure><p>结果为：</p><blockquote><p>source: 3 May 1979<br>predict 1979-05-03<br>source: 5 April 09<br>predict 2009-04-05<br>source: 21th of August 2016<br>predict 2016-08-21<br>source: Tue 10 Jul 2007<br>predict 2007-07-10<br>source: Saturday May 9 2018<br>predict 2018-05-09<br>source: March 3 2001<br>predict 2001-03-03<br>source: March 3rd 2001<br>predict 2001-03-03<br>source: 1 March 2001<br>predict 2001-03-01</p></blockquote><p>从结果看出模型很好的翻译了人类日期。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;模型简介&quot;&gt;&lt;a href=&quot;#模型简介&quot; class=&quot;headerlink&quot; title=&quot;模型简介&quot;&gt;&lt;/a&gt;模型简介&lt;/h2&gt;&lt;p&gt;（1）sequence to sequence模型：根据给定的句子得到翻译的结果&lt;/p&gt;&lt;p&gt;模型由编码器和解码器构成：&lt;/p&gt;&lt;p&gt;编码器部分读入输入的句子中每个单词，每步不输出结果，仅在最后一步输出一个编码向量。&lt;/p&gt;&lt;p&gt;解码器部分接受编码部分传入的编码向量，每一步输出一个翻译单词即结果。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630110532.png&quot; alt&gt;&lt;/p&gt;&lt;p&gt;（2）image sequence模型：根据给出的图片输出一个描述&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="循环神经网络" scheme="https://www.xiapf.com/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>序列模型RNN——使用词嵌入搭建NLP系统</title>
    <link href="https://www.xiapf.com/blogs/rnn2/"/>
    <id>https://www.xiapf.com/blogs/rnn2/</id>
    <published>2020-06-23T12:27:36.000Z</published>
    <updated>2020-10-27T06:03:02.630Z</updated>
    
    <content type="html"><![CDATA[<h2 id="词嵌入"><a href="#词嵌入" class="headerlink" title="词嵌入"></a>词嵌入</h2><h3 id="什么是词嵌入"><a href="#什么是词嵌入" class="headerlink" title="什么是词嵌入"></a>什么是词嵌入</h3><p>以词的特征来表示各个单词，即单词的高维度特征表示。</p><p>与one-hot表示词相比：</p><p>（1）能更好的反映词的特征，增强相关词的泛化能力（例如学习了orange juice，后面会很容易根据apple推测出apple juice，因为apple和orange的特征表示很像），而使用one-hot表示，词之前没有联系，无法泛化</p><a id="more"></a><p>（2）节约了表示词向量所占的内存，one-hot表示方法，每个词向量的大小和字典的大小相同，词嵌入方法中每个词的大小和特征维度相关。</p><p>使用t-SNE可以将高维度的特征向量映射到二维空间，可以更容易观察各个向量之间的联系，靠的更近的联系更紧密。</p><h3 id="为什么使用词嵌入"><a href="#为什么使用词嵌入" class="headerlink" title="为什么使用词嵌入"></a>为什么使用词嵌入</h3><p>词嵌入可以用在迁移学习中，目前的NLP问题中已标记的数据集较少，此时使用词嵌入可以很快速又准确的构建NLP系统。</p><p>（1）先从大量文本中学习词嵌入（或者从网上下载别人训练好的词嵌入模型）    task A</p><p>（2）将词嵌入模型迁移到具有少量标记样本的数据集中     task B</p><p>（3）微调：当自己已标记的数据量很大时，可以对已有的词嵌入模型进行微调</p><p>迁移学习原理：将在task A中学习的知识迁移到B中，此时A数据量很大，B数据量很小，训练出的模型即使在以后出现了不在B中的词的情况下也能很好的预测，因为拥有从很大数据量中学习到的知识（task A）</p><h2 id="搭建词嵌入模型的方法（Word2Vec）"><a href="#搭建词嵌入模型的方法（Word2Vec）" class="headerlink" title="搭建词嵌入模型的方法（Word2Vec）"></a>搭建词嵌入模型的方法（Word2Vec）</h2><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200623162528.png" alt></p><p>总体思路：学习词嵌入模型即学习嵌入矩阵E，将每个词使用one-hot表示为Ox，再乘以嵌入矩阵E即可得到每个词的词嵌入表示方法。</p><p>方法：通过选择上下文-目标词对来学习词嵌入。</p><p>（1）前面的4个单词-预测词</p><p>（2）前面的4个单词-预测词-后面的4个单词</p><p>（3）前面的1个单词-预测词</p><p>（4）与预测词关系最近的一个单词-预测词</p><h3 id="skip-gram模型"><a href="#skip-gram模型" class="headerlink" title="skip gram模型"></a>skip gram模型</h3><p>（1）训练集构建</p><p>上下文-目标词：任意选择一个词-选择的词的前后距离内随机再选择一个词</p><p>（2）模型</p><p>Ox-&gt;E-&gt;ec-&gt;softmax-&gt;yhat</p><p>softmax中的概率为目标词t出现在词c上下文的概率：<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200623162639.png" alt></p><p>损失函数为：L=-∑y * log(yhat)</p><p>存在问题：计算每个词的概率都需要对词汇表中的词进行求和，成本高</p><p>解决：采用分级softmax树形分类器，常用词放在顶部，非常用词放在树的深处，每个节点都是一个二分类器。</p><p>将多分类问题转换为多个二分类问题。</p><h3 id="负采样"><a href="#负采样" class="headerlink" title="负采样"></a>负采样</h3><p>（1）训练集构建</p><p>上下文-目标词：每组k+1个词，给出一组词判断是否是一对上下文-目标词对</p><p>第一组词（正样本）：当前词-在句子中前后距离内随机选择一个词，标签为1</p><p>剩余k-1个词（负样本）：当前词-在字典中随机选择一个词，标签为0</p><p>（2）模型</p><p>Ox-&gt;E-&gt;ec-&gt;二分类器-&gt;yhat</p><p>每次训练k+1个，softmax中是二分类问题。 </p><p>二分类中的概率为目标词t出现在词c上下文的概率：<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200623162745.png" alt></p><p>（3）如何选择负样本</p><p>即考虑词出现的概率又考虑词的分布，以词出现概率的3/4除以总体得到的概率来选择负样本（单一以概率，容易一直选择到the a of等词）：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200623162853.png" alt></p><h3 id="glove向量"><a href="#glove向量" class="headerlink" title="glove向量"></a>glove向量</h3><p>（1）训练集构建</p><p>上下文-目标词：用Xij表示单词i出现在单词j上下文的次数，衡量两个单词彼此接近频率的计数器</p><p>（2）模型</p><p>最小化两个单词之前的距离：<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200623162915.png" alt></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>有以上三种方法搭建词嵌入模型，得到从大量文本中学习到的词向量知识。</p><p>为了快速搭建NLP系统，可以从网上下载别人已训练好的词嵌入，用此进行系统搭建。</p><h3 id="采用词嵌入进行词的类比"><a href="#采用词嵌入进行词的类比" class="headerlink" title="采用词嵌入进行词的类比"></a>采用词嵌入进行词的类比</h3><p>描述：解决a与b类似于c与？的问题</p><p>（1）导入词嵌入</p><p>这里的词嵌入采用glove向量，导入词嵌入模型中的所有单词，及单词对应的词嵌入向量表示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.导入词嵌入中的词汇和对应表</span></span><br><span class="line"><span class="comment">#word代表字符 word_to_vec_map代表向量表</span></span><br><span class="line">word,word_to_vec_map=w2v_utils.read_glove_vecs(<span class="string">'./data/glove.6B.50d.txt'</span>)</span><br></pre></td></tr></table></figure><p>（2）定义相似度度量函数</p><p>词之间的相似度采用余弦相似度来表示，余弦值越大，当等于1时，此时角度为0，说明两个词最相似，余弦值越小两个词越不相似，根据计算余弦相似度定义函数：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200623170723.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#2.计算词之间的相似度</span></span><br><span class="line"><span class="comment">#采用余弦相似度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cosine_similarity</span><span class="params">(u,v)</span>:</span></span><br><span class="line">np_dot=np.dot(u,v)</span><br><span class="line">linalg_u=np.linalg.norm(u)</span><br><span class="line">linalg_v=np.linalg.norm(v)</span><br><span class="line">linalg_uv=np.dot(linalg_u,linalg_v)</span><br><span class="line"></span><br><span class="line">cos_sim=np.divide(np_dot,linalg_uv)</span><br><span class="line"><span class="keyword">return</span> cos_sim</span><br></pre></td></tr></table></figure><p>（3）进行词的类比</p><p>目标：ea-eb≈eb-e?，为了让等式成立就要使得等式前后的式子代表的向量越相似，即余弦值要最大</p><p>1° 将输入的三个单词转换为向量表示</p><p>2°对词汇表中所有单词进行遍历，计算ea-eb≈eb-e?</p><p>3°选择余弦值最大的即最相似时的单词作为推出的单词</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">complete_analogy</span><span class="params">(word_a,word_b,word_c,word_to_vec_map)</span>:</span></span><br><span class="line"><span class="comment">#因为词嵌入模型中单词均为小写，所以这里需要转换</span></span><br><span class="line">word_a,word_b,word_c=word_a.lower(),word_b.lower(),word_c.lower()</span><br><span class="line"><span class="comment">#将单词转换为向量进行运算</span></span><br><span class="line">e_a,e_b,e_c=word_to_vec_map[word_a],word_to_vec_map[word_b],word_to_vec_map[word_c]</span><br><span class="line"></span><br><span class="line"><span class="comment">#得到字典里所有单词</span></span><br><span class="line">words=word_to_vec_map.keys()</span><br><span class="line"></span><br><span class="line"><span class="comment">#找到最相近的距离</span></span><br><span class="line">max_cosine_sim=<span class="number">-100</span></span><br><span class="line">cosine_word=word_c</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line"><span class="keyword">if</span> word <span class="keyword">in</span> [word_a,word_b,word_c]:</span><br><span class="line"><span class="keyword">continue</span></span><br><span class="line"><span class="comment">#余弦相似度：越相似，角度越小，值越大</span></span><br><span class="line"><span class="comment">#cos值为1的时候最大，此时角度为0</span></span><br><span class="line">cosine_sim=cosine_similarity((e_a-e_b),(e_c-word_to_vec_map[word]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> cosine_sim&gt;max_cosine_sim:</span><br><span class="line">max_cosine_sim=cosine_sim</span><br><span class="line">cosine_word=word</span><br><span class="line"><span class="keyword">return</span> cosine_word</span><br></pre></td></tr></table></figure><p>结果：</p><p>输入一组词，进行类比推理：</p><blockquote><p>triads_to_try = [(‘italy’, ‘italian’, ‘spain’), (‘india’, ‘delhi’, ‘japan’), (‘man’, ‘woman’, ‘boy’), (‘happy’, ‘baby’, ‘beautiful’)]<br>for triad in triads_to_try:<br>    print (‘{} -&gt; {} &lt;====&gt; {} -&gt; {}’.format( * triad, complete_analogy( * triad,word_to_vec_map)))</p></blockquote><p>得出结果：</p><blockquote><p>italy -&gt; italian &lt;====&gt; spain -&gt; spanish<br>india -&gt; delhi &lt;====&gt; japan -&gt; tokyo<br>man -&gt; woman &lt;====&gt; boy -&gt; girl<br>happy -&gt; baby &lt;====&gt; beautiful -&gt; newborn</p></blockquote><p>可以看出类比出的词距离最相近，即相似。</p><p>（4）消除词嵌入中的偏差（选学，了解即可）</p><p>因为社会中存在年龄、性别等的歧视，所以在大量文本中学习到的词嵌入容易存在偏差，因此从以下两方面可以消除存在的偏差。</p><p>以性别歧视带来的偏差为例子</p><p>1°中和步：没有明确性别含义的词</p><p>设单词word的词嵌入为e，消除e和g之间的偏差，因为e与性别无关，即需要减少其到非性别轴上的距离差，即将e沿着g的方向归0即可</p><p>公式：<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200623171310.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#4.消除词嵌入中的偏差</span></span><br><span class="line"><span class="comment">#中和步:没有明确性别的单词</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">neutralize</span><span class="params">(word,g,word_to_vec_map)</span>:</span></span><br><span class="line">word=word.lower()</span><br><span class="line">e=word_to_vec_map[word]</span><br><span class="line">np_dot=np.dot(e,g)</span><br><span class="line"><span class="comment">#square求平方</span></span><br><span class="line">e_bias=np.multiply(np.divide(np_dot,np.square(np.linalg.norm(g))),g)</span><br><span class="line">e_disbias=e-e_bias</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> e_disbias</span><br></pre></td></tr></table></figure><p>2°均衡步：有明确性别含义的词</p><p>将具有明确性别含义的一对单词，使得他们到非性别轴的距离相等，只有性别才能将其区分。</p><p>公式：<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200623171348.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#均衡步：明确性别的单词</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">equalize</span><span class="params">(pairs,bias_axis,word_to_vec_map)</span>:</span></span><br><span class="line">w1,w2=pairs</span><br><span class="line">e_w1,e_w2=word_to_vec_map[w1],word_to_vec_map[w2]</span><br><span class="line">mu=np.divide((e_w1+e_w2),<span class="number">2</span>)</span><br><span class="line">mub=np.divide(np.multiply(np.dot(mu,bias_axis),bias_axis),np.square(np.linalg.norm(bias_axis)))</span><br><span class="line">mu_orth=mu-mub</span><br><span class="line">e_w1b=np.divide(np.multiply(np.dot(e_w1,bias_axis),bias_axis),np.square(np.linalg.norm(bias_axis)))</span><br><span class="line">e_w2b=np.divide(np.multiply(np.dot(e_w2,bias_axis),bias_axis),np.square(np.linalg.norm(bias_axis)))</span><br><span class="line">e_w1cor=np.multiply(np.sqrt(np.abs(<span class="number">1</span>-np.square(np.linalg.norm(mu_orth)))),np.divide(e_w1b-mub,np.abs((e_w1-mu_orth)-mub)))</span><br><span class="line">e_w2cor=np.multiply(np.sqrt(np.abs(<span class="number">1</span>-np.square(np.linalg.norm(mu_orth)))),np.divide(e_w2b-mub,np.abs((e_w2-mu_orth)-mub)))</span><br><span class="line">e1=e_w1cor+mu_orth</span><br><span class="line">e2=e_w2cor+mu_orth</span><br><span class="line"><span class="keyword">return</span> e1,e2</span><br></pre></td></tr></table></figure><h2 id="利用词嵌入搭建NLP系统"><a href="#利用词嵌入搭建NLP系统" class="headerlink" title="利用词嵌入搭建NLP系统"></a>利用词嵌入搭建NLP系统</h2><h3 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h3><p>利用已训练的词嵌入模型，根据输入的句子，判断句子代表的emoji表情，设定emoji标签为5个。</p><h3 id="词嵌入与数据集"><a href="#词嵌入与数据集" class="headerlink" title="词嵌入与数据集"></a>词嵌入与数据集</h3><p>（1）导入词嵌入</p><p>将单词和单词代表的golve向量作为该模型中的词嵌入，并将每个单词在单词表中的索引进行存储。num=len(word_to_vec_map[‘.’])</p><p>每个使用词嵌入的词的维度为(num,1)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#0.定义词向量</span></span><br><span class="line">word_to_index, index_to_word, word_to_vec_map=emo_utils.read_glove_vecs(<span class="string">'./data/glove.6B.50d.txt'</span>)</span><br></pre></td></tr></table></figure><p>（2）训练集</p><p>1°x：从表格中读取每行的句子构成句子数组</p><p>2°y：读入每个句子的标签值，因为emoji有五个标签，这里将y标签值转换为1 * 5的独热向量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.导入数据</span></span><br><span class="line">X_train,Y_train=emo_utils.read_csv(<span class="string">'./data/train_emoji.csv'</span>)</span><br><span class="line">X_test,Y_test=emo_utils.read_csv(<span class="string">'./data/test.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将y转换为独热向量</span></span><br><span class="line">Y_oh_train=emo_utils.convert_to_one_hot(Y_train,C=<span class="number">5</span>)</span><br><span class="line">Y_oh_test=emo_utils.convert_to_one_hot(Y_test,C=<span class="number">5</span>)</span><br></pre></td></tr></table></figure><h3 id="简单模型"><a href="#简单模型" class="headerlink" title="简单模型"></a>简单模型</h3><p>将句子中每个单词使用词嵌入表示，每个词的维度为（num,1），假设句子中有k个单词，则将这k个单词的词嵌入竖向堆叠起来，取均值，再经过softmax进行预测得到结果。</p><p>网络模型</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200623163047.png" alt></p><p>softmax网络传播</p><p>1°定义softmax层的权重和偏置量，由于输出值为（1，5）的独热向量，偏置量b的维度为（n_y,1）n_y=5，权重w的维度为（n_h,n_y），均值后avg的维度为（50,），n_h=50</p><p>w:（50，5）</p><p>b:（5，1）</p><p>2°在每次迭代中，分别对所有句子遍历，得到每个单词词嵌入的平均数，再进行正向传播，计算损失，反向传播，计算梯度</p><p>网络结构：avg-&gt;linear-&gt;a-&gt;softmax-&gt;yhat</p><p>3°得到句子中每个单词词嵌入的平均数：拆分句子中的单词，对所有单词遍历，得到每个单词的词嵌入再除以单词总数，得到平均值作为网络的输入</p><p>（1）将输入的单词转换为glove向量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#2.将输入的句子拆分为单词并用glove词向量，并对句子中每个单词取平均得到整个句子代表的向量</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sentence_to_avg</span><span class="params">(sentence,word_to_vec_map)</span>:</span></span><br><span class="line"><span class="comment">#拆分句子</span></span><br><span class="line">words=sentence.lower().split()</span><br><span class="line"></span><br><span class="line">m=len(word_to_vec_map[<span class="string">'.'</span>])</span><br><span class="line">add_sum=np.zeros((m,))</span><br><span class="line">count=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">add_sum+=word_to_vec_map[word]</span><br><span class="line">count+=<span class="number">1</span></span><br><span class="line">avg=add_sum/count</span><br><span class="line"><span class="keyword">return</span> avg</span><br></pre></td></tr></table></figure><p>（2）搭建模型，将每个句子代表的向量输入softmax单元中进行训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#3.构建模型</span></span><br><span class="line"><span class="comment">#x:(m,1)   y: (m,1)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X,Y,word_to_vec_map,learning_rate=<span class="number">0.01</span>,num_iteritions=<span class="number">400</span>)</span>:</span></span><br><span class="line"><span class="comment">#初始化参数</span></span><br><span class="line"></span><br><span class="line">m=Y.shape[<span class="number">0</span>]</span><br><span class="line">n_y=<span class="number">5</span></span><br><span class="line">n_h=len(word_to_vec_map[<span class="string">'.'</span>])</span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">W=np.random.randn(n_y,n_h)/np.sqrt(n_h)</span><br><span class="line">b=np.zeros((n_y,))</span><br><span class="line"></span><br><span class="line"><span class="comment">#y值转换为独热向量</span></span><br><span class="line">Y_oh=emo_utils.convert_to_one_hot(Y,C=n_y)</span><br><span class="line">costs=[]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#每次迭代</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(num_iteritions):</span><br><span class="line"><span class="comment">#拆分每个句子</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">avg=sentence_to_avg(X[i],word_to_vec_map)</span><br><span class="line"></span><br><span class="line"><span class="comment">#正向传播计算softmax 利用得到的均值</span></span><br><span class="line">z=np.dot(W,avg)+b</span><br><span class="line">a=emo_utils.softmax(z)</span><br><span class="line">cost=-np.sum(Y_oh[i]*np.log(a))</span><br><span class="line"></span><br><span class="line"><span class="comment">#反向传播</span></span><br><span class="line">dz=a-Y_oh[i]</span><br><span class="line">dw=np.dot(dz.reshape(n_y,<span class="number">1</span>),avg.reshape(<span class="number">1</span>,n_h))</span><br><span class="line">db=dz</span><br><span class="line"></span><br><span class="line">W=W-learning_rate*dw</span><br><span class="line">b=b-learning_rate*db</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> t%<span class="number">100</span>==<span class="number">0</span>:</span><br><span class="line">print(<span class="string">"第"</span>+str(t)+<span class="string">"次，损失为："</span>+str(cost))</span><br><span class="line">costs.append(cost)</span><br><span class="line">pred=emo_utils.predict(X,Y,W,b,word_to_vec_map)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> pred,W,b</span><br></pre></td></tr></table></figure><p>（3）结果</p><p>将训练集和词嵌入模型传入，定义学习率为0.01，迭代次数400次，最终得到训练好的网络参数（w,b），传入训练集和测试集，得到准确率为</p><blockquote><p>pred, W, b = model(X_train, Y_train, word_to_vec_map)<br>print(“训练集：”)<br>pred_train=emo_utils.predict(X_train,Y_train,W,b,word_to_vec_map)<br>print(“测试集：”)<br>pred_test=emo_utils.predict(X_test,Y_test,W,b,word_to_vec_map)</p></blockquote><blockquote><p>训练集：<br>Accuracy: 0.9772727272727273<br>测试集：<br>Accuracy: 0.8571428571428571</p></blockquote><p>选择一些句子转换为数组进行预测，得到emoji表情，发现you are not happy预测为 ❤️，可见该模型没有考虑词的顺序，只根据happy得到了预测，没有考虑前面的not，因此采用考虑词序的RNN模型对其进行改进。</p><h3 id="RNN模型"><a href="#RNN模型" class="headerlink" title="RNN模型"></a>RNN模型</h3><p>网络模型</p><p>总体思路：将每个句子乘上词嵌入矩阵得到特征向量，作为RNN的输入，每个时刻都考虑了上一个顺序的输入，最终的到输出预测值。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200623163134.png" alt></p><p>实际中网络结构稍有变化，采用两层隐藏层，为了防止过拟合，每层的输出都会经过dropout，随机丢弃一些节点，第一层隐藏层输入Tx个数据，输出Tx个数据。第二层隐藏层输入Tx个数据，输出1个数据，即最终的预测结果。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200623194624.png" alt></p><p>1°按照最大长度的句子单词数补0：由于传入网络的单词数量固定，所以需要补0</p><p>对m个句子，按照max_len大小生成（m,max_len）的补0后的单词矩阵（以单词位置作为矩阵的值）：</p><p>对每个句子，拆分其中单词，得到每个单词的索引进行存储</p><p>2°将单词矩阵乘以词嵌入矩阵E得到嵌入的特征向量表示e：每一行是一个向量表示：</p><p>根据每个单词的索引位置得到在词嵌入中的向量表示，并存储在嵌入矩阵E中；</p><p>利用keras.layer中的Embedding层得到每个单词的词嵌入表示，该层不需要训练，则设置trainable=False，输入词汇表的大小，输出词嵌入向量的大小，建立该层，并设置该层的权重为嵌入矩阵E，最终能返回每个单词的词嵌入。</p><p>注：此步骤是根据Ox * E =ec，求其中的E，而简单模型中是直接使用了词向量。</p><p>3°搭建模型：</p><p>根据输入的最大单词的长度，定义网络的输入；</p><p>根据已经训练的词向量得到词嵌入矩阵E，将网络输入传入其中，得到词嵌入，作为真正的输入LSTM单元中的数据；</p><p>定义LSTM-&gt;Dropout-&gt;LSTM-&gt;Dropout-&gt;Dense-&gt;Activation(sotfmax)，最终的到输出</p><p>（1）填充每个句子：以最大长度的句子为基准，在末尾进行补0</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#采用RNN模型构建考虑词顺序的预测网络</span></span><br><span class="line"><span class="comment">#1.将短的词按照最长的长度填充</span></span><br><span class="line"><span class="comment">#传入句子数组及其索引</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sentences_to_indices</span><span class="params">(X,word_to_index,max_len)</span>:</span></span><br><span class="line">m=X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">X_indices=np.zeros((m,max_len))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">words=X[i].lower().split()</span><br><span class="line">j=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">X_indices[i,j]=word_to_index[word]</span><br><span class="line">j=j+<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> X_indices</span><br></pre></td></tr></table></figure><p>（2）得到词嵌入矩阵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#2.构建embedding层，构建词向量矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pretrained_embedding_layer</span><span class="params">(word_to_vec_map,word_to_index)</span>:</span></span><br><span class="line">vocab_size=len(word_to_index)+<span class="number">1</span> <span class="comment">#加1是为了考虑句子没有到最大长度的0向量</span></span><br><span class="line">em_size=len(word_to_vec_map[<span class="string">'.'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#每个单词的词向量进行堆叠</span></span><br><span class="line">emb_matrix=np.zeros((vocab_size,em_size))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> word,index <span class="keyword">in</span> word_to_index.items():</span><br><span class="line">emb_matrix[index,:]=word_to_vec_map[word]</span><br><span class="line"></span><br><span class="line"><span class="comment">#输入词汇表大小，词向量维度,不用训练</span></span><br><span class="line">embedding_layer=Embedding(vocab_size,em_size,trainable=<span class="literal">False</span>)</span><br><span class="line">embedding_layer.build((<span class="literal">None</span>,))</span><br><span class="line">embedding_layer.set_weights([emb_matrix])</span><br><span class="line"><span class="keyword">return</span> embedding_layer</span><br></pre></td></tr></table></figure><p>（3）搭建模型，根据词嵌入矩阵进行两层lstm传播</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#3.搭建lstm模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Emojifier_V2</span><span class="params">(input_shape,word_to_vec_map,word_to_index)</span>:</span></span><br><span class="line">sentence_indices=Input(input_shape,dtype=<span class="string">'float32'</span>)</span><br><span class="line"></span><br><span class="line">embedding_layer=pretrained_embedding_layer(word_to_vec_map,word_to_index)</span><br><span class="line"></span><br><span class="line">embeddings=embedding_layer(sentence_indices)</span><br><span class="line"></span><br><span class="line">X=LSTM(<span class="number">128</span>,return_sequences=<span class="literal">True</span>)(embeddings)</span><br><span class="line">X=Dropout(<span class="number">0.5</span>)(X)</span><br><span class="line">X=LSTM(<span class="number">128</span>,return_sequences=<span class="literal">False</span>)(X)</span><br><span class="line">X=Dropout(<span class="number">0.5</span>)(X)</span><br><span class="line"></span><br><span class="line">X=Dense(<span class="number">5</span>)(X)</span><br><span class="line">X=Activation(activation=<span class="string">'softmax'</span>)(X)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model=Model(inputs=sentence_indices,outputs=X)</span><br><span class="line"><span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><p>（4）结果</p><p>综上，将得到的模型进行编译（compile），将输入的训练集进行补0，输入模型中（fit）（指明batch_size,epochs，则模型训练完毕，得到训练集精度接近1：</p><blockquote><p>Epoch 50/50</p><p> 32/132 [======&gt;…………………..] - ETA: 0s - loss: 0.2743 - accuracy: 0.9688<br>132/132 [==============================] - 0s 558us/step - loss: 0.1291 - accuracy: 0.9697</p><p>32/56 [================&gt;………….] - ETA: 0s<br>56/56 [==============================] - 0s 2ms/step</p></blockquote><p>输入测试集数据（evaluate），得到模型预测的损失和精度为：</p><blockquote><p>X_test_indices=sentences_to_indices(X_test,word_to_index,max_len)</p><p>loss,acc=model.evaluate(X_test_indices,Y_oh_test)</p><p>print(“test accuracy:”,acc)</p></blockquote><blockquote><p>test accuracy: 0.875</p></blockquote><p>自己写一些句子进行预测，发现not happy可以正确进行预测了，改善了上面基本模型出现的问题：预测结果i am not happy 😞，正确率也比基本模型提高了。</p><h3 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h3><p>（1）简单模型中是将训练的词向量求和得到均值作为网络的输入，而RNN模型需要将每个句子补成一样长，通过训练的词向量得到一个总的嵌入矩阵E，通过keras得到每个单词的词嵌入（一个个进行计算的），得到的结果作为网络的输入。</p><p>（2）简单模型没有考虑词序，RNN模型考虑了词序，所以预测的时候回准确些。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;词嵌入&quot;&gt;&lt;a href=&quot;#词嵌入&quot; class=&quot;headerlink&quot; title=&quot;词嵌入&quot;&gt;&lt;/a&gt;词嵌入&lt;/h2&gt;&lt;h3 id=&quot;什么是词嵌入&quot;&gt;&lt;a href=&quot;#什么是词嵌入&quot; class=&quot;headerlink&quot; title=&quot;什么是词嵌入&quot;&gt;&lt;/a&gt;什么是词嵌入&lt;/h3&gt;&lt;p&gt;以词的特征来表示各个单词，即单词的高维度特征表示。&lt;/p&gt;&lt;p&gt;与one-hot表示词相比：&lt;/p&gt;&lt;p&gt;（1）能更好的反映词的特征，增强相关词的泛化能力（例如学习了orange juice，后面会很容易根据apple推测出apple juice，因为apple和orange的特征表示很像），而使用one-hot表示，词之前没有联系，无法泛化&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="循环神经网络" scheme="https://www.xiapf.com/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>序列模型RNN——构建即兴演奏的爵士音乐的LSTM模型</title>
    <link href="https://www.xiapf.com/blogs/rnn1Music/"/>
    <id>https://www.xiapf.com/blogs/rnn1Music/</id>
    <published>2020-06-19T06:31:48.000Z</published>
    <updated>2020-10-27T06:03:09.299Z</updated>
    
    <content type="html"><![CDATA[<p>为了解决梯度消失和获得更深层的连接，采用keras.layers.recurrent中的LSTM模型构建神经单元。</p><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>利用已有的音乐片段，搭建能够生成即兴音乐的模型。</p><h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><h3 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h3><p>导入本地的原始音乐，并从中提取音节和音调</p><p>这里的x的维度为（m,T_x,n_values），m代表样本个数，T_x代表时间步，n_values代表字典大小，每个样本被表示成n_values大小的独热向量，这里隐藏层大小设置为64，即n_a=64。</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.导入数据</span></span><br><span class="line"><span class="comment">#x.shape: (60, 30, 78)</span></span><br><span class="line"><span class="comment">#y.shape (30, 60, 78)</span></span><br><span class="line"><span class="comment">#n_values代表字典数量，indies_values代表字典</span></span><br><span class="line">X,Y,n_values,indies_values=load_music_utils()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_music_utils</span><span class="params">()</span>:</span></span><br><span class="line">    chords, abstract_grammars = get_musical_data(<span class="string">'data/original_metheny.mid'</span>)</span><br><span class="line">    corpus, tones, tones_indices, indices_tones = get_corpus_data(abstract_grammars)</span><br><span class="line">    N_tones = len(set(corpus))</span><br><span class="line">    X, Y, N_tones = data_processing(corpus, tones_indices, <span class="number">60</span>, <span class="number">30</span>)   </span><br><span class="line">    <span class="keyword">return</span> (X, Y, N_tones, indices_tones)</span><br></pre></td></tr></table></figure><p>导入数据后得到总样本、实际输出、字典的个数、具体字典的内容。</p><h3 id="利用keras搭建基本神经单元"><a href="#利用keras搭建基本神经单元" class="headerlink" title="利用keras搭建基本神经单元"></a>利用keras搭建基本神经单元</h3><p>（1）重构每个时间步的x</p><p>（2）定义单步传播的神经单元LSTM</p><p>（3）定义最终输出的预测函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#2.利用keras搭建lstm模型</span></span><br><span class="line">n_a=<span class="number">64</span></span><br><span class="line">reshapor=Reshape((<span class="number">1</span>,n_values))</span><br><span class="line">LSTM_cell=LSTM(n_a,return_state=<span class="literal">True</span>)</span><br><span class="line">densor=Dense(n_values,activation=<span class="string">'softmax'</span>)</span><br></pre></td></tr></table></figure><h3 id="搭建模型"><a href="#搭建模型" class="headerlink" title="搭建模型"></a>搭建模型</h3><p>（0）定义输入的X，当前初始的激活值的记忆细胞值</p><p>（1）在每个时间步下</p><p>（2）创建一个临时函数，提取出适当的独热向量作为keras.layers对象：x=Lambda(lambda x:X[:,t,])(X)</p><p>（3）重构当前时间步x大小</p><p>（4）调用LSTM_cell，经过一个lstm单元传播，得到激活值和记忆细胞值</p><p>（5）调用densor，得到输出值out</p><p>（6）将每个时间步的输出值进行保存</p><p>（7）最终生成模型：model=Model(inputs=[X,a0,c0],outputs=outputs)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">djmodel</span><span class="params">(T_x,n_a,n_values)</span>:</span></span><br><span class="line"><span class="comment">#定义输入数据的维度</span></span><br><span class="line">X=Input((T_x,n_values))</span><br><span class="line">a0=Input(shape=(n_a,),name=<span class="string">'a0'</span>)</span><br><span class="line">c0=Input(shape=(n_a,),name=<span class="string">'c0'</span>)</span><br><span class="line">a=a0</span><br><span class="line">c=c0</span><br><span class="line"></span><br><span class="line">outputs=[]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(T_x):</span><br><span class="line">x=Lambda(<span class="keyword">lambda</span> x:X[:,t,])(X)</span><br><span class="line">x=reshapor(x)</span><br><span class="line"></span><br><span class="line">a,_,c=LSTM_cell(x,initial_state=[a,c])</span><br><span class="line"></span><br><span class="line">out=densor(a)</span><br><span class="line"></span><br><span class="line">outputs.append(out)</span><br><span class="line">model=Model(inputs=[X,a0,c0],outputs=outputs)</span><br><span class="line"><span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><h3 id="编译并训练模型"><a href="#编译并训练模型" class="headerlink" title="编译并训练模型"></a>编译并训练模型</h3><p>（1）编译模型，指定优化算法（包含其餐宿）、损失和需要打印的准确度</p><p>（2）定义初始伪激活值，伪记忆细胞（零向量）</p><p>（3）将之前的数据集放入模型中训练，并使用ecophs指定迭代次数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#编译模型</span></span><br><span class="line">m,T_x,_=X.shape</span><br><span class="line">model=djmodel(T_x,n_a,n_values)</span><br><span class="line">opt=Adam(lr=<span class="number">0.01</span>,beta_1=<span class="number">0.9</span>,beta_2=<span class="number">0.99</span>,decay=<span class="number">0.01</span>)</span><br><span class="line">model.compile(optimizer=opt,loss=<span class="string">"categorical_crossentropy"</span>,metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">a0=np.zeros((m,n_a))</span><br><span class="line">c0=np.zeros((m,n_a))</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练模型</span></span><br><span class="line">start=time.clock()</span><br><span class="line">model.fit([X,a0,c0],list(Y),epochs=<span class="number">100</span>)</span><br><span class="line">end=time.clock()</span><br><span class="line">print(<span class="string">"模型训练时间："</span>,end-start)</span><br></pre></td></tr></table></figure><h3 id="采样和预测"><a href="#采样和预测" class="headerlink" title="采样和预测"></a>采样和预测</h3><p>采样</p><p>（1）采用已经训练好的模型的lstm单元，densor输出单元</p><p>（2）定义初始输入、伪激活值，伪记忆细胞（零向量）</p><p>（3）lstm单元：在每个时间步，根据lstm单元得出激活值和记忆细胞值</p><p>（4）densor单元：根据激活值得到输出，</p><p>（5）定义一个Lambda层，将输出转换为独热向量，作为下一个时间步的输入：</p><blockquote><p>x=Lambda(one_hot)(out)</p><p>def one_hot(x):<br>    x = K.argmax(x)<br>    x = tf.one_hot(x, 78)<br>    x = RepeatVector(1)(x)<br>    return x</p></blockquote><p>最终得到采样后的模型结果，通过输入的LSTM_cell单元，densor单元，字典长度，隐藏层大小，输出值大小得到采样的结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#采样与预测</span></span><br><span class="line"><span class="comment">#采样</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">music_inference_model</span><span class="params">(LSTM_cell,densor,n_values,n_a,T_y)</span>:</span></span><br><span class="line"><span class="comment">#T_y时间步长</span></span><br><span class="line">x0=Input((<span class="number">1</span>,n_values))</span><br><span class="line">a0=Input(shape=(n_a,),name=<span class="string">'a0'</span>)</span><br><span class="line">c0=Input(shape=(n_a,),name=<span class="string">'c0'</span>)</span><br><span class="line">a=a0</span><br><span class="line">c=c0</span><br><span class="line">x=x0</span><br><span class="line"></span><br><span class="line">outputs=[]</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(T_y):</span><br><span class="line">a,_,c=LSTM_cell(x,initial_state=[a,c])</span><br><span class="line">out=densor(a)</span><br><span class="line"><span class="comment">#上一个输出作为下一个输出入</span></span><br><span class="line">x=Lambda(one_hot)(out)</span><br><span class="line">outputs.append(out)</span><br><span class="line">inference_model=Model(inputs=[x0,a0,c0],outputs=outputs)</span><br><span class="line"><span class="keyword">return</span> inference_model</span><br><span class="line"></span><br><span class="line">inference_model=music_inference_model(LSTM_cell,densor,n_values,n_a,T_y=<span class="number">50</span>)</span><br><span class="line">x_initializer=np.zeros((<span class="number">1</span>,<span class="number">1</span>,n_values))</span><br><span class="line">a_initializer=np.zeros((<span class="number">1</span>,n_a))</span><br><span class="line">c_initializer=np.zeros((<span class="number">1</span>,n_a))</span><br></pre></td></tr></table></figure><p>预测</p><p>（1）根据采样的模型，输入的样本，激活值、记忆单元</p><p>（2）使用采样模型预测，得到输出值</p><p>（3）使用argmax取最大的索引</p><p>（4）最终将输出表示为独热向量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#预测</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_and_sanmple</span><span class="params">(inference_model,x_initializer=x_initializer,a_initializer=a_initializer,c_initializer=c_initializer)</span>:</span></span><br><span class="line"><span class="comment">#模型预测</span></span><br><span class="line">pred=inference_model.predict([x_initializer,a_initializer,c_initializer])</span><br><span class="line"><span class="comment">#取最大值</span></span><br><span class="line">pred_value=np.argmax(pred,axis=<span class="number">-1</span>)</span><br><span class="line"><span class="comment">#转换为独热向量</span></span><br><span class="line">one_hot=to_categorical(pred_value,num_classes=n_values)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> one_hot,pred_value</span><br><span class="line"></span><br><span class="line">results,indices=predict_and_sanmple(inference_model,x_initializer,a_initializer,c_initializer)</span><br><span class="line">print(<span class="string">"np.argmax(results[12]) ="</span>, np.argmax(results[<span class="number">12</span>]))</span><br><span class="line">print(<span class="string">"np.argmax(results[17]) ="</span>, np.argmax(results[<span class="number">17</span>]))</span><br><span class="line">print(<span class="string">"list(indices[12:18]) ="</span>, list(indices[<span class="number">12</span>:<span class="number">18</span>]))</span><br></pre></td></tr></table></figure><p>预测结果：</p><p>np.argmax(results[12]) = 72<br>np.argmax(results[17]) = 39<br>list(indices[12:18]) = [array([72]), array([39]), array([46]), array([57]), array([72]), array([39])]</p><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p><code>out_stream=generate_music(inference_model)</code>，利用已经生成的模型得到的采样结果在本地生成即兴的音乐，序列模型生成音乐值，采用额外库函数生成midi文件。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200619122826.png" alt></p><p>可以利用在线midi转mp3工具：<a href="https://www.ofoct.com/audio-converter/convert-midi-to-mp3-or-wav-ogg-aac-wma.html" target="_blank" rel="external nofollow noopener noreferrer">https://www.ofoct.com/audio-converter/convert-midi-to-mp3-or-wav-ogg-aac-wma.html</a> 转换文件格式，并进行试听。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;为了解决梯度消失和获得更深层的连接，采用keras.layers.recurrent中的LSTM模型构建神经单元。&lt;/p&gt;&lt;h2 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;问题描述&lt;/h2&gt;&lt;p&gt;利用已有的音乐片段，搭建能够生成即兴音乐的模型。&lt;/p&gt;&lt;h2 id=&quot;步骤&quot;&gt;&lt;a href=&quot;#步骤&quot; class=&quot;headerlink&quot; title=&quot;步骤&quot;&gt;&lt;/a&gt;步骤&lt;/h2&gt;&lt;h3 id=&quot;导入数据&quot;&gt;&lt;a href=&quot;#导入数据&quot; class=&quot;headerlink&quot; title=&quot;导入数据&quot;&gt;&lt;/a&gt;导入数据&lt;/h3&gt;&lt;p&gt;导入本地的原始音乐，并从中提取音节和音调&lt;/p&gt;&lt;p&gt;这里的x的维度为（m,T_x,n_values），m代表样本个数，T_x代表时间步，n_values代表字典大小，每个样本被表示成n_values大小的独热向量，这里隐藏层大小设置为64，即n_a=64。&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="循环神经网络" scheme="https://www.xiapf.com/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>序列模型RNN——构建字符级语言模型</title>
    <link href="https://www.xiapf.com/blogs/rnn1DN/"/>
    <id>https://www.xiapf.com/blogs/rnn1DN/</id>
    <published>2020-06-19T03:17:25.000Z</published>
    <updated>2020-10-27T06:03:16.388Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>（1）字符级语言模型：指每个字母（字符）采用一个独热向量表示，即每个时间步中输入的是一个字符，来使用RNN构建模型。</p><p>（2）使用多对多构建一个能给恐龙岛上恐龙命名的语言模型。</p><h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><h3 id="构建字典"><a href="#构建字典" class="headerlink" title="构建字典"></a>构建字典</h3><p>（1）打开所有可以命名的名字数据集，将名字全部转换为小写字母</p><p>（2）按照字母：索引和索引：字母形式生成字典对应表，方便查找字母</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#数据集处理</span></span><br><span class="line"><span class="comment">#得到不重复的字典</span></span><br><span class="line">data=open(<span class="string">'dinos.txt'</span>,<span class="string">'r'</span>).read()</span><br><span class="line">data=data.lower()</span><br><span class="line">char=list(set(data))</span><br><span class="line">data_size,vocab_size=len(data),len(char)</span><br><span class="line">print(data_size,vocab_size)</span><br><span class="line">ix_to_char=&#123;ix:char <span class="keyword">for</span> ix,char <span class="keyword">in</span> enumerate(sorted(char))&#125;</span><br><span class="line">char_to_ix=&#123;char:ix <span class="keyword">for</span> ix,char <span class="keyword">in</span> enumerate(sorted(char))&#125;</span><br><span class="line">print(char_to_ix)</span><br><span class="line">print(ix_to_char)</span><br></pre></td></tr></table></figure><blockquote><p>19909 27<br>{‘\n’: 0, ‘a’: 1, ‘b’: 2, ‘c’: 3, ‘d’: 4, ‘e’: 5, ‘f’: 6, ‘g’: 7, ‘h’: 8, ‘i’: 9, ‘j’: 10, ‘k’: 11, ‘l’: 12, ‘m’: 13, ‘n’: 14, ‘o’: 15, ‘p’: 16, ‘q’: 17, ‘r’: 18, ‘s’: 19, ‘t’: 20, ‘u’: 21, ‘v’: 22, ‘w’: 23, ‘x’: 24, ‘y’: 25, ‘z’: 26}<br>{0: ‘\n’, 1: ‘a’, 2: ‘b’, 3: ‘c’, 4: ‘d’, 5: ‘e’, 6: ‘f’, 7: ‘g’, 8: ‘h’, 9: ‘i’, 10: ‘j’, 11: ‘k’, 12: ‘l’, 13: ‘m’, 14: ‘n’, 15: ‘o’, 16: ‘p’, 17: ‘q’, 18: ‘r’, 19: ‘s’, 20: ‘t’, 21: ‘u’, 22: ‘v’, 23: ‘w’, 24: ‘x’, 25: ‘y’, 26: ‘z’}</p></blockquote><p>得到19909个名字中27个不重复的字符，将字符按照字典顺序排序，回车键在第0个，后面字符依次排列。</p><h3 id="梯度修剪"><a href="#梯度修剪" class="headerlink" title="梯度修剪"></a>梯度修剪</h3><p>将梯度中过大或者过小的梯度直接设置为最大值和最小值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#梯度修剪</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clip</span><span class="params">(gradients,maxValue)</span>:</span></span><br><span class="line">dWax=gradients[<span class="string">"dWax"</span>]</span><br><span class="line">dWaa=gradients[<span class="string">"dWaa"</span>]</span><br><span class="line">dWya=gradients[<span class="string">"dWya"</span>]</span><br><span class="line"></span><br><span class="line">db=gradients[<span class="string">"db"</span>]</span><br><span class="line">dby=gradients[<span class="string">"dby"</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> gradient <span class="keyword">in</span> [dWax,dWaa,dWya,db,dby]:</span><br><span class="line">np.clip(gradient,-maxValue,maxValue,out=gradient)</span><br><span class="line">gradients=&#123;<span class="string">'dWax'</span>:dWax,<span class="string">'dWaa'</span>:dWaa,<span class="string">'dWya'</span>:dWya,<span class="string">'db'</span>:db,<span class="string">'dby'</span>:dby&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><h3 id="采样"><a href="#采样" class="headerlink" title="采样"></a>采样</h3><p>采样时，已经得到当前模型的训练参数，将回车键作为采样结束的标志</p><p>（1）当采样未结束的时候</p><p>（2）输入独热向量0向量和第0层的激活值，根据RNN基本单元公式，得到预测值y</p><p>（3）进行随机采样（np.random.choice）选择最大概率的索引值</p><p>（4）将选中的值转换为新的独热向量，作为下一个时间步的输入</p><p>（5）不断循环，当到末尾时得到当前的采样结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#采样</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(parameters,char_to_ix,seed)</span>:</span></span><br><span class="line">Wax=parameters[<span class="string">'Wax'</span>]</span><br><span class="line">Waa=parameters[<span class="string">'Waa'</span>]</span><br><span class="line">b=parameters[<span class="string">'b'</span>]</span><br><span class="line">Wya=parameters[<span class="string">'Wya'</span>]</span><br><span class="line">by=parameters[<span class="string">'by'</span>]</span><br><span class="line"></span><br><span class="line">vocab_size=by.shape[<span class="number">0</span>]</span><br><span class="line">n_a=Waa.shape[<span class="number">1</span>]</span><br><span class="line">a_prev=np.zeros((n_a,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#创造独热向量</span></span><br><span class="line">x=np.zeros((vocab_size,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">counter=<span class="number">0</span></span><br><span class="line"></span><br><span class="line">idx=<span class="number">-1</span></span><br><span class="line">indies=[]</span><br><span class="line">check_character=char_to_ix[<span class="string">'\n'</span>]</span><br><span class="line"><span class="keyword">while</span> idx!=check_character <span class="keyword">and</span> counter&lt;<span class="number">50</span>:</span><br><span class="line">a=np.tanh(np.dot(Waa,a_prev)+np.dot(Wax,x)+b)</span><br><span class="line">z=np.dot(Wya,a)+by</span><br><span class="line">y=cu.softmax(z)</span><br><span class="line"></span><br><span class="line"><span class="comment">#按照概率采样</span></span><br><span class="line">np.random.seed(seed+counter)</span><br><span class="line">idx=np.random.choice(list(range(vocab_size)),p=y.ravel())</span><br><span class="line">indies.append(idx)</span><br><span class="line"></span><br><span class="line"><span class="comment">#按照选择的位置生成独热向量</span></span><br><span class="line">x=np.zeros((vocab_size,<span class="number">1</span>))</span><br><span class="line">x[idx]=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#将上一个预测值直接作为下一个的输入</span></span><br><span class="line">a_prev=a</span><br><span class="line">seed+=<span class="number">1</span></span><br><span class="line">counter+=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> counter==<span class="number">50</span>:</span><br><span class="line">indies.append(char_to_ix[<span class="string">'\n'</span>])</span><br><span class="line"><span class="keyword">return</span> indies</span><br></pre></td></tr></table></figure><h3 id="搭建语言模型（一个样本）"><a href="#搭建语言模型（一个样本）" class="headerlink" title="搭建语言模型（一个样本）"></a>搭建语言模型（一个样本）</h3><p>（1）输入当前的样本的所有字符，实际输出值，上一层的激活值，参数</p><p>（2）进行一次RNN前向传播，将当前样本所有字符一一对应每个时间步，得到损失和缓存</p><p>（3）进行一次RNN反向传播，得到梯度和激活值</p><p>（4）进行一次梯度更新，得到新的梯度</p><p>（5）对得到的新梯度进行修剪，防止梯度爆炸</p><p>（6）最终返回当前样本的损失，梯度，最终的激活值输出和参数</p><p>tips：当字典中值被改变，无论在函数内还是函数外都会影响字典中的值，因此这里可以不返回parameters</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#构建语言模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span><span class="params">(x,y,a_prev,parameters,learning_rate=<span class="number">0.01</span>,maxValue=<span class="number">5</span>)</span>:</span></span><br><span class="line"><span class="comment">#前向传播</span></span><br><span class="line">loss,cache=cu.rnn_forward(x,y,a_prev,parameters,vocab_size=<span class="number">27</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#反向传播</span></span><br><span class="line">gradients,a=cu.rnn_backward(x,y,parameters,cache)</span><br><span class="line"></span><br><span class="line"><span class="comment">#修剪梯度</span></span><br><span class="line">gradients=clip(gradients,maxValue)</span><br><span class="line"></span><br><span class="line"><span class="comment">#更新参数</span></span><br><span class="line">parameters=cu.update_parameters(parameters,gradients,learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> loss,gradients,a[len(x)<span class="number">-1</span>],parameters</span><br></pre></td></tr></table></figure><h3 id="生成模型（所有样本）"><a href="#生成模型（所有样本）" class="headerlink" title="生成模型（所有样本）"></a>生成模型（所有样本）</h3><p>（1）读入当前的数据，并打乱</p><p>（2）初始化参数和损失</p><p>（3）在每次迭代中，每次随机选择一个样本进行RNN模型构建</p><p>随机选择样本：</p><p>​        index=i%len(examples)<br>​        x=[None]+[char_to_ix[ch] for ch in examples[index]]<br>​        y=x[1:]+[char_to_ix[‘\n’]]</p><p>0<br>[None, 20, 21, 18, 9, 1, 19, 1, 21, 18, 21, 19]<br>[20, 21, 18, 9, 1, 19, 1, 21, 18, 21, 19, 0]</p><p>为了保证，输入输出一样长，这里将x最前面加上空字符，输出值取实际名字并加上回车符作为结束。</p><p>（4）根据每次选择的一个样本，进行一个语言模型构建</p><p>这里采用延迟来平滑损失，加速训练</p><p>（5）每2000次进行一次采样进行模型预测，最终得到模型训练的参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#运行算法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(data,ix_to_char,char_to_ix,num_iterations=<span class="number">3500</span>,n_a=<span class="number">50</span>,vocab_size=<span class="number">27</span>,dinos_name=<span class="number">7</span>)</span>:</span></span><br><span class="line">n_x,n_y=vocab_size,vocab_size</span><br><span class="line"></span><br><span class="line"><span class="comment">#读入当前样本</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'dinos.txt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">examples=f.readlines()</span><br><span class="line">examples=[x.lower().strip() <span class="keyword">for</span> x <span class="keyword">in</span> examples]</span><br><span class="line">print(len(examples))</span><br><span class="line"></span><br><span class="line"><span class="comment">#打乱样本</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">np.random.shuffle(examples)</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化参数和损失</span></span><br><span class="line">parameters=cu.initialize_parameters(n_a,n_x,n_y)</span><br><span class="line">loss=cu.get_initial_loss(vocab_size,dinos_name)</span><br><span class="line"></span><br><span class="line">a_prev=np.zeros((n_a,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#进行迭代</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations):</span><br><span class="line">index=i%len(examples)</span><br><span class="line">x=[<span class="literal">None</span>]+[char_to_ix[ch] <span class="keyword">for</span> ch <span class="keyword">in</span> examples[index]]</span><br><span class="line">y=x[<span class="number">1</span>:]+[char_to_ix[<span class="string">'\n'</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment">#字典无论在函数内或函数外发生改变，这些改变都会作用于字典</span></span><br><span class="line">cur_loss,gradients,a_prev,parameters=optimize(x,y,a_prev,parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment">#平滑损失</span></span><br><span class="line">loss=cu.smooth(loss,cur_loss)</span><br><span class="line"></span><br><span class="line"><span class="comment">#每2000次进行一次采样</span></span><br><span class="line"><span class="keyword">if</span> i%<span class="number">2000</span>==<span class="number">0</span>:</span><br><span class="line">print(<span class="string">"第"</span>+str(i)+<span class="string">"次，损失为"</span>+str(loss))</span><br><span class="line">seed=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> name <span class="keyword">in</span> range(dinos_name):</span><br><span class="line">sample_indies=sample(parameters,char_to_ix,seed)</span><br><span class="line">cu.print_sample(sample_indies,ix_to_char)</span><br><span class="line">seed+=<span class="number">1</span></span><br><span class="line">print(<span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>（6）结果</p><p>输入字典对应表和迭代次数，进行训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(data, ix_to_char, char_to_ix, num_iterations=<span class="number">3500</span>)</span><br></pre></td></tr></table></figure><p>模型训练的结果：</p><p>得到每2000次的损失和当前预测的7个恐龙的名字</p><blockquote><p>第0次，损失为23.087336085484605<br>Nkzxwtdmfqoeyhsqwasjkjvu<br>Kneb<br>Kzxwtdmfqoeyhsqwasjkjvu<br>Neb<br>Zxwtdmfqoeyhsqwasjkjvu<br>Eb<br>Xwtdmfqoeyhsqwasjkjvu</p><p>第2000次，损失为27.884160491415773<br>Liusskeomnolxeros<br>Hmdaairus<br>Hytroligoraurus<br>Lecalosapaus<br>Xusicikoraurus<br>Abalpsamantisaurus<br>Tpraneronxeros</p></blockquote><p>使用模型训练参数进行预测：</p><p>根据需要预测的恐龙的名字数量进行迭代，输入训练好的参数和字典表得到预测索引，根据索引：字典得到对应的名字。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dinos_name=<span class="number">7</span></span><br><span class="line">seed=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> name <span class="keyword">in</span> range(dinos_name):</span><br><span class="line">sample_indies=sample(parameters,char_to_ix,seed)</span><br><span class="line">cu.print_sample(sample_indies,ix_to_char)</span><br><span class="line">seed+=<span class="number">1</span></span><br><span class="line">print(<span class="string">"\n"</span>)</span><br></pre></td></tr></table></figure><p>得到预测的恐龙的名字为：</p><blockquote><p>Livusaton<br>Hola<br>Ivushandoraunpsacrophus<br>Lecakosaunus<br>Wusnchepokupros<br>Acalpsasaurus<br>Torandor</p></blockquote><h2 id="附：构造莎士比亚风格的语言模型"><a href="#附：构造莎士比亚风格的语言模型" class="headerlink" title="附：构造莎士比亚风格的语言模型"></a>附：构造莎士比亚风格的语言模型</h2><p>大致思路：</p><p>（1）导入莎士比亚的诗歌，作为训练样本</p><p>（2）利用已搭建大莎士比亚诗歌模型，该模型上已训练1000次，只需要导入，再训练一次即可，即将数据导入到模型中使用，再根据输入的数据利用会掉函数输入模型中进行训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print_callback=LambdaCallback(on_epoch_end=on_epoch_end)</span><br><span class="line">model.fit(x,y,batch_size=<span class="number">128</span>,epochs=<span class="number">1</span>,callbacks=[print_callback])</span><br></pre></td></tr></table></figure><p>（3）该模型需要输入初始激活值，得到最终的预测结果</p><p>这里输入初始单词为：bajie，得到生成的诗歌如下：</p><blockquote><p>Write the beginning of your poem, the Shakespeare machine will complete it. Your input is: bajie</p><p>Here is your poem: </p><p>bajienst fauch is the grace,<br>the roud by sorst then every where gise,<br>gave your lover wadese bust may o ficcle,<br>and ou may but would relaved be ta niell<br>live anbsad i forse itferse mads,<br> from ho strenst my grast of thight crors,<br>and to thop hersiang thises to hesp ather,<br>and ho my to thy filens express wold’s faver,<br>do this bay of dowh i connied too,<br>my live the wherat be forse thishuror,<br>it som as wh</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;问题描述&lt;/h2&gt;&lt;p&gt;（1）字符级语言模型：指每个字母（字符）采用一个独热向量表示，即每个时间步中输入的是一个字符，来使用RNN构建模型。&lt;/p&gt;&lt;p&gt;（2）使用多对多构建一个能给恐龙岛上恐龙命名的语言模型。&lt;/p&gt;&lt;h2 id=&quot;步骤&quot;&gt;&lt;a href=&quot;#步骤&quot; class=&quot;headerlink&quot; title=&quot;步骤&quot;&gt;&lt;/a&gt;步骤&lt;/h2&gt;&lt;h3 id=&quot;构建字典&quot;&gt;&lt;a href=&quot;#构建字典&quot; class=&quot;headerlink&quot; title=&quot;构建字典&quot;&gt;&lt;/a&gt;构建字典&lt;/h3&gt;&lt;p&gt;（1）打开所有可以命名的名字数据集，将名字全部转换为小写字母&lt;/p&gt;&lt;p&gt;（2）按照字母：索引和索引：字母形式生成字典对应表，方便查找字母&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="循环神经网络" scheme="https://www.xiapf.com/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>序列模型RNN——基本网络结构</title>
    <link href="https://www.xiapf.com/blogs/rnn1/"/>
    <id>https://www.xiapf.com/blogs/rnn1/</id>
    <published>2020-06-18T14:10:26.000Z</published>
    <updated>2020-10-27T06:03:29.357Z</updated>
    
    <content type="html"><![CDATA[<h2 id="RNN原理"><a href="#RNN原理" class="headerlink" title="RNN原理"></a>RNN原理</h2><p>（0）标记符号含义</p><p>输入的样本为（X,Y），其中X的维度为（m，T_x，n），m代表样本个数，T_x代表时间步，n代表字典长度。</p><p>RNN中每个样本采用独热向量表示，在训练集中选择不重复数据作为字典，根据字典构造每个独热向量（当前词在字典中的位置标为1，其余标为0）。</p><p>（1）网络结构</p><p>RNN网络结果如图，在t时刻输入当前时刻的x值，经过隐藏层，通过计算前一层的激活值和输入值x得到预测的y。</p><a id="more"></a><p>每个时刻的输出取决于当前时刻的输入和前一个时刻的激活值。</p><p>1° 多对多结构</p><p>很多输入对应很多输出，当Tx=Ty，即输入和输出数量相同，例如命名实体识别（根据一个句子识别出其中的主语）</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618165122.png" alt></p><p>2° 多对多结构</p><p>很多输入对应很多输出，当Tx不等于Ty，即输入和输出数量不相同，例如机器翻译。</p><p>因为机器翻译需要考虑整个句子的前后联系，所以要将x全部输入后才能进行预测。因此，前半部分按照时间步输入x，作为编码器，后半部分输出预测的y，作为解码器。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618195021.png" alt></p><p>3° 一对多结构</p><p>输入部分片段或者不输入（0向量），输出形成的完整的输出，例如生成音乐。</p><p>根据输入的x和激活值得到预测的y，上一层预测的y作为下一层的输入。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618195910.png" alt></p><p>4° 多对一结构</p><p>很多输入对应有个输出，例如情感分类问题，给一段文字，给出评分。</p><p>每次输入一个时间步的x，但不输出预测值，当所有的x均按照时间步输入之后，最后得到一个评分，该评分综合考虑了签名所有的文字。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618200114.png" alt></p><p>（2）前向传播</p><p>以多对多中，Tx=Ty为例说明RNN的前向传播。</p><p>在t=1的时间步下：</p><p>1° 有输入a&lt;0&gt;激活值（伪激活值，常设置为0），和时间步为1的x值：x&lt;1&gt;，将两者乘以对应权重，加上偏置量得到线性值，再使用激活函数（tanh/Relu），得到当前单元输出的激活值。</p><p>2° 将激活值a&lt;1&gt;乘以对应权重，加上偏置量得到线性值，再使用激活函数（softmax）,得到最终预测的时间步为1的y的预测值。</p><p>后面的神经元步骤类似：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618165220.png" alt></p><p>编码时，首先根据前向传播公式得到一个基本RNN单元处理的结果，将前一个和当前激活值，当前输入的x和参数存成缓存，将当前输出的激活值、预测值、缓存作为最终的输出。</p><p>接着，按照时间步，传入当前输入和上一层的激活值输出，得到所有时间步的激活值和预测值，最终将所有时间步的激活值和预测值和缓存输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#搭建标准rnn单元</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_cell_forward</span><span class="params">(xt,a_prev,parameters)</span>:</span></span><br><span class="line">Wax=parameters[<span class="string">"Wax"</span>]</span><br><span class="line">Waa=parameters[<span class="string">"Waa"</span>]</span><br><span class="line">Wya=parameters[<span class="string">"Wya"</span>]</span><br><span class="line"></span><br><span class="line">ba=parameters[<span class="string">"ba"</span>]</span><br><span class="line">by=parameters[<span class="string">"by"</span>]</span><br><span class="line"></span><br><span class="line">a_next=np.tanh(np.dot(Waa,a_prev)+np.dot(Wax,xt)+ba)</span><br><span class="line"></span><br><span class="line">y_next=ru.softmax(np.dot(Wya,a_next)+by)</span><br><span class="line"></span><br><span class="line">cache=(a_next,a_prev,xt,parameters)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> a_next,y_next,cache</span><br><span class="line"></span><br><span class="line"><span class="comment">#将所有标准单元连接起来，进行前向传播</span></span><br><span class="line"><span class="comment">#这里Tx=Ty</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_forward</span><span class="params">(x,a0,parameters)</span>:</span></span><br><span class="line">n_x,m,T_x=x.shape</span><br><span class="line">Wya=parameters[<span class="string">"Wya"</span>]</span><br><span class="line">n_y,n_a=Wya.shape</span><br><span class="line"></span><br><span class="line">a_next=a0</span><br><span class="line">a=np.zeros((n_a,m,T_x))</span><br><span class="line">y_hat=np.zeros((n_y,m,T_x))</span><br><span class="line">caches=[]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(T_x):</span><br><span class="line">a_next,y_next,cache=rnn_cell_forward(x[:,:,t],a_next,parameters)</span><br><span class="line"></span><br><span class="line">a[:,:,t]=a_next</span><br><span class="line"></span><br><span class="line">y_hat[:,:,t]=y_next</span><br><span class="line"></span><br><span class="line">caches.append(cache)</span><br><span class="line">caches=(caches,x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> a,y_hat,caches</span><br></pre></td></tr></table></figure><p>（3）反向传播</p><p>反向传播需要从最后开始（for t in reversed(range(T_x))），求得所有参数的梯度。计算和激活值和输入值相关的梯度即可，通过损失调整相应权重和偏置量的值。</p><p>根据对tanh(a)求导等于(1-tanh^2) * da，易求得Wax,Waa,xt,ba,a&lt; t-1 &gt;的梯度如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618170023.png" alt></p><p>编码时，也是先得出一个基本神经元的反向传播过程，再根据时间步以此类推得到其他的，并将梯度修正并进行爆粗。</p><p>注：对激活值的推导一直要得到最开始的激活值即da0</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#标准rnn的反向传播</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#单个单元的反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_cell_backward</span><span class="params">(da_next,cache)</span>:</span></span><br><span class="line">a_next,a_prev,xt,parameters=cache</span><br><span class="line"></span><br><span class="line">Wax=parameters[<span class="string">"Wax"</span>]</span><br><span class="line">Waa=parameters[<span class="string">"Waa"</span>]</span><br><span class="line">ba=parameters[<span class="string">"ba"</span>]</span><br><span class="line"><span class="comment"># Wya=parameters["Wya"]</span></span><br><span class="line"><span class="comment"># by=parameters["by"]</span></span><br><span class="line"></span><br><span class="line">dtanh=(<span class="number">1</span>-np.square(a_next))*da_next</span><br><span class="line"></span><br><span class="line">dxt=np.dot(Wax.T,dtanh)</span><br><span class="line">dWax=np.dot(dtanh,xt.T)</span><br><span class="line"></span><br><span class="line">da_prev=np.dot(Waa.T,dtanh)</span><br><span class="line">dWaa=np.dot(dtanh,a_prev.T)</span><br><span class="line"></span><br><span class="line">dba=np.sum(dtanh,axis=<span class="number">-1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># dWya=np.dot(dy,dtanh.T)</span></span><br><span class="line"><span class="comment"># dyb=np.sum(dtanh,axis=1,keepdims=True)</span></span><br><span class="line"></span><br><span class="line">gradients=&#123;<span class="string">'dxt'</span>:dxt,<span class="string">'da_prev'</span>:da_prev,<span class="string">'dWaa'</span>:dWaa,<span class="string">'dWax'</span>:dWax,<span class="string">'dba'</span>:dba&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> gradients</span><br><span class="line"></span><br><span class="line"><span class="comment">#一个序列的反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_backward</span><span class="params">(da,caches)</span>:</span></span><br><span class="line">caches,x=caches</span><br><span class="line">a1,a0,x1,parameters=caches[<span class="number">0</span>]</span><br><span class="line">n_x,m=x1.shape</span><br><span class="line">n_a,m,T_x=da.shape</span><br><span class="line"></span><br><span class="line">dx=np.zeros((n_x,m,T_x))</span><br><span class="line">dWaa=np.zeros((n_a,n_a))</span><br><span class="line">dWax=np.zeros((n_a,n_x))</span><br><span class="line">dba=np.zeros((n_a,<span class="number">1</span>))</span><br><span class="line">da0=np.zeros((n_a,m))</span><br><span class="line">da_prevt=np.zeros((n_a,m))</span><br><span class="line"></span><br><span class="line"><span class="comment">#倒序</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> reversed(range(T_x)):</span><br><span class="line">gradient=rnn_cell_backward(da[:,:,t]+da_prevt,caches[t])</span><br><span class="line">dxt,da_prevt,dWaat,dWaxt,dbat=gradient[<span class="string">'dxt'</span>],gradient[<span class="string">'da_prev'</span>],gradient[<span class="string">'dWaa'</span>],gradient[<span class="string">'dWax'</span>],gradient[<span class="string">'dba'</span>]</span><br><span class="line"></span><br><span class="line">dx[:,:,t]=dxt</span><br><span class="line">dWaa+=dWaat</span><br><span class="line">dWax+=dWaxt</span><br><span class="line">dba+=dbat</span><br><span class="line">da0=da_prevt</span><br><span class="line"></span><br><span class="line">gradients=&#123;<span class="string">'dx'</span>:dx,<span class="string">'da0'</span>:da0,<span class="string">'dWaa'</span>:dWaa,<span class="string">'dWax'</span>:dWax,<span class="string">'dba'</span>:dba&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><h2 id="基本RNN网络存在问题"><a href="#基本RNN网络存在问题" class="headerlink" title="基本RNN网络存在问题"></a>基本RNN网络存在问题</h2><p>当网络深度变深，权重将以指数形式扩大或减小，会出现以下几个问题：</p><p>（1）梯度爆炸——解决方法：梯度修剪</p><p>当出现梯度爆炸时，在使用当前梯度更新参数之后，对更新后的参数判断是否在[- 最大值，最大值]之间（这里的最大值和最小值需要用户给出），当不在时，按照最大值和最小值设置，这样保证梯度不会变的很大。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#梯度修剪</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clip</span><span class="params">(gradients,maxValue)</span>:</span></span><br><span class="line">dWax=gradients[<span class="string">"dWax"</span>]</span><br><span class="line">dWaa=gradients[<span class="string">"dWaa"</span>]</span><br><span class="line">dWya=gradients[<span class="string">"dWya"</span>]</span><br><span class="line"></span><br><span class="line">db=gradients[<span class="string">"db"</span>]</span><br><span class="line">dby=gradients[<span class="string">"dby"</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> gradient <span class="keyword">in</span> [dWax,dWaa,dWya,db,dby]:</span><br><span class="line">np.clip(gradient,-maxValue,maxValue,out=gradient)</span><br><span class="line">gradients=&#123;<span class="string">'dWax'</span>:dWax,<span class="string">'dWaa'</span>:dWaa,<span class="string">'dWya'</span>:dWya,<span class="string">'db'</span>:db,<span class="string">'dby'</span>:dby&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><p>（2）梯度消失——解决方法：修改网络的基本单元</p><p>采用GRU单元或者LSTM单元，能更好捕捉深层连接</p><h2 id="LSTM原理"><a href="#LSTM原理" class="headerlink" title="LSTM原理"></a>LSTM原理</h2><p>（1）网络结构</p><p>LSTM单元和基本RNN单元类似，但其中加入了记忆细胞，每个神经元都会有一个记忆细胞，并输出一个新记忆细胞值，这样保证即使很深，后面的值也会有前面神经元的值。</p><p>LSTM中设置更新门，遗忘门，根据更新门决定保留新的记忆细胞的部分，根据遗忘门决定保留前一个记忆细胞的部分，通过记忆细胞贯穿所有时间步，从而获得更远的连接。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618170626.png" alt></p><p>（2）前向传播</p><p>1° 记忆细胞根据当前时间步x&lt; t &gt;和上一个输出激活值得出新的记忆细胞</p><p>2° 将更新门、遗忘门、输出门按照权重得到最新的值</p><p>3° 根据更新门、遗忘门得到当前的记忆细胞</p><p>4° 根据输出门和当前记忆细胞德奥激活值</p><p>5° 最终经过softmax得到最终的预测</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618170550.png" alt></p><p>编码同上面基本RNN单元，得出单一LSTM单元的前向传播，再对所有时间步求得预测值。</p><p>注：这里有个小技巧，将前一个激活值a_prev和当前x进行堆叠，减少参数个数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#加入LTSM解决梯度消失的问题</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#搭建单个lstm单元</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_cell_forward</span><span class="params">(xt,a_prev,c_prev,parameters)</span>:</span></span><br><span class="line">Wf=parameters[<span class="string">'Wf'</span>]</span><br><span class="line">bf=parameters[<span class="string">'bf'</span>]</span><br><span class="line">Wu=parameters[<span class="string">'Wu'</span>]</span><br><span class="line">bu=parameters[<span class="string">'bu'</span>]</span><br><span class="line">Wc=parameters[<span class="string">'Wc'</span>]</span><br><span class="line">bc=parameters[<span class="string">'bc'</span>]</span><br><span class="line">Wo=parameters[<span class="string">'Wo'</span>]</span><br><span class="line">bo=parameters[<span class="string">'bo'</span>]</span><br><span class="line">Wy=parameters[<span class="string">'Wy'</span>]</span><br><span class="line">by=parameters[<span class="string">'by'</span>]</span><br><span class="line"></span><br><span class="line">n_a,m=a_prev.shape</span><br><span class="line">n_xt,m=xt.shape</span><br><span class="line"></span><br><span class="line">ax_prev=np.zeros(((n_a+n_xt),m))</span><br><span class="line"></span><br><span class="line"><span class="comment">#将a_prev和xt堆叠起来</span></span><br><span class="line">ax_prev[:n_a,:]=a_prev</span><br><span class="line">ax_prev[n_a:,:]=xt</span><br><span class="line"></span><br><span class="line"><span class="comment">#遗忘门</span></span><br><span class="line">ft=ru.sigmoid(np.dot(Wf,ax_prev)+bf)</span><br><span class="line"><span class="comment">#更新门</span></span><br><span class="line">ut=ru.sigmoid(np.dot(Wu,ax_prev)+bu)</span><br><span class="line"></span><br><span class="line"><span class="comment">#候选值</span></span><br><span class="line">cct=np.tanh(np.dot(Wc,ax_prev)+bc)</span><br><span class="line"><span class="comment">#记忆细胞</span></span><br><span class="line">c_next=ut*cct+ft*c_prev</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出门</span></span><br><span class="line">ot=ru.sigmoid(np.dot(Wo,ax_prev)+bo)</span><br><span class="line"></span><br><span class="line">a_next=ot*np.tanh(c_next)</span><br><span class="line"></span><br><span class="line">y_next=ru.softmax(np.dot(Wy,a_next)+by)</span><br><span class="line"></span><br><span class="line">cache=(a_next,c_next,a_prev,c_prev,ft,ut,cct,ot,xt,parameters)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> a_next,c_next,y_next,cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#lstm的前向传播</span></span><br><span class="line"><span class="comment">#c0使用0来初始化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_forward</span><span class="params">(x,a0,parameters)</span>:</span></span><br><span class="line">caches=[]</span><br><span class="line"></span><br><span class="line">n_x,m,T_x=x.shape</span><br><span class="line">Wy=parameters[<span class="string">'Wy'</span>]</span><br><span class="line">n_y,n_a=Wy.shape</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a=np.zeros((n_a,m,T_x))</span><br><span class="line">c=np.zeros((n_a,m,T_x))</span><br><span class="line">y_hat=np.zeros((n_y,m,T_x))</span><br><span class="line"></span><br><span class="line">a_next=a0</span><br><span class="line">c_next=np.zeros((n_a,m))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(T_x):</span><br><span class="line">a_next,c_next,y_next,cache=lstm_cell_forward(x[:,:,t],a_next,c_next,parameters)</span><br><span class="line"></span><br><span class="line">a[:,:,t]=a_next</span><br><span class="line">c[:,:,t]=c_next</span><br><span class="line">y_hat[:,:,t]=y_next</span><br><span class="line"></span><br><span class="line">caches.append(cache)</span><br><span class="line"></span><br><span class="line">caches=(caches,x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> a,y_hat,c,caches</span><br></pre></td></tr></table></figure><p>（3）反向传播</p><p>反向从最后前向求得所有的梯度值（for t in reversed(range(T_x))），输出值y中的权重，偏置量不更新。</p><p>1° 求三个门和记忆细胞的梯度</p><p>输出门：根据最终输出的激活值公式以及对sigmoid的求导得出</p><p>记忆细胞候选值、更新门、遗忘门：一部分受求当前记忆细胞的影响，一部分受求激活值的影响</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618170127.png" alt></p><p>2° 各个权重系数的梯度，根据1中求得的梯度能很容易求到</p><p>同理，各个偏置量梯度也可求得</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618170142.png" alt></p><p>3° 上一个激活值，上一个记忆细胞候选值，当前输入值的梯度</p><p>上一个激活值和记忆细胞候选值、更新门、遗忘门、输出门均有联系，所以将各个梯度相加</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618170213.png" alt></p><p>上一个记忆细胞候选值一部分受求当前记忆细胞的影响，一部分受求激活值的影响</p><p>当前输入值的梯度和记忆细胞候选值、更新门、遗忘门、输出门均有联系，所以将各个梯度相加</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618170231.png" alt></p><p>编码同上面基本RNN单元，得出单一LSTM单元的反向传播，再对所有时间步求得梯度。</p><p>注：前向传播中用的小技巧，将前一个激活值a_prev和当前x进行堆叠，减少参数个数。求梯度的时候要将值进行拆分求梯度，即将权重前部分用来求a_prev，后部分用于求xt。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#lstm单个单元的反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_cell_backward</span><span class="params">(da_next,dc_next,cache)</span>:</span></span><br><span class="line">a_next,c_next,a_prev,c_prev,ft,ut,cct,ot,xt,parameters=cache</span><br><span class="line">n_a,m=a_next.shape</span><br><span class="line"></span><br><span class="line">Wf=parameters[<span class="string">'Wf'</span>]</span><br><span class="line">bf=parameters[<span class="string">'bf'</span>]</span><br><span class="line">Wu=parameters[<span class="string">'Wu'</span>]</span><br><span class="line">bu=parameters[<span class="string">'bu'</span>]</span><br><span class="line">Wc=parameters[<span class="string">'Wc'</span>]</span><br><span class="line">bc=parameters[<span class="string">'bc'</span>]</span><br><span class="line">Wo=parameters[<span class="string">'Wo'</span>]</span><br><span class="line">bo=parameters[<span class="string">'bo'</span>]</span><br><span class="line">Wy=parameters[<span class="string">'Wy'</span>]</span><br><span class="line">by=parameters[<span class="string">'by'</span>]</span><br><span class="line"></span><br><span class="line">dot=da_next*np.tanh(c_next)*ot*(<span class="number">1</span>-ot)</span><br><span class="line">dcct=(dc_next*ut+ot*(<span class="number">1</span>-np.square(np.tanh(c_next)))*ut*da_next)*(<span class="number">1</span>-np.square(cct))</span><br><span class="line">dut=(dc_next*cct+ot*(<span class="number">1</span>-np.square(np.tanh(c_next)))*cct*da_next)*ut*(<span class="number">1</span>-ut)</span><br><span class="line">dft=(dc_next*c_prev+ot*(<span class="number">1</span>-np.square(np.tanh(c_next)))*c_prev*da_next)*ft*(<span class="number">1</span>-ft)</span><br><span class="line"></span><br><span class="line">concat=np.concatenate((a_prev,xt),axis=<span class="number">0</span>).T</span><br><span class="line"></span><br><span class="line">dWf=np.dot(dft,concat)</span><br><span class="line">dbf=np.sum(dft,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">dWu=np.dot(dut,concat)</span><br><span class="line">dbu=np.sum(dut,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">dWc=np.dot(dcct,concat)</span><br><span class="line">dbc=np.sum(dcct,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">dWo=np.dot(dot,concat)</span><br><span class="line">dbo=np.sum(dot,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">da_prev=np.dot(Wf[:,:n_a].T,dft)+np.dot(Wu[:,:n_a].T,dut)+np.dot(Wc[:,:n_a].T,dcct)+np.dot(Wo[:,:n_a].T,dot)</span><br><span class="line">dc_prev=dc_next*ft+ot*(<span class="number">1</span>-np.square(np.tanh(c_next)))*ft*da_next</span><br><span class="line">dxt=np.dot(Wf[:,n_a:].T,dft)+np.dot(Wu[:,n_a:].T,dut)+np.dot(Wc[:,n_a:].T,dcct)+np.dot(Wo[:,n_a:].T,dot)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">gradients=&#123;<span class="string">'dxt'</span>:dxt,<span class="string">'da_prev'</span>:da_prev,<span class="string">'dc_prev'</span>:dc_prev,<span class="string">'dWf'</span>:dWf,<span class="string">'dbf'</span>:dbf,<span class="string">'dWu'</span>:dWu,<span class="string">'dbu'</span>:dbu,<span class="string">'dWc'</span>:dWc,<span class="string">'dbc'</span>:dbc,<span class="string">'dWo'</span>:dWo,<span class="string">'dbo'</span>:dbo&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> gradients</span><br><span class="line"></span><br><span class="line"><span class="comment">#lstm一个时间序列的反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_backward</span><span class="params">(da,caches)</span>:</span></span><br><span class="line">caches,x=caches</span><br><span class="line">a_next1,c_next1,a_prev1,c_prev1,ft1,ut1,cct1,ot1,xt1,parameters=caches[<span class="number">0</span>]</span><br><span class="line">n_a,m,T_x=da.shape</span><br><span class="line">n_x,m=xt1.shape</span><br><span class="line">n_c,m=c_next1.shape</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dx=np.zeros((n_x,m,T_x))</span><br><span class="line">da0=np.zeros((n_a,m))</span><br><span class="line"></span><br><span class="line">dWf=np.zeros((n_a,n_a+n_x))</span><br><span class="line">dWu=np.zeros((n_a,n_a+n_x))</span><br><span class="line">dWc=np.zeros((n_a,n_a+n_x))</span><br><span class="line">dWo=np.zeros((n_a,n_a+n_x))</span><br><span class="line">dbf=np.zeros((n_a,<span class="number">1</span>))</span><br><span class="line">dbu=np.zeros((n_a,<span class="number">1</span>))</span><br><span class="line">dbc=np.zeros((n_a,<span class="number">1</span>))</span><br><span class="line">dbo=np.zeros((n_a,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">da_prevt=np.zeros((n_a,m))</span><br><span class="line">dc_prevt=np.zeros((n_c,m))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> reversed(range(T_x)):</span><br><span class="line">gradient=lstm_cell_backward(da[:,:,t]+da_prevt,dc_prevt,caches[t])</span><br><span class="line">dxt,da_prevt,dc_prevt,dWft,dbft,dWut,dbut,dWct,dbct,dWot,dbot=gradient[<span class="string">'dxt'</span>],gradient[<span class="string">'da_prev'</span>],gradient[<span class="string">'dc_prev'</span>],gradient[<span class="string">'dWf'</span>],gradient[<span class="string">'dbf'</span>],gradient[<span class="string">'dWu'</span>],gradient[<span class="string">'dbu'</span>],gradient[<span class="string">'dWc'</span>],gradient[<span class="string">'dbc'</span>],gradient[<span class="string">'dWo'</span>],gradient[<span class="string">'dbo'</span>]</span><br><span class="line">dx[:,:,t]=dxt</span><br><span class="line">dWf+=dWft</span><br><span class="line">dbf+=dbft</span><br><span class="line">dWu+=dWut</span><br><span class="line">dbu+=dbut</span><br><span class="line">dWc+=dWct</span><br><span class="line">dWo+=dWot</span><br><span class="line">dbo+=dbot</span><br><span class="line">da0=da_prevt</span><br><span class="line"><span class="comment"># dc0=dc_prevt</span></span><br><span class="line">gradients=&#123;<span class="string">'dx'</span>:dx,<span class="string">'da0'</span>:da0,<span class="string">'dWf'</span>:dWf,<span class="string">'dbf'</span>:dbf,<span class="string">'dWu'</span>:dWu,<span class="string">'dbu'</span>:dbu,<span class="string">'dWc'</span>:dWc,<span class="string">'dbc'</span>:dbc,<span class="string">'dWo'</span>:dWo,<span class="string">'dbo'</span>:dbo&#125;</span><br><span class="line"><span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><h2 id="GRU原理"><a href="#GRU原理" class="headerlink" title="GRU原理"></a>GRU原理</h2><p>（1）网络结构</p><p>GRU可以看成是简化版的LSTM，去除了遗忘门，并且将激活值就作为当前记忆细胞，简化神经单元。</p><p>GRU中同样使用记忆细胞，并使用更新门，当前的新的记忆细胞使用更新门保留新的候选值，使用（1-更新门）保留之前的记忆细胞，并将得到的新记忆细胞作为激活值输出，并经过softmax得到预测值y。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618170407.png" alt></p><p>（2）前向传播</p><p>GRU采用a&lt; t &gt;=c&lt; t &gt;</p><p>1° 得到记忆细胞候选值，使用相关门衡量前一个记忆细胞和当前候选值的相关性</p><p>2° 计算更新门、更新门</p><p>3° 按照更新门来计算新的记忆细胞</p><p>4° 将记忆细胞新的值作为激活值，采用softmax得到预测值</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618170430.png" alt></p><h2 id="采样"><a href="#采样" class="headerlink" title="采样"></a>采样</h2><p>作用：合成数据，得到每个时间步的预测并传递给下一个神经元，从而得到训练模型后的预测结果。</p><p>当模型经过搭建、编译、训练之后，通过采样得到模型对新的数据（或者不输入数据）的预测值。</p><p>0° 初始采用零向量（独热向量）作为初始输入，根据模型训练好的参数进入循环</p><p>1° 根据RNN网络前向传播得到预测值y</p><p>2° 按照随机采样的方式取其中一个值作为预测值</p><p>3° 按照当前预测值的位置，生成独热向量作为下一个时间步的输入</p><p>4° 不断循环，就得到了合成数据，即使用训练好的模型预测的结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#采样</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(parameters,char_to_ix,seed)</span>:</span></span><br><span class="line">Wax=parameters[<span class="string">'Wax'</span>]</span><br><span class="line">Waa=parameters[<span class="string">'Waa'</span>]</span><br><span class="line">b=parameters[<span class="string">'b'</span>]</span><br><span class="line">Wya=parameters[<span class="string">'Wya'</span>]</span><br><span class="line">by=parameters[<span class="string">'by'</span>]</span><br><span class="line"></span><br><span class="line">vocab_size=by.shape[<span class="number">0</span>]</span><br><span class="line">n_a=Waa.shape[<span class="number">1</span>]</span><br><span class="line">a_prev=np.zeros((n_a,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#创造独热向量</span></span><br><span class="line">x=np.zeros((vocab_size,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">counter=<span class="number">0</span></span><br><span class="line"></span><br><span class="line">idx=<span class="number">-1</span></span><br><span class="line">indies=[]</span><br><span class="line">check_character=char_to_ix[<span class="string">'\n'</span>]</span><br><span class="line"><span class="keyword">while</span> idx!=check_character <span class="keyword">and</span> counter&lt;<span class="number">50</span>:</span><br><span class="line">a=np.tanh(np.dot(Waa,a_prev)+np.dot(Wax,x)+b)</span><br><span class="line">z=np.dot(Wya,a)+by</span><br><span class="line">y=cu.softmax(z)</span><br><span class="line"></span><br><span class="line"><span class="comment">#按照概率采样</span></span><br><span class="line">np.random.seed(seed+counter)</span><br><span class="line">idx=np.random.choice(list(range(vocab_size)),p=y.ravel())</span><br><span class="line">indies.append(idx)</span><br><span class="line"></span><br><span class="line"><span class="comment">#按照选择的位置生成独热向量</span></span><br><span class="line">x=np.zeros((vocab_size,<span class="number">1</span>))</span><br><span class="line">x[idx]=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#将上一个预测值直接作为下一个的输入</span></span><br><span class="line">a_prev=a</span><br><span class="line">seed+=<span class="number">1</span></span><br><span class="line">counter+=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> counter==<span class="number">50</span>:</span><br><span class="line">indies.append(char_to_ix[<span class="string">'\n'</span>])</span><br><span class="line"><span class="keyword">return</span> indies</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;RNN原理&quot;&gt;&lt;a href=&quot;#RNN原理&quot; class=&quot;headerlink&quot; title=&quot;RNN原理&quot;&gt;&lt;/a&gt;RNN原理&lt;/h2&gt;&lt;p&gt;（0）标记符号含义&lt;/p&gt;&lt;p&gt;输入的样本为（X,Y），其中X的维度为（m，T_x，n），m代表样本个数，T_x代表时间步，n代表字典长度。&lt;/p&gt;&lt;p&gt;RNN中每个样本采用独热向量表示，在训练集中选择不重复数据作为字典，根据字典构造每个独热向量（当前词在字典中的位置标为1，其余标为0）。&lt;/p&gt;&lt;p&gt;（1）网络结构&lt;/p&gt;&lt;p&gt;RNN网络结果如图，在t时刻输入当前时刻的x值，经过隐藏层，通过计算前一层的激活值和输入值x得到预测的y。&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="循环神经网络" scheme="https://www.xiapf.com/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络——神经风格转换</title>
    <link href="https://www.xiapf.com/blogs/convNet5/"/>
    <id>https://www.xiapf.com/blogs/convNet5/</id>
    <published>2020-06-10T03:10:55.000Z</published>
    <updated>2020-10-27T06:05:16.933Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>将一个图像作为内容图像，另一幅图作为风格图像，两个图像经过神经网络生辰一个新的图像，该图像既有前一个图像的内容，又有后一个图像的风格，这称为神经风格转换。</p><h2 id="损失函数的确定"><a href="#损失函数的确定" class="headerlink" title="损失函数的确定"></a>损失函数的确定</h2><p>神经网络浅层学习图片的点、线或者颜色等基础特征，深层学习图片中物体或者更具体的特征。</p><p>神经风格转换是让合成的图片中的内容更接近内容图片，风格更接近风格图片，即让合成的图片中的内容和内容图片相比损失更小，让合成的图片中的风格和风格图片相比损失更小，最终得到的总成本最小，通过成本函数调整网络的参数。</p><a id="more"></a><p>因此需要定义网络中的内容损失函数，风格损失函数，总成本函数。</p><h3 id="内容损失函数"><a href="#内容损失函数" class="headerlink" title="内容损失函数"></a>内容损失函数</h3><p>将每层的通道按照长宽进行展开，每个位置和内容图片的相应位置之间使用二者之差范数求内容之间的差异：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200610105338.png" alt></p><p>即将两个矩阵对应元素相减再求平方</p><p>说明：（1）最终内容损失函数前的系数可定义为归一化1/2，或者其他值。（因为最终成本函数会通过alpha调整系数）</p><p>（2）这里需要将激活值中的通道数放在前面，因为网络中导入模型的时候是将通道放在最前面处理的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#内容函数成本</span></span><br><span class="line"><span class="comment">#输入内容图像和合成图像的激活值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_content_cost</span><span class="params">(a_C,a_G)</span>:</span></span><br><span class="line">m,n_h,n_w,n_c=a_G.get_shape().as_list()</span><br><span class="line"></span><br><span class="line"><span class="comment">#保证通道在前</span></span><br><span class="line">a_C=tf.transpose(tf.reshape(a_C,(n_h*n_w,n_c)))</span><br><span class="line">a_G=tf.transpose(tf.reshape(a_G,(n_h*n_w,n_c)))</span><br><span class="line">C_G=tf.reduce_sum(tf.square(tf.subtract(a_C,a_G)))</span><br><span class="line"></span><br><span class="line">content_cost=<span class="number">1</span>/(<span class="number">4</span>*n_h*n_w*n_c)*C_G</span><br><span class="line"><span class="keyword">return</span> content_cost</span><br></pre></td></tr></table></figure><h3 id="风格损失函数"><a href="#风格损失函数" class="headerlink" title="风格损失函数"></a>风格损失函数</h3><p>（1）求单层的风格差异</p><p>得到当前图片按照通道展开的风格矩阵（使用相关系数衡量不同位置出现相应风格的概率），再按照范数定义风格之间的差异：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200610110834.png" alt></p><p>1° 求风格矩阵：即将展开的当前通道的矩阵对应元素相乘</p><p>2° 按照范数，将风格图片和合成图片的风格矩阵求差，取平方得到范数值，以此作为风格的损失，即风格之间的差异</p><p>（2）将各层的风格差异合并</p><p>多层风格合并的原因：多考虑几层的风格能使得结果更准确，选择的层数：style_layers=[(“conv1_1”,0.2),(“conv2_1”,0.2),(“conv3_1”,0.2),(“conv4_1”,0.2),(“conv5_1”,0.2)]</p><p>方法：遍历每层，得到激活值输入单层风格求差异，累加每层的风格差异，并乘上每层的系数（衡量每层的风格所占的比重，这里权重定量选择0.2），最终构成总体风格损失函数</p><p>注：为什么内容函数中没有使用多层合并，因为浅层网站中只学习了图形的基本样式，深层网络才学习到真正的图像内容。（个人的理解）</p><p>说明：（1）最终风格损失函数前的系数可定义为归一化1/(2 * nh * nw * nc)^2，或者其他值。（因为最终成本函数会通过beta调整系数）</p><p>（2）这里需要将激活值中的通道数放在前面，因为网络中导入模型的时候是将通道放在最前面处理的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#风格函数成本</span></span><br><span class="line"><span class="comment">#矩阵内元素相乘</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gram_matrix</span><span class="params">(A)</span>:</span></span><br><span class="line">gram=tf.matmul(A,tf.transpose(A))</span><br><span class="line"><span class="keyword">return</span> gram</span><br><span class="line"></span><br><span class="line"><span class="comment">#某一层的风格函数成本</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_layer_style_cost</span><span class="params">(a_S,a_G)</span>:</span></span><br><span class="line">m,n_h,n_w,n_c=a_G.get_shape().as_list()</span><br><span class="line"></span><br><span class="line">a_S=tf.transpose(tf.reshape(a_S,(n_h*n_w,n_c)))</span><br><span class="line">a_G=tf.transpose(tf.reshape(a_G,(n_h*n_w,n_c)))</span><br><span class="line"></span><br><span class="line">a_Svalue=gram_matrix(a_S)</span><br><span class="line">a_Gvalue=gram_matrix(a_G)</span><br><span class="line"></span><br><span class="line">S_G=tf.reduce_sum(tf.square(tf.subtract(a_Svalue,a_Gvalue)))</span><br><span class="line"></span><br><span class="line">style_layer_cost=<span class="number">1</span>/(<span class="number">4</span>*(n_c**<span class="number">2</span>)*((n_h*n_w))**<span class="number">2</span>)*S_G</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> style_layer_cost</span><br><span class="line"></span><br><span class="line"><span class="comment">#从不同的层得到风格函数成本进行合并，效果会更好</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_style_cost</span><span class="params">(model,style_layers)</span>:</span></span><br><span class="line">style_cost=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> layer,coeff <span class="keyword">in</span> style_layers:</span><br><span class="line">out=model[layer]</span><br><span class="line">a_S=sess.run(out)</span><br><span class="line">a_G=out</span><br><span class="line"></span><br><span class="line">style_layer_cost=compute_layer_style_cost(a_S,a_G)</span><br><span class="line"></span><br><span class="line">style_cost+=coeff*style_layer_cost</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> style_cost</span><br></pre></td></tr></table></figure><h3 id="总体损失函数"><a href="#总体损失函数" class="headerlink" title="总体损失函数"></a>总体损失函数</h3><p>将内容损失和风格损失乘以相应系数得到总成本函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#得到总成本</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">total_cost</span><span class="params">(J_content,J_style,alpha=<span class="number">10</span>,beta=<span class="number">40</span>)</span>:</span></span><br><span class="line">J_cost=alpha*J_content+J_style*beta</span><br><span class="line"><span class="keyword">return</span> J_cost</span><br></pre></td></tr></table></figure><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>（1）建立交互会话</p><p>使用交互式对话，可以先定义会话，再定义操作，较直接会话方式使用更方便</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#0.使用交互式会话及导入模型</span></span><br><span class="line">tf.reset_default_graph()</span><br><span class="line">sess=tf.InteractiveSession()<span class="comment">#tf.session是需要将操作都建立好才能进行会话操作，使用交互式会话可以先定义一个会话，再定义操作</span></span><br></pre></td></tr></table></figure><p>（2）导入模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model=nu.load_vgg_model(<span class="string">"./pretrained-model/imagenet-vgg-verydeep-19.mat"</span>)</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure><p>模型采用神经风格转换提出者论文中的模型，采用VGG-19模型，本地保存了该模型网络，打印如下，可见模型存储在字典中，字典的键中保存每个变量，值保存包含该变量值的张量，当需要通过网络运行图像的时候，使用tf.assign将图像输入网络即可</p><p>模型结构如下：</p><blockquote><p>{‘input’: &lt;tf.Variable ‘Variable:0’ shape=(1, 300, 400, 3) dtype=float32_ref&gt;, ‘conv1_1’: &lt;tf.Tensor ‘Relu:0’ shape=(1, 300, 400, 64) dtype=float32&gt;, ‘conv1_2’: &lt;tf.Tensor ‘Relu_1:0’ shape=(1, 300, 400, 64) dtype=float32&gt;, ‘avgpool1’: &lt;tf.Tensor ‘AvgPool:0’ shape=(1, 150, 200, 64) dtype=float32&gt;, ‘conv2_1’: &lt;tf.Tensor ‘Relu_2:0’ shape=(1, 150, 200, 128) dtype=float32&gt;, ‘conv2_2’: &lt;tf.Tensor ‘Relu_3:0’ shape=(1, 150, 200, 128) dtype=float32&gt;, ‘avgpool2’: &lt;tf.Tensor ‘AvgPool_1:0’ shape=(1, 75, 100, 128) dtype=float32&gt;, ‘conv3_1’: &lt;tf.Tensor ‘Relu_4:0’ shape=(1, 75, 100, 256) dtype=float32&gt;, ‘conv3_2’: &lt;tf.Tensor ‘Relu_5:0’ shape=(1, 75, 100, 256) dtype=float32&gt;, ‘conv3_3’: &lt;tf.Tensor ‘Relu_6:0’ shape=(1, 75, 100, 256) dtype=float32&gt;, ‘conv3_4’: &lt;tf.Tensor ‘Relu_7:0’ shape=(1, 75, 100, 256) dtype=float32&gt;, ‘avgpool3’: &lt;tf.Tensor ‘AvgPool_2:0’ shape=(1, 38, 50, 256) dtype=float32&gt;, ‘conv4_1’: &lt;tf.Tensor ‘Relu_8:0’ shape=(1, 38, 50, 512) dtype=float32&gt;, ‘conv4_2’: &lt;tf.Tensor ‘Relu_9:0’ shape=(1, 38, 50, 512) dtype=float32&gt;, ‘conv4_3’: &lt;tf.Tensor ‘Relu_10:0’ shape=(1, 38, 50, 512) dtype=float32&gt;, ‘conv4_4’: &lt;tf.Tensor ‘Relu_11:0’ shape=(1, 38, 50, 512) dtype=float32&gt;, ‘avgpool4’: &lt;tf.Tensor ‘AvgPool_3:0’ shape=(1, 19, 25, 512) dtype=float32&gt;, ‘conv5_1’: &lt;tf.Tensor ‘Relu_12:0’ shape=(1, 19, 25, 512) dtype=float32&gt;, ‘conv5_2’: &lt;tf.Tensor ‘Relu_13:0’ shape=(1, 19, 25, 512) dtype=float32&gt;, ‘conv5_3’: &lt;tf.Tensor ‘Relu_14:0’ shape=(1, 19, 25, 512) dtype=float32&gt;, ‘conv5_4’: &lt;tf.Tensor ‘Relu_15:0’ shape=(1, 19, 25, 512) dtype=float32&gt;, ‘avgpool5’: &lt;tf.Tensor ‘AvgPool_4:0’ shape=(1, 10, 13, 512) dtype=float32&gt;}</p></blockquote><p>（3）输入内容图片</p><p>从网络结构中可以看出，网络输入是300 * 400大小的图像，所以要定义好图像大小</p><p>使用PIL 中的open函数读取图片</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#图片大小为300*400</span></span><br><span class="line"><span class="comment">#1.导入内容图片</span></span><br><span class="line">image_path=<span class="string">"./images/"</span></span><br><span class="line">content_image=Image.open(image_path+<span class="string">"cat.jpg"</span>)</span><br><span class="line">content_image=nu.reshape_and_normalize_image(np.array(content_image))</span><br></pre></td></tr></table></figure><p>（4）输入风格图片</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#2.导入风格图片</span></span><br><span class="line">style_image=Image.open(image_path+<span class="string">"camp-nou.jpg"</span>)</span><br><span class="line">style_image=nu.reshape_and_normalize_image(np.array(style_image))</span><br></pre></td></tr></table></figure><p>（5）计算成本函数</p><p>内容成本：1° 将内容图片通过assign方法输入到模型中，并使用会话运行网络；</p><p>2° 网络一共5层，选择第四层的内容作为比对的内容，将模型运行该层得到的激活值作为a_C，即内容图片输出（需要比对的内容）</p><p>3° a_G这里使用对应层数的线性值作为占位符，当运行模型的时候，更新值</p><p>4° 最终输入内容损失函数中</p><p>风格成本：1° 将风格图片通过assign方法输入到模型中，并使用会话运行网络；</p><p>2° 网络一共5层，选择第1-5层的风格和系数作为比对的内容，将模型和需要比对的网络层输入到风格损失函数中</p><p>综上，将二者成本输入到总成本中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#4.分别计算三个成本</span></span><br><span class="line">sess.run(model[<span class="string">"input"</span>].assign(content_image))</span><br><span class="line">out=model[<span class="string">'conv4_2'</span>]</span><br><span class="line">a_C=sess.run(out)</span><br><span class="line">a_G=out</span><br><span class="line">J_content=compute_content_cost(a_C,a_G)</span><br><span class="line"></span><br><span class="line">sess.run(model[<span class="string">"input"</span>].assign(style_image))</span><br><span class="line">J_style=compute_style_cost(model,style_layers)</span><br><span class="line">J_cost=total_cost(J_content,J_style,alpha=<span class="number">10</span>,beta=<span class="number">40</span>)</span><br></pre></td></tr></table></figure><p>（6）定义网络的优化算法</p><p>选择adam优化算法和学习率，以及需要最小化的成本函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#5.定义优化算法</span></span><br><span class="line">optimizer=tf.train.AdamOptimizer(learning_rate=<span class="number">2</span>).minimize(J_cost)</span><br></pre></td></tr></table></figure><p>（7）搭建网络</p><p>0° 此时运行会话初始化所有变量</p><p>1° 将合成图片作为网络输入，并运行会话</p><p>2° 每次迭代都按照优化算法，并更新合成的图片，将每次的合成图片按照路径保存</p><p>3° 迭代结束得到最终合成效果图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#6.搭建总体模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_nn</span><span class="params">(sess,input_image,iteration=<span class="number">200</span>)</span>:</span></span><br><span class="line"><span class="comment">#初始化所有变量</span></span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line"><span class="comment">#将输入的图片作为初始图片输入网络</span></span><br><span class="line">generate_image=sess.run(model[<span class="string">"input"</span>].assign(input_image))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(iteration):</span><br><span class="line">sess.run(optimizer)</span><br><span class="line"></span><br><span class="line"><span class="comment">#每次迭代更新合成的图片</span></span><br><span class="line">generate_image=sess.run(model[<span class="string">"input"</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> i%<span class="number">20</span>==<span class="number">0</span>:</span><br><span class="line">Jc,Js,Jt=sess.run([J_content,J_style,J_cost])</span><br><span class="line">print(<span class="string">"iteration:"</span>,i)</span><br><span class="line">print(<span class="string">"内容函数损失："</span>,Jc)</span><br><span class="line">print(<span class="string">"风格函数损失："</span>,Js)</span><br><span class="line">print(<span class="string">"总体损失："</span>,Jt)</span><br><span class="line">nu.save_image(<span class="string">"./out/"</span>+str(i)+<span class="string">".png"</span>,generate_image)</span><br><span class="line"></span><br><span class="line">nu.save_image(<span class="string">"./out/generate_image.jpg"</span>,generate_image)</span><br><span class="line"><span class="keyword">return</span> generate_image</span><br></pre></td></tr></table></figure><p>（8）最终结果</p><p>内容图片：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200610093228.png" alt></p><p>风格图片：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200610093247.png" alt></p><p>输入model_nn(sess,generate_image)进行测试，每次迭代得到的结果输出在out文件夹下，迭代结束后最终得到的合成结果如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200610093114.png" alt></p><p>从合成图像可以看出，很好的保留了内容图像中内容，也很好的融合了风格图片中的风格。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;问题描述&lt;/h2&gt;&lt;p&gt;将一个图像作为内容图像，另一幅图作为风格图像，两个图像经过神经网络生辰一个新的图像，该图像既有前一个图像的内容，又有后一个图像的风格，这称为神经风格转换。&lt;/p&gt;&lt;h2 id=&quot;损失函数的确定&quot;&gt;&lt;a href=&quot;#损失函数的确定&quot; class=&quot;headerlink&quot; title=&quot;损失函数的确定&quot;&gt;&lt;/a&gt;损失函数的确定&lt;/h2&gt;&lt;p&gt;神经网络浅层学习图片的点、线或者颜色等基础特征，深层学习图片中物体或者更具体的特征。&lt;/p&gt;&lt;p&gt;神经风格转换是让合成的图片中的内容更接近内容图片，风格更接近风格图片，即让合成的图片中的内容和内容图片相比损失更小，让合成的图片中的风格和风格图片相比损失更小，最终得到的总成本最小，通过成本函数调整网络的参数。&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="卷积神经网络" scheme="https://www.xiapf.com/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络——人脸识别</title>
    <link href="https://www.xiapf.com/blogs/convNet4/"/>
    <id>https://www.xiapf.com/blogs/convNet4/</id>
    <published>2020-06-09T12:48:18.000Z</published>
    <updated>2020-10-27T06:05:29.329Z</updated>
    
    <content type="html"><![CDATA[<p>人脸识别系统包括人脸识别和活体检测（使用监督学习），这里着重介绍人脸识别部分。</p><h2 id="人脸验证和人脸识别区别"><a href="#人脸验证和人脸识别区别" class="headerlink" title="人脸验证和人脸识别区别"></a>人脸验证和人脸识别区别</h2><p>（1）人脸验证</p><p>输入：图片和ID/姓名</p><p>输出：验证这个图片是否是ID/姓名代表的这个人</p><p>这是个1：1问题</p><p>（2）人脸识别</p><p>输入：图片</p><p>输出：查找输入的图片是否是k个人中的一个</p><p>这是个1:K问题</p><p>总结：人脸识别是以人脸验证作为基础模块</p><a id="more"></a><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>人脸识别是为了解决one shot learning问题，即一次学习就能识别出人的身份。</p><p>一次学习即使用一张图片训练出来的神经网络识别率很差，而且当有新的人图片出现还需要重新训练网络，这样很麻烦。</p><p>因此，人脸识别的核心：</p><p>（1）使用能够识别人脸图片的网络</p><p>（2）将输入的图片输入网络得到图片编码与数据库内图片对比，差异性小的就认为两者一致，反之不一致</p><p>所以，重点在于训练的网络要使得相同人的差异性小，不同人的差异性大。</p><p>于是得到损失函数为三元组损失函数，网络结构采用Siamese网络结构能够对图片编码，下面两部分分别介绍</p><h3 id="“similarity”函数-gt-三元组损失函数的确定"><a href="#“similarity”函数-gt-三元组损失函数的确定" class="headerlink" title="“similarity”函数-&gt;三元组损失函数的确定"></a>“similarity”函数-&gt;三元组损失函数的确定</h3><p>“similarity”函数：用于衡量两个图片的相似度，定义为d(img1,img2)=两张图片的不同，即输入两张图片，输出两张图片的相似度。</p><p>人脸识别的三元组损失函数triplets为：</p><p>（1）相同的人：anchor和positive差异小</p><p>（2）不同的人：anchor和negative差异大</p><p>即d(A,P)&lt;=d(A,N)，为了让两者相差多一点，这里加上一个间隔alpha，同时为了防止出现损失函数为0的情况（这样输出无意义），即需要保证：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200609203322.png" alt></p><p>即满足triplet&lt;=0，则最终网络的损失函数在triplets和0之间取较大值，当triplets小于等于0时，即损失等于0，则此时网络训练结束，当大于0时，损失不为0，此时需要让d(A,P)更小，或者d(A,N)更大，以满足条件。</p><p>成本函数是损失函数求和，则网络的损失和成本定义为：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200609203435.png" alt></p><h3 id="Siamese网络结构"><a href="#Siamese网络结构" class="headerlink" title="Siamese网络结构"></a>Siamese网络结构</h3><p>将输入的图像转换为编码形式，即网络结构和convnet相似，但是最后全连接层输出部分不输入softmax，直接将全连接层的输出作为输入图像的编码。</p><p>总结：</p><p>（1）使用Siamese网络得到输入图片的编码</p><p>（2）选择难和输入图片区分的图片作为negative图片，同时选择相同图片，加上输入图片构成三元组</p><p>（3）计算这三元组的距离，从而得出损失，采用反向传播算法更新网络参数</p><p>最终得到能够识别人脸的网络。</p><h3 id="附：可以将人脸识别转换为监督学习（二分类）问题"><a href="#附：可以将人脸识别转换为监督学习（二分类）问题" class="headerlink" title="附：可以将人脸识别转换为监督学习（二分类）问题"></a>附：可以将人脸识别转换为监督学习（二分类）问题</h3><p>将一对图片经过卷积池化等操作，经过全连接层后输入logitic回归，最终输出是否是同一个人，即输出0或者1，这样转换成了监督学习方法。</p><p>这里的输入是一对图片，logistic回归内的预测函数为：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200609204309.png" alt></p><p>对编码后的向量每个元素取绝对值，再加上权重和偏置量，训练w,b，得到最终的预测结果。</p><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>（）网络架构</p><p>网络的架构上遵循Szegedy et al.等人的初始模型，应用该网络结构并且指明优化算法和损失函数，将模型进行编译，最后导入本地的权重，生成得出的预训练模型。</p><p>这里的图片格式是通道在前，因此在最开始需要设置通道的位置：K.set_image_data_format(“channels_first”)，同时输入网络的图片长宽为96 * 96</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.加载模型结构</span></span><br><span class="line">FRmodel=ibv2.faceRecoModel(input_shape=(<span class="number">3</span>,<span class="number">96</span>,<span class="number">96</span>))</span><br><span class="line"><span class="comment">#编译模型，损失采用三元组损失</span></span><br><span class="line">FRmodel.compile(<span class="string">"adam"</span>,loss=triplet_loss,metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"><span class="comment">#导入权重</span></span><br><span class="line">fu.load_weights_from_FaceNet(FRmodel)</span><br></pre></td></tr></table></figure><p>（1）使用三元组损失函数</p><p>分别求出d(A,P),d(A,N)的值，加上间隔并求和得到网络的成本函数，公式参照三元损失函数的定义。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#triplets_loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">triplet_loss</span><span class="params">(yred,alpha=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">anchor,postive,negative=yred[<span class="number">0</span>],yred[<span class="number">1</span>],yred[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">AP_loss=tf.reduce_sum(tf.square(tf.subtract(anchor,postive)),axis=<span class="number">-1</span>)</span><br><span class="line">AN_loss=tf.reduce_sum(tf.square(tf.subtract(anchor,negative)),axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">APN_loss=tf.subtract(AP_loss,AN_loss)</span><br><span class="line">APN_loss=tf.add(APN_loss,alpha)</span><br><span class="line"></span><br><span class="line">loss=tf.reduce_sum(tf.maximum(APN_loss,<span class="number">0</span>))</span><br><span class="line"><span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><p>（2）保存模型</p><p>将具有网格结构与权重的预训练模型进行保存</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FRmodel.save(<span class="string">'model.h5'</span>)</span><br></pre></td></tr></table></figure><p>（3）导入训练的模型</p><p>由于模型中的损失函数是使用的自定义损失函数，因此在导入模型时，需要指明保存的模型中的损失函数的具体实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FRmodel=load_model(<span class="string">"model.h5"</span>,custom_objects=&#123;<span class="string">"triplet_loss"</span>:triplet_loss&#125;)</span><br></pre></td></tr></table></figure><p>（4）建立员工人脸编码数据库</p><p>将现有员工的人脸图像经过模型得到128位的编码，之后将数据库内的编码和输入的图像编码比对，预先将这些编码保存好能节省人脸识别的时间</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据库：生成128维向量编码</span></span><br><span class="line"><span class="comment">#加载自定义的loss函数</span></span><br><span class="line">FRmodel=load_model(<span class="string">"model.h5"</span>,custom_objects=&#123;<span class="string">"triplet_loss"</span>:triplet_loss&#125;)</span><br><span class="line">img_path=<span class="string">'./images/'</span></span><br><span class="line">dataset=&#123;&#125;</span><br><span class="line">dataset[<span class="string">'bajie'</span>]=fu.img_to_encoding(img_path+<span class="string">'bajie.jpg'</span>,FRmodel)</span><br><span class="line"></span><br><span class="line">dataset[<span class="string">'andrew'</span>]=fu.img_to_encoding(img_path+<span class="string">'andrew.jpg'</span>,FRmodel)</span><br><span class="line">dataset[<span class="string">'arnaud'</span>]=fu.img_to_encoding(img_path+<span class="string">'arnaud.jpg'</span>,FRmodel)</span><br><span class="line">dataset[<span class="string">'benoit'</span>]=fu.img_to_encoding(img_path+<span class="string">'benoit.jpg'</span>,FRmodel)</span><br><span class="line">dataset[<span class="string">'bertrand'</span>]=fu.img_to_encoding(img_path+<span class="string">'bertrand.jpg'</span>,FRmodel)</span><br><span class="line">dataset[<span class="string">'dan.jpg'</span>]=fu.img_to_encoding(img_path+<span class="string">'dan.jpg'</span>,FRmodel)</span><br><span class="line">dataset[<span class="string">'younes'</span>]=fu.img_to_encoding(img_path+<span class="string">'younes.jpg'</span>,FRmodel)</span><br><span class="line">dataset[<span class="string">'kian'</span>]=fu.img_to_encoding(img_path+<span class="string">'kian.jpg'</span>,FRmodel)</span><br></pre></td></tr></table></figure><p>（5）人脸验证 1：1问题</p><p>将输入的图像经过网络转换为编码形式</p><p>根据用户身份找到数据库中的编码</p><p>将二者编码对比，如果距离小于阈值（这里设定0.7），则人脸和身份符合，反之不符合</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#根据输入的门禁id和当前识别的人脸比对</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">verfy</span><span class="params">(img,model,identity,dataset)</span>:</span></span><br><span class="line"><span class="comment">#将验证的图片转换为编码形式</span></span><br><span class="line">img_code=fu.img_to_encoding(img,model)</span><br><span class="line"></span><br><span class="line"><span class="comment">#计算验证图片和数据库内图片的距离</span></span><br><span class="line">distance=np.linalg.norm(img_code-dataset[identity])</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> distance&lt;<span class="number">0.7</span>:</span><br><span class="line">print(<span class="string">"验证通过，欢迎你，"</span>+str(identity))</span><br><span class="line">is_open=<span class="literal">True</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">print(<span class="string">"验证失败，"</span>+<span class="string">"你与"</span>+str(identity)+<span class="string">"不符合"</span>)</span><br><span class="line">is_open=<span class="literal">False</span></span><br><span class="line"><span class="keyword">return</span> distance,is_open</span><br></pre></td></tr></table></figure><p>将本人图片编码保存在数据库中，图片信息如图：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200609173436.png" alt></p><p>进行测试</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test_img=img_path+<span class="string">'camera_6.jpg'</span></span><br><span class="line">distance,is_open=verfy(test_img,FRmodel,<span class="string">'bajie'</span>,dataset)</span><br><span class="line">print(distance,is_open)</span><br></pre></td></tr></table></figure><p>使用非本人图片测试</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200609173550.png" alt></p><p>结果：验证失败，你与bajie不符合<br>1.041191 False</p><p>使用本人的其他图片测试</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200609173613.png" alt></p><p>结果：验证通过，欢迎你，bajie<br>0.6963635 True</p><p>最终输出的是差异度及是否开门</p><p>（6）人脸识别 1：K问题</p><p>和人脸验证相比，这里不输入用户的身份，直接根据输入的图像判断：</p><p>将输入的图像经过网络转换为编码形式</p><p>遍历数据库，找到当前编码和数据库内距离最近的编码</p><p>当距离小于阈值（这里设定0.7），则说明在数据库中找到了与之相似的人脸，则找到了该员工，反之则未找到，即不开门</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 人脸识别系统</span></span><br><span class="line"><span class="comment">#当前识别的人脸与数据库内所有人员进行比对，得到最小的距离则为最相似的人</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">config</span><span class="params">(img,model,dataset)</span>:</span></span><br><span class="line">img_code=fu.img_to_encoding(img,model)</span><br><span class="line">min_dist=<span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (name,dcode) <span class="keyword">in</span> dataset.items():</span><br><span class="line">dist=np.linalg.norm(img_code-dcode)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(dist&lt;min_dist):</span><br><span class="line">min_dist=dist</span><br><span class="line">identity=name</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> min_dist&lt;<span class="number">0.7</span>:</span><br><span class="line">print(<span class="string">"验证通过，欢迎你，"</span>+str(identity))</span><br><span class="line">is_open=<span class="literal">True</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">print(<span class="string">"验证失败，你不是本系统成员"</span>)</span><br><span class="line">is_open=<span class="literal">False</span></span><br><span class="line"><span class="keyword">return</span> min_dist,is_open</span><br></pre></td></tr></table></figure><p>进行测试</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test_img=img_path+<span class="string">'camera_7.jpg'</span></span><br><span class="line">distance,is_open=config(test_img,FRmodel,dataset)</span><br><span class="line">print(distance,is_open)</span><br></pre></td></tr></table></figure><p>当使用上述camera_7非系统内成员图像时：</p><p>验证失败，你不是本系统成员<br>0.79701996 False</p><p>当使用上述camera_6系统内成员的其他图像时：</p><p>验证通过，欢迎你，bajie<br>0.6963635 True</p><p>说明：最终输出当前识别出的人的身份，以及门是否打开</p><h2 id="附：将图像转换为想要的尺寸大小"><a href="#附：将图像转换为想要的尺寸大小" class="headerlink" title="附：将图像转换为想要的尺寸大小"></a>附：将图像转换为想要的尺寸大小</h2><p>使用PIL中Image将对应路径的图像打开，并进行尺寸调整，最终使用save对调整好的图像进行保存</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">img_path=<span class="string">"./images/"</span></span><br><span class="line">file_img=img_path+<span class="string">"camera_7.jpg"</span></span><br><span class="line">img=Image.open(file_img).resize((<span class="number">96</span>,<span class="number">96</span>))</span><br><span class="line">img.save(img_path+<span class="string">"camera_7.jpg"</span>)</span><br></pre></td></tr></table></figure><p>上述实验中使用的自己的图片需要转换为网络能识别的大小，即96 * 96</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;人脸识别系统包括人脸识别和活体检测（使用监督学习），这里着重介绍人脸识别部分。&lt;/p&gt;&lt;h2 id=&quot;人脸验证和人脸识别区别&quot;&gt;&lt;a href=&quot;#人脸验证和人脸识别区别&quot; class=&quot;headerlink&quot; title=&quot;人脸验证和人脸识别区别&quot;&gt;&lt;/a&gt;人脸验证和人脸识别区别&lt;/h2&gt;&lt;p&gt;（1）人脸验证&lt;/p&gt;&lt;p&gt;输入：图片和ID/姓名&lt;/p&gt;&lt;p&gt;输出：验证这个图片是否是ID/姓名代表的这个人&lt;/p&gt;&lt;p&gt;这是个1：1问题&lt;/p&gt;&lt;p&gt;（2）人脸识别&lt;/p&gt;&lt;p&gt;输入：图片&lt;/p&gt;&lt;p&gt;输出：查找输入的图片是否是k个人中的一个&lt;/p&gt;&lt;p&gt;这是个1:K问题&lt;/p&gt;&lt;p&gt;总结：人脸识别是以人脸验证作为基础模块&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="卷积神经网络" scheme="https://www.xiapf.com/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络——识别并定位车辆</title>
    <link href="https://www.xiapf.com/blogs/convNet3/"/>
    <id>https://www.xiapf.com/blogs/convNet3/</id>
    <published>2020-06-08T03:03:00.000Z</published>
    <updated>2020-10-27T06:05:42.157Z</updated>
    
    <content type="html"><![CDATA[<h2 id="YOLO算法"><a href="#YOLO算法" class="headerlink" title="YOLO算法"></a>YOLO算法</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>（1）目标检测与定位</p><p>目标检测：输入一个图像，经过convnet，判读图片中是否有物体（或者输出该物体是几个类别中的哪个）。方法类似于对图片的分类。</p><p>目标定位：输入一个图像，经过convert，将图片中物体使用定位框将其标注出来。方法：此时的训练集中标签项需要重新设置，设置是否有物体pc，物体的位置参数（中心点bx,by，长宽bw,bh），物体属于的类别。</p><a id="more"></a><p>（2）滑动窗口一次卷积</p><p>上述的目标定位适用于图片中只有一个物体，当有多个物体时，可以将图像划分成一个个网格，将每个格子输入convert，在每个格子上进行目标检测。</p><p>缺点：计算成本大，耗时</p><p>此时可以采用一次卷积的方法：图片按照网格划分，每个格子对应一个输出标签，将图片整体输入convert中，最终得到每个格子的结果，即每个格子输出一个定位框。这样比将每个格子输入网络中的计算成本小。</p><h3 id="物体中点认定——让一个物体只被一个格子检测"><a href="#物体中点认定——让一个物体只被一个格子检测" class="headerlink" title="物体中点认定——让一个物体只被一个格子检测"></a>物体中点认定——让一个物体只被一个格子检测</h3><p>为了保证一个物体只被一个格子检测，定义物体的中点所在的格子识别出该对象，其余有该对象的格子不识别该对象。</p><h3 id="anchor-box——一个格子能检测出多个物体"><a href="#anchor-box——一个格子能检测出多个物体" class="headerlink" title="anchor box——一个格子能检测出多个物体"></a>anchor box——一个格子能检测出多个物体</h3><p>当有多个物体的中点落在同一个格子时，物体识别的类别可能有多个，此时网络无法识别输出，为了能让一个格子检测出多个物体，引入anchor box，按照物体的形状设置anchor box，和设置的anchor box的Iou值高说明该物体匹配该anchor box，类别设置在该anchor box中，因此每个标签y中有多个anchor box，对应不同形状的物体，这样一个格子就能检测出多个物体。</p><h3 id="非最大值抑制——只输出一个定位框"><a href="#非最大值抑制——只输出一个定位框" class="headerlink" title="非最大值抑制——只输出一个定位框"></a>非最大值抑制——只输出一个定位框</h3><p>（1）阈值过滤</p><p>使用pc * 每个类别得到每个类别出现的概率，取最大的概率作为该格子识别出的类别及该格子判定的物体的概率，将该概率和过滤的阈值比较，保留大于等于该阈值的类别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#分类阈值过滤</span></span><br><span class="line"><span class="comment">#box_confidence  （19，19，5，1）</span></span><br><span class="line"><span class="comment">#boxes  （19，19，5，4）</span></span><br><span class="line"><span class="comment">#box_classes_prob    （19，19，5，80）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">yolo_filter_boxes</span><span class="params">(box_confidence,boxes,box_classes_prob,threshold)</span>:</span></span><br><span class="line"><span class="comment">#得到每个概率</span></span><br><span class="line">box_scores=box_confidence*box_classes_prob</span><br><span class="line"></span><br><span class="line"><span class="comment">#找到最大的概率的分类和对应的pc</span></span><br><span class="line"><span class="comment">#用最后的80个分类来比较</span></span><br><span class="line">box_class=K.argmax(box_scores,axis=<span class="number">-1</span>)</span><br><span class="line">box_class_scores=K.max(box_scores,axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#根据每个的pc和阈值比较</span></span><br><span class="line">box_max=(box_class_scores&gt;=threshold)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scores=tf.boolean_mask(box_class_scores,box_max)</span><br><span class="line">boxes=tf.boolean_mask(boxes,box_max)</span><br><span class="line">classes=tf.boolean_mask(box_class,box_max)</span><br><span class="line"><span class="keyword">return</span> scores,boxes,classes</span><br></pre></td></tr></table></figure><p>（2）Iou</p><p>定义：Iou=两个目标定位框的相交的面积/两个目标定位框的总面积</p><p>使用Iou去除不符合要求的定位框，一般Iou设置为0.5，当小于该值的定位框直接删除</p><p>方法：输入两个定位框的左上角和右下角坐标，得到相交的面积再除以总面积。</p><p>原理：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iou</span><span class="params">(box1,box2)</span>:</span></span><br><span class="line"><span class="comment">#box1(x1,y1,x2,y2)</span></span><br><span class="line"><span class="comment">#box2(x3,y3,x4,y4)</span></span><br><span class="line"><span class="comment">#左上角</span></span><br><span class="line">xi1=np.maximum(box1[<span class="number">0</span>],box2[<span class="number">0</span>])</span><br><span class="line">yi1=np.maximum(box1[<span class="number">1</span>],box2[<span class="number">1</span>])</span><br><span class="line"><span class="comment">#右下角</span></span><br><span class="line">xi2=np.minimum(box1[<span class="number">2</span>],box2[<span class="number">2</span>])</span><br><span class="line">yi2=np.minimum(box1[<span class="number">3</span>],box2[<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#计算面积</span></span><br><span class="line">box1_area=(box1[<span class="number">2</span>]-box1[<span class="number">0</span>])*(box1[<span class="number">3</span>]-box1[<span class="number">1</span>])</span><br><span class="line">box2_area=(box2[<span class="number">2</span>]-box2[<span class="number">0</span>])*(box2[<span class="number">3</span>]-box2[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">box_inter_area=(xi2-xi1)*(yi2-yi1)</span><br><span class="line">box_union_area=box1_area+box2_area-box_inter_area</span><br><span class="line"></span><br><span class="line">iou_area=box_inter_area/box_union_area</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> iou_area</span><br></pre></td></tr></table></figure><p>yolo自带non_max_suppression方法，定义一个图像中输出的最大定位框个数和iou阈值，自动得到过滤之后的定位框</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">yolo_non_max_suppression</span><span class="params">(scores,boxes,classes,max_box=<span class="number">10</span>,iou_threshold=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">max_box_tensor=K.variable(max_box,dtype=<span class="string">"int32"</span>)</span><br><span class="line">K.get_session().run(tf.variables_initializer([max_box_tensor]))</span><br><span class="line"></span><br><span class="line">num_dims=tf.image.non_max_suppression(boxes,scores,max_box,iou_threshold)</span><br><span class="line"></span><br><span class="line">scores=K.gather(scores,num_dims)</span><br><span class="line">boxes=K.gather(boxes,num_dims)</span><br><span class="line">classes=K.gather(classes,num_dims)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> scores,boxes,classes</span><br></pre></td></tr></table></figure><p>（3）总结：对所有框进行过滤</p><p>根据官方的yolo模型经过convnet得到的输出转换为中心点形式</p><p>将其经过阈值过滤，得到定位框并将其扩展以适应新图像的大小（模型得到的定位框是680x680）</p><p>经过非最大值抑制得到最终每个格子过滤之后的物体定位框</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#对所有框进行过滤</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">yolo_eval</span><span class="params">(output,image_shape=<span class="params">(<span class="number">720.</span>,<span class="number">1280.</span>)</span>,max_box=<span class="number">10</span>,scores_threshold=<span class="number">0.6</span>,iou_threshold=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">box_confidence,box_xy,boxhw,box_classes_prob=output</span><br><span class="line"></span><br><span class="line"><span class="comment">#将中心点转换为坐标形式</span></span><br><span class="line">boxes=yolo_boxes_to_corners(box_xy,boxhw)</span><br><span class="line"></span><br><span class="line"><span class="comment">#阈值过滤</span></span><br><span class="line">scores,boxes,classes=yolo_filter_boxes(box_confidence,boxes,box_classes_prob,scores_threshold)</span><br><span class="line"></span><br><span class="line"><span class="comment">#缩放anchor box以适应原始图像</span></span><br><span class="line">boxes=yu.scale_boxes(boxes,image_shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#nms过滤</span></span><br><span class="line">scores,boxes,classes=yolo_non_max_suppression(scores,boxes,classes,max_box,iou_threshold)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> scores,boxes,classes</span><br></pre></td></tr></table></figure><h3 id="训练集设置"><a href="#训练集设置" class="headerlink" title="训练集设置"></a>训练集设置</h3><p>将每个图像划分为n * n的网格，常用的网格个数n=19，每一个网格对应一对数据（x,y）</p><p>x：代表每个网格中输入的图片</p><p>y：代表当前格子对应物体的概率，定位框以及类别。假设类别数为80，则y的维度为pc+定位框系数（4个）+类别数（80个）=85个，当有5个anchor box时，每个anchor box中都有85个数据，填充在y中，y的维度=85 * 5=425</p><h3 id="算法如何预测"><a href="#算法如何预测" class="headerlink" title="算法如何预测"></a>算法如何预测</h3><p>（1）输入图像，经过convert，得到预测的标签y：每个格子对应一个425的向量，19 * 19的格子即一张图片对应（19，19，425）维的向量，每个图片的y输出（19，19，425）维的向量，预测对应格子中物体出现的概率以及定位框</p><p>（2）过滤定位框：通过计算每个格子中识别出的物体的概率，进行阈值过滤，小于阈值的定位框去除，每个类别的无图再根据非最大值抑制，删除Iou高的定位框，保证一个物体只有一个定位框。</p><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>（1）导入模型</p><p>使用官网yolo模型，导入anchor box的类型及分类的类别数量，使用keras.models中的load_model得到训练好的模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#加载模型</span></span><br><span class="line">sess=K.get_session()</span><br><span class="line"><span class="comment">#定义anchor box,class,输入图像大小</span></span><br><span class="line">boxes=yu.read_anchors(<span class="string">"./model_data/yolo_anchors.txt"</span>)</span><br><span class="line">classes_name=yu.read_classes(<span class="string">"./model_data/object_classes.txt"</span>)</span><br><span class="line">image_shape=(<span class="number">720.</span>,<span class="number">1280.</span>)</span><br><span class="line">car_model=load_model(<span class="string">"./model_data/yolov2.h5"</span>)</span><br><span class="line">car_model.summary()</span><br></pre></td></tr></table></figure><p>（2）批量预测</p><p>1° 输入图像，并将图像按照模型中680x680尺寸换算，并输入模型中</p><p>2° 运行模型，将yolo模型的数据转换为边框坐标形式，经过阈值与非最大值抑制过滤边框，得到预测的物体即边框</p><p>3° 根据得到的边框进行过滤，并且边框需要适应原始图像大小，最终得到过滤后的边框</p><p>4° 根据模型类别得到颜色，并将边框在原始图像上画出来</p><p>5° 使用os.path.join得出预测后图像的保存地址，进行保存 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将模型输出转换为边框</span></span><br><span class="line">yolo_outputs=yolo_head(car_model.output,boxes,len(classes_name))</span><br><span class="line"></span><br><span class="line"><span class="comment">#调用eval进行边框过滤</span></span><br><span class="line">scores,boxes,classes=yolo_eval(yolo_outputs,image_shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用模型进行车辆识别</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(sess,img_file)</span>:</span></span><br><span class="line"><span class="comment">#图片处理（归一化和延展）</span></span><br><span class="line">file=<span class="string">'./images/'</span></span><br><span class="line">img_path=file+img_file</span><br><span class="line"></span><br><span class="line">img,img_data=yu.preprocess_image(img_path,model_image_size=(<span class="number">608</span>,<span class="number">608</span>))</span><br><span class="line"><span class="comment">#设置占位符</span></span><br><span class="line">out_scores,out_boxes,out_classes=sess.run([scores,boxes,classes],feed_dict=&#123;car_model.input:img_data,K.learning_phase():<span class="number">0</span>&#125;)</span><br><span class="line"></span><br><span class="line">print(img_file+<span class="string">"中找到了anchor box："</span>+str(len(out_boxes)))</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置输出的颜色</span></span><br><span class="line">colors=yu.generate_colors(classes_name)</span><br><span class="line"></span><br><span class="line"><span class="comment">#画边框</span></span><br><span class="line">yu.draw_boxes(img,out_scores,out_boxes,out_classes,classes_name,colors)</span><br><span class="line"></span><br><span class="line"><span class="comment">#保存画好边框的图片</span></span><br><span class="line">img.save(os.path.join(<span class="string">'out'</span>,img_file),quatity=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#展示图片</span></span><br><span class="line">p_image=Image.open(os.path.join(<span class="string">'out'</span>,img_file))</span><br><span class="line">plt.imshow(p_image)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> out_scores,out_boxes,out_classes</span><br></pre></td></tr></table></figure><p>（3）结果</p><p>zfill:字符串右对齐，不够长度的左边补0，调用predict，输入需要的图片进行物体识别定位</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>,<span class="number">18</span>):</span><br><span class="line">img=str(i).zfill(<span class="number">4</span>)+<span class="string">".jpg"</span></span><br><span class="line">out_scores,out_boxes,out_classes=predict(sess,img)</span><br><span class="line">print(<span class="string">"识别结束"</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200605162058.png" alt></p><p>0066.jpg中找到了anchor box：2<br>traffic light 0.62 (532, 68) (564, 113)<br>car 0.77 (372, 281) (454, 333)</p><p>结果输出该图像中找到的物体以及物体的坐标</p><p>说明：从结果中可以看出，识别出了车辆和红灯，并用方框将其定位出来了。</p><p>再测试另一个图片</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200608105320.png" alt></p><p>0017.jpg中找到了anchor box：3<br>truck 0.64 (666, 274) (1063, 392)<br>car 0.79 (1103, 300) (1271, 356)<br>car 0.82 (1, 358) (183, 427)</p><p>说明：此时识别出一个卡车和两个汽车，并用不同颜色框将其定位。</p><h2 id="附：R-CNN带区域的卷积神经网络"><a href="#附：R-CNN带区域的卷积神经网络" class="headerlink" title="附：R-CNN带区域的卷积神经网络"></a>附：R-CNN带区域的卷积神经网络</h2><p>思路来源于基于滑动窗口的目标检测算法</p><p>思想：</p><p>（1）使用某种算法（1.分割算法；2.卷积神经网络）得到图像的候选区域</p><p>（2）在候选区域中运行分类器，得到每个候选其余的类别及定位框</p><p>说明：因为该种目标定位与识别是分两步进行，时间效率比yolo总体要低一些。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;YOLO算法&quot;&gt;&lt;a href=&quot;#YOLO算法&quot; class=&quot;headerlink&quot; title=&quot;YOLO算法&quot;&gt;&lt;/a&gt;YOLO算法&lt;/h2&gt;&lt;h3 id=&quot;原理&quot;&gt;&lt;a href=&quot;#原理&quot; class=&quot;headerlink&quot; title=&quot;原理&quot;&gt;&lt;/a&gt;原理&lt;/h3&gt;&lt;p&gt;（1）目标检测与定位&lt;/p&gt;&lt;p&gt;目标检测：输入一个图像，经过convnet，判读图片中是否有物体（或者输出该物体是几个类别中的哪个）。方法类似于对图片的分类。&lt;/p&gt;&lt;p&gt;目标定位：输入一个图像，经过convert，将图片中物体使用定位框将其标注出来。方法：此时的训练集中标签项需要重新设置，设置是否有物体pc，物体的位置参数（中心点bx,by，长宽bw,bh），物体属于的类别。&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="卷积神经网络" scheme="https://www.xiapf.com/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>springBoot部署问题</title>
    <link href="https://www.xiapf.com/blogs/tomcatNoWebXml/"/>
    <id>https://www.xiapf.com/blogs/tomcatNoWebXml/</id>
    <published>2020-06-05T06:01:49.000Z</published>
    <updated>2020-06-08T03:02:31.895Z</updated>
    
    <content type="html"><![CDATA[<p>springboot项目通过外置tomcat部署，不需要web.xml的原因分析：</p><p>首先可以看到在spring-web.jar的META-INF有一个配置文件</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200604155440.png" alt="spi配置文件"></p><p>这里是通过SPI机制，执行了ServletContainerInitializer接口的实现类SpringServletContainerInitializer中的onStartup方法</p><a id="more"></a><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onStartup</span><span class="params">(Set&lt;Class&lt;?&gt;&gt; webAppInitializerClasses, ServletContext servletContext)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> ServletException </span>&#123;</span><br><span class="line"></span><br><span class="line">List&lt;WebApplicationInitializer&gt; initializers = <span class="keyword">new</span> LinkedList&lt;WebApplicationInitializer&gt;();</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (webAppInitializerClasses != <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="keyword">for</span> (Class&lt;?&gt; waiClass : webAppInitializerClasses) &#123;</span><br><span class="line"><span class="comment">// Be defensive: Some servlet containers provide us with invalid classes,</span></span><br><span class="line"><span class="comment">// no matter what @HandlesTypes says...</span></span><br><span class="line"><span class="keyword">if</span> (!waiClass.isInterface() &amp;&amp; !Modifier.isAbstract(waiClass.getModifiers()) &amp;&amp;</span><br><span class="line">WebApplicationInitializer<span class="class">.<span class="keyword">class</span>.<span class="title">isAssignableFrom</span>(<span class="title">waiClass</span>)) </span>&#123;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">initializers.add((WebApplicationInitializer) waiClass.newInstance());</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">catch</span> (Throwable ex) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> ServletException(<span class="string">"Failed to instantiate WebApplicationInitializer class"</span>, ex);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (initializers.isEmpty()) &#123;</span><br><span class="line">servletContext.log(<span class="string">"No Spring WebApplicationInitializer types detected on classpath"</span>);</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">servletContext.log(initializers.size() + <span class="string">" Spring WebApplicationInitializers detected on classpath"</span>);</span><br><span class="line">AnnotationAwareOrderComparator.sort(initializers);</span><br><span class="line"><span class="keyword">for</span> (WebApplicationInitializer initializer : initializers) &#123;</span><br><span class="line">initializer.onStartup(servletContext);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>代码最后的循环里是执行WebApplicationInitializer接口的所有实现类onStartup方法</p><p>我们springboot的入口方法，也是继承了抽象方法SpringBootServletInitializer，而SpringBootServletInitializer实现了WebApplicationInitializer接口</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@SpringBootApplication</span></span><br><span class="line"><span class="meta">@EnableEurekaClient</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Application</span> <span class="keyword">extends</span> <span class="title">SpringBootServletInitializer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Application</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        SpringApplication.run(Application<span class="class">.<span class="keyword">class</span>, <span class="title">args</span>)</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> SpringApplicationBuilder <span class="title">configure</span><span class="params">(SpringApplicationBuilder builder)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> builder.sources(Application<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里要重写configure方法，将这个入口类放到source里</p><p>SpringBootServletInitializer的onStartup会调用createRootApplicationContext方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onStartup</span><span class="params">(ServletContext servletContext)</span> <span class="keyword">throws</span> ServletException </span>&#123;</span><br><span class="line"><span class="comment">// Logger initialization is deferred in case a ordered</span></span><br><span class="line"><span class="comment">// LogServletContextInitializer is being used</span></span><br><span class="line"><span class="keyword">this</span>.logger = LogFactory.getLog(getClass());</span><br><span class="line">WebApplicationContext rootAppContext = createRootApplicationContext(</span><br><span class="line">servletContext);</span><br><span class="line"><span class="keyword">if</span> (rootAppContext != <span class="keyword">null</span>) &#123;</span><br><span class="line">servletContext.addListener(<span class="keyword">new</span> ContextLoaderListener(rootAppContext) &#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">contextInitialized</span><span class="params">(ServletContextEvent event)</span> </span>&#123;</span><br><span class="line"><span class="comment">// no-op because the application context is already initialized</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">this</span>.logger.debug(<span class="string">"No ContextLoaderListener registered, as "</span></span><br><span class="line">+ <span class="string">"createRootApplicationContext() did not "</span></span><br><span class="line">+ <span class="string">"return an application context"</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>createRootApplicationContext方法里会调用run方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> WebApplicationContext <span class="title">createRootApplicationContext</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">ServletContext servletContext)</span> </span>&#123;</span><br><span class="line">SpringApplicationBuilder builder = createSpringApplicationBuilder();</span><br><span class="line">builder.main(getClass());</span><br><span class="line">ApplicationContext parent = getExistingRootWebApplicationContext(servletContext);</span><br><span class="line"><span class="keyword">if</span> (parent != <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="keyword">this</span>.logger.info(<span class="string">"Root context already created (using as parent)."</span>);</span><br><span class="line">servletContext.setAttribute(</span><br><span class="line">WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE, <span class="keyword">null</span>);</span><br><span class="line">builder.initializers(<span class="keyword">new</span> ParentContextApplicationContextInitializer(parent));</span><br><span class="line">&#125;</span><br><span class="line">builder.initializers(</span><br><span class="line"><span class="keyword">new</span> ServletContextApplicationContextInitializer(servletContext));</span><br><span class="line">builder.contextClass(AnnotationConfigEmbeddedWebApplicationContext<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">builder = configure(builder);</span><br><span class="line">builder.listeners(<span class="keyword">new</span> WebEnvironmentPropertySourceInitializer(servletContext));</span><br><span class="line">SpringApplication application = builder.build();</span><br><span class="line"><span class="keyword">if</span> (application.getSources().isEmpty() &amp;&amp; AnnotationUtils</span><br><span class="line">.findAnnotation(getClass(), Configuration<span class="class">.<span class="keyword">class</span>) !</span>= <span class="keyword">null</span>) &#123;</span><br><span class="line">application.getSources().add(getClass());</span><br><span class="line">&#125;</span><br><span class="line">Assert.state(!application.getSources().isEmpty(),</span><br><span class="line"><span class="string">"No SpringApplication sources have been defined. Either override the "</span></span><br><span class="line">+ <span class="string">"configure method or add an @Configuration annotation"</span>);</span><br><span class="line"><span class="comment">// Ensure error pages are registered</span></span><br><span class="line"><span class="keyword">if</span> (<span class="keyword">this</span>.registerErrorPageFilter) &#123;</span><br><span class="line">application.getSources().add(ErrorPageFilterConfiguration<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> run(application);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>run方法里最后就是调用application.run，同原来的入口执行方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> WebApplicationContext <span class="title">run</span><span class="params">(SpringApplication application)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> (WebApplicationContext) application.run();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;springboot项目通过外置tomcat部署，不需要web.xml的原因分析：&lt;/p&gt;&lt;p&gt;首先可以看到在spring-web.jar的META-INF有一个配置文件&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200604155440.png&quot; alt=&quot;spi配置文件&quot;&gt;&lt;/p&gt;&lt;p&gt;这里是通过SPI机制，执行了ServletContainerInitializer接口的实现类SpringServletContainerInitializer中的onStartup方法&lt;/p&gt;
    
    </summary>
    
    
      <category term="java" scheme="https://www.xiapf.com/categories/java/"/>
    
    
      <category term="java" scheme="https://www.xiapf.com/tags/java/"/>
    
      <category term="springBoot" scheme="https://www.xiapf.com/tags/springBoot/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络——构建残差网络</title>
    <link href="https://www.xiapf.com/blogs/convNet2/"/>
    <id>https://www.xiapf.com/blogs/convNet2/</id>
    <published>2020-06-03T04:28:01.000Z</published>
    <updated>2020-10-27T06:05:37.207Z</updated>
    
    <content type="html"><![CDATA[<h2 id="不同数据量下的迁移学习"><a href="#不同数据量下的迁移学习" class="headerlink" title="不同数据量下的迁移学习"></a>不同数据量下的迁移学习</h2><p>可以使用开源的别人的训练好的神经网络，将其网络中学习的知识迁移到自己的网络中，通常开源的网络配置在prototxt中</p><p>（1）少量的标定数据</p><p>将softmax输出类别层之前的神经元进行冻结，即直接使用别人训练好的参数，在softmax层重新用自己的标定数据进行训练。</p><p>（2）充足的标定数据</p><a id="more"></a><p>从softmax层向前选择一些层数进行训练，其余层进行冻结直接使用别人训练好的参数。</p><p>（3）巨多的标定数据</p><p>将别人的网络参数作为初始化，使用自己的标定数据重新训练网络。</p><p>总结，当标定数据越多，需要冻结的网络层数越少，训练的网络层数越多。</p><h2 id="数据扩充的方法"><a href="#数据扩充的方法" class="headerlink" title="数据扩充的方法"></a>数据扩充的方法</h2><p>计算机视觉需要大量数据，可以通过以下方式在原有数据基础上进行扩充</p><p>（1）垂直镜像翻转</p><p>将图像镜像翻转</p><p>（2）随机图像修剪</p><p>在原有图像中剪裁一小部分图像，但不影响数据的特征及标签情况</p><p>还有旋转图像，扭曲图像等措施</p><p>（3）颜色变换</p><p>将图像的三通道颜色按照一定概率分布进行数值加减得到新的图像</p><h2 id="Incepton-Net"><a href="#Incepton-Net" class="headerlink" title="Incepton Net"></a>Incepton Net</h2><h3 id="1x1卷积的作用"><a href="#1x1卷积的作用" class="headerlink" title="1x1卷积的作用"></a>1x1卷积的作用</h3><p>可以减少输入的通道数即nC，降低计算的成本</p><p>注：池化层是减少输入的长宽，即nH,nW</p><h3 id="谷歌的Incepton-Net"><a href="#谷歌的Incepton-Net" class="headerlink" title="谷歌的Incepton Net"></a>谷歌的Incepton Net</h3><p>不需要手工设置网络的结构，直接将过滤器类型或者池化层等加入到网络中，让网络自动学习，将所有的输出堆叠在一起，将所有incepetion模型连接在一起就是Incepton Net，该网络中间层也能输出图像特征和预测结果，防止了过拟合。</p><p>注：如果一个模型中有一种情况输出的通道数过大，可以使用1x1卷积来减小通道数</p><p>以下是一个inception模块：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200603123833.png" alt></p><h2 id="构建残差块"><a href="#构建残差块" class="headerlink" title="构建残差块"></a>构建残差块</h2><h3 id="恒等块"><a href="#恒等块" class="headerlink" title="恒等块"></a>恒等块</h3><p>输入的激活值a[l]和输出的激活值a[l+1]维度相同</p><p>网络结构：直接从输入点加一个shortcut捷径到输出</p><p>（1）使用keras.layers自带的Conv2D,BatchNormalization,Activation,MaxPooling2D构造正常网络结果，指明过滤器个数和维度，以及参数初始化方法（均匀正态分布），并进行命名。</p><p>（2）将捷径输入的x和正常网络得出的x使用Add函数进行合并，并通过激活函数得到最终输出</p><p>注：BatchNormalization的作用是对隐藏层进行归一化，使得每层都能尽量单独学习，限制了前层参数变化对后层参数的数值分布程度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#恒等块 连接前后维度相同</span></span><br><span class="line"><span class="comment">#stage - 整数，根据每层的位置来命名每一层，与block参数一起使用。</span></span><br><span class="line"><span class="comment">#block - 字符串，据每层的位置来命名每一层，与stage参数一起使用。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">identity_block</span><span class="params">(X,filters,f,stage,block)</span>:</span></span><br><span class="line"><span class="comment">#结构：conv-bn-relu,conv-bn-relu,conv-bn    relu</span></span><br><span class="line">conv_name_base=<span class="string">"res"</span>+str(stage)+block+<span class="string">"_branch"</span></span><br><span class="line">bn_name_base=<span class="string">"bn"</span>+str(stage)+block+<span class="string">"_branch"</span></span><br><span class="line"></span><br><span class="line">F1,F2,F3=filters</span><br><span class="line">X_input=X</span><br><span class="line"><span class="comment">#均匀正态分布初始化</span></span><br><span class="line">X=Conv2D(filters=F1,kernel_size=(<span class="number">1</span>,<span class="number">1</span>),strides=(<span class="number">1</span>,<span class="number">1</span>),padding=<span class="string">'valid'</span>,kernel_initializer=glorot_uniform(seed=<span class="number">0</span>),name=conv_name_base+<span class="string">'2a'</span>)(X)</span><br><span class="line">X=BatchNormalization(axis=<span class="number">3</span>,name=bn_name_base+<span class="string">'2a'</span>)(X)</span><br><span class="line">X=Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line">X=Conv2D(filters=F2,kernel_size=(f,f),strides=(<span class="number">1</span>,<span class="number">1</span>),padding=<span class="string">'same'</span>,kernel_initializer=glorot_uniform(seed=<span class="number">0</span>),name=conv_name_base+<span class="string">'2b'</span>)(X)</span><br><span class="line">X=BatchNormalization(axis=<span class="number">3</span>,name=bn_name_base+<span class="string">'2b'</span>)(X)</span><br><span class="line">X=Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line">X=Conv2D(filters=F3,kernel_size=(<span class="number">1</span>,<span class="number">1</span>),strides=(<span class="number">1</span>,<span class="number">1</span>),padding=<span class="string">'valid'</span>,kernel_initializer=glorot_uniform(seed=<span class="number">0</span>),name=conv_name_base+<span class="string">'2c'</span>)(X)</span><br><span class="line">X=BatchNormalization(axis=<span class="number">3</span>,name=bn_name_base+<span class="string">'2c'</span>)(X)</span><br><span class="line"></span><br><span class="line">X=Add()([X_input,X])</span><br><span class="line">X=Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><h3 id="卷积块"><a href="#卷积块" class="headerlink" title="卷积块"></a>卷积块</h3><p>输入的激活值a[l]和输出的激活值a[l+1]维度不同</p><p>网络结构：从输入点出发的一个shortcut捷径需要增加卷积层，如何再到输出</p><p>（1）使用keras.layers自带的Conv2D,BatchNormalization,Activation,MaxPooling2D构造正常网络结果，指明过滤器个数和维度，以及参数初始化方法（均匀正态分布），并进行命名。</p><p>（2）捷径输入x使用Conv2D,BatchNormalization,经过一个卷积层得到捷径的输出x</p><p>（3）将捷径输出的x和正常网络得出的x使用Add函数进行合并，并通过激活函数得到最终输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#卷积块 连接前后维度不同,需要在捷径里加上conv层</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convolutional_block</span><span class="params">(X,filters,f,stage,block,s=<span class="number">2</span>)</span>:</span></span><br><span class="line"><span class="comment">#结构：conv-bn-relu,conv-bn-relu,conv-bn    relu</span></span><br><span class="line">conv_name_base=<span class="string">"res"</span>+str(stage)+block+<span class="string">"_branch"</span></span><br><span class="line">bn_name_base=<span class="string">"bn"</span>+str(stage)+block+<span class="string">"_branch"</span></span><br><span class="line"></span><br><span class="line">F1,F2,F3=filters</span><br><span class="line">X_input=X</span><br><span class="line"><span class="comment">#均匀正态分布初始化</span></span><br><span class="line">X=Conv2D(filters=F1,kernel_size=(<span class="number">1</span>,<span class="number">1</span>),strides=(s,s),padding=<span class="string">'valid'</span>,kernel_initializer=glorot_uniform(seed=<span class="number">0</span>),name=conv_name_base+<span class="string">'2a'</span>)(X)</span><br><span class="line">X=BatchNormalization(axis=<span class="number">3</span>,name=bn_name_base+<span class="string">'2a'</span>)(X)</span><br><span class="line">X=Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line">X=Conv2D(filters=F2,kernel_size=(f,f),strides=(<span class="number">1</span>,<span class="number">1</span>),padding=<span class="string">'same'</span>,kernel_initializer=glorot_uniform(seed=<span class="number">0</span>),name=conv_name_base+<span class="string">'2b'</span>)(X)</span><br><span class="line">X=BatchNormalization(axis=<span class="number">3</span>,name=bn_name_base+<span class="string">'2b'</span>)(X)</span><br><span class="line">X=Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line">X=Conv2D(filters=F3,kernel_size=(<span class="number">1</span>,<span class="number">1</span>),strides=(<span class="number">1</span>,<span class="number">1</span>),padding=<span class="string">'valid'</span>,kernel_initializer=glorot_uniform(seed=<span class="number">0</span>),name=conv_name_base+<span class="string">'2c'</span>)(X)</span><br><span class="line">X=BatchNormalization(axis=<span class="number">3</span>,name=bn_name_base+<span class="string">'2c'</span>)(X)</span><br><span class="line"></span><br><span class="line"><span class="comment">#捷径</span></span><br><span class="line">X_input=Conv2D(filters=F3,kernel_size=(<span class="number">1</span>,<span class="number">1</span>),strides=(s,s),padding=<span class="string">'valid'</span>,kernel_initializer=glorot_uniform(seed=<span class="number">0</span>),name=conv_name_base+<span class="string">'1'</span>)(X_input)</span><br><span class="line">X_input=BatchNormalization(axis=<span class="number">3</span>,name=bn_name_base+<span class="string">'1'</span>)(X_input)</span><br><span class="line"></span><br><span class="line">X=Add()([X_input,X])</span><br><span class="line">X=Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><h2 id="构建残差网络"><a href="#构建残差网络" class="headerlink" title="构建残差网络"></a>构建残差网络</h2><h3 id="将残差块一个个连接起来即为残差网络"><a href="#将残差块一个个连接起来即为残差网络" class="headerlink" title="将残差块一个个连接起来即为残差网络"></a>将残差块一个个连接起来即为残差网络</h3><p>按照如图的网络结构搭建残差网络</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200603120554.png" alt></p><p>（1）先定义一个tensor的占位符，并根据输入图像的大小定义维度</p><p>（2）使用kears.layers自带的ZeroPadding2D对图像进行填充</p><p>（3）网络第一个模块结构为conv-bn-maxpoool-relu</p><p>（4）后面的模块是使用卷积块和恒等块进行组合</p><p>（5）最终的到的结果通过平均池化，并进行拉伸后，将图像向量传入全连接层</p><p>（6）根据输入和输出，得到ResNet50的模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#搭建模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ResNet50</span><span class="params">(input_shape=<span class="params">(<span class="number">64</span>,<span class="number">64</span>,<span class="number">3</span>)</span>,classes=<span class="number">6</span>)</span>:</span></span><br><span class="line"><span class="comment">#定义一个tensor的占位符，并制定维度</span></span><br><span class="line">X_input=Input(input_shape)</span><br><span class="line">X=ZeroPadding2D((<span class="number">3</span>,<span class="number">3</span>))(X_input)</span><br><span class="line"></span><br><span class="line">X=Conv2D(<span class="number">64</span>,kernel_size=(<span class="number">7</span>,<span class="number">7</span>),strides=(<span class="number">2</span>,<span class="number">2</span>),name=<span class="string">'conv1'</span>,kernel_initializer=glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">X=BatchNormalization(axis=<span class="number">3</span>,name=<span class="string">'bn_conv1'</span>)(X)</span><br><span class="line">X=Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line">X=MaxPooling2D((<span class="number">3</span>,<span class="number">3</span>),strides=(<span class="number">2</span>,<span class="number">2</span>),name=<span class="string">'max_pool'</span>)(X)</span><br><span class="line"></span><br><span class="line">filters=[<span class="number">64</span>,<span class="number">64</span>,<span class="number">256</span>]</span><br><span class="line">f=<span class="number">3</span></span><br><span class="line">stage=<span class="number">2</span></span><br><span class="line">X=convolutional_block(X,filters,f,stage,block=<span class="string">'a'</span>)</span><br><span class="line">X=identity_block(X,filters,f,stage,block=<span class="string">'b'</span>)</span><br><span class="line">X=identity_block(X,filters,f,stage,block=<span class="string">'c'</span>)</span><br><span class="line"></span><br><span class="line">filters=[<span class="number">128</span>,<span class="number">128</span>,<span class="number">512</span>]</span><br><span class="line">f=<span class="number">3</span></span><br><span class="line">stage=<span class="number">3</span></span><br><span class="line">X=convolutional_block(X,filters,f,stage,block=<span class="string">'a'</span>)</span><br><span class="line">X=identity_block(X,filters,f,stage,block=<span class="string">'b'</span>)</span><br><span class="line">X=identity_block(X,filters,f,stage,block=<span class="string">'c'</span>)</span><br><span class="line">X=identity_block(X,filters,f,stage,block=<span class="string">'d'</span>)</span><br><span class="line"></span><br><span class="line">filters=[<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>]</span><br><span class="line">f=<span class="number">3</span></span><br><span class="line">stage=<span class="number">4</span></span><br><span class="line">X=convolutional_block(X,filters,f,stage,block=<span class="string">'a'</span>)</span><br><span class="line">X=identity_block(X,filters,f,stage,block=<span class="string">'b'</span>)</span><br><span class="line">X=identity_block(X,filters,f,stage,block=<span class="string">'c'</span>)</span><br><span class="line">X=identity_block(X,filters,f,stage,block=<span class="string">'d'</span>)</span><br><span class="line">X=identity_block(X,filters,f,stage,block=<span class="string">'e'</span>)</span><br><span class="line">X=identity_block(X,filters,f,stage,block=<span class="string">'f'</span>)</span><br><span class="line"></span><br><span class="line">filters=[<span class="number">512</span>,<span class="number">512</span>,<span class="number">2048</span>]</span><br><span class="line">f=<span class="number">3</span></span><br><span class="line">stage=<span class="number">5</span></span><br><span class="line">X=convolutional_block(X,filters,f,stage,block=<span class="string">'a'</span>)</span><br><span class="line">X=identity_block(X,filters,f,stage,block=<span class="string">'b'</span>)</span><br><span class="line">X=identity_block(X,filters,f,stage,block=<span class="string">'c'</span>)</span><br><span class="line"></span><br><span class="line">X=AveragePooling2D((<span class="number">2</span>,<span class="number">2</span>),padding=<span class="string">'same'</span>,name=<span class="string">'avg_pool'</span>)(X)</span><br><span class="line">X=Flatten()(X)</span><br><span class="line">X=Dense(classes,activation=<span class="string">'softmax'</span>,name=<span class="string">'fc'</span>+str(classes),kernel_initializer=glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line"></span><br><span class="line">model=Model(inputs=X_input,outputs=X,name=<span class="string">'ResNet50'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><h3 id="残差网络作用"><a href="#残差网络作用" class="headerlink" title="残差网络作用"></a>残差网络作用</h3><p>当网络很深时，训练集的误差反而会增大，误差先减小后增大。</p><p>（1）使用残差网络能很好的学习恒等式，并不降低效率</p><p>（2）能将输入的复杂的非线性函数传递到更深层，轻松学习输入输出之间的映射</p><p>最终当网络很深，训练集误差随着层数增加而减少。</p><p>a[l+2]=g(z[l+2]+a[l])=g(w[l+2] * a[l+1]+b[l+2]+ws * a[l])，当输入输出维度不同，可以在a[l]前面加上ws进行维度调整</p><p>其中a[l]是捷径输入，z[l+2]是正常网络输出，a[l+2]是最终输出</p><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>（1）在手势图片上使用残差网络进行模型训练</p><p>输入每个图片的维度和输出标签数量，进行模型训练。</p><p>选择模型中采用的算法和损失函数。</p><p>最后使用fit将数据输入模型进行训练。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#编译模型</span></span><br><span class="line">mymodel=ResNet50(input_shape=(<span class="number">64</span>,<span class="number">64</span>,<span class="number">3</span>),classes=<span class="number">6</span>)</span><br><span class="line">mymodel.compile(optimizer=<span class="string">'adam'</span>,loss=<span class="string">'categorical_crossentropy'</span>,metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"><span class="comment">#标签转换为独热矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_one_hot</span><span class="params">(Y,C)</span>:</span></span><br><span class="line">Y=np.eye(C)[Y.reshape(<span class="number">-1</span>).T]</span><br><span class="line"><span class="keyword">return</span> Y</span><br><span class="line"><span class="comment">#导入数据</span></span><br><span class="line">train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes=ru.load_dataset()</span><br><span class="line"><span class="comment">#归一化</span></span><br><span class="line">train_x=train_set_x_orig/<span class="number">255</span></span><br><span class="line">test_x=test_set_x_orig/<span class="number">255</span></span><br><span class="line"></span><br><span class="line">train_y=convert_one_hot(train_set_y_orig,<span class="number">6</span>)</span><br><span class="line">test_y=convert_one_hot(test_set_y_orig,<span class="number">6</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#输入数据，训练模型</span></span><br><span class="line">mymodel.fit(train_x,train_y,epochs=<span class="number">3</span>,batch_size=<span class="number">64</span>)</span><br></pre></td></tr></table></figure><p>（2）保存模型</p><p>使用save保存模型：将模型的结构，参数，导出的模型可以直接进行模型预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mymodel.save(<span class="string">"my_resnet50.h5"</span>)</span><br></pre></td></tr></table></figure><p>（3）导入保存的模型，得到模型的准确率</p><p>使用keras.models下的load_model方法将之前训练好的模型导入，并计算准确度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mymodel=load_model(<span class="string">'my_resnet50.h5'</span>)</span><br><span class="line">p=mymodel.evaluate(test_x,test_y)</span><br><span class="line">print(<span class="string">"误差率："</span>,p[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">"准确率："</span>,p[<span class="number">1</span>])mymodel.save(<span class="string">"my_resnet50.h5"</span>)</span><br></pre></td></tr></table></figure><p>误差率： 0.7191567063331604<br>准确率： 0.7833333611488342</p><p>由于原算法迭代数仅2次，所以准确率不是特别高。</p><p>（4）利用已经训练好的模型测试</p><p>使用keras.preprocessing下的image导入图像，将图像转换为数组，并进行归一化后输入训练好的网络，最终通过np.argmax得到分类的结果</p><p>注：这里图像需要拓展维度，从(64, 64, 3)拓展为(1, 64, 64, 3)，代码为x=np.expand_dims(x,axis=0)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用自己的图像测试</span></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">image_path=<span class="string">'./fingers/2.jpg'</span></span><br><span class="line">x=image.load_img(image_path,target_size=(<span class="number">64</span>,<span class="number">64</span>))</span><br><span class="line"></span><br><span class="line">x=image.img_to_array(x)</span><br><span class="line"><span class="comment">#拓展维度</span></span><br><span class="line">x=np.expand_dims(x,axis=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># x=preprocess_input(x)</span></span><br><span class="line">x=x/<span class="number">255</span></span><br><span class="line"></span><br><span class="line">res=np.argmax(mymodel.predict(x))</span><br><span class="line">print(res)</span><br><span class="line">print(mymodel.predict(x))</span><br><span class="line"></span><br><span class="line">test_image=Image.open(image_path).convert(<span class="string">'RGB'</span>).resize((<span class="number">64</span>,<span class="number">64</span>))</span><br><span class="line">plt.imshow(test_image)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>输入图像</p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200602210826.jpg" style="zoom:25%;"><p>预测结果为：</p><p>2<br>[[0.1477255  0.15454543 0.18347658 0.16707171 0.18196131 0.16521947]]</p><p>输入的手势是2，预测也是2，说明预测正确</p><p>说明：由于训练集中的手势图片不是自己拍摄的，所以预测的效果不一定很准确。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;不同数据量下的迁移学习&quot;&gt;&lt;a href=&quot;#不同数据量下的迁移学习&quot; class=&quot;headerlink&quot; title=&quot;不同数据量下的迁移学习&quot;&gt;&lt;/a&gt;不同数据量下的迁移学习&lt;/h2&gt;&lt;p&gt;可以使用开源的别人的训练好的神经网络，将其网络中学习的知识迁移到自己的网络中，通常开源的网络配置在prototxt中&lt;/p&gt;&lt;p&gt;（1）少量的标定数据&lt;/p&gt;&lt;p&gt;将softmax输出类别层之前的神经元进行冻结，即直接使用别人训练好的参数，在softmax层重新用自己的标定数据进行训练。&lt;/p&gt;&lt;p&gt;（2）充足的标定数据&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="卷积神经网络" scheme="https://www.xiapf.com/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络——识别手势图片</title>
    <link href="https://www.xiapf.com/blogs/convNet1/"/>
    <id>https://www.xiapf.com/blogs/convNet1/</id>
    <published>2020-05-28T07:47:32.000Z</published>
    <updated>2020-10-27T06:05:46.540Z</updated>
    
    <content type="html"><![CDATA[<h2 id="算法简介"><a href="#算法简介" class="headerlink" title="算法简介"></a>算法简介</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>使用普通的神经网络时，当输入的图片维度大，假设为1000 * 1000 * 3，第一层的神经元为1000个，则权重参数达到三亿个（1000 * 1000 * 3 * 1000），这时候网络会变得很大，有巨大的参数，因此需要使用卷积神经网络。</p><p>卷积神经网络通过参数共享和稀疏连接的方式使得参数变得很少，可以用很少的训练集训练，避免网络过拟合。</p><a id="more"></a><p>参数共享：图片的不同区域都可以用相同的过滤器计算</p><p>稀疏连接：每个输出的像素仅和部分输入像素相关，其他像素不影响相应输出</p><p>假设为1000 * 1000 * 3，过滤器为4 * 4 * 3，加上偏置项，共10个过滤器，则需要的参数为（4 * 4 * 3 +1） * 10=490个，原小于原使用普通的神经网络</p><h3 id="网络构成"><a href="#网络构成" class="headerlink" title="网络构成"></a>网络构成</h3><p>卷积神经网络通常有三层（也有只有卷积层的网络，具体网络的配置，如层数，超参设置可以参照文献中其他人的方法），这里将卷积层+池化层作为神经网络的一层。</p><p>（1）卷积层</p><p>作用：使用过滤器对输入的图像进行卷积操作（按照步长，在图像相应区域做乘积并求和），功能类似于求Z=W * X +b</p><p>经过卷积层的图像使用非线性函数进行计算得到A=relu(Z)</p><p>padding的作用：由于卷积之后图像会变小，可以通过事先在输入图像外部进行填充0。有两种填充方式：valid padding即p=0，不填充；same padding，即填充完成后，输入输出的维度相同，即n+2p-f+1=n，由此德驰p=(f-1)/2</p><p>附（公式）：</p><p>当输入的维度为n * n * n_c，过滤器为f * f，步长为s，填充为p，则输出的数据维度为：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200528143258.png" alt></p><p>（2）池化层</p><p>作用：缩小模型，加快计算速度</p><p>分类：最大池化，取图像对应区域内的最大值；平均池化，取图像区域内的平均值</p><p>接收经过卷积层的输出A，通过池化层，选择池化方式最终得到P，作为一层网络的输出。</p><p>注：池化层中参数不在反向传播中更新，即池化层没有要学习的参数</p><p>常用参数：f=2，s=2（长宽缩小一半）</p><p>（3）全连接层</p><p>作用：全连接层类似于单神经网络，假设通过两层网络（卷积+池化）得到图像将其拉伸成一维向量，其维度为n，全连接层的维度为m，则中间的权重维度为（m,n），可见卷积神经网络中参数主要集中在全连接层。</p><p>经过全连接层的图像会最终输出一个一维向量作为结果，当最后接softmax函数时，则最终输出一个结果。</p><h3 id="简单网络结构示意图"><a href="#简单网络结构示意图" class="headerlink" title="简单网络结构示意图"></a>简单网络结构示意图</h3><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200527180339.png" alt></p><p>如图所示，是一个简单的二层卷积神经网络，输入的图像为32 * 32 * 3</p><p>（1）卷积层+池化层</p><p>第一层网络：卷积层设置过滤器为5 * 5，数量为6个，步长s=1，p=0，则输出28 * 28 * 6的图像（套用公式即可得到输出的维度），第三个维度6代表6个过滤器</p><p>池化层选择最大池化，f=2，s=2（长宽缩小一半），则输出14 * 14 *6的图像</p><p>第二层网络同理，最终得到5 * 5 * 16的图像，将其拉伸为400 * 1的一维向量，输入全连接层中</p><p>（2）全连接层</p><p>可以设置多个全连接层，这里设置了两层，将拉伸了的图像输入全连接层中，最终得到的数据输入softmax函数中得到最终输出，数字最大的数字代表输入图像代表的类别</p><h3 id="网络中各数据的维度"><a href="#网络中各数据的维度" class="headerlink" title="网络中各数据的维度"></a>网络中各数据的维度</h3><p>填充：p</p><p>步长：s</p><p>每个过滤器：f * f * n_c_pre （这里的n_c_pre是指过滤器维度需要和输入，即上一层的输入保持一致）</p><p>本层过滤器数量：n_c</p><p>权重W：f * f *n_c_pre * n_c （n_c是指本层过滤器数量，权重的维度等于每个过滤器维度 * 过滤器数量）</p><p>偏置量b：1 * 1 * 1 * n_c</p><p>激活值a：n_h * n_w * n_c（本层的输出）</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200527180726.png" alt></p><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>（1）padding 填充数据</p><p>使用numpy中的pad，填充过滤器的长宽，填充大小为pad，当不指定constant_value时，自动填充0</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.将输入的图像，按照指定的padding进行填充</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zero_pad</span><span class="params">(X,pad)</span>:</span></span><br><span class="line">  X=np.pad(X,((<span class="number">0</span>,<span class="number">0</span>),(pad,pad),(pad,pad),(<span class="number">0</span>,<span class="number">0</span>)),<span class="string">'constant'</span>) <span class="comment">#不指定constant_value,默认填充0</span></span><br><span class="line">  <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><p>（2）前向传播</p><p>1.卷积层</p><p>1° 计算需要输出的图像Z的维度，并填充输入的图像</p><p>2° 对每个输入的图像，计算需要卷积的区域：</p><p>h_start=h * stride<br>h_end=h_start+f_h<br>w_start=w * stride<br>w_end=w_start+f_w</p><p>按照卷积的区域计算每个样本的卷积值</p><p>3° 保存卷积层的上一层输入，当前层的权重和超参（包含s和pad），保存为cache</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#卷积层的前向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_forward</span><span class="params">(A_pre,W,b,hparameters)</span>:</span></span><br><span class="line">  pad=hparameters[<span class="string">'pad'</span>]</span><br><span class="line">  stride=hparameters[<span class="string">'stride'</span>]</span><br><span class="line">  m,n_h_pre,n_w_pre,n_c_pre=A_pre.shape</span><br><span class="line">  f_h,f_w,n_c_pre,n_c=W.shape</span><br><span class="line"></span><br><span class="line">  <span class="comment">#将每个输入样本进行填充</span></span><br><span class="line">  A_pre_single=zero_pad(A_pre,pad)</span><br><span class="line"></span><br><span class="line">  <span class="comment">#计算输出的维度</span></span><br><span class="line">  n_h=int(((n_h_pre+<span class="number">2</span>*pad-f_h)/stride)+<span class="number">1</span>)</span><br><span class="line">  n_w=int(((n_w_pre+<span class="number">2</span>*pad-f_w)/stride)+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  Z=np.zeros((m,n_h,n_w,n_c))</span><br><span class="line"></span><br><span class="line">  <span class="comment">#对填充的样本一一用过滤器处理得到结果</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">    a_pre_single=A_pre_single[i]</span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> range(n_h):</span><br><span class="line">      <span class="keyword">for</span> w <span class="keyword">in</span> range(n_w):</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> range(n_c):</span><br><span class="line">          h_start=h*stride</span><br><span class="line">          h_end=h_start+f_h</span><br><span class="line">          w_start=w*stride</span><br><span class="line">          w_end=w_start+f_w</span><br><span class="line">          <span class="comment">#将对应通道的矩阵进行一次卷积</span></span><br><span class="line">          Z[i,h,w,c]=conv_single_step(a_pre_single[h_start:h_end,w_start:w_end,:],W[:,:,:,c],b[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,c])</span><br><span class="line">          <span class="comment">#激活值可以在这里计算</span></span><br><span class="line">          <span class="comment">#....</span></span><br><span class="line"></span><br><span class="line">    cache=(A_pre,W,b,hparameters)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> Z,cache</span><br></pre></td></tr></table></figure><p>2.池化层</p><p>接收卷积层的输入，按照卷积方式，在指定区域进行取最大值或者取平均值，最终得到P输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#池化层前向传播 A_pre是卷积之后的值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pool_forward</span><span class="params">(A_pre,hparameters,pool=<span class="string">'max'</span>,pad=<span class="number">0</span>)</span>:</span></span><br><span class="line">  m,n_h_pre,n_w_pre,n_c_pre=A_pre.shape</span><br><span class="line"></span><br><span class="line">  f=hparameters[<span class="string">'f'</span>]</span><br><span class="line">  stride=hparameters[<span class="string">'stride'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#计算输出的维度</span></span><br><span class="line">  n_h=int(((n_h_pre+<span class="number">2</span>*pad-f)/stride)+<span class="number">1</span>)</span><br><span class="line">  n_w=int(((n_w_pre+<span class="number">2</span>*pad-f)/stride)+<span class="number">1</span>)</span><br><span class="line">  n_c=n_c_pre</span><br><span class="line"></span><br><span class="line">  Z_pool=np.zeros((m,n_h,n_w,n_c))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> range(n_h):</span><br><span class="line">      <span class="keyword">for</span> w <span class="keyword">in</span> range(n_w):</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> range(n_c):</span><br><span class="line">          h_start=h*stride</span><br><span class="line">          h_end=h_start+f</span><br><span class="line">          w_start=w*stride</span><br><span class="line">          w_end=w_start+f</span><br><span class="line">            <span class="comment">#指定第i个样本和其维度</span></span><br><span class="line">          a_single=A_pre[i,h_start:h_end,w_start:w_end,c]</span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span>(pool==<span class="string">"max"</span>):</span><br><span class="line">            Z_pool[i,h,w,c]=np.max(a_single)</span><br><span class="line">          <span class="keyword">elif</span>(pool==<span class="string">"average"</span>):</span><br><span class="line">            Z_pool[i,h,w,c]=np.mean(a_single)</span><br><span class="line">  cache=(A_pre,hparameters)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> Z_pool,cache</span><br></pre></td></tr></table></figure><p>（3）反向传播（了解）</p><p>1.卷积层</p><p>利用公式dA=W * DZ，dW= 1/m *(A_pre * dZ)，db=dZ</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#卷积层反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_backward</span><span class="params">(dZ,cache)</span>:</span></span><br><span class="line">  m,n_h,n_w,n_c=dZ.shape</span><br><span class="line">  A_pre,W,b,hparameters=cache</span><br><span class="line">  m,n_h_pre,n_w_pre,n_c_pre=A_pre.shape</span><br><span class="line">  f,f,n_c_pre,n_c=W.shape</span><br><span class="line"></span><br><span class="line">  pad=hparameters[<span class="string">'pad'</span>]</span><br><span class="line">  stride=hparameters[<span class="string">'stride'</span>]</span><br><span class="line"></span><br><span class="line">  dA_pre=np.zeros((m,n_h_pre,n_w_pre,n_c_pre))</span><br><span class="line">  dW=np.zeros((f,f,n_c_pre,n_c))</span><br><span class="line">  db=np.zeros((<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,n_c))</span><br><span class="line"></span><br><span class="line">  <span class="comment">#填充A</span></span><br><span class="line">  A_pre_pad=zero_pad(A_pre,pad)</span><br><span class="line">  dA_pre_pad=zero_pad(dA_pre,pad)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">    a_pre_single=A_pre_pad[i]</span><br><span class="line">    da_pre_single=dA_pre_pad[i]</span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> range(n_h):</span><br><span class="line">      <span class="keyword">for</span> w <span class="keyword">in</span> range(n_w):</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> range(n_c):</span><br><span class="line">          h_start=h*stride</span><br><span class="line">          h_end=h_start+f</span><br><span class="line">          w_start=w*stride</span><br><span class="line">          w_end=w_start+f</span><br><span class="line"></span><br><span class="line">          a_single=a_pre_single[h_start:h_end,w_start:w_end,:]</span><br><span class="line"></span><br><span class="line">          da_pre_single[h_start:h_end,w_start:w_end,:]+=W[:,:,:,c]*dZ[i,h,w,c]</span><br><span class="line"></span><br><span class="line">          dW[:,:,:,c]+=a_single*dZ[i,h,w,c]</span><br><span class="line">          db[:,:,:,c]+=dZ[i,h,w,c]</span><br><span class="line">    dA_pre[i,:,:,:]=da_pre_single[pad:-pad,pad:-pad,:]</span><br><span class="line">  grads=(dA_pre,dW,db)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure><p>2.池化层</p><p>最大池化使用掩码矩阵标记最大值，得到在最大值作用下的梯度，平均池化是根据图像维度计算出平均值，得到在平均值作用下的梯度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#池化层反向传播</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#最大池化</span></span><br><span class="line"><span class="comment">#创建矩阵保存最大值所在的位置，即创建掩码矩阵</span></span><br><span class="line"><span class="comment">#为了反向传播的时候，用值定位到卷积层中的位置</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_mask_from_window</span><span class="params">(x)</span>:</span></span><br><span class="line">  mask=(x==np.max(x))</span><br><span class="line">  <span class="keyword">return</span> mask</span><br><span class="line"></span><br><span class="line"><span class="comment">#平均池化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distribute_value</span><span class="params">(dz,shape)</span>:</span></span><br><span class="line">  n_h,n_w=shape</span><br><span class="line">  average=dz/(n_h*n_w)</span><br><span class="line">  a=np.ones((shape))*average</span><br><span class="line">  <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line"><span class="comment">#反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pool_backward</span><span class="params">(dA,cache,pool=<span class="string">"max"</span>)</span>:</span></span><br><span class="line">  A_pre,hparameters=cache</span><br><span class="line">  m,n_h,n_w,n_c=dA.shape</span><br><span class="line"></span><br><span class="line">  dA_pre=np.zeros_like(A_pre)</span><br><span class="line"></span><br><span class="line">  f=hparameters[<span class="string">'f'</span>]</span><br><span class="line">  stride=hparameters[<span class="string">'stride'</span>]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> range(n_h):</span><br><span class="line">      <span class="keyword">for</span> w <span class="keyword">in</span> range(n_w):</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> range(n_c):</span><br><span class="line">          h_start=h*stride</span><br><span class="line">          h_end=h_start+f</span><br><span class="line">          w_start=w*stride</span><br><span class="line">          w_end=w_start+f</span><br><span class="line">          <span class="comment">#指定第i个样本和其维度</span></span><br><span class="line">          a_single=A_pre[i,h_start:h_end,w_start:w_end,c]</span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span>(pool==<span class="string">"max"</span>):</span><br><span class="line">            mask=create_mask_from_window(a_single)</span><br><span class="line">            dA_pre[i,h_start:h_end,w_start:w_end,c]+=np.multiply(mask,dA[i,h,w,c])</span><br><span class="line">            </span><br><span class="line">          <span class="keyword">elif</span>(pool==<span class="string">"average"</span>):</span><br><span class="line">            shape=(f,f)</span><br><span class="line">            dA_pre[i,h_start:h_end,w_start:w_end,c]+=distribute_value(dA[i,h,w,c],shape)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> dA_pre</span><br></pre></td></tr></table></figure><h2 id="应用实例——使用tensorflow框架"><a href="#应用实例——使用tensorflow框架" class="headerlink" title="应用实例——使用tensorflow框架"></a>应用实例——使用tensorflow框架</h2><p>（0）导入数据</p><p>导入h5文件下的数据集，并进行归一化</p><p>这里输入数据的格式是（m,n_h,n_w,n_c），所以不需要修正大小</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将输入的标签转换为独热矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_one_hot</span><span class="params">(Y,C)</span>:</span></span><br><span class="line"><span class="comment">#Y.reshape(-1)转成一维的</span></span><br><span class="line">one_hot=np.eye(C)[Y.reshape(<span class="number">-1</span>)].T</span><br><span class="line"><span class="keyword">return</span> one_hot</span><br><span class="line">train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes=cu.load_dataset()</span><br><span class="line">train_x=train_set_x_orig/<span class="number">255</span></span><br><span class="line">test_x=test_set_x_orig/<span class="number">255</span></span><br></pre></td></tr></table></figure><p>（1）建立占位符，初始化参数</p><p>因为不确定输入图片的个数，第一位设置为none</p><p>使用tf下的get_variable初始化权重</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.创建占位符</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_placeholders</span><span class="params">(n_h,n_w,n_c,n_y)</span>:</span></span><br><span class="line">  <span class="comment">#m不确定</span></span><br><span class="line">  X=tf.placeholder(tf.float32,[<span class="literal">None</span>,n_h,n_w,n_c])</span><br><span class="line">  Y=tf.placeholder(tf.float32,[<span class="literal">None</span>,n_y])</span><br><span class="line">  <span class="keyword">return</span> X,Y</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.初始化权重</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">()</span>:</span></span><br><span class="line">  n_h1,n_w1,n_c_pre1,n_c1=[<span class="number">4</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">8</span>] <span class="comment">#f*f*n_c_pre*n_c</span></span><br><span class="line">  n_h2,n_w2,n_c_pre2,n_c2=[<span class="number">2</span>,<span class="number">2</span>,<span class="number">8</span>,<span class="number">16</span>]</span><br><span class="line">  tf.set_random_seed(<span class="number">1</span>)</span><br><span class="line">  W1=tf.get_variable(<span class="string">"W1"</span>,[n_h1,n_w1,n_c_pre1,n_c1],initializer=tf.contrib.layers.xavier_initializer(seed=<span class="number">0</span>))</span><br><span class="line">  W2=tf.get_variable(<span class="string">"W2"</span>,[n_h2,n_w2,n_c_pre2,n_c2],initializer=tf.contrib.layers.xavier_initializer(seed=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">  parameters=&#123;<span class="string">"W1"</span>:W1,<span class="string">"W2"</span>:W2&#125;</span><br><span class="line">  <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>（2）前向传播</p><p>使用tf.nn下的conv2d、relu、max_pool实现卷积和池化，其中步长[1,s,s,1]是指指对于输入 (m, n_H_prev, n_W_prev, n_C_prev)而言，每次滑动的步伐，即长宽滑动s步，同理池化中的ksize的窗口大小也是值长宽滑动的大小。</p><p>最终得到结果使用tf.contrib.layer下的flatten进行拉伸，最后调用fully_connected输出全连接层的值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#3.前向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(X,parameters)</span>:</span></span><br><span class="line">  W1=parameters[<span class="string">"W1"</span>]</span><br><span class="line">  W2=parameters[<span class="string">'W2'</span>]</span><br><span class="line">  <span class="comment">#1.1卷积</span></span><br><span class="line">  <span class="comment">#[1,s,s,1]是指对于输入 (m, n_H_prev, n_W_prev, n_C_prev)而言，每次滑动的步伐</span></span><br><span class="line">  <span class="comment">#步长1，指定填充方式</span></span><br><span class="line">  Z1=tf.nn.conv2d(X,W1,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line">  A1=tf.nn.relu(Z1)</span><br><span class="line">  <span class="comment">#1.2池化</span></span><br><span class="line">  <span class="comment">#窗口大小8*8，步长8</span></span><br><span class="line">  P1=tf.nn.max_pool(A1,ksize=[<span class="number">1</span>,<span class="number">8</span>,<span class="number">8</span>,<span class="number">1</span>],strides=[<span class="number">1</span>,<span class="number">8</span>,<span class="number">8</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line"></span><br><span class="line">  Z2=tf.nn.conv2d(P1,W2,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line">  A2=tf.nn.relu(Z2)</span><br><span class="line">  P2=tf.nn.max_pool(A2,ksize=[<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">1</span>],strides=[<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line"></span><br><span class="line">  P3=tf.contrib.layers.flatten(P2)</span><br><span class="line">  <span class="comment">#6是输出的维度</span></span><br><span class="line">  Z3=tf.contrib.layers.fully_connected(P3,<span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> Z3</span><br></pre></td></tr></table></figure><p>（3）计算损失</p><p>调用tf.nn下计算softmax的交叉熵模型，输入的数据维度是（数据集大小，分布）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#4.计算成本</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(Z3,Y)</span>:</span></span><br><span class="line">  <span class="comment">#参数维度[数据集大小，分布]-&gt;logits,labels</span></span><br><span class="line">  cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Z3,labels=Y))</span><br><span class="line">  <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><p>（4）反向传播</p><p>使用tf。train下的adam函数</p><p>optimizer=tf.train.AdamOptimizer(learning_rate).minimize(cost)</p><p>（5）建立会话，导入数据</p><p>1° 重现建立模型而不覆盖tf变量：ops.reset_default_graph()</p><p>2° 构建占位符，初始化参数，前向传播，计算成本，反向传播</p><p>3° 创建会话，对每个批量计算其损失</p><p>4° 保存模型的参数，将得到的结果使用tf下的argmax得到最大值和真实值比较，并计算其准确度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#5.构建模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X_train,Y_train,X_test,Y_test,num_ecpom=<span class="number">50</span>,learning_rate=<span class="number">0.001</span>,mini_batches_size=<span class="number">64</span>)</span>:</span></span><br><span class="line">  <span class="comment">#重新运行模型不覆盖tf变量</span></span><br><span class="line">  ops.reset_default_graph()</span><br><span class="line">  tf.set_random_seed(<span class="number">1</span>)</span><br><span class="line">  seed=<span class="number">3</span></span><br><span class="line">  costs=[]</span><br><span class="line">  m,n_h,n_w,n_c=X_train.shape</span><br><span class="line">  m,n_y=Y_train.shape</span><br><span class="line"></span><br><span class="line">  X,Y=create_placeholders(n_h,n_w,n_c,n_y)</span><br><span class="line"></span><br><span class="line">  parameters=initialize_parameters()</span><br><span class="line"></span><br><span class="line">  Z3=forward_propagation(X,parameters)</span><br><span class="line"></span><br><span class="line">  cost=compute_cost(Z3,Y)</span><br><span class="line"></span><br><span class="line">  optimizer=tf.train.AdamOptimizer(learning_rate).minimize(cost)</span><br><span class="line"></span><br><span class="line">  <span class="comment">#创建会话，导入训练集数据</span></span><br><span class="line">  init=tf.global_variables_initializer()</span><br><span class="line">  session=tf.Session()</span><br><span class="line">  session.run(init)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(num_ecpom):</span><br><span class="line">    <span class="comment">#按照小批量梯度下降</span></span><br><span class="line">    mini_batches=cu.random_mini_batches(X_train,Y_train,mini_batches_size,seed)</span><br><span class="line">    <span class="comment">#每个数据块的大小-&gt;得到在该数据块上的错误率</span></span><br><span class="line">    num_mini_batch=int(m/mini_batches_size)</span><br><span class="line">    mini_batch_cost=<span class="number">0</span></span><br><span class="line">    seed=seed+<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> mini_batch <span class="keyword">in</span> mini_batches:</span><br><span class="line">      mini_batch_x,mini_batch_y=mini_batch</span><br><span class="line">      _,ecpom_cost=session.run([optimizer,cost],feed_dict=&#123;X:X_train,Y:Y_train&#125;)</span><br><span class="line">      mini_batch_cost=mini_batch_cost+ecpom_cost/num_mini_batch</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> i%<span class="number">1</span>==<span class="number">0</span>:</span><br><span class="line">      costs.append(mini_batch_cost)</span><br><span class="line">      <span class="keyword">if</span> i%<span class="number">5</span>==<span class="number">0</span>:</span><br><span class="line">        print(<span class="string">"第"</span>+str(i)+<span class="string">"次，cost:"</span>+str(mini_batch_cost))</span><br><span class="line">  plt.plot(costs)</span><br><span class="line">  plt.title(<span class="string">"learning_rate="</span>+str(learning_rate))</span><br><span class="line">  plt.xlabel(<span class="string">"iteration"</span>)</span><br><span class="line">  plt.ylabel(<span class="string">"cost"</span>)</span><br><span class="line">  plt.show()</span><br><span class="line"></span><br><span class="line">  <span class="comment">#保存得到的参数</span></span><br><span class="line">  parameters=session.run(parameters)</span><br><span class="line">  print(<span class="string">"参数已经保存至session中"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">#计算准确率</span></span><br><span class="line">  <span class="comment">#取一行中最大的那个</span></span><br><span class="line">  <span class="comment">#predict=tf.argmax(Z3,1)</span></span><br><span class="line">  predict_matrix=tf.equal(tf.argmax(Z3,<span class="number">1</span>),tf.argmax(Y,<span class="number">1</span>))</span><br><span class="line">  accuary=tf.reduce_mean(tf.cast(predict_matrix,<span class="string">"float32"</span>))</span><br><span class="line"></span><br><span class="line">  print(<span class="string">"训练集准确率："</span>,session.run(accuary,&#123;X:X_train,Y:Y_train&#125;))</span><br><span class="line">  print(<span class="string">"测试集准确率："</span>,session.run(accuary,&#123;X:X_test,Y:Y_test&#125;))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>结果：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200527180218.jpg" alt></p><p>训练集准确率： 0.8074074<br>测试集准确率： 0.7416667</p><p>实验中选择迭代50次，学习率为0.001，为取得更好的结果，可以将迭代次数增加或者修改学习率</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;算法简介&quot;&gt;&lt;a href=&quot;#算法简介&quot; class=&quot;headerlink&quot; title=&quot;算法简介&quot;&gt;&lt;/a&gt;算法简介&lt;/h2&gt;&lt;h3 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h3&gt;&lt;p&gt;使用普通的神经网络时，当输入的图片维度大，假设为1000 * 1000 * 3，第一层的神经元为1000个，则权重参数达到三亿个（1000 * 1000 * 3 * 1000），这时候网络会变得很大，有巨大的参数，因此需要使用卷积神经网络。&lt;/p&gt;&lt;p&gt;卷积神经网络通过参数共享和稀疏连接的方式使得参数变得很少，可以用很少的训练集训练，避免网络过拟合。&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="卷积神经网络" scheme="https://www.xiapf.com/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>图像数据集：将本地图片批量写入h5文件</title>
    <link href="https://www.xiapf.com/blogs/imgToH5/"/>
    <id>https://www.xiapf.com/blogs/imgToH5/</id>
    <published>2020-05-27T14:45:26.000Z</published>
    <updated>2020-10-27T06:04:57.991Z</updated>
    
    <content type="html"><![CDATA[<h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p>当需要用大量图片训练神经网络时，保存为图像格式的图片所占的内存大，因此将本地图片存入h5文件中减少数据集所占内存。</p><p>使用手机拍摄剪刀、石头、布共三组图片，每组图片四张，共12张图像，选择11张作为训练集，剩余的作为测试集。</p><h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><p>（1）读取本地图片并将数据打乱</p><p>1° 读取文件：使用os的listdir函数读取目录下的文件：这里读取并存储的的是本地图像的路径，同时将图片的标签进行存储</p><a id="more"></a><p>2° 合并图像和标签：使用numpy下的hstack将不同标签的图像数据进行堆叠保存，并合并</p><p>3° 打乱数据集：使用numpy.random下的shuffle打乱数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_files</span><span class="params">(file_dir)</span>:</span></span><br><span class="line">stone=[]</span><br><span class="line">label_stone=[]</span><br><span class="line">cut=[]</span><br><span class="line">label_cut=[]</span><br><span class="line">cloth=[]</span><br><span class="line">label_cloth=[]</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取指定目录下的文件</span></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> os.listdir(file_dir+<span class="string">'/stone'</span>):</span><br><span class="line">stone.append(file_dir+<span class="string">'/stone/'</span>+file)</span><br><span class="line">label_stone.append(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> os.listdir(file_dir+<span class="string">'/cut'</span>):</span><br><span class="line">cut.append(file_dir+<span class="string">'/cut/'</span>+file)</span><br><span class="line">label_cut.append(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> os.listdir(file_dir+<span class="string">'/cloth'</span>):</span><br><span class="line">cloth.append(file_dir+<span class="string">'/cloth/'</span>+file)</span><br><span class="line">label_cloth.append(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将数据集合并</span></span><br><span class="line">merge_x=np.hstack((stone,cut,cloth))</span><br><span class="line">merge_y=np.hstack((label_stone,label_cut,label_cloth))</span><br><span class="line">merge=np.array([merge_x,merge_y]).T</span><br><span class="line"></span><br><span class="line"><span class="comment">#打乱数据集</span></span><br><span class="line">np.random.shuffle(merge)</span><br><span class="line"></span><br><span class="line">image_list=merge[:,<span class="number">0</span>]</span><br><span class="line">label_list=merge[:,<span class="number">1</span>]</span><br><span class="line"><span class="comment">#将标签化为整型数据</span></span><br><span class="line">label_list=[int(i) <span class="keyword">for</span> i <span class="keyword">in</span> label_list]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> image_list,label_list</span><br></pre></td></tr></table></figure><p>（2）划分训练集和测试集</p><p>1° 随机初始化：设置四个数组分别保存训练集和测试集数据</p><p>这里需要将数组的类型使用astype转换为int，因为Matplotlib显示图像，如果是0-1区间，值为float，如果是0-255区间，值为int，需要转换，否则无法显示，空白图像，报错：Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).</p><p>这里的图像值在0-255之间，所以需要转换为int.</p><p>2° 按比例划分数据</p><p>输入的图片可能大小不一样，使用PIL中的Image中的resize转换图像到合适的大小</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#3个训练，1个测试</span></span><br><span class="line">num_px=<span class="number">64</span></span><br><span class="line">train_x=np.random.rand(m<span class="number">-1</span>,num_px,num_px,<span class="number">3</span>).astype(<span class="string">'int'</span>)</span><br><span class="line">train_y=np.random.rand(m<span class="number">-1</span>,<span class="number">1</span>).astype(<span class="string">'int'</span>)</span><br><span class="line">test_x=np.random.rand(<span class="number">1</span>,num_px,num_px,<span class="number">3</span>).astype(<span class="string">'int'</span>)</span><br><span class="line">test_y=np.random.rand(<span class="number">1</span>,<span class="number">1</span>).astype(<span class="string">'int'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m<span class="number">-1</span>):</span><br><span class="line">image=image_list[i]</span><br><span class="line">image_convert=Image.open(image).convert(<span class="string">'RGB'</span>).resize((num_px,num_px))</span><br><span class="line">train_x[i]=np.array(image_convert)</span><br><span class="line">train_y[i]=np.array(label_list[i])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m<span class="number">-1</span>,m):</span><br><span class="line">image=image_list[i]</span><br><span class="line">image_convert=Image.open(image).convert(<span class="string">'RGB'</span>).resize((num_px,num_px))</span><br><span class="line">test_x[i+<span class="number">1</span>-m]=np.array(image_convert)</span><br><span class="line">test_y[i+<span class="number">1</span>-m]=np.array(label_list[i])</span><br></pre></td></tr></table></figure><p>（3）将图像数据写入h5文件并测试</p><p>使用create_dataset将数据按照对应名称存储，最后以只读方式读取h5对应名称中的图像数据，用plt方法显示指定图片。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将图像写入h5文件</span></span><br><span class="line">f=h5py.File(<span class="string">'data.h5'</span>,<span class="string">'w'</span>)</span><br><span class="line">f.create_dataset(<span class="string">'X_train'</span>,data=train_x)</span><br><span class="line">f.create_dataset(<span class="string">'y_train'</span>,data=train_y)</span><br><span class="line">f.create_dataset(<span class="string">'X_test'</span>,data=test_x)</span><br><span class="line">f.create_dataset(<span class="string">'y_test'</span>,data=test_y)</span><br><span class="line">f.close()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span><span class="params">()</span>:</span></span><br><span class="line"><span class="comment">#读取h5文件中的图像</span></span><br><span class="line">dataset=h5py.File(<span class="string">'data.h5'</span>,<span class="string">'r'</span>)</span><br><span class="line">my_train_x=np.array(dataset[<span class="string">'X_train'</span>][:])</span><br><span class="line">my_train_y=np.array(dataset[<span class="string">'y_train'</span>][:])</span><br><span class="line">my_test_x=np.array(dataset[<span class="string">'X_test'</span>][:])</span><br><span class="line">my_test_y=np.array(dataset[<span class="string">'y_test'</span>][:])</span><br><span class="line"><span class="keyword">return</span> my_train_x,my_train_y,my_test_x,my_test_y</span><br><span class="line"></span><br><span class="line">my_train_x,my_train_y,my_test_x,my_test_y=load_dataset()</span><br><span class="line">print(my_train_x.shape)</span><br><span class="line">print(my_train_y.shape)</span><br><span class="line">print(my_test_x.shape)</span><br><span class="line">print(my_test_y.shape)</span><br><span class="line">index=<span class="number">1</span></span><br><span class="line">plt.imshow(my_train_x[index])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>（1）将图像保存为h5文件后</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200527212520.png" alt></p><p>可见，原图像大小58.5MB，保存为h5文件后只有1.5MB，明显所占内存减少了</p><p>（2）指定查看第一个图像显示如下</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200527212728.png" alt></p><p>（3）训练集和测试集维度为：</p><p>(11, 64, 64, 3)<br>(11, 1)<br>(1, 64, 64, 3)<br>(1, 1)</p><p>参考：</p><p><a href="https://blog.csdn.net/chenkz123/article/details/79640658" target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/chenkz123/article/details/79640658</a></p><p><a href="https://blog.csdn.net/weixin_43615222/article/details/84577293" target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/weixin_43615222/article/details/84577293</a></p><p><a href="https://www.jianshu.com/p/778d78463028" target="_blank" rel="external nofollow noopener noreferrer">https://www.jianshu.com/p/778d78463028</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;描述&quot;&gt;&lt;a href=&quot;#描述&quot; class=&quot;headerlink&quot; title=&quot;描述&quot;&gt;&lt;/a&gt;描述&lt;/h2&gt;&lt;p&gt;当需要用大量图片训练神经网络时，保存为图像格式的图片所占的内存大，因此将本地图片存入h5文件中减少数据集所占内存。&lt;/p&gt;&lt;p&gt;使用手机拍摄剪刀、石头、布共三组图片，每组图片四张，共12张图像，选择11张作为训练集，剩余的作为测试集。&lt;/p&gt;&lt;h2 id=&quot;步骤&quot;&gt;&lt;a href=&quot;#步骤&quot; class=&quot;headerlink&quot; title=&quot;步骤&quot;&gt;&lt;/a&gt;步骤&lt;/h2&gt;&lt;p&gt;（1）读取本地图片并将数据打乱&lt;/p&gt;&lt;p&gt;1° 读取文件：使用os的listdir函数读取目录下的文件：这里读取并存储的的是本地图像的路径，同时将图片的标签进行存储&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>结构化机器学习项目——第二周学习笔记</title>
    <link href="https://www.xiapf.com/blogs/struLearning2/"/>
    <id>https://www.xiapf.com/blogs/struLearning2/</id>
    <published>2020-05-25T03:22:54.000Z</published>
    <updated>2020-10-27T06:02:17.495Z</updated>
    
    <content type="html"><![CDATA[<h2 id="误差分析"><a href="#误差分析" class="headerlink" title="误差分析"></a>误差分析</h2><h3 id="如何进误差分析"><a href="#如何进误差分析" class="headerlink" title="如何进误差分析"></a>如何进误差分析</h3><p>当需要提高系统的正确率时，需要一一看发生错误的样本并进行记录（主要是看假阳和假阴样本），列成如下的表格，纵向代表每个出错的样本，横向代表出错的类型：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200525102748.png" alt></p><p><strong>最后一行记录每个错误类型的百分比，根据百分比确定不同错误类型对错误率的影响，选择影响大的作为后续系统改进的方向。</strong></p><p>注：可以将百分比理解为是确定每个错误类型的优先级顺序，同时要考虑解决错误类型的难易度，即添加数据的难易度，获取难可以采用人工合成数据</p><a id="more"></a><h3 id="对标记错误的例子如何处理"><a href="#对标记错误的例子如何处理" class="headerlink" title="对标记错误的例子如何处理"></a>对标记错误的例子如何处理</h3><p>在误差分析之后，有些错误样本可能是初始标记的时候发生了错误，这时候分类处理：</p><p>（1）如果是训练集中原始标记错误，当训练集足够大，可以忽略错误，不进行改正</p><p>（2）如果是开发集或者测试集，就需要查看原始标记错误的百分比是否很大，即是否严重影响了系统的正确率，如果是，则需要修正，反之不需要。</p><h2 id="快速搭建自己的系统"><a href="#快速搭建自己的系统" class="headerlink" title="快速搭建自己的系统"></a>快速搭建自己的系统</h2><h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><p>（1）确定<strong>目标</strong>（后续出现问题也可修改）：确定开发集、测试集和衡量算法的单一实数指标</p><p>（2）搭建一个机器学习系统<strong>原型</strong>：在训练集上进行训练看效果，在开发集、测试集上测试看表现</p><p>（3）用<strong>偏差</strong>和<strong>方差</strong>分析，确定下一步做什么，进行<strong>误差分析</strong>，了解大部分出错的样本是什么，提高系统正确率</p><h3 id="数据不够如何处理"><a href="#数据不够如何处理" class="headerlink" title="数据不够如何处理"></a>数据不够如何处理</h3><p>当真正关心的数据比较少，假设只有10000条，非真正关心的数据有500000条，如何分配训练集、开发集、测试集：</p><p>根据“<u>瞄准需要处理的目标即真正关系的数据</u>”的原则划分数据集，保证开发集和测试集数据来自同分布</p><p>（1）选择1：500000条作为训练集，真正关系的数据中5000条作为开发集，5000条作为测试集</p><p>（2）选择2：500000条+真正关心数据中5000条=505000条作为训练集，真正关心的数据中2500条作为开发集，2500条作为测试集</p><h3 id="数据不匹配如何处理"><a href="#数据不匹配如何处理" class="headerlink" title="数据不匹配如何处理"></a>数据不匹配如何处理</h3><p>当按照数据不够的处理原则处理后即数据不匹配的系统的方差偏差分析：</p><p>由于训练集和开发集数据不同分布，为了了解训练集上的模型的推广程度（即方差），从原始数据中取一部分作物训练—开发集，来衡量方差的值。（设定500000条作为训练集，真正关系的数据中5000条作为开发集，5000条作为测试集）</p><table><thead><tr><th>误差类型</th><th>误差</th><th>数据量</th></tr></thead><tbody><tr><td>1</td><td>人类表现（即贝叶斯误差）</td><td>/</td></tr><tr><td>2</td><td>训练集误差</td><td>495000（在该数据上进行训练）</td></tr><tr><td>3</td><td>训练—开发集误差</td><td>5000（该数据上不进行训练）</td></tr><tr><td>4</td><td>开发集误差</td><td>5000</td></tr><tr><td>5</td><td>训练集误差</td><td>5000</td></tr></tbody></table><p>（1）1，2之间是可避免误差，措施：训练更大的网络或者训练时间更长</p><p>（2）2，3之间是方差，措施：正则化或者获取更多的训练集数据</p><p>（3）3，4之间是数据不匹配，措施：人工合成数据</p><p>快速制造更多训练数据，例如语音识别，用大量清晰数据+噪音数据制造出关心的数据：在噪音下说话的声音。</p><p>缺点：当噪音数据过小，容易对当前噪音数据过拟合。</p><p>（4）4，5之间是对开发集过拟合程度，措施：需要更大的开发集或者开发集数据更多些</p><h2 id="迁移学习和多任务学习"><a href="#迁移学习和多任务学习" class="headerlink" title="迁移学习和多任务学习"></a>迁移学习和多任务学习</h2><h3 id="迁移学习——串行"><a href="#迁移学习——串行" class="headerlink" title="迁移学习——串行"></a>迁移学习——串行</h3><p>神经网络从一个任务中学习到知识，并将这些知识应用到其他任务中叫做迁移学习。</p><h4 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h4><p>A-&gt;B（将A任务中知识迁移到B任务中）</p><p>（1）A的数据量比B多很多</p><p>（2）A，B有相同的输入</p><p>（3）A任务中低层次特征的学习能够帮助B任务</p><p>注：当A的数据量比B小，迁移学习效果不太好</p><h4 id="example"><a href="#example" class="headerlink" title="example"></a>example</h4><p>将对猫的识别迁移到对x射线图像的识别中去</p><p>A:对猫的识别</p><p>B:对x射线图像的识别</p><p>两者都是输入图像，满足条件（2），同时A任务中能够学习图像的轮廓等能保住B任务，满足条件（3），任务A的数据量很大。</p><p>方法：预训练+微调</p><p>预训练：使用数据量大的A任务训练的模型</p><p>微调：使用数据量小的B任务去掉最后输出层，重新训练权重的过程</p><p>（1）方法1（x射线图像的识别数据量小）：将对猫识别网络最后一层去掉，初始化最后一层的权重等参数（随机权重），使用原始前n-1层参数进行训练</p><p>（2）方法2（x射线图像的识别数据量大）：将对猫识别网络最后一层去掉，初始化所有层的权重等参数进行训练</p><p>注：去掉最后一层之后，可以加几层隐藏层再接输出</p><h3 id="多任务学习——并行"><a href="#多任务学习——并行" class="headerlink" title="多任务学习——并行"></a>多任务学习——并行</h3><p>同时开始学习，神经网络每次学习几件事，并希望每个任务都能帮助其他任务称为多任务学习</p><h4 id="应用场景-1"><a href="#应用场景-1" class="headerlink" title="应用场景"></a>应用场景</h4><p>（1）训练的一组任务，可以共用低层次特征（例如自动车驾驶，需要训练对物体的识别，识别行人，标志，斑马线等，此时可以共用低层次特征）</p><p>（2）每个单独的任务数据量接近</p><p>（3）建立一个大的神经网络同时训练一组任务比单独训练每个任务效率高</p><p>一般网络最后输出代表是否有该组任务，即每个任务赋予一个标签。（和softmaxt层不同，softmax最后数据代表单个样本的分类情况）</p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200525111013.png" alt></p><p>红色框中代表对四个任务分类的求和</p><h2 id="端到端的深度学习"><a href="#端到端的深度学习" class="headerlink" title="端到端的深度学习"></a>端到端的深度学习</h2><h3 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h3><p>语音识别任务（x,y），x代表输入的音频，y代表听写文本</p><p>一般深度学习：从x-&gt;提取特征-&gt;提取音位-&gt;生成词-&gt;y，中间包含很多手工生成的组件，里面包含人类对知识的分析，从而传递给网络</p><p>端到端的深度学习：从x直接映射到y，完全依靠数据来学习知识，没有人类的干预即手工生成组件</p><h3 id="应用场景-2"><a href="#应用场景-2" class="headerlink" title="应用场景"></a>应用场景</h3><p>从x直接映射到y有大量训练样本</p><p>当从x直接映射到y没有很多样本，这时候可以换分为子任务，在子任务中进行端到端的深度学习</p><p>例如：人脸识别系统（x,y），x代表输入的人像图片，y代表这个人的身份</p><p>由于人脸识别任务中的输入图片有很多，各种角度和各种距离的，这样的数据集较少，因此可以将任务分为两个子任务：</p><p>（1）（x1,y1），x代表输入的人脸图片，y1代表将输入的图片裁剪出人脸并居中</p><p>（2）（x2,y2），x2代表输入的居中人脸图片，y2代表这个人的身份（通过x2人脸和库中人脸进行匹配训练模型）</p><p>子任务（1）、（2）数据量充足，划分之后任务简单也易训练模型。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;误差分析&quot;&gt;&lt;a href=&quot;#误差分析&quot; class=&quot;headerlink&quot; title=&quot;误差分析&quot;&gt;&lt;/a&gt;误差分析&lt;/h2&gt;&lt;h3 id=&quot;如何进误差分析&quot;&gt;&lt;a href=&quot;#如何进误差分析&quot; class=&quot;headerlink&quot; title=&quot;如何进误差分析&quot;&gt;&lt;/a&gt;如何进误差分析&lt;/h3&gt;&lt;p&gt;当需要提高系统的正确率时，需要一一看发生错误的样本并进行记录（主要是看假阳和假阴样本），列成如下的表格，纵向代表每个出错的样本，横向代表出错的类型：&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200525102748.png&quot; alt&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;最后一行记录每个错误类型的百分比，根据百分比确定不同错误类型对错误率的影响，选择影响大的作为后续系统改进的方向。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;注：可以将百分比理解为是确定每个错误类型的优先级顺序，同时要考虑解决错误类型的难易度，即添加数据的难易度，获取难可以采用人工合成数据&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://www.xiapf.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>结构化机器学习项目——第一周学习笔记</title>
    <link href="https://www.xiapf.com/blogs/struLearning1/"/>
    <id>https://www.xiapf.com/blogs/struLearning1/</id>
    <published>2020-05-21T08:38:04.000Z</published>
    <updated>2020-10-27T06:02:27.283Z</updated>
    
    <content type="html"><![CDATA[<h2 id="正交化"><a href="#正交化" class="headerlink" title="正交化"></a>正交化</h2><p>正交化在数学中是垂直的含义，每个方法单独影响一个功能。</p><p>一个监督学习系统</p><p>（1）训练集</p><p>当训练集拟合不佳时，采用更大的网络或者优化的算法（如Adam算法），这些措施单一的影响训练集训练的效果，这样能方便对模型修改。</p><p>（2）开发集</p><p>当开发集错误率高，说明存在较大的方差，可能是训练集过拟合了，可以采用正则化或者获取更多的训练数据，这些事正交化措施。如果使用早停的方法，虽然可以抑制过拟合，但是可能会导致训练集训练不完全，这种非正交化的措施不建议使用。</p><a id="more"></a><p>（3）测试集</p><p>当测试集错误率高，说明在开发集上效果良好的模型没有很好的推广到测试集，说明开发集可能过拟合了，这时就需要更多的开发集数据，这也是正交化措施，只单独影响测试集。</p><p>（4）用户体验（即成本函数）</p><p>根据原有的成本函数生成的模型效果好，但是在实际应用中效果不好，说明可能是成本函数的评价标准发生错误，这时就需要重新定义成本函数，或者改变验证集。</p><p>总结：因此，在搭建模型的时候，需要采用正交化的方法，以便以方便的解决问题。</p><p>正交化步骤：</p><p>（1）首先设定一个目标来衡量想做的事</p><p>（2）调试算法表现：分开考虑改善系统在指标上的表现</p><h2 id="单一实数指标"><a href="#单一实数指标" class="headerlink" title="单一实数指标"></a>单一实数指标</h2><p>单一的实数指标能很方便的判断众多模型中最优的那个。</p><p>一般评估模型常用查准率P，即模型标记为真的数据中有多少确实是真；指标查全率T，即在真的数据中模型标记出了多少。然而这两个指标具有平衡性，所以常采用F1分数评估模型，即F和P的调和平均数：F1=2/(1/P+1/T)</p><p>总结：通过设置开发集和单一实数指标能够快速判断模型的好坏，加快迭代速度。</p><p>注：当指标不能很好的衡量模型的好坏时，需要更换新的指标。例如对于假阳的例子，对其设置惩罚权重，提高其错误率。</p><h2 id="满足指标和优化指标"><a href="#满足指标和优化指标" class="headerlink" title="满足指标和优化指标"></a>满足指标和优化指标</h2><p>单一的实数指标不一定能够满足模型的所有条件，当需要符合N个条件时，选择一个作为优化指标，剩余的N-1个作为满足指标。</p><p>优化指标：尽可能优化，即精度越高，数值越大即可</p><p>满足指标：设定一个阈值，在阈值线以内即可，不要求最优</p><p>这样模型就转换为尽可能优化设定的优化指标即可，满足指标只要达到条件就行。</p><h2 id="如何设定训练集，开发集，测试集"><a href="#如何设定训练集，开发集，测试集" class="headerlink" title="如何设定训练集，开发集，测试集"></a>如何设定训练集，开发集，测试集</h2><p>（1）三者分别作用</p><p>训练集作用：根据不同思路训练出不同模型</p><p>开发集作用：用于评估不同模型中最好的模型</p><p>测试集作用：系统开发之后评估性能（评估成本函数），能否满足用户需求</p><p>（2）大小如何设置</p><p>数据量很大的情况，训练集占98%以上，开发集和测试集占很少部分（1%左右）。</p><p>（3）设置注意点</p><p>为了让算法集中于想要的目标数据，开发集和测试集的数据分布要相同。</p><p>为了让在开发集上运行的模型能很好的推广到测试集上。</p><h2 id="人的表现"><a href="#人的表现" class="headerlink" title="人的表现"></a>人的表现</h2><p>（1）估计bayes error</p><p>人的表现误差可以用来贝叶斯最优误差，贝叶斯最优误差时理论上最小的误差</p><p>（2）制定算法的策略</p><p>假定hunman error(约等于bayes error)=a%，training eror=b%，dev error=c%</p><p>则可避免偏差为b%-a%，方差为c%-b%，以此可以可以根据不同的场景确定不同的策略。当可避免偏差大时，采取训练更大的网络或者训练时间更长，当方差大时，可以采取正则化或者选取更多的训练集数据。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;正交化&quot;&gt;&lt;a href=&quot;#正交化&quot; class=&quot;headerlink&quot; title=&quot;正交化&quot;&gt;&lt;/a&gt;正交化&lt;/h2&gt;&lt;p&gt;正交化在数学中是垂直的含义，每个方法单独影响一个功能。&lt;/p&gt;&lt;p&gt;一个监督学习系统&lt;/p&gt;&lt;p&gt;（1）训练集&lt;/p&gt;&lt;p&gt;当训练集拟合不佳时，采用更大的网络或者优化的算法（如Adam算法），这些措施单一的影响训练集训练的效果，这样能方便对模型修改。&lt;/p&gt;&lt;p&gt;（2）开发集&lt;/p&gt;&lt;p&gt;当开发集错误率高，说明存在较大的方差，可能是训练集过拟合了，可以采用正则化或者获取更多的训练数据，这些事正交化措施。如果使用早停的方法，虽然可以抑制过拟合，但是可能会导致训练集训练不完全，这种非正交化的措施不建议使用。&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://www.xiapf.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>改善深层神经网络——识别手势图片</title>
    <link href="https://www.xiapf.com/blogs/ipvNNet3/"/>
    <id>https://www.xiapf.com/blogs/ipvNNet3/</id>
    <published>2020-05-19T12:57:17.000Z</published>
    <updated>2020-10-27T06:04:29.661Z</updated>
    
    <content type="html"><![CDATA[<h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p>使用TensorFlow深度学习框架建立识别手势图片的网络</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>注：（1）— （5）步骤并没有实际执行，所有训练模型过程都在会话（session）中进行，即模型训练基于会话的创建，<strong>所有输入的数据在会话中赋值，其他地方均处理的是变量</strong>。</p><p>网络结构为三层网络，各层的激活函数relu，relu，softmax，即模型结构为linear-&gt;relu-&gt;linear-&gt;relu-&gt;linear-&gt;softmax</p><a id="more"></a><h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><p>（0）数据处理</p><p>导入数据，并将数据归一化，对标签项转换为独热矩阵编码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.导入数据并归一化</span></span><br><span class="line">train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes=tu.load_dataset()</span><br><span class="line"><span class="comment">#每幅图片64*64*3，将每个样本堆叠起来</span></span><br><span class="line">train_x_flattern=train_set_x_orig.reshape(train_set_x_orig.shape[<span class="number">0</span>],<span class="number">-1</span>).T</span><br><span class="line">test_x_flattern=test_set_x_orig.reshape(test_set_x_orig.shape[<span class="number">0</span>],<span class="number">-1</span>).T</span><br><span class="line"><span class="comment">#归一化</span></span><br><span class="line">train_x=train_x_flattern/<span class="number">255</span></span><br><span class="line">test_x=test_x_flattern/<span class="number">255</span></span><br><span class="line"><span class="comment">#y全部转换为独热矩阵</span></span><br><span class="line">train_y=tu.convert_to_one_hot(train_set_y_orig,<span class="number">6</span>)</span><br><span class="line">test_y=tu.convert_to_one_hot(test_set_y_orig,<span class="number">6</span>)</span><br></pre></td></tr></table></figure><p>注：使用np.eye(C)[Y.reshape(-1)].T，eye后面跟一维数组，数组中每个元素代表1的偏移量，由于每一列代表一个样本，所以最后需要转置</p><p>（1）创建占位符用于输入训练集数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#2.构造占位符，便于输入训练集数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_placeholder</span><span class="params">(n_x,n_y)</span>:</span></span><br><span class="line">X=tf.placeholder(tf.float32,[n_x,<span class="literal">None</span>])</span><br><span class="line">Y=tf.placeholder(tf.float32,[n_y,<span class="literal">None</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> X,Y</span><br></pre></td></tr></table></figure><p>（2）初始化权重等参数</p><p>利用tf创建变量：tf.get_variable()</p><p>权重初始化：initializer=tf.contrib.layers.xavaier_initializer</p><p>偏置量初始化：initializer=tf.zeros_initializer</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#3.初始化参数</span></span><br><span class="line"><span class="comment">#构建变量</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(layer_dims)</span>:</span></span><br><span class="line">n_0,n_1,n_2,n_3=layer_dims</span><br><span class="line">parameters=&#123;&#125;</span><br><span class="line">tf.set_random_seed(<span class="number">1</span>)</span><br><span class="line">W1=tf.get_variable(<span class="string">"W1"</span>,[n_1,n_0],initializer=tf.contrib.layers.xavier_initializer(seed=<span class="number">1</span>))</span><br><span class="line">b1=tf.get_variable(<span class="string">"b1"</span>,[n_1,<span class="number">1</span>],initializer=tf.zeros_initializer())</span><br><span class="line">W2=tf.get_variable(<span class="string">"W2"</span>,[n_2,n_1],initializer=tf.contrib.layers.xavier_initializer(seed=<span class="number">1</span>))</span><br><span class="line">b2=tf.get_variable(<span class="string">"b2"</span>,[n_2,<span class="number">1</span>],initializer=tf.zeros_initializer())</span><br><span class="line">W3=tf.get_variable(<span class="string">"W3"</span>,[n_3,n_2],initializer=tf.contrib.layers.xavier_initializer(seed=<span class="number">1</span>))</span><br><span class="line">b3=tf.get_variable(<span class="string">"b3"</span>,[n_3,<span class="number">1</span>],initializer=tf.zeros_initializer())</span><br><span class="line"></span><br><span class="line">parameters=&#123;<span class="string">"W1"</span>:W1,<span class="string">"b1"</span>:b1,<span class="string">"W2"</span>:W2,<span class="string">"b2"</span>:b2,<span class="string">"W3"</span>:W3,<span class="string">"b3"</span>:b3&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>（3）前向传播</p><p>根据传入的参数和上一层数据计算Z，A</p><p>注：最后一层不用计算激活值，根据得出的Z3，利用tf.argmax得出概率最大（最大值概率最大），即为当前分类结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#4.前向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propogation</span><span class="params">(parameters,X)</span>:</span></span><br><span class="line">W1=parameters[<span class="string">"W1"</span>]</span><br><span class="line">b1=parameters[<span class="string">"b1"</span>]</span><br><span class="line">W2=parameters[<span class="string">"W2"</span>]</span><br><span class="line">b2=parameters[<span class="string">"b2"</span>]</span><br><span class="line">W3=parameters[<span class="string">"W3"</span>]</span><br><span class="line">b3=parameters[<span class="string">"b3"</span>]</span><br><span class="line"></span><br><span class="line">Z1=tf.add(tf.matmul(W1,X),b1)</span><br><span class="line">A1=tf.nn.relu(Z1)</span><br><span class="line">Z2=tf.add(tf.matmul(W2,A1),b2)</span><br><span class="line">A2=tf.nn.relu(Z2)</span><br><span class="line"><span class="comment">#Z3最大相当于概率最大</span></span><br><span class="line">Z3=tf.add(tf.matmul(W3,A2),b3)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> Z3</span><br></pre></td></tr></table></figure><p>（4）计算损失</p><p>利用框架中softmax的损失函数：tf.softmax_cross_entropy_with_logits(logits=….，labels=…..)，该损失函数数据输入的维度是（数据集大小，分布），所以输入的堆叠数据需要转置</p><p>使用reduce_mean计算均值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#5.计算损失</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(Z3,Y)</span>:</span></span><br><span class="line">logits=tf.transpose(Z3)</span><br><span class="line">labels=tf.transpose(Y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#最后一层用softmax,并求平均</span></span><br><span class="line"><span class="comment">#参数维度[数据集大小，分布]</span></span><br><span class="line">cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=labels))</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><p>（5）反向传播</p><p>tensorflow框架在前向传播时，自动计算导数，因此反向传播用一行代码即可，这里使用adam算法</p><p>tf.train.AdamOptimizer(学习率)).minimize(cost)</p><p>（6）创建会话执行训练模型过程</p><p>1° 创建会话，并初始化所有变量</p><p>2° 将训练数据输入占位符中进行模型训练</p><p>利用小批量梯度下降法：</p><p>a）每次获取一小批样本数据，将其输入占位符中；</p><p>b）在会话中输入反向传播的算法和损失函数，得到当前总损失</p><p>c）根据样本总数得到当前批量的损失，并进行记录</p><p>注：模型的参数需要保存到会话中，不然parameters一直是只有物理空间，没有赋值的状态：</p><p>{‘W1’: &lt;tf.Variable ‘W1:0’ shape=(25, 12288) dtype=float32_ref&gt;, ‘b1’: &lt;tf.Variable ‘b1:0’ shape=(25, 1) dtype=float32_ref&gt;, ‘W2’: &lt;tf.Variable ‘W2:0’ shape=(12, 25) dtype=float32_ref&gt;, ‘b2’: &lt;tf.Variable ‘b2:0’ shape=(12, 1) dtype=float32_ref&gt;, ‘W3’: &lt;tf.Variable ‘W3:0’ shape=(6, 12) dtype=float32_ref&gt;, ‘b3’: &lt;tf.Variable ‘b3:0’ shape=(6, 1) dtype=float32_ref&gt;}</p><p>最后利用argmax得到输出最大概率对应的类别，将其和正确输出对比，即可得到准确率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#6.构建模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(train_x,train_y,test_x,test_y,layer_dims,num_ecpho=<span class="number">100</span>,mini_batch_size=<span class="number">32</span>,learning_rate=<span class="number">0.0001</span>)</span>:</span></span><br><span class="line"><span class="comment">#重新运行模型不覆盖tf变量</span></span><br><span class="line">ops.reset_default_graph()</span><br><span class="line">tf.set_random_seed(<span class="number">1</span>)</span><br><span class="line">costs=[]</span><br><span class="line">seed=<span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#1.占位符：输入训练数据</span></span><br><span class="line">n_x,m=train_x.shape</span><br><span class="line">n_y=train_y.shape[<span class="number">0</span>]</span><br><span class="line">X,Y=create_placeholder(n_x,n_y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.初始化参数</span></span><br><span class="line">parameters=initialize_parameters(layer_dims)</span><br><span class="line"></span><br><span class="line"><span class="comment">#3.前向传播</span></span><br><span class="line">Z3=forward_propogation(parameters,X)</span><br><span class="line"></span><br><span class="line"><span class="comment">#4.计算损失</span></span><br><span class="line">cost=compute_cost(Z3,Y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#5.反向传播</span></span><br><span class="line">optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)</span><br><span class="line"></span><br><span class="line">init=tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment">#5.创建会话，训练模型</span></span><br><span class="line">session=tf.Session()</span><br><span class="line">session.run(init)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_ecpho):</span><br><span class="line">seed=seed+<span class="number">1</span></span><br><span class="line">mini_batchs=tu.random_mini_batches(train_x,train_y,mini_batch_size,seed)</span><br><span class="line"><span class="comment">#每一代的成本</span></span><br><span class="line">ecpho_cost=<span class="number">0</span></span><br><span class="line">num_mini_batch=int(m/mini_batch_size)</span><br><span class="line"><span class="keyword">for</span> mini_batch <span class="keyword">in</span> mini_batchs:</span><br><span class="line">mini_batch_x,mini_batch_y=mini_batch</span><br><span class="line">_,mini_cost=session.run([optimizer,cost],feed_dict=&#123;X:mini_batch_x,Y:mini_batch_y&#125;)</span><br><span class="line">ecpho_cost=ecpho_cost+mini_cost/num_mini_batch</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> i%<span class="number">5</span>==<span class="number">0</span>:</span><br><span class="line">costs.append(ecpho_cost)</span><br><span class="line"><span class="keyword">if</span> i%<span class="number">100</span>==<span class="number">0</span>:</span><br><span class="line">print(<span class="string">"第"</span>+str(i)+<span class="string">"次，cost:"</span>+str(ecpho_cost))</span><br><span class="line"></span><br><span class="line"><span class="comment">#6.画图</span></span><br><span class="line">plt.plot(costs)</span><br><span class="line">plt.xlabel(<span class="string">"iteration"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"cost"</span>)</span><br><span class="line">plt.title(<span class="string">"learning_rate:"</span>+str(learning_rate))</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#7.保存参数 ！！！！！要在session中保存，不然parameters一直是只有物理空间，没有赋值的状态</span></span><br><span class="line">parameters=session.run(parameters)</span><br><span class="line">print(<span class="string">"参数已保存到session"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#8.计算准确率</span></span><br><span class="line">predit_matrix=tf.equal(tf.argmax(Z3),tf.argmax(Y))</span><br><span class="line">accury=tf.reduce_mean(tf.cast(predit_matrix,<span class="string">"float32"</span>))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"训练集正确率:"</span>,session.run(accury,&#123;X:train_x,Y:train_y&#125;))</span><br><span class="line">print(<span class="string">"测试集正确率:"</span>,session.run(accury,&#123;X:test_x,Y:test_y&#125;))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>（7）输入自己数据进行测试</p><p>利用matplotlib中的图片工厂读入图片，并进行大小转换，将上述得出的模型参数和当前图像输入预测函数中（预测函数是进行一次前向传播，并根据最终的输出判断类别）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> image <span class="keyword">as</span> mpimg</span><br><span class="line">my_image=<span class="string">"1.png"</span></span><br><span class="line">file_name=<span class="string">"./datasets/fingers/"</span>+my_image</span><br><span class="line">img=mpimg.imread(file_name)</span><br><span class="line">plt.imshow(img)</span><br><span class="line">plt.show()</span><br><span class="line">my_test=img.reshape(<span class="number">1</span>,<span class="number">64</span>*<span class="number">64</span>*<span class="number">3</span>).T</span><br><span class="line">predict=tu.predict(my_test,parameters)</span><br><span class="line">print(<span class="string">"预测结果为："</span>+str(predict))</span><br></pre></td></tr></table></figure><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>（1）训练结果</p><p>即算法中第6步结果</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200519160728.png" alt></p><p>训练集正确率: 0.9990741<br>测试集正确率: 0.71666664</p><p>可见损失不断减少，训练集和测试集正确率效果可以。</p><p>（2）测试结果</p><p>输入新的图片</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200519205603.png" alt></p><p>预测结果为</p><p>CPU运行时间407.137855s.<br>预测结果为：[1]</p><p>可见预测无误，训练好的网络能识别出手势。</p><h2 id="附"><a href="#附" class="headerlink" title="附"></a>附</h2><h3 id="超参调试"><a href="#超参调试" class="headerlink" title="超参调试"></a>超参调试</h3><p>（1）超参调试采用随机取值方式：因为不知道哪个参数更重要</p><p>（2）采取从粗糙到精细的策略：先在大范围内随机取值，在其中找到最好效果的区间，在这个区间进行精细化，即更密集的取值</p><p>（3）注意标尺的选择：当小的变化对结果影响很大时，需要使用对数标尺</p><p>如参数取值为0.9 ~ 0.99，则转换为1-（0.1 ~ 0.11）,最大值0.1对应10^-1，最小值0.11对应10^-2</p><p>则取值范围指数r=[-2，-1]，参数取值为1-10^r</p><p>附：各超参的重要性排序：</p><p>学习率learning_rate &gt; momentum中的beta(常用取值0.9) = 隐藏层神经元数量 = mini_batch_size &gt; 网络层数 = 学习率衰减率</p><p>最不重要Adam算法中的beta1(取0.9)，beta2(取0.99)，epsilon(取1e-8)</p><h3 id="batch-norm归一化"><a href="#batch-norm归一化" class="headerlink" title="batch norm归一化"></a>batch norm归一化</h3><p>当深层网络层数很多，即网络很深时，可以使用batch norm归一化对隐藏层数据进行归一化。</p><p>作用：使得每层隐藏层的输入更趋于稳定范围内，即限制了在前层网络参数变化时，会影响数值的分布，使得后层的值趋于稳定，即后面单元不过分依赖前面单元，具有一定的独立性，可单独进行学习。</p><p>方法：用参数β,γ控制隐藏层的方差和均值</p><p>（为了使得方差和均值不全为0，1，加入的这两个参数，当方差和均值为0，1，则激活函数只在某一段变化，不能完全学习到所有特征）</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200519202110.png" alt></p><p>更新参数时和更新权重一样，可以使用梯度下降或者Adam等其他算法。</p><p>注：测试时没有均值和方差，则需要在训练时，根据指数加权平均方法得到训练集整体的均值和方差，作为测试的均值和方差使用</p><h3 id="多分类使用softmax"><a href="#多分类使用softmax" class="headerlink" title="多分类使用softmax"></a>多分类使用softmax</h3><p>当输出层的分类不止2个时，使用softmax函数：对输出的数值进行判断，将最大概率对应的数值作为当前分类（输出层得到的最大的数即概率最大）</p><p>当到最后一层时，得到线性值Z[l]，激活函数使用对幂指数取平均的方式，如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200519203045.png" alt></p><p>得到最大的值作为输出值。</p><p>损失函数：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200519203054.png" alt></p><h3 id="其他深度学习框架"><a href="#其他深度学习框架" class="headerlink" title="其他深度学习框架"></a>其他深度学习框架</h3><p>对不同领域的不同应用有不同的深度学习框架</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200519203341.png" alt></p><p>（深度学习框架学习待续…）</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;描述&quot;&gt;&lt;a href=&quot;#描述&quot; class=&quot;headerlink&quot; title=&quot;描述&quot;&gt;&lt;/a&gt;描述&lt;/h2&gt;&lt;p&gt;使用TensorFlow深度学习框架建立识别手势图片的网络&lt;/p&gt;&lt;h2 id=&quot;算法&quot;&gt;&lt;a href=&quot;#算法&quot; class=&quot;headerlink&quot; title=&quot;算法&quot;&gt;&lt;/a&gt;算法&lt;/h2&gt;&lt;p&gt;注：（1）— （5）步骤并没有实际执行，所有训练模型过程都在会话（session）中进行，即模型训练基于会话的创建，&lt;strong&gt;所有输入的数据在会话中赋值，其他地方均处理的是变量&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;网络结构为三层网络，各层的激活函数relu，relu，softmax，即模型结构为linear-&amp;gt;relu-&amp;gt;linear-&amp;gt;relu-&amp;gt;linear-&amp;gt;softmax&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>改善深层神经网络——第二周编程作业</title>
    <link href="https://www.xiapf.com/blogs/ipvNNet2/"/>
    <id>https://www.xiapf.com/blogs/ipvNNet2/</id>
    <published>2020-05-15T07:11:21.000Z</published>
    <updated>2020-10-27T06:04:42.467Z</updated>
    
    <content type="html"><![CDATA[<p>可以通过优化算法提高网络训练速度</p><h2 id="Mini-batch梯度下降算法"><a href="#Mini-batch梯度下降算法" class="headerlink" title="Mini_batch梯度下降算法"></a>Mini_batch梯度下降算法</h2><p>梯度下降算法将样本全部遍历一遍后，再计算梯度，当样本很多时，训练时间太长。</p><p>这时候需要用Mini_batch梯度下降算法：</p><p>（1）将原始的样本打乱，随机分布，使用numpy库中permutation生成随机数组，打乱样本集</p><p>（2）将样本按照用户输入的大小mini_batch_size（一般是64 ~ 512，即符合计算机内存大小的2的幂次方，这样训练速度会快），将样本重新分为x{1}，x{2} …x{n}，其中划分的个数num_mini_batch=样本总个数m / mini_batch_size，则第一个mini_batch，即x{1}=train_x[:，0:mini_batch_size * 1]，第二个mini_batch，即x{2}=train_x[:，mini_batch_size * 1：mini_batch_size * 2]</p><a id="more"></a><p>（3）由于划分的个数num_mini_batch可能有小数部分，第二步是处理其整数部分，当处理结束后，需要判断是否有余下部分，有余下部分需要将数据补全在最后x{n}=train_x[:，mini_batch_size * num_mini_batch：]</p><p>（4）注：当用户输入的大小mini_batch_size=样本总数时，算法变为梯度下降法，即一次遍历整个样本集，当用户输入的大小mini_batch_size==1时，算法变为随机梯度下降算法，即一次遍历一个样本，此时失去了将样本向量化的加速作用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用mini_batch的梯度下降（小批量梯度下降）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_mini_batches</span><span class="params">(X,Y,mini_batch_size,seed)</span>:</span></span><br><span class="line">np.random.seed(seed)</span><br><span class="line">mini_batchs=[]</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取样本的数量</span></span><br><span class="line">m=X.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#1.打乱训练集</span></span><br><span class="line"><span class="comment">#返回长度为m的随机数组</span></span><br><span class="line">permutation=list(np.random.permutation(m))</span><br><span class="line">shuffle_X=X[:,permutation]</span><br><span class="line">shuffle_Y=Y[:,permutation].reshape((<span class="number">1</span>,m))</span><br><span class="line"></span><br><span class="line">num_mini_batch=math.floor(m/mini_batch_size)</span><br><span class="line"><span class="comment">#2.按照大小切分数据</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(num_mini_batch):</span><br><span class="line">mini_batch_X=shuffle_X[:,mini_batch_size*k:mini_batch_size*(k+<span class="number">1</span>)]</span><br><span class="line">mini_batch_Y=shuffle_Y[:,mini_batch_size*k:mini_batch_size*(k+<span class="number">1</span>)]</span><br><span class="line">mini_batch=(mini_batch_X,mini_batch_Y)</span><br><span class="line"></span><br><span class="line">mini_batchs.append(mini_batch)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#3.处理余下部分的数据</span></span><br><span class="line"><span class="keyword">if</span>(m%mini_batch_size!=<span class="number">0</span>):</span><br><span class="line">mini_batch_X=shuffle_X[:,mini_batch_size*num_mini_batch:]</span><br><span class="line">mini_batch_Y=shuffle_Y[:,mini_batch_size*num_mini_batch:]</span><br><span class="line">mini_batch=(mini_batch_X,mini_batch_Y)</span><br><span class="line">mini_batchs.append(mini_batch)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> mini_batchs</span><br></pre></td></tr></table></figure><p>a ）当mini_batch_size设置为样本总数时</p><p>损失变化情况</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200515143104.png" alt></p><p>train data:<br>Accuracy: 0.66</p><p>打印出的决策边界</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200515143125.png" alt></p><p>b ） 当mini_batch_size设置为64时</p><p>损失变化情况</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200515142350.png" alt></p><p>train data:<br>Accuracy: 0.7966666666666666</p><p>打印出的决策边界</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200515142739.png" alt></p><p>分析</p><p>可以看出梯度下降算法基本无摆动，但是损失下降到0.64左右变化幅度很小，从分类情况来看误分角度，而mini_batch梯度下降算法摆动较大，分类准确率比梯度下降要高一些。</p><h2 id="Momentun算法（带动量的梯度下降算法）"><a href="#Momentun算法（带动量的梯度下降算法）" class="headerlink" title="Momentun算法（带动量的梯度下降算法）"></a>Momentun算法（带动量的梯度下降算法）</h2><p>Momentun算法使用了指数加权平均项来计算梯度，动量参数beta设置为0.9，每个梯度的变化前都加入了指数项，随着时间增加，梯度前的指数项衰减，即每次梯度的变化都参照了前面梯度的变化方向（越近的梯度权重越高），从而减少向最低点前进时的摆动。</p><p>注：指数加权平均项 v(t)=beta * v(t-1)+(1-beta) * θ</p><p>（1）在初始化参数的时候，按照参数W，b的维度生成动量项</p><p>（2）当选择带动量的梯度下降算法时，按照指数加权平均公式，计算动量项，当单纯使用momentum算法时，不需要偏差修正，根据动量项更新参数W，b</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#momentum算法</span></span><br><span class="line"><span class="comment">#通过指数加权平均方法改变梯度，减少摆动幅度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_momentum</span><span class="params">(parameters)</span>:</span></span><br><span class="line">L=len(parameters)//<span class="number">2</span></span><br><span class="line">v=&#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(L):</span><br><span class="line">v[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]=np.zeros_like(parameters[<span class="string">"W"</span>+str(i+<span class="number">1</span>)])</span><br><span class="line">v[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]=np.zeros_like(parameters[<span class="string">"b"</span>+str(i+<span class="number">1</span>)])</span><br><span class="line"><span class="keyword">return</span> v</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameter_momentum</span><span class="params">(parameters,grads,v,beta,learning_rate)</span>:</span></span><br><span class="line">L=len(parameters)//<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(L):</span><br><span class="line">v[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]=beta*v[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]+(<span class="number">1</span>-beta)*grads[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]</span><br><span class="line">v[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]=beta*v[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]+(<span class="number">1</span>-beta)*grads[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">parameters[<span class="string">"W"</span>+str(i+<span class="number">1</span>)]=parameters[<span class="string">"W"</span>+str(i+<span class="number">1</span>)]-learning_rate*v[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]</span><br><span class="line">parameters[<span class="string">"b"</span>+str(i+<span class="number">1</span>)]=parameters[<span class="string">"b"</span>+str(i+<span class="number">1</span>)]-learning_rate*v[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]</span><br><span class="line"><span class="keyword">return</span> parameters,v</span><br></pre></td></tr></table></figure><p>结果</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200515143700.png" alt></p><p>train data:<br>Accuracy: 0.8766666666666667</p><p>打印出的决策边界</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200515143712.png" alt></p><p>分析</p><p>momentun算法中由于每次梯度的变化都是参考之前的梯度进行变化，即增加了动量，摆动幅度比只使用mini_batch算法较小，并且分类正确率也提高了。</p><h2 id="Adam算法（结合Momentun和RMSprop算法）"><a href="#Adam算法（结合Momentun和RMSprop算法）" class="headerlink" title="Adam算法（结合Momentun和RMSprop算法）"></a>Adam算法（结合Momentun和RMSprop算法）</h2><p>RMSprop算法是保留微分平方的加权平均数的梯度下降算法，也是用于较少梯度变化中的摆动。</p><p>计算公式：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200515144636.png" alt></p><p>在对每个mini_batch迭代中，按照指数加权平均的衰减方式计算梯度，但是这里梯度是需要将微分平方，然后再更新参数，为了让更新参数中的分母为0，这里将分母加上epsilon(默认值为1e-8)</p><p>Adam算法是结合Momentun和RMSprop算法</p><p>（1）momentun部分：在初始化参数的时候，按照参数W，b的维度生成动量项；RMSprop部分：在初始化参数的时候，按照参数W，b生成生成初始项Sdw，Sdb</p><p>（2）momentun部分：按照指数加权平均公式，计算动量项，并进行偏差修正(v /（1-np.power（beta1,t）))，得到修正之后的动量项；RMSprop部分：按照保留微分平方的加权平均数方式，得到Sdw，Sdb，并进行偏差修正(s /（1-np.power（beta2,t）))，其中momentum部分的beat1默认为0.9，RMSprop部分的beta2默认为0.99</p><p>（3）使用指数衰减之后的梯度值更新参数W，b</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200515145910.png" alt></p><p>注：偏差修正是将需要修正的项目v或者s除以（1-np.power（beta,t）），其中t代表第几个mini_catch，这里的beta都是小于1的（由于是进行指数衰减），当t越大，np.power（beta,t）的值越接近于0，分母就接近1，此时基本没有修正，当t越小，修正越多，这是由于算法初始的时候，由于迭代次数少，数值出现偏差可能大，所以在初始时需要修正多一些。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#adam算法</span></span><br><span class="line"><span class="comment">#结合了momentun算法和RMSprop算法，并进行偏差修正</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_adam</span><span class="params">(parameters)</span>:</span></span><br><span class="line">L=len(parameters)//<span class="number">2</span></span><br><span class="line">v=&#123;&#125;</span><br><span class="line">s=&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(L):</span><br><span class="line">v[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]=np.zeros_like(parameters[<span class="string">"W"</span>+str(i+<span class="number">1</span>)])</span><br><span class="line">v[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]=np.zeros_like(parameters[<span class="string">"b"</span>+str(i+<span class="number">1</span>)])</span><br><span class="line">s[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]=np.zeros_like(parameters[<span class="string">"W"</span>+str(i+<span class="number">1</span>)])</span><br><span class="line">s[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]=np.zeros_like(parameters[<span class="string">"b"</span>+str(i+<span class="number">1</span>)])</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> v,s</span><br><span class="line"></span><br><span class="line"><span class="comment">#t表示迭代的mini_batch的次数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameter_adam</span><span class="params">(parameters,grads,v,s,t,learning_rate=<span class="number">0.01</span>,beta1=<span class="number">0.9</span>,beta2=<span class="number">0.999</span>,epsilon=<span class="number">1e-8</span>)</span>:</span></span><br><span class="line">L=len(parameters)//<span class="number">2</span></span><br><span class="line">vcorrect=&#123;&#125;</span><br><span class="line">scorrect=&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(L):</span><br><span class="line">v[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]=beta1*v[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]+(<span class="number">1</span>-beta1)*grads[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]</span><br><span class="line">v[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]=beta1*v[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]+(<span class="number">1</span>-beta1)*grads[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]</span><br><span class="line">vcorrect[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]=v[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]/(<span class="number">1</span>-np.power(beta1,t))</span><br><span class="line">vcorrect[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]=v[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]/(<span class="number">1</span>-np.power(beta1,t))</span><br><span class="line"></span><br><span class="line">s[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]=beta2*s[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]+(<span class="number">1</span>-beta2)*(np.square(grads[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]))</span><br><span class="line">s[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]=beta2*s[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]+(<span class="number">1</span>-beta2)*(np.square(grads[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]))</span><br><span class="line">scorrect[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]=s[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]/(<span class="number">1</span>-np.power(beta2,t))</span><br><span class="line">scorrect[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]=s[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]/(<span class="number">1</span>-np.power(beta2,t))</span><br><span class="line"></span><br><span class="line">parameters[<span class="string">"W"</span>+str(i+<span class="number">1</span>)]=parameters[<span class="string">"W"</span>+str(i+<span class="number">1</span>)]-learning_rate*vcorrect[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]/np.sqrt(scorrect[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]+epsilon)</span><br><span class="line">parameters[<span class="string">"b"</span>+str(i+<span class="number">1</span>)]=parameters[<span class="string">"b"</span>+str(i+<span class="number">1</span>)]-learning_rate*vcorrect[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]/np.sqrt(scorrect[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]+epsilon)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> parameters,v,s</span><br></pre></td></tr></table></figure><p>结果</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200515150033.png" alt></p><p>train data:<br>Accuracy: 0.94</p><p>打印出的决策边界</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200515150045.png" alt></p><p>分析</p><p>可以看出，adam算法基本无摆动，分类准确率也很高</p><h2 id="附：学习率衰减"><a href="#附：学习率衰减" class="headerlink" title="附：学习率衰减"></a>附：学习率衰减</h2><p>在学习初期，学习率大；在收敛时，学习率小，按照以下公式设置学习率</p><p>alpha=(1 / （1+衰减率 * 迭代次数）) * alpha0</p><p>还有其他指数衰减或者离散衰减方式</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200515162005.png" alt></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>（1）当样本集数据量很大时，使用mini_batch算法加快训练速度，每次遍历mini_batch_size大小的数据，当用户输入的大小mini_batch_size=样本总数时，算法变为梯度下降法，即一次遍历整个样本集，当用户输入的大小mini_batch_size==1时，算法变为随机梯度下降算法，即一次遍历一个样本，此时失去了将样本向量化的加速作用。</p><p>（2）由于梯度下降时存在摆动，即噪声很大，此时学习率需要很小，即向最低点前进的步长小，这时也会导致网络训练速度减慢，这时候就需要使 用指数加权平均项来得到当前的梯度：每个梯度的变化前都加入了指数项，随着时间增加，梯度前的指数项衰减，即每次梯度的变化都参照了前面梯度的变化方向（越近的梯度权重越高），从而减少向最低点前进时的摆动。其中momentun算法和Adam算法能有效减少摆动，可以增加学习率，即加大每次参数变化的步长，从而加快了网络训练速度，其中Adam算法由于结合了结合Momentun和RMSprop算法，优化效果最好，并且分类准确率也高。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;可以通过优化算法提高网络训练速度&lt;/p&gt;&lt;h2 id=&quot;Mini-batch梯度下降算法&quot;&gt;&lt;a href=&quot;#Mini-batch梯度下降算法&quot; class=&quot;headerlink&quot; title=&quot;Mini_batch梯度下降算法&quot;&gt;&lt;/a&gt;Mini_batch梯度下降算法&lt;/h2&gt;&lt;p&gt;梯度下降算法将样本全部遍历一遍后，再计算梯度，当样本很多时，训练时间太长。&lt;/p&gt;&lt;p&gt;这时候需要用Mini_batch梯度下降算法：&lt;/p&gt;&lt;p&gt;（1）将原始的样本打乱，随机分布，使用numpy库中permutation生成随机数组，打乱样本集&lt;/p&gt;&lt;p&gt;（2）将样本按照用户输入的大小mini_batch_size（一般是64 ~ 512，即符合计算机内存大小的2的幂次方，这样训练速度会快），将样本重新分为x{1}，x{2} …x{n}，其中划分的个数num_mini_batch=样本总个数m / mini_batch_size，则第一个mini_batch，即x{1}=train_x[:，0:mini_batch_size * 1]，第二个mini_batch，即x{2}=train_x[:，mini_batch_size * 1：mini_batch_size * 2]&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>（转载）windows10专业版,我们无法在此设备上激活windows因为无法连接到你的组织的激活服务器解决方法</title>
    <link href="https://www.xiapf.com/blogs/sovConSever/"/>
    <id>https://www.xiapf.com/blogs/sovConSever/</id>
    <published>2020-05-15T02:27:50.000Z</published>
    <updated>2020-05-15T02:37:38.047Z</updated>
    
    <content type="html"><![CDATA[<p>转载于 <a href="https://www.jianshu.com/p/953c1517e436" target="_blank" rel="external nofollow noopener noreferrer">https://www.jianshu.com/p/953c1517e436</a> </p><p>1.管理员运行cmd，然后输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">slmgr /ipk W269N-WFGWX-YVC9B-4J6C9-T83GX</span><br></pre></td></tr></table></figure><p>2.再输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">slmgr.vbs -skms zh.us.to</span><br></pre></td></tr></table></figure><p>3.再设置–更新和安全–激活中，点击更新产品密钥，输入以上密钥 W269N-WFGWX-YVC9B-4J6C9-T83GX，并点击下一步激活该密钥。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转载于 &lt;a href=&quot;https://www.jianshu.com/p/953c1517e436&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;https://www.jianshu.com/
      
    
    </summary>
    
    
      <category term="win10" scheme="https://www.xiapf.com/categories/win10/"/>
    
    
      <category term="win10" scheme="https://www.xiapf.com/tags/win10/"/>
    
  </entry>
  
  <entry>
    <title>改善深层神经网络——第一周编程作业</title>
    <link href="https://www.xiapf.com/blogs/ipvNNet1/"/>
    <id>https://www.xiapf.com/blogs/ipvNNet1/</id>
    <published>2020-05-13T03:29:13.000Z</published>
    <updated>2020-10-27T06:04:51.715Z</updated>
    
    <content type="html"><![CDATA[<h2 id="训练集、验证集、测试集选取"><a href="#训练集、验证集、测试集选取" class="headerlink" title="训练集、验证集、测试集选取"></a>训练集、验证集、测试集选取</h2><p>训练集用于根据算法训练不同的模型</p><p>验证集用于验证不同的算法中哪一种更有效</p><p>测试集用于评估算法的性能</p><p>对于大数据（如一万条数据以上），训练集，验证集，测试集的比列为98%，1%，1%，训练集占绝大部分。</p><h2 id="提高训练速度"><a href="#提高训练速度" class="headerlink" title="提高训练速度"></a>提高训练速度</h2><h3 id="归一化输入特征"><a href="#归一化输入特征" class="headerlink" title="归一化输入特征"></a>归一化输入特征</h3><p>未归一化的输入特征得到的损失函数是比较狭长的，而且在反向传播中使用梯度下降时，步长需要很小，否则无法找到最小值。</p><a id="more"></a><p>所有需要归一化输入特征，将输入特征统一减去数据均值并除以方差，使得得出的损失函数更匀称，无论从什么步长都能得到最小值。</p><p>需要注意的是，测试集，验证集都需要很训练集一样的归一化方式</p><h3 id="初始权重的选取"><a href="#初始权重的选取" class="headerlink" title="初始权重的选取"></a>初始权重的选取</h3><p>不同初始化方法的对比</p><p>（1）权重全都初始化为0</p><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.初始化参数为0</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_zeros</span><span class="params">(layer_dims)</span>:</span></span><br><span class="line">L=len(layer_dims)</span><br><span class="line">parameters=&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,L):</span><br><span class="line">parameters[<span class="string">"W"</span>+str(i)]=np.zeros((layer_dims[i],layer_dims[i<span class="number">-1</span>]))</span><br><span class="line">parameters[<span class="string">"b"</span>+str(i)]=np.zeros((layer_dims[i],<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>结果</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200513104721.png" alt></p><p>分析</p><p>可以看出使用该种初始化权重的方式，损失没有变化，并且预测精度也很低：训练集Accuracy: 0.5，测试集Accuracy: 0.5。主要是因为权重初始化为0，使得所有神经元都做相同的事，存在对称性。</p><p>（2）权重用很大的随机值初始化</p><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#2.初始化参数为很大的随机数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_random</span><span class="params">(layer_dims)</span>:</span></span><br><span class="line">L=len(layer_dims)</span><br><span class="line">parameters=&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,L):</span><br><span class="line">parameters[<span class="string">"W"</span>+str(i)]=np.random.randn(layer_dims[i],layer_dims[i<span class="number">-1</span>])*<span class="number">10</span></span><br><span class="line">parameters[<span class="string">"b"</span>+str(i)]=np.zeros((layer_dims[i],<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>结果</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200513105002.png" alt></p><p>分析</p><p>可以看出使用该种初始化权重的方式，损失一开始由于权重较大，下降快，但到后半段，损失下降很慢，因为w变大，z变大，此时的梯度变化下，训练速度减慢。预测精度也不是很高：训练集Accuracy: 0.8833333333333333，测试集Accuracy：0.85。</p><p>（3）采用抑制梯度爆炸和梯度消失的方式初始化</p><p>当权重大于1时，梯度以指数方式增长，当权重小于1时，梯度递减，为了防止出现这种梯度爆炸和梯度消失的出现，需要将权重初始化为不比1大很多，不比1小很多的值：w=np.random.randn(shape) * sqrt(1/n[l-1])，即需要除以上一层神经元的个数（n[l-1]代表上一层神经元个数），将权重w归一化。（当激活函数是tanh时，使用sqrt(2/n[l-1]效果更好)</p><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#3.初始化参数使用抑梯度异常方式  乘以sqrt(2/上一层神经元数量)  把参数w初始化到了1附近</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_he</span><span class="params">(layer_dims)</span>:</span></span><br><span class="line">L=len(layer_dims)</span><br><span class="line">parameters=&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,L):</span><br><span class="line">parameters[<span class="string">"W"</span>+str(i)]=np.random.randn(layer_dims[i],layer_dims[i<span class="number">-1</span>])*np.sqrt(<span class="number">2</span>/layer_dims[i<span class="number">-1</span>])</span><br><span class="line">parameters[<span class="string">"b"</span>+str(i)]=np.zeros((layer_dims[i],<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>结果</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200513104411.png" alt></p><p>分析</p><p>可以看出使用该种初始化权重的方式，损失逐渐减少，并且预测精度也很高：训练集Accuracy: 0.9933333333333333，测试集Accuracy: 0.93。</p><p>总结</p><p>采用抑制梯度爆炸和梯度消失的方式初始化权重</p><h2 id="高偏差高方差的解决"><a href="#高偏差高方差的解决" class="headerlink" title="高偏差高方差的解决"></a>高偏差高方差的解决</h2><h3 id="高偏差"><a href="#高偏差" class="headerlink" title="高偏差"></a>高偏差</h3><p>高偏差是指训练集的误差大，即模型没有学到训练集的所有特征，不能很好的拟合训练数据。</p><p>解决方法：</p><p>（1）选择更大的网络（增加网络层数或者神经元数量）</p><p>（2）训练时间更长</p><h3 id="高方差"><a href="#高方差" class="headerlink" title="高方差"></a>高方差</h3><p>高方差是指训练的模型在测试集的误差大，即模型存在过拟合，过度拟合训练集数据。</p><p>解决方法：</p><p>（1）正则化</p><p>（2）更多训练数据</p><p>一般第二种方法成本较高，当出现高方差的时候选择正则化方法</p><p>正则化方法简单来说就是减少对输入数据的学习，以防止过拟合。</p><p>不同正则化方法对比：</p><p>（0）当不使用正则化</p><p>lambd是L2正则化中的参数,keep_prop是dropout正则化中的参数，lambd=0,keep_prop=1.0代表不使用正则化</p><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#是否加入正则化的模型  lambd是L2正则化中的参数,keep_prop是dropout正则化中的参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_regularization</span><span class="params">(X,Y,layer_dims,iter=<span class="number">30000</span>,learning_rate=<span class="number">0.3</span>,lambd=<span class="number">0</span>,keep_prop=<span class="number">1.0</span>)</span>:</span></span><br><span class="line"><span class="comment">#1.初始化参数 防止梯度爆炸和梯度消失</span></span><br><span class="line">parameters=ru.initialize_parameters(layer_dims)</span><br><span class="line">costs=[]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(iter):</span><br><span class="line"><span class="comment">#2.前向传播 是否进行keep_prop正则化</span></span><br><span class="line"><span class="keyword">if</span>(keep_prop==<span class="number">1.0</span>):</span><br><span class="line">A,cache=ru.forward_propagation(X,parameters)</span><br><span class="line"><span class="keyword">elif</span>(keep_prop&lt;<span class="number">1.0</span>):</span><br><span class="line">A,cache=forward_propagation_dropout(X,parameters,keep_prop)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">print(<span class="string">"keep_prop参数错误"</span>)</span><br><span class="line">exit</span><br><span class="line"></span><br><span class="line"><span class="comment">#3.计算损失，判断是否使用L2正则化</span></span><br><span class="line"><span class="keyword">if</span>(lambd==<span class="number">0</span>):</span><br><span class="line">cost=ru.compute_cost(A,Y)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">cost=compute_loss_L2(A,Y,parameters,lambd)</span><br><span class="line"></span><br><span class="line"><span class="comment">#4.反向传播 是否进行正则化(dropout或者L2)</span></span><br><span class="line"><span class="keyword">if</span>(keep_prop==<span class="number">1</span>) <span class="keyword">and</span> (lambd==<span class="number">0</span>):</span><br><span class="line">grad=ru.backward_propagation(X,Y,cache)</span><br><span class="line"><span class="keyword">elif</span>(lambd!=<span class="number">0</span>):</span><br><span class="line">grad=backward_propagation_L2(X,Y,cache,lambd)</span><br><span class="line"><span class="keyword">elif</span>(keep_prop&lt;<span class="number">1</span>):</span><br><span class="line">grad=backward_propagation_dropout(X,Y,cache,keep_prop)</span><br><span class="line"></span><br><span class="line"><span class="comment">#5.更新参数</span></span><br><span class="line">parameters=ru.update_parameters(parameters,grad,learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(i%<span class="number">1000</span>==<span class="number">0</span>):</span><br><span class="line">costs.append(cost)</span><br><span class="line">print(<span class="string">"第"</span>+str(i)+<span class="string">"次，cost:"</span>+str(cost))</span><br><span class="line"></span><br><span class="line"><span class="comment">#绘制曲线</span></span><br><span class="line">plt.plot(costs)</span><br><span class="line">plt.xlabel(<span class="string">"iteration"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"cost"</span>)</span><br><span class="line">plt.title(<span class="string">"learning_rate="</span>+str(learning_rate))</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>结果</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200513110235.png" alt></p><p>训练集<br>Accuracy: 0.9478672985781991<br>测试集<br>Accuracy: 0.915</p><p>分析</p><p>从图中可以看出模型存在过拟合，训练集准确率明显高于测试集很多。</p><p>（1）L2范数</p><p>在损失函数中加入L2参数，使得权重缩减，即减少对一些不重要特征的学习。</p><p><strong>L2范数正则化是在计算损失时加入正则化参数，并在反向传播结果调整权重。</strong></p><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用L2范数进行正则化</span></span><br><span class="line"><span class="comment">#计算损失</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_loss_L2</span><span class="params">(A,Y,parameters,lambd)</span>:</span></span><br><span class="line">m=A.shape[<span class="number">1</span>]</span><br><span class="line">W1=parameters[<span class="string">"W1"</span>]</span><br><span class="line">W2=parameters[<span class="string">"W2"</span>]</span><br><span class="line">W3=parameters[<span class="string">"W3"</span>]</span><br><span class="line"></span><br><span class="line">loss=ru.compute_cost(A,Y)</span><br><span class="line">L2_regularization=(lambd/(<span class="number">2</span>*m))*(np.sum(np.square(W1))+np.sum(np.square(W2))+np.sum(np.square(W3)))</span><br><span class="line">all_loss=loss+L2_regularization</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> all_loss</span><br><span class="line"></span><br><span class="line"><span class="comment">#反向传播过程</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation_L2</span><span class="params">(X,Y,cache,lambd)</span>:</span></span><br><span class="line">Z1,A1,W1,b1,Z2,A2,W2,b2,Z3,A3,W3,b3=cache</span><br><span class="line"></span><br><span class="line">m=X.shape[<span class="number">1</span>]</span><br><span class="line">dZ3=A3-Y</span><br><span class="line">dW3=(<span class="number">1</span>/m)*np.dot(dZ3,A2.T)+(lambd/m)*W3</span><br><span class="line">db3=(<span class="number">1</span>/m)*np.sum(dZ3,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">dA2=np.dot(W3.T,dZ3)</span><br><span class="line">dZ2=np.multiply(dA2,np.int64(A2&gt;<span class="number">0</span>))</span><br><span class="line">dW2=(<span class="number">1</span>/m)*np.dot(dZ2,A1.T)+(lambd/m)*W2</span><br><span class="line">db2=(<span class="number">1</span>/m)*np.sum(dZ2,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">dA1=np.dot(W2.T,dZ2)</span><br><span class="line">dZ1=np.multiply(dA1,np.int64(A1&gt;<span class="number">0</span>))</span><br><span class="line">dW1=(<span class="number">1</span>/m)*np.dot(dZ1,X.T)+(lambd/m)*W1</span><br><span class="line">db1=(<span class="number">1</span>/m)*np.sum(dZ1,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">grad=&#123;<span class="string">"dW1"</span>:dW1,<span class="string">"dW2"</span>:dW2,<span class="string">"dW3"</span>:dW3,<span class="string">"db1"</span>:db1,<span class="string">"db2"</span>:db2,<span class="string">"db3"</span>:db3&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> grad</span><br></pre></td></tr></table></figure><p>结果</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200513110551.png" alt></p><p>训练集<br>Accuracy: 0.9383886255924171<br>测试集<br>Accuracy: 0.93</p><p>分析</p><p>从图中可以看出决策边界变得平滑，训练集和测试集误差相近，减少了模型的过拟合情况</p><p>（b）dropout随机失活</p><p><strong>dropout随机失活方法是在每次训练模型时，按照一定概率随机删除一些节点，即每次都训练一个小型的网络，keep_prop是保留节点的概率。</strong></p><p>但由于随机删除一些节点，在随机失活方法中成本函数没有明确的定义</p><p>1°前向传播</p><p>在前向传播过程中根据阈值keep_prop决定每个节点是否保留：D=np.random.rand(A.shape[0],A.shape[1])&lt;keep_prop</p><p>过滤所有为0的节点，并且按照阈值修补缺失节点后的函数值：A=A * D / keep_prop</p><p>2°反向传播</p><p>根据前向传播中每个节点的保留情况，删除缺失的节点，并计算缺失节点后的梯度值：dA=dA * D / keep_prop</p><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用dropout进行正则化，随机删除一些节点</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#正向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation_dropout</span><span class="params">(X,parameters,keep_prop=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">W1=parameters[<span class="string">"W1"</span>]</span><br><span class="line">W2=parameters[<span class="string">"W2"</span>]</span><br><span class="line">W3=parameters[<span class="string">"W3"</span>]</span><br><span class="line">b1=parameters[<span class="string">"b1"</span>]</span><br><span class="line">b2=parameters[<span class="string">"b2"</span>]</span><br><span class="line">b3=parameters[<span class="string">"b3"</span>]</span><br><span class="line"></span><br><span class="line">Z1=np.dot(W1,X)+b1</span><br><span class="line">A1=ru.relu(Z1)</span><br><span class="line">D1=np.random.rand(A1.shape[<span class="number">0</span>],A1.shape[<span class="number">1</span>])</span><br><span class="line"><span class="comment">#小于keep_prop的设置为1，反之为0</span></span><br><span class="line">D1=D1&lt;keep_prop</span><br><span class="line">A1=A1*D1</span><br><span class="line">A1=A1/keep_prop</span><br><span class="line"></span><br><span class="line">Z2=np.dot(W2,A1)+b2</span><br><span class="line">A2=ru.relu(Z2)</span><br><span class="line">D2=np.random.rand(A2.shape[<span class="number">0</span>],A2.shape[<span class="number">1</span>])</span><br><span class="line">D2=D2&lt;keep_prop</span><br><span class="line">A2=A2*D2</span><br><span class="line">A2=A2/keep_prop</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Z3=np.dot(W3,A2)+b3</span><br><span class="line">A3=ru.sigmoid(Z3)</span><br><span class="line"><span class="comment"># D3=np.random.rand(A3.shape[0],A3.shape[1])</span></span><br><span class="line"><span class="comment"># D3=D3&lt;keep_prop</span></span><br><span class="line"><span class="comment"># A3=A3*D3</span></span><br><span class="line"><span class="comment"># A3=A3/keep_prop</span></span><br><span class="line"></span><br><span class="line">cache=[Z1,D1,A1,W1,b1,Z2,D2,A2,W2,b2,Z3,A3,W3,b3]</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> A3,cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation_dropout</span><span class="params">(X,Y,cache,keep_prop)</span>:</span></span><br><span class="line">Z1,D1,A1,W1,b1,Z2,D2,A2,W2,b2,Z3,A3,W3,b3=cache</span><br><span class="line"></span><br><span class="line">m=X.shape[<span class="number">1</span>]</span><br><span class="line">dZ3=A3-Y</span><br><span class="line">dW3=(<span class="number">1</span>/m)*np.dot(dZ3,A2.T)</span><br><span class="line">db3=(<span class="number">1</span>/m)*np.sum(dZ3,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">dA2=np.dot(W3.T,dZ3)</span><br><span class="line">dA2=dA2*D2</span><br><span class="line">dA2=dA2/keep_prop</span><br><span class="line"></span><br><span class="line">dZ2=np.multiply(dA2,np.int64(A2&gt;<span class="number">0</span>))</span><br><span class="line">dW2=(<span class="number">1</span>/m)*np.dot(dZ2,A1.T)</span><br><span class="line">db2=(<span class="number">1</span>/m)*np.sum(dZ2,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">dA1=np.dot(W2.T,dZ2)</span><br><span class="line">dA1=dA1*D1</span><br><span class="line">dA1=dA1/keep_prop</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dZ1=np.multiply(dA1,np.int64(A1&gt;<span class="number">0</span>))</span><br><span class="line">dW1=(<span class="number">1</span>/m)*np.dot(dZ1,X.T)</span><br><span class="line">db1=(<span class="number">1</span>/m)*np.sum(dZ1,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">grad=&#123;<span class="string">"dW1"</span>:dW1,<span class="string">"dW2"</span>:dW2,<span class="string">"dW3"</span>:dW3,<span class="string">"db1"</span>:db1,<span class="string">"db2"</span>:db2,<span class="string">"db3"</span>:db3&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> grad</span><br></pre></td></tr></table></figure><p>结果</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200513111546.png" alt></p><p>训练集<br>Accuracy: 0.943127962085308<br>测试集<br>Accuracy: 0.935</p><p>分析</p><p>从图中可以看出决策边界变得平滑，训练集和测试集误差相近，减少了模型的过拟合情况。</p><h2 id="梯度检验"><a href="#梯度检验" class="headerlink" title="梯度检验"></a>梯度检验</h2><p>通过双边误差检验反向传播是否很好的实现，主要用于调试</p><p>（1）得到算法中的梯度grads</p><p>（2）计算双边误差gradprox=J_plus(θ+eplison)-J_minus(θ+eplison)/2 * eplison</p><p>（3）比较双边误差和梯度的距离d=||grads-gradprox||2 / (||grads||2+||gradprox||2)</p><p>当距离小于1e-7说明反向传播中梯度正确实现了，反之则没有</p><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#梯度校验</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grandient_check_n</span><span class="params">(parameters,grad,X,Y,eplison=<span class="number">1e-7</span>)</span>:</span></span><br><span class="line">parameters_values=gu.dictionary_to_vector(parameters)</span><br><span class="line">grad_values=gu.gradients_to_vector(grad)</span><br><span class="line">num_iter=parameters_values.shape[<span class="number">0</span>]</span><br><span class="line">J_min=np.zeros((num_iter,<span class="number">1</span>))</span><br><span class="line">J_max=np.zeros((num_iter,<span class="number">1</span>))</span><br><span class="line">gradprox=np.zeros((num_iter,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_iter):</span><br><span class="line">temp_para=np.copy(parameters_values)</span><br><span class="line">temp_para[i][<span class="number">0</span>]=temp_para[i][<span class="number">0</span>]-eplison</span><br><span class="line">J_min[i],cache=forward_propagation_n(X,Y,gu.vector_to_dictionary(temp_para))</span><br><span class="line"></span><br><span class="line">temp_para2=np.copy(parameters_values)</span><br><span class="line">temp_para2[i][<span class="number">0</span>]=temp_para2[i][<span class="number">0</span>]+eplison</span><br><span class="line">J_max[i],cache=forward_propagation_n(X,Y,gu.vector_to_dictionary(temp_para))</span><br><span class="line"></span><br><span class="line">gradprox[i]=J_max[i]-J_min[i]/<span class="number">2</span>*eplison</span><br><span class="line"></span><br><span class="line"><span class="comment">#向量之间的距离</span></span><br><span class="line">denum=np.liang.norm(grad_values)+np.liang.norm(gradprox)</span><br><span class="line">nenum=np.liang.norm(grad_values-gradprox)</span><br><span class="line">difference=nenum/denum</span><br><span class="line"><span class="keyword">if</span> difference&lt;<span class="number">1e-7</span>:</span><br><span class="line">print(<span class="string">"梯度检验正确"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">print(<span class="string">"梯度检验有误"</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;训练集、验证集、测试集选取&quot;&gt;&lt;a href=&quot;#训练集、验证集、测试集选取&quot; class=&quot;headerlink&quot; title=&quot;训练集、验证集、测试集选取&quot;&gt;&lt;/a&gt;训练集、验证集、测试集选取&lt;/h2&gt;&lt;p&gt;训练集用于根据算法训练不同的模型&lt;/p&gt;&lt;p&gt;验证集用于验证不同的算法中哪一种更有效&lt;/p&gt;&lt;p&gt;测试集用于评估算法的性能&lt;/p&gt;&lt;p&gt;对于大数据（如一万条数据以上），训练集，验证集，测试集的比列为98%，1%，1%，训练集占绝大部分。&lt;/p&gt;&lt;h2 id=&quot;提高训练速度&quot;&gt;&lt;a href=&quot;#提高训练速度&quot; class=&quot;headerlink&quot; title=&quot;提高训练速度&quot;&gt;&lt;/a&gt;提高训练速度&lt;/h2&gt;&lt;h3 id=&quot;归一化输入特征&quot;&gt;&lt;a href=&quot;#归一化输入特征&quot; class=&quot;headerlink&quot; title=&quot;归一化输入特征&quot;&gt;&lt;/a&gt;归一化输入特征&lt;/h3&gt;&lt;p&gt;未归一化的输入特征得到的损失函数是比较狭长的，而且在反向传播中使用梯度下降时，步长需要很小，否则无法找到最小值。&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>神经网络和深度学习——搭建识别图片的多层神经网络</title>
    <link href="https://www.xiapf.com/blogs/NNet4/"/>
    <id>https://www.xiapf.com/blogs/NNet4/</id>
    <published>2020-05-09T08:22:07.000Z</published>
    <updated>2020-10-27T06:03:41.721Z</updated>
    
    <content type="html"><![CDATA[<h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p>利用猫咪图片搭建能够识别猫图片的神经网络，并使用新的图片测试网络效果</p><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>神经网络的搭建主要分为两部分：前向传播和反向传播</p><p>（1）前向传播</p><p>根据输入的参数及上一层的输入得出每层的函数值，通过每层不同的激活函数得到输出，最终输出结果根据交叉熵模型得出前向传播中的损失。</p><p>（2）反向传播</p><p>根据损失值，从最后一层向前得到每层的梯度，根据梯度下降原则更新参数，向着损失减小的方向变化。</p><a id="more"></a><p>前向传播和反向传播不断循环，当损失达到最小或者趋于稳定时，网络模型则训练结束</p><p>具体原理流程图参照下图：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200509151835.png" alt></p><h2 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h2><p>为了使得网络不陷入当数值增大，激活函数梯度减小而导致学习速度减慢，所有的隐藏层选择使用Relu激活函数，由于是识别分类问题，输出层使用sigmoid函数。</p><p>设搭建L层网络，则输入层是第0层，隐藏层为第1层 ~ 第L-2层，输出层为第L-1层</p><p>搭建多层神经网络步骤</p><p>（1）初始化各层参数（W，b）</p><p>需要初始化参数的层数为第1层 ~ 第L-1层，即参数索引为1 ~ L-1</p><p>设有L层网络，则第l层的权重W和偏置量b的维度满足：</p><p>shape(W)=(n[l],n[l-1])</p><p>shpe(b)=(n[l],1)</p><p>使用正态分布数据初始化每层神经元权重，为防止梯度爆炸，将每层的权重数值除以前一层权重数值的二分之一次方，偏置量初始化为0，将初始化参数进行保存使用。</p><p>（2）前向传播</p><p>第1层 ~ 第L-2层，每层计算前向函数的线性值，并传入relu函数中计算每层的激活值，将前一层的输入和当前层的参数保存为linear_cahe,将每层的函数线性值保存为active_cache，最终得到每一层的cache和每层的输出值。第L-1层，同理，调用sigmoid激活函数。</p><p>最终，将缓存cache保存在caches中，在反向传播中的计算梯度时使用。</p><p>（3）计算损失</p><p>每次迭代，根据最后输出层数据和实际数据，代入交叉熵模型公式中得出损失值，并每百次输出。</p><p>（4）反向传播</p><p>从第L-1层向前更新参数梯度：</p><p>第L-1层，计算输出层的导数，即dAL（交叉熵公式对输出值的导数），根据最后一层的导数，及最后一层的cache（保存了前一层的前向函数线性值，本层的参数，本层的前向函数线性值），根据sigmoid激活函数求导得出dZ，根据导数公式dW=(1/m) * multiply（dZ,dA_pre），db=(1/m) * sum（dZ）,得到参数的梯度，将参数梯度保存在grad中，索引为L（为了和参数索引对应）。</p><p>第L-2层 ~ 第1层，同理，根据后一层的dA值（从保存的grads中取，当前grads索引为i+1，后一层的梯度索引为i+2），当前层的cache，得到梯度保存在索引为i+1的梯度中。</p><p>（5）更新参数</p><p>根据得出的梯度，按照梯度下降公式，即损失减小的方向变化。</p><p>（5）模型预测</p><p>将输入的图像向量和类别，以及之前训练的各层参数及已经训练好的模型传入预测函数中，根据参数和输入的图像向量得出最终的输出A，得出每个输入图像向量对应的输出值，并根据sigmoid特性进行分类，大于0.5的则判定为猫，反之不是猫，将预测的类别和真实类别比较得出最终正确率。</p><p>效果</p><p>搭建5层神经网络，隐藏层神经元个数分别为20，7，5，迭代2500次，学习率为0.0075，最终随着迭代次数的增加损失变化为：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200509160309.png" alt></p><p>训练集正确率为：99.52%</p><p>测试集正确率为：78%</p><p>当输入新的测试图像时，由于神经网络中处理的图像都转换为向量处理，这里将读入的图像使用PIL中的Image模块读取，并将其转换为RGB类型的图片，使用array将图像转换为和训练图片一样大小的矩阵。</p><p>当输入图片是猫的图像时：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200509160906.jpg" alt></p><p>最终得出：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200509161327.png" alt></p><p>当输入图片不是猫的图像时：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200509161223.jpg" alt></p><p>最终得出：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200509161153.png" alt></p><p>可见网络正确识别了猫和非猫的图片</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#搭建一个两层网络和深层网络</span></span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> dnn_utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> testCases <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">from</span> lr_utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> misc,ndimage</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#1.导入数据</span></span><br><span class="line">train_x_original,train_y_original,test_x_original,test_y_original,classes=load_dataset()</span><br><span class="line"></span><br><span class="line"><span class="comment">#把向量竖起来排列</span></span><br><span class="line">train_x=train_x_original.reshape(train_x_original.shape[<span class="number">0</span>],<span class="number">-1</span>).T</span><br><span class="line">test_x=test_x_original.reshape(test_x_original.shape[<span class="number">0</span>],<span class="number">-1</span>).T</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#归一化</span></span><br><span class="line">train_x=train_x/<span class="number">255</span></span><br><span class="line">print()</span><br><span class="line">test_x=test_x/<span class="number">255</span></span><br><span class="line">train_y=train_y_original</span><br><span class="line">test_y=test_y_original</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.参数初始化</span></span><br><span class="line"><span class="comment">#2.0两层网络参数初始化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inital_parameter</span><span class="params">(n_x,n_h,n_y)</span>:</span></span><br><span class="line">W1=random.randn(n_h,n_x)*<span class="number">0.01</span></span><br><span class="line">b1=zeros((n_h,<span class="number">1</span>))</span><br><span class="line">W2=random.randn(n_y,n_h)*<span class="number">0.01</span></span><br><span class="line">b2=zeros((n_y,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">parameter=&#123;<span class="string">"W1"</span>:W1,<span class="string">"b1"</span>:b1,<span class="string">"W2"</span>:W2,<span class="string">"b2"</span>:b2&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> parameter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#2.1深层网络参数初始化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inital_parameter_deep</span><span class="params">(layer_dims)</span>:</span></span><br><span class="line">np.random.seed(<span class="number">3</span>)</span><br><span class="line">parameter=&#123;&#125;</span><br><span class="line">L=len(layer_dims)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,L):</span><br><span class="line">parameter[<span class="string">"W"</span>+str(i)]=random.randn(layer_dims[i],layer_dims[i<span class="number">-1</span>])/sqrt(layer_dims[i<span class="number">-1</span>])</span><br><span class="line">parameter[<span class="string">"b"</span>+str(i)]=zeros((layer_dims[i],<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> parameter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#3.前向传播</span></span><br><span class="line"><span class="comment">#3.1线性值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_forward</span><span class="params">(A,W,b)</span>:</span></span><br><span class="line"></span><br><span class="line">Z=dot(W,A)+b</span><br><span class="line">cache=(A,W,b)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> Z,cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#3.2激活值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward</span><span class="params">(A_pre,W,b,active)</span>:</span></span><br><span class="line"><span class="keyword">if</span> active==<span class="string">"sigmoid"</span>:</span><br><span class="line">Z,linear_cache=linear_forward(A_pre,W,b)</span><br><span class="line">A,active_cache=sigmoid(Z)</span><br><span class="line"></span><br><span class="line"><span class="keyword">elif</span> active==<span class="string">"relu"</span>:</span><br><span class="line">Z,linear_cache=linear_forward(A_pre,W,b)</span><br><span class="line">A,active_cache=relu(Z)</span><br><span class="line"></span><br><span class="line">cache=(linear_cache,active_cache)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> A,cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#深层网络的前向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_forward</span><span class="params">(A,parameter)</span>:</span></span><br><span class="line"><span class="comment">#前L-1个用relu激活，最后一个用sigmoid激活</span></span><br><span class="line">L=len(parameter)//<span class="number">2</span></span><br><span class="line">caches=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,L):</span><br><span class="line">A_pre=A</span><br><span class="line">A,cache=linear_activation_forward(A_pre,parameter[<span class="string">"W"</span>+str(i)],parameter[<span class="string">"b"</span>+str(i)],<span class="string">"relu"</span>)</span><br><span class="line">caches.append(cache)</span><br><span class="line"></span><br><span class="line">A_pre=A</span><br><span class="line">A,cache=linear_activation_forward(A_pre,parameter[<span class="string">"W"</span>+str(L)],parameter[<span class="string">"b"</span>+str(L)],<span class="string">"sigmoid"</span>)</span><br><span class="line">caches.append(cache)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> A,caches</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#4.计算损失</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_loss</span><span class="params">(Yhat,Y)</span>:</span></span><br><span class="line">m=shape(Y)[<span class="number">1</span>]</span><br><span class="line">cost=-sum(multiply(log(Yhat),Y)+multiply(log(<span class="number">1</span>-Yhat),(<span class="number">1</span>-Y)))</span><br><span class="line">cost=(<span class="number">1</span>/m)*cost</span><br><span class="line"><span class="comment"># AL=Yhat</span></span><br><span class="line"><span class="comment"># cost = -sum(multiply(np.log(Yhat),Y) + multiply(np.log(1 - Yhat), 1 - Y)) / m</span></span><br><span class="line"><span class="comment"># cost = squeeze(cost)</span></span><br><span class="line"><span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#5.反向传播</span></span><br><span class="line"><span class="comment">#5.2线性值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_backward</span><span class="params">(dZ,linear_cache)</span>:</span></span><br><span class="line">A_pre,W,b=linear_cache</span><br><span class="line">m=shape(A_pre)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">dW=(<span class="number">1</span>/m)*dot(dZ,A_pre.T)</span><br><span class="line">db=(<span class="number">1</span>/m)*sum(dZ,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">dA_pre=dot(W.T,dZ)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> dA_pre,dW,db</span><br><span class="line"></span><br><span class="line"><span class="comment">#5.1激活值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_backward</span><span class="params">(dA,cache,active)</span>:</span></span><br><span class="line">linear_cache,active_cache=cache</span><br><span class="line"><span class="keyword">if</span> active==<span class="string">"sigmoid"</span>:</span><br><span class="line">dZ=sigmoid_backward(dA,active_cache) <span class="comment">#activecache 保存z</span></span><br><span class="line">dA_pre,dW,db=linear_backward(dZ,linear_cache) <span class="comment">#linear_cache保存A,w,b</span></span><br><span class="line"><span class="keyword">elif</span> active==<span class="string">"relu"</span>:</span><br><span class="line">dZ=relu_backward(dA,active_cache)</span><br><span class="line">dA_pre,dW,db=linear_backward(dZ,linear_cache)</span><br><span class="line"><span class="keyword">return</span> dA_pre,dW,db</span><br><span class="line"></span><br><span class="line"><span class="comment">#深层网络的反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward</span><span class="params">(AL,Y,caches)</span>:</span></span><br><span class="line">L=len(caches)</span><br><span class="line"></span><br><span class="line">grads=&#123;&#125;</span><br><span class="line"></span><br><span class="line">dAL=-(divide(Y,AL)-divide(<span class="number">1</span>-Y,<span class="number">1</span>-AL))</span><br><span class="line">dA_pre,dW,db=linear_activation_backward(dAL,caches[L<span class="number">-1</span>],<span class="string">"sigmoid"</span>)</span><br><span class="line">grads[<span class="string">"dA"</span>+str(L)]=dA_pre</span><br><span class="line">grads[<span class="string">"dW"</span>+str(L)]=dW</span><br><span class="line">grads[<span class="string">"db"</span>+str(L)]=db</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> reversed(range(L<span class="number">-1</span>)): <span class="comment">#注意反转</span></span><br><span class="line">dA_pre,dW,db=linear_activation_backward(grads[<span class="string">"dA"</span>+str(i+<span class="number">2</span>)],caches[i],<span class="string">"relu"</span>)</span><br><span class="line">grads[<span class="string">"dA"</span>+str(i+<span class="number">1</span>)]=dA_pre</span><br><span class="line">grads[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]=dW</span><br><span class="line">grads[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]=db</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> grads</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#6.更新参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameter</span><span class="params">(parameter,grads,learning_rate)</span>:</span></span><br><span class="line">L=len(parameter)//<span class="number">2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(L):</span><br><span class="line">parameter[<span class="string">"W"</span>+str(i+<span class="number">1</span>)]=parameter[<span class="string">"W"</span>+str(i+<span class="number">1</span>)]-learning_rate*grads[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]</span><br><span class="line">parameter[<span class="string">"b"</span>+str(i+<span class="number">1</span>)]=parameter[<span class="string">"b"</span>+str(i+<span class="number">1</span>)]-learning_rate*grads[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> parameter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#7.搭建模型</span></span><br><span class="line"><span class="comment">#两层神经网络</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">two_layer_model</span><span class="params">(X,Y,layer_dims,learning_rate=<span class="number">0.0075</span>,num_iter=<span class="number">3000</span>)</span>:</span></span><br><span class="line"><span class="comment">#初始化参数</span></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">grads=&#123;&#125;</span><br><span class="line">costs=[]</span><br><span class="line">(n_x,n_h,n_y)=layer_dims</span><br><span class="line">parameter=inital_parameter(n_x,n_h,n_y)</span><br><span class="line">W1=parameter[<span class="string">"W1"</span>]</span><br><span class="line">b1=parameter[<span class="string">"b1"</span>]</span><br><span class="line">W2=parameter[<span class="string">"W2"</span>]</span><br><span class="line">b2=parameter[<span class="string">"b2"</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_iter):</span><br><span class="line"><span class="comment">#前向传播</span></span><br><span class="line">A1,cache1=linear_activation_forward(X,W1,b1,<span class="string">"relu"</span>)</span><br><span class="line">A2,cache2=linear_activation_forward(A1,W2,b2,<span class="string">"sigmoid"</span>)</span><br><span class="line"><span class="comment">#计算损失</span></span><br><span class="line">cost=compute_loss(A2,Y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#反向传播</span></span><br><span class="line">dAL=-(divide(Y,A2)-divide(<span class="number">1</span>-Y,<span class="number">1</span>-A2))</span><br><span class="line">dA_pre1,dW2,db2=linear_activation_backward(dAL,cache2,<span class="string">"sigmoid"</span>)</span><br><span class="line">dA_pre0,dW1,db1=linear_activation_backward(dA_pre1,cache1,<span class="string">"relu"</span>)</span><br><span class="line"></span><br><span class="line">grads[<span class="string">"dW1"</span>]=dW1</span><br><span class="line">grads[<span class="string">"dW2"</span>]=dW2</span><br><span class="line">grads[<span class="string">"db1"</span>]=db1</span><br><span class="line">grads[<span class="string">"db2"</span>]=db2</span><br><span class="line"></span><br><span class="line"><span class="comment">#更新参数</span></span><br><span class="line">parameter=update_parameter(parameter,grads,learning_rate)</span><br><span class="line"></span><br><span class="line">W1=parameter[<span class="string">"W1"</span>]</span><br><span class="line">b1=parameter[<span class="string">"b1"</span>]</span><br><span class="line">W2=parameter[<span class="string">"W2"</span>]</span><br><span class="line">b2=parameter[<span class="string">"b2"</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># print("第 "+str(i)+" 次，cost: "+str(cost))</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(i%<span class="number">100</span>==<span class="number">0</span>):</span><br><span class="line">costs.append(cost)</span><br><span class="line">print(<span class="string">"第 "</span>+str(i)+<span class="string">" 次，cost: "</span>+str(cost))</span><br><span class="line"></span><br><span class="line">plt.plot(costs)</span><br><span class="line">plt.xlabel(<span class="string">"iter"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"cost"</span>)</span><br><span class="line">plt.title(<span class="string">"learning_rate:"</span>+str(learning_rate))</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> parameter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#深层神经网络</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_layer_model</span><span class="params">(X,Y,layer_dims,learning_rate,num_iter)</span>:</span></span><br><span class="line"><span class="comment">#初始化参数</span></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">costs=[]</span><br><span class="line">parameter=inital_parameter_deep(layer_dims)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_iter):</span><br><span class="line"><span class="comment">#前向传播</span></span><br><span class="line">A,caches=L_model_forward(X,parameter)</span><br><span class="line"></span><br><span class="line"><span class="comment">#计算损失</span></span><br><span class="line">cost=compute_loss(A,Y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#反向传播</span></span><br><span class="line">grads=L_model_backward(A,Y,caches)</span><br><span class="line"></span><br><span class="line"><span class="comment">#更新参数</span></span><br><span class="line">parameter=update_parameter(parameter,grads,learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(i%<span class="number">100</span>==<span class="number">0</span>):</span><br><span class="line">costs.append(cost)</span><br><span class="line">print(<span class="string">"第 "</span>+str(i)+<span class="string">" 次，cost: "</span>+str(cost))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.plot(costs)</span><br><span class="line">plt.xlabel(<span class="string">"iter"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"cost"</span>)</span><br><span class="line">plt.title(<span class="string">"learning_rate:"</span>+str(learning_rate))</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> parameter</span><br><span class="line"><span class="comment">#7.预测</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(X,Y,parameter)</span>:</span></span><br><span class="line"><span class="comment">#按照参数走一次前向传播，查看预测值和实际值是否相同</span></span><br><span class="line">A,caches=L_model_forward(X,parameter)</span><br><span class="line"></span><br><span class="line">m=shape(A)[<span class="number">1</span>]</span><br><span class="line">n=shape(X)[<span class="number">1</span>]</span><br><span class="line">p=zeros((<span class="number">1</span>,n))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line"><span class="keyword">if</span>(A[<span class="number">0</span>,i]&gt;<span class="number">0.5</span>):</span><br><span class="line">p[<span class="number">0</span>,i]=<span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">p[<span class="number">0</span>,i]=<span class="number">0</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"正确率为："</span>+str(sum(Y==p)/n))</span><br><span class="line"><span class="keyword">return</span> p</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#查看分类错误的图片</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_mislabeled_image</span><span class="params">(X,y,p)</span>:</span></span><br><span class="line"><span class="comment">#y:实际类别  #p:预测类别</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#得到预测错误的下标</span></span><br><span class="line">a=p+y</span><br><span class="line">print(a)</span><br><span class="line">error_index=asarray(where(a==<span class="number">1</span>))</span><br><span class="line">print(error_index)</span><br><span class="line">num_range=len(error_index[<span class="number">0</span>])</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>]=(<span class="number">40.0</span>,<span class="number">40.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#打印错误图片</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_range):</span><br><span class="line">index=error_index[<span class="number">1</span>][i]</span><br><span class="line">plt.subplot(<span class="number">2</span>,num_range,i+<span class="number">1</span>)</span><br><span class="line">plt.imshow(X[:,index].reshape(<span class="number">64</span>,<span class="number">64</span>,<span class="number">3</span>))</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#8.两层网络测试</span></span><br><span class="line"><span class="comment"># n_x=shape(train_x)[0]</span></span><br><span class="line"><span class="comment"># n_h=7</span></span><br><span class="line"><span class="comment"># n_y=shape(train_y)[0]</span></span><br><span class="line"><span class="comment"># layer_dims = [12288,7,1] </span></span><br><span class="line"><span class="comment"># parameter=two_layer_model(train_x,train_y,layer_dims,learning_rate=0.0075,num_iter=2500)</span></span><br><span class="line"><span class="comment"># p_train=predict(train_x,train_y,parameter)</span></span><br><span class="line"><span class="comment"># p_test=predict(test_x,test_y,parameter)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#9.多层网络测试</span></span><br><span class="line">layer_dims = [<span class="number">12288</span>,<span class="number">20</span>,<span class="number">7</span>,<span class="number">5</span>,<span class="number">1</span>] </span><br><span class="line">parameter=L_layer_model(train_x,train_y,layer_dims,learning_rate=<span class="number">0.0075</span>,num_iter=<span class="number">2500</span>)</span><br><span class="line">p_train=predict(train_x,train_y,parameter)</span><br><span class="line">p_test=predict(test_x,test_y,parameter)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#10.使用自己图像测试</span></span><br><span class="line"><span class="comment">#导入图像处理模块</span></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="comment">#输入自己图像的数据特征及标签</span></span><br><span class="line">my_image=<span class="string">"my_image.jpg"</span></span><br><span class="line">my_image_y=[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#将自己的图像转换为向量，以便网络处理</span></span><br><span class="line">num_px=<span class="number">64</span></span><br><span class="line">image=Image.open(my_image).convert(<span class="string">'RGB'</span>).resize((num_px,num_px))</span><br><span class="line">my_input=array(image).reshape((num_px*num_px*<span class="number">3</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#输入网络进行测试</span></span><br><span class="line">my_predict=predict(my_input,my_image_y,parameter)</span><br><span class="line">plt.imshow(image)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y = "</span> + str(squeeze(my_predict)) + <span class="string">", your L-layer model predicts a \""</span> + classes[int(squeeze(my_predict)),].decode(<span class="string">"utf-8"</span>) +  <span class="string">"\" picture."</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;描述&quot;&gt;&lt;a href=&quot;#描述&quot; class=&quot;headerlink&quot; title=&quot;描述&quot;&gt;&lt;/a&gt;描述&lt;/h2&gt;&lt;p&gt;利用猫咪图片搭建能够识别猫图片的神经网络，并使用新的图片测试网络效果&lt;/p&gt;&lt;h2 id=&quot;原理&quot;&gt;&lt;a href=&quot;#原理&quot; class=&quot;headerlink&quot; title=&quot;原理&quot;&gt;&lt;/a&gt;原理&lt;/h2&gt;&lt;p&gt;神经网络的搭建主要分为两部分：前向传播和反向传播&lt;/p&gt;&lt;p&gt;（1）前向传播&lt;/p&gt;&lt;p&gt;根据输入的参数及上一层的输入得出每层的函数值，通过每层不同的激活函数得到输出，最终输出结果根据交叉熵模型得出前向传播中的损失。&lt;/p&gt;&lt;p&gt;（2）反向传播&lt;/p&gt;&lt;p&gt;根据损失值，从最后一层向前得到每层的梯度，根据梯度下降原则更新参数，向着损失减小的方向变化。&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深层网络" scheme="https://www.xiapf.com/tags/%E6%B7%B1%E5%B1%82%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>react+springboot前后台分离项目</title>
    <link href="https://www.xiapf.com/blogs/react-springboot/"/>
    <id>https://www.xiapf.com/blogs/react-springboot/</id>
    <published>2020-05-09T06:41:07.000Z</published>
    <updated>2020-05-09T06:59:08.836Z</updated>
    
    <content type="html"><![CDATA[<h2 id="实验描述"><a href="#实验描述" class="headerlink" title="实验描述"></a>实验描述</h2><p>训练部分：使用面向对象的方法实现两种机器学习算法BP、SVM，并将两种算法进行封装，通过接口调用两种算法，利用已知1-5月气象数据训练2个气温预测模型，并预测后续3日气温值。</p><p>测试部分：利用训练好的模型，当输入最大温度，最小温度，湿度和光照时，可以得到预测得气温。</p><h2 id="通过dubbo-zookeeper实现通过接口远程调用算法模型"><a href="#通过dubbo-zookeeper实现通过接口远程调用算法模型" class="headerlink" title="通过dubbo+zookeeper实现通过接口远程调用算法模型"></a>通过dubbo+zookeeper实现通过接口远程调用算法模型</h2><a id="more"></a><p>实现功能：生产者实现两种机器学习算法BP,SVM，消费者通过远程调用生产者的接口，能够用自己的数据训练BP和SVM的模型，并根据训练的模型，使用数据进行测试。</p><h3 id="安装配置zookeeper"><a href="#安装配置zookeeper" class="headerlink" title="安装配置zookeeper"></a>安装配置zookeeper</h3><p>在官网<a href="http://zookeeper.apache.org/" target="_blank" rel="external nofollow noopener noreferrer">http://zookeeper.apache.org/</a>选择一个版本下载，我下载的是zookeeper-3.5.7，解压并修改配置文件，配置文件中dataDir，我使用的默认路径/bmp/zookeeper。切换至bin目录下，使用命令./zkServer.sh start，启动服务</p><p>出现如下字样说明服务启动成功：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /Users/Local/Resource/zookeeper/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br></pre></td></tr></table></figure><h3 id="新建生产者pc2（providerCenter）项目"><a href="#新建生产者pc2（providerCenter）项目" class="headerlink" title="新建生产者pc2（providerCenter）项目"></a>新建生产者pc2（providerCenter）项目</h3><p>（1）提供接口</p><p>在provider项目中新建client模块，其中定义IUseModels接口，该接口就是要注册到zookeeper中心。供远程消费者调用。</p><p>IUseModels接口：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">IUseModels</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  Map&lt;String, <span class="keyword">double</span>[][]&gt; bpTrain(<span class="keyword">double</span>[][] inData,<span class="keyword">double</span>[][] target, <span class="keyword">int</span> inputSize, <span class="keyword">int</span> hideSize, <span class="keyword">int</span> outputSize, <span class="keyword">int</span> iter);</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">double</span> <span class="title">bpTest</span><span class="params">(<span class="keyword">double</span>[] inData,Map&lt;String, <span class="keyword">double</span>[][]&gt; map)</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">double</span>[] svmTrain(<span class="keyword">double</span>[][] inData,<span class="keyword">double</span>[] y,<span class="keyword">int</span> iter,<span class="keyword">double</span> parameter);</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">double</span> <span class="title">svmTest</span><span class="params">(<span class="keyword">double</span>[] inData,<span class="keyword">double</span>[] w)</span></span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接口定义了bp算法的训练模型部分、测试数据功能，svm算法的训练模型部分、测试数据功能。</p><p>接口的具体实现在主模块的UseModelsImpl中：</p><p>bpTrain根据用户输入的训练数据，输入层、隐藏层、输出层神经元个数，训练次数进行模型训练，并返回训练的模型。</p><p>bpTest根据用户输入的测试数据，之前训练的模型得出输出的结果返回。</p><p>svmTrain根据用户输入的训练数据，损失函数的参数，训练次数进行模型训练，并返回训练的模型</p><p>svmTest根据用户输入的测试数据，之前训练的模型得出输出的结果返回。</p><p>（2）引用服务</p><p>生产者配置文件中添加服务引用，需要配置dubbo依赖和zookeeper依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.dubbo<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>dubbo<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.6<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.zookeeper<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>zookeeper<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.5.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>（3）生产者在zookeeper上注册</p><p>在项目内新建配置文件provider.xml，指定当前服务的名字，指定注册中心的地址，这里默认注册中心在本地，指定通信规则是dubbo，通信端口为7070，声明需要暴露的服务接口，同时指向容器中的接口实现对象。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">dubbo:application</span> <span class="attr">name</span> = <span class="string">"dubbo_provider"</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dubbo:protocol</span> <span class="attr">name</span> = <span class="string">"dubbo"</span> <span class="attr">port</span>=<span class="string">"7070"</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dubbo:registry</span> <span class="attr">address</span> = <span class="string">"zookeeper://127.0.0.1:2181"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dubbo:service</span> <span class="attr">interface</span> = <span class="string">"com.cl.client.IUseModels"</span> <span class="attr">ref</span> = <span class="string">"UseModelsImpl"</span>/&gt;</span></span><br></pre></td></tr></table></figure><p>使用zkui-master查看注册中心上已注册的生产者节点如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507202451.png" alt></p><p>（4）暴露接口</p><p>使用maven中的install命令将接口单独进行编译，并把接口编译的jar包安装至本地maven仓库。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507202521.png" alt></p><p>查看本地maven仓库，发现接口成功上传至仓库，以供消费者使用。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507202529.png" alt></p><h3 id="新建消费者oc2（consumerCenter）项目"><a href="#新建消费者oc2（consumerCenter）项目" class="headerlink" title="新建消费者oc2（consumerCenter）项目"></a>新建消费者oc2（consumerCenter）项目</h3><p>（1）消费者在zookeeper上调用</p><p>在项目内新建配置文件consumer.xml，指定当前服务的名字，指定注册中心的地址，指定需要调用的服务的接口名称，以便调用。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dubbo:application</span> <span class="attr">name</span>=<span class="string">"demo_consumer"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dubbo:registry</span> <span class="attr">address</span>=<span class="string">"zookeeper://127.0.0.1:2181"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dubbo:reference</span> <span class="attr">id</span>=<span class="string">"UseModelsImpl"</span> <span class="attr">interface</span>=<span class="string">"com.cl.client.IUseModels"</span> <span class="attr">timeout</span>=<span class="string">"3000"</span> <span class="attr">check</span>=<span class="string">"false"</span>/&gt;</span></span><br></pre></td></tr></table></figure><p>（2）导入生产者接口jar包</p><p>在pom.xml添加对应包的依赖，maven自动下载本地仓库中的jar包</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.cl<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.0.1-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>（3）实现远程调用</p><p>启动生产者端后，生产者端通过dubbo服务将自己的url地址注册到zookeeper上，消费者端通过调用dubbo服务，根据配置的注册中心以及接口名称到zookeeper上查找对应的生产者端中的接口实现。</p><p>消费者端将IUseModels接口实例化，编写方法testbp用于训练bp模型并进行气温预测：导入本地数据，并调用IUseModels中的bpTrain方法，消费者端找到生产者端中的端口实现，传入训练的样本，输入层、隐藏层、输出层的结构进行模型训练，训练结束后，得到完成的模型，并根据测试数据返回预测得后三天气温。</p><p>同理，消费者端的testsvm方法调用IUseModels中的svmTrain方法得到预测得气温。</p><p>bp和svm模型训练后进行保存，消费者端的calcTemp方法通过以训练的模型和前台传入的测试数据预测六月某天的气温。</p><p>使用zkui-master查看注册中心上已注册的消费者节点如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507204011.png" alt></p><p>可以看到在bubbo节点下的consumers消费者节点中包含了当前提供服务的生产者提供的接口名称和接口方法。</p><h2 id="使用springboot-react-antd实现项目前后台分离并发布训练的模型及测试的结果"><a href="#使用springboot-react-antd实现项目前后台分离并发布训练的模型及测试的结果" class="headerlink" title="使用springboot+react+antd实现项目前后台分离并发布训练的模型及测试的结果"></a>使用springboot+react+antd实现项目前后台分离并发布训练的模型及测试的结果</h2><h3 id="使用react-antd进行前台搭建"><a href="#使用react-antd进行前台搭建" class="headerlink" title="使用react+antd进行前台搭建"></a>使用react+antd进行前台搭建</h3><p>（1）创建前台页面工程</p><p>使用react脚手架create-react-app创建工程test。</p><p>步骤参照官网<a href="https://ant.design/docs/react/use-with-create-react-app-cn" target="_blank" rel="external nofollow noopener noreferrer">https://ant.design/docs/react/use-with-create-react-app-cn</a></p><p>安装脚手架：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install -g create-react-app</span><br></pre></td></tr></table></figure><p>使用脚手架创建项目：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create react-app test</span><br></pre></td></tr></table></figure><p>引入antd：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn add antd</span><br></pre></td></tr></table></figure><p>修改package.json,让项目可以自定义配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&quot;scripts&quot;: &#123;</span><br><span class="line">  &quot;start&quot;: &quot;react-app-rewired start&quot;,</span><br><span class="line">  &quot;build&quot;: &quot;react-app-rewired build&quot;,</span><br><span class="line">  &quot;test&quot;: &quot;react-app-rewired test&quot;,</span><br><span class="line">  &quot;eject&quot;: &quot;react-scripts eject&quot;</span><br><span class="line"> &#125;,</span><br></pre></td></tr></table></figure><p>在项目根目录创建一个 config-overrides.js 用于修改默认配置</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> &#123; override, fixBabelImports &#125; = <span class="built_in">require</span>(<span class="string">'customize-cra'</span>);</span><br><span class="line"><span class="built_in">module</span>.exports = override(</span><br><span class="line"> fixBabelImports(<span class="string">'import'</span>, &#123;</span><br><span class="line">  libraryName: <span class="string">'antd'</span>,</span><br><span class="line">  libraryDirectory: <span class="string">'es'</span>,</span><br><span class="line">  style: <span class="string">'css'</span>,</span><br><span class="line"> &#125;),</span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>利用yarn安装项目需要的依赖：yarn install</p><p>进入项目，并启动：cd antd-demo  yarn start</p><p>项目默认端口是3000，当没有程序占用时，直接打开浏览器，显示如下，说明运行成功。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507204025.png" alt></p><p>当端口被占用时，会出现如下界面，选择y,即用其他端口打开即可</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507204034.png" alt></p><p>如果需要修改默认端口号，可以找到项目下/node_modules/react-scripts/scripts/start.js，找到DEFAULT_PORT，修改端口即可。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507204201.png" alt></p><p>（2）react脚手架创建项目结构</p><p>创建的项目默认目录如下</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507204210.png" alt></p><p>其中node_mudules文件夹内是安装的所有依赖模块，package.json里包含项目基本设置，如名称、版本号等，最主要的是src文件夹，里面包含的js文件是页面展示的内容,css文件是样式文件，项目默认index.js是项目的入口。index.js内容如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507204219.png" alt></p><p>使用 React 构建的应用通常只有单一的根 DOM 节点，通过‘root’也可判断index是代码入口，其中ReactDOM.render后面是显示在页面上的内容，这里是直接显示App页面，App页面中简单设置了文字和图像，即项目初始运行时显示的内容。</p><p>（3）创建页面</p><p>在src文件夹下新建js文件，通过class组件建立显示的页面内容，class组件格式如下：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">App</span> <span class="keyword">extends</span> <span class="title">React</span>.<span class="title">Component</span></span>&#123;</span><br><span class="line">  内容<span class="number">1</span></span><br><span class="line">  render() &#123;</span><br><span class="line">    <span class="keyword">return</span>(</span><br><span class="line">    内容<span class="number">2</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  &#125;</span><br><span class="line"><span class="keyword">export</span> <span class="keyword">default</span> App;</span><br></pre></td></tr></table></figure><p>内容1部分设置需要用到的对象，构造函数，触发的事件，与后台数据交互等内容。</p><p>内容2部分是设置页面显示的部分，这里直接使用antd组件。</p><p>1°内容2部分:使用antd组件创建页面内容</p><p>进入antd官网<a href="https://ant.design/components/button-cn/" target="_blank" rel="external nofollow noopener noreferrer">https://ant.design/components/button-cn/</a> ，选择组件，以button组件为例：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507204229.png" alt></p><p>第一步，选择需要的组件button</p><p>第二步，在“代码演示”部分找到自己需要的样式</p><p>第三步，显示当前组件的代码</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507204238.png" alt></p><p>如图所示，展示了四种按钮代码，分别对应上面四种不同样式的按钮，选择第一种按钮，赋值代码，并把需要的库文件导入，选择方框内的代码复制进App.js文件中</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507204248.png" alt></p><p>页面显示如下</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507204257.png" alt></p><p>antd组件进行按需选择，项目中构建了如下文件：</p><p>index.js:项目入口，显示Demo.js页面。</p><p>Demo.js:主页面，设置为侧边布局，当页面横向空间有限时，侧边导航可以收起，当点击导航上对应菜单跳转对应页面（首页页面show.js、训练页面Train.js,测试页面Res.js,关于页面About.js）。</p><p>show.js: 首页页面，项目默认显示首页页面，由Card组件和文字组成。</p><p>Train.js: 训练页面，实现获取后台数据功能，当点击按钮时，后台训练对应算法模型，并将结果传回前台进行显示。</p><p>Res.js: 测试页面，以表单形式实现通过前台提交数据，后台接收数据并计算，最终将结果返回前台的功能。</p><p>About.js:关于页面，显示项目及作者信息。</p><p>最终搭建的项目页面如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507204304.png" alt></p><p>（4）前台页面转发</p><p>项目主页面Demo.js通过点击侧边导航栏，能够跳转对应页面，主要是使用react-router-dom,从react-router-dom中导入路由Router,Demo页面使用路由组件渲染，使用link组件实现至路由跳转。</p><p>第一步，将需要跳转的导航菜单上设置Link组件，设置需要跳转的页面<link to="/">首页</p><p>第二步，由页面路由设置连接需要跳转的页面<route exact path="/" component="{show}">，component后面就是跳转的页面内容,这里exact path设置系统默认进入的页面。</route></p><p>转发前台页面核心代码：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;Router&gt;</span><br><span class="line">   &lt;Menu theme=<span class="string">"dark"</span> defaultSelectedKeys=&#123;[<span class="string">'0'</span>]&#125; mode=<span class="string">"inline"</span>&gt;</span><br><span class="line">        &lt;Menu.Item key=<span class="string">"0"</span>&gt;</span><br><span class="line">            &lt;Link to=<span class="string">"/"</span>&gt;首页&lt;<span class="regexp">/Link&gt;</span></span><br><span class="line"><span class="regexp">         &lt;/</span>Menu.Item&gt;</span><br><span class="line">         &lt;Menu.Item key=<span class="string">"1"</span>&gt;</span><br><span class="line">             &lt;Link to=<span class="string">"/Train"</span>&gt;训练&lt;<span class="regexp">/Link&gt;</span></span><br><span class="line"><span class="regexp">          &lt;/</span>Menu.Item&gt;</span><br><span class="line">   &lt;<span class="regexp">/Menu&gt;</span></span><br><span class="line"><span class="regexp">   &lt;Route exact path="/</span><span class="string">" component=&#123;show&#125; /&gt;</span></span><br><span class="line"><span class="string">   &lt;Route  path="</span>/Train<span class="string">" component=&#123;Train&#125; /&gt;</span></span><br><span class="line"><span class="string">&lt;/Router&gt;</span></span><br></pre></td></tr></table></figure><h3 id="前台和后台数据交互"><a href="#前台和后台数据交互" class="headerlink" title="前台和后台数据交互"></a>前台和后台数据交互</h3><p>后台开发使用springboot框架,开发软件为IntelliJ IDEA。</p><p>（1）跨域问题</p><p>跨域是指当客户端向服务器发起一个网络请求，url会有包含三个主要信息：协议（protocol），域名（host），端口号（port）。当三部分都和服务器相同的情况下，属于同源。但是只要有一个不同,就属于构成了跨域调用。会受到同源策略的限制。同源策略限制从一个源加载的文档或脚本如何与来自另一个源的资源进行交互。springboot项目默认端口是8080，react默认端口是3000,前后端端口不一样，所以有跨域问题。</p><p>这里采用代理方式进行处理：（这里springboot项目的端口为9091），在react项目中的package.json中设置转发”proxy”:”<a href="http://localhost:9091&quot;" target="_blank" rel="external nofollow noopener noreferrer">http://localhost:9091&quot;</a> ,将对前端的所有请求全部转发到9091端口即后台项目的入口中，这样就解决了跨域问题。</p><p>（2）数据交互</p><p>这里的项目是采用前后端分离的思想，前端从后端剥离，形成一个前端工程，前端只利用Json来和后端进行交互，后端不返回页面，只返回Json数据。前后端之间完全通过public API约定。前后端之间使用fetch进行数据交互。</p><p>1°前台接收后台数据</p><p>后台：springboot设置control层，返回json数据。这里的testbp调用生产者端的bp算法，得到模型后使用数据测试，最后返回“后三天的温度”的json数据。</p><p>后台核心代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="meta">@RequestMapping</span>(<span class="string">"/submit"</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SubmitController</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> IUseModels useModels;</span><br><span class="line">    <span class="keyword">public</span> Map&lt;String,<span class="keyword">double</span>[][]&gt; mapRes;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@RequestMapping</span>(<span class="string">"/testbp"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">testbp</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">//导入数据</span></span><br><span class="line">        ..</span><br><span class="line">        <span class="comment">//训练网络</span></span><br><span class="line">        mapRes= useModels.bpTrain(inData,target,inputSize,hideSize,outputSize,iter);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//测试数据</span></span><br><span class="line">        <span class="keyword">double</span>[] x = <span class="keyword">new</span> <span class="keyword">double</span>[]&#123;<span class="number">14.7</span>,<span class="number">0</span>,<span class="number">11.7</span>,<span class="number">22.4</span>&#125;;</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">double</span> res=useModels.bpTest(x,mapRes);</span><br><span class="line">    <span class="comment">//设置输出精度</span></span><br><span class="line">        DecimalFormat df = <span class="keyword">new</span> DecimalFormat( <span class="string">"0.00 "</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"后三天的温度为："</span>+df.format(res)+<span class="string">" , "</span>+df.format(res2)+<span class="string">" , "</span>+df.format(res3);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>前台：前台就需要将请求转发到<a href="http://localhost:9091/submit/testbp" target="_blank" rel="external nofollow noopener noreferrer">http://localhost:9091/submit/testbp</a> 上，因为上面设置了跨域转发，所以使用fetch(‘/submit/testbp’)即可以将请求转发至后台，当获取到json数据后将其转换成text（then(response =&gt; response.text())），并读取其中的message（then(message =&gt; {this.setState({message: message});});）,即后台传回的后三天的温度数据。</p><p>项目中设置的是，当点击按钮时，向后台获取数据，因此将fetch写在事件中，当点击按钮时，触发该事件，这里使用button组件自带的onclick方法绑定事件。</p><p>需要注意的是在 JavaScript 中，class 的方法默认不会绑定 this。如果忘记绑定 this.handleClick 并把它传入了 onClick，当调用这个函数的时候 this 的值为 undefined，所以需要在构造函数中绑定事件。</p><p>注：如果需要进入页面就从后台获取数据，则需要在生命周期函数componentDidMount()中添加fetch</p><p>前台获取后台数据过程：</p><p>第一步，编写事件，通过fetch获取json数据，并通过setState设置文本值</p><p>第二步，将时间传入button的onclick属性中，当点击按钮时进行触发</p><p>第三步，在构造函数中绑定触发的事件</p><p>第四步，将当前state中的文本值显示在页面上</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507204315.png" alt></p><p>注：state是组件内部进行传递数据，具有从上到下的特点，在构造函数中进行初始化，当获得后台数据时，通过setState设置新的state的值，最终在页面上进行显示。（state不能直接赋值，只能通过setState设置新的值）</p><p>前台运行结果：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507204328.png" alt></p><p>前台通过点击训练模型按钮，后台分别通过接口调用生产者端的算法，进行模型训练，得到模型后，根据输入的测试数据得到后三天的气温情况，并将数据传送给前台页面进行显示。</p><p>后台将传输的数据打印在控制台</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507204347.png" alt></p><p>2°后台数据接收前台数据，并计算返回结果给前台</p><p>后台：springboot设置controller层，通过requestbody接收前台传入的json数据，依次获取数据，并调用训练的模型进行测试，并将结果封装成json数据传回后台。</p><p>后台核心代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="meta">@RequestMapping</span>(<span class="string">"/submit"</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SubmitController</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> IUseModels useModels;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@RequestMapping</span>(<span class="string">"/calc"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">calcTemp</span><span class="params">(@RequestBody String str)</span> </span>&#123;</span><br><span class="line">        Map&lt;String, String&gt; map = JSON.parseObject(str, Map<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        <span class="comment">//解析json数据并传入x中</span></span><br><span class="line">  …</span><br><span class="line">        Map result = <span class="keyword">new</span> HashMap();</span><br><span class="line">        <span class="keyword">double</span> res=useModels.svmTest(x,weight);</span><br><span class="line">        <span class="keyword">double</span> res2=useModels.bpTest(x,mapRes);</span><br><span class="line">        DecimalFormat df = <span class="keyword">new</span> DecimalFormat( <span class="string">"0.00 "</span>);</span><br><span class="line">        result.put(<span class="string">"result"</span>, df.format(res));</span><br><span class="line">        result.put(<span class="string">"result2"</span>,df.format(res2));</span><br><span class="line">        <span class="keyword">return</span> JSON.toJSONString(result);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>前台：</p><p>a）提交前台数据：前台页面主要设置了一个表单，每个输入框都设置name属性，利用input框的onchange属性绑定handleInputChange事件，当输入框内数据发生变化时即触发handleInputChange事件，通过setState将当前变化值作为输入框对应name新value值。</p><p>当点击提交按钮时，通过onClick属性绑定calc事件，每次点击提交，则触发该事件，事件中定义需要提交的对象，将需要提交的数据封装到该对象中，并通过fetch转发到对应后台页面，为方便后台处理，将提交的内容转换为字符串提交。</p><p>b）接收后台数据：通过fetch提交结束后，接收返回的json数据，并通过setState获取数值并赋值给相应组件。</p><p>这里所有事件都需要在构造函数中进行绑定，否则触发事件时会找不到。因为react内部通过state设置组件的显示的内容，所以需要在构造函数中设置state的初值。由于算法中处理的是double数据，当前台提交double数据至后台时，系统会自动转换为BigDecimal，不方便后续处理，所以这里将各个输入框的初始设置为非数值类型，通过字符串类型传回后台再转换为double类型处理。</p><p>前台运行结果：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507204339.png" alt></p><p>后台将接受的前台数据打印在控制台中</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507204355.png" alt></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>项目中采用dubbo+zookeeper方式实现面向接口编程，在生产者端通过面向对象的编程实现了两种机器学习算法：BP和SVM，设置接口可以调用者两种算法，并通过dubbo协议将生产者端的接口实现注册至zookeeper端，以供消费者使用；在消费者端通过dubbo协议，根据生产者的接口方法在zookeeper中找到接口实现方法，并进行调用，实现模型训练。</p><p>采用springboot+react+antd实现前后端分离，前端采用react+antd编写项目，使用react脚手架创建前端页面工程，利用antd组件方便快速的搭建页面显示内容，后端采用springboot框架，编写训练模型和测试数据的功能，两者之间的跨域问题通过代理转发解决，同时前端和后端之间通过fetch传输json数据，前后端之间完全通过public API约定。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;实验描述&quot;&gt;&lt;a href=&quot;#实验描述&quot; class=&quot;headerlink&quot; title=&quot;实验描述&quot;&gt;&lt;/a&gt;实验描述&lt;/h2&gt;&lt;p&gt;训练部分：使用面向对象的方法实现两种机器学习算法BP、SVM，并将两种算法进行封装，通过接口调用两种算法，利用已知1-5月气象数据训练2个气温预测模型，并预测后续3日气温值。&lt;/p&gt;&lt;p&gt;测试部分：利用训练好的模型，当输入最大温度，最小温度，湿度和光照时，可以得到预测得气温。&lt;/p&gt;&lt;h2 id=&quot;通过dubbo-zookeeper实现通过接口远程调用算法模型&quot;&gt;&lt;a href=&quot;#通过dubbo-zookeeper实现通过接口远程调用算法模型&quot; class=&quot;headerlink&quot; title=&quot;通过dubbo+zookeeper实现通过接口远程调用算法模型&quot;&gt;&lt;/a&gt;通过dubbo+zookeeper实现通过接口远程调用算法模型&lt;/h2&gt;
    
    </summary>
    
    
      <category term="java" scheme="https://www.xiapf.com/categories/java/"/>
    
    
      <category term="java" scheme="https://www.xiapf.com/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>用zookeeper+dubbo实现rpc远程调用</title>
    <link href="https://www.xiapf.com/blogs/dubboZkNote/"/>
    <id>https://www.xiapf.com/blogs/dubboZkNote/</id>
    <published>2020-05-07T13:46:12.000Z</published>
    <updated>2020-05-09T06:40:36.725Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是zookeeper"><a href="#什么是zookeeper" class="headerlink" title="什么是zookeeper"></a>什么是zookeeper</h2><p>zookeeper是一个分布式服务框架，是Apache Hadoop 的一个子项目，它主要是用来解决分布式应用中经常遇到的一些数据管理问题，如：统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等。简单来说zookeeper是文件系统加上监听通知机制.</p><p>zookeeper维护着文件系统的数据结构，里面包含了很多目录节点，例如NameService、GroupMember等，这些子目录想可以增加删除以及存储数据。客户端注册监听关心的目录节点，当目录节点发生变化，zookeeper会通知客户端。</p><a id="more"></a><p>zookeeper的功能比较多，这里主要用到的是分布式应用配置管理功能。当程序是分布式部署在多台机器上，如果要改变程序的配置文件，需要逐台机器去修改，这里就需要用到zookeeper，将配置放到zookeeper上去，保存其目录节点中，然后客户端对这个目录节点进行监听，一旦配置信息发生变化，客户端就会收到 zookeeper 的通知，然后从 zookeeper 获取新的配置信息。</p><h2 id="什么是dubbo"><a href="#什么是dubbo" class="headerlink" title="什么是dubbo"></a>什么是dubbo</h2><p>dubbo是阿里巴巴于2011年10月正式开源的一个由Java语言编写的分布式服务框架。它是一个提供高性能和透明化的远程服务调用方案和基于服务框架展开的完整SOA服务治理方案。</p><p>dubbo的架构如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507205533.png" alt></p><p>这个过程类似生产者-消费者模型。在此基础上加上了注册中心和监控中心，用于管理提供方提供的url和管理整个过程。</p><p>整个过程就是先启动容器，加载，运行服务提供者。生产者在启动时，在注册中心发布注册自己提供的服务。消费者在启动时，在注册中心订阅自己所需的服务。</p><h2 id="zookeeper-dubbo实现rpc框架"><a href="#zookeeper-dubbo实现rpc框架" class="headerlink" title="zookeeper+dubbo实现rpc框架"></a>zookeeper+dubbo实现rpc框架</h2><p>zookeeper是类似文件系统的数据结果，支持修改删除同时还可以进行变更推送操作，以服务名和类型作为节点路径，符合dubbo订阅和通知的需求,因此此适合做dubbo服务的注册中心.</p><p>zookeeper上的服务节点设计是树形结构，dubbo会在zookeeper上创建一个根节点，下一层的子节点代表dubbo的一个服务，服务的左节点是生产者的根节点，右节点是消费者的根节点，。</p><p>当zookeeper启动时，生产者在服务节点下创建一个子节点，将自己的url地址注册到该节点。此时消费者会读取并订阅生产者根节点下的所有子节点，解析出所有生产者的url地址来作为该服务地址列表，然后发起正常调用，同时在消费者根节点下创建一个子节点写入自己的url地址，代表了使用生产者服务的消费者。监控中心指向此时的服务获取所有生产者和消费者的url地址，并监听其子节点的变化进行计数。</p><p>rpc是远程过程调用，满足分布式系统架构中不同的系统之间的远程通信和相互调用。以zookeeper服务为基础,使用dubbo监视和调度各个服务，生产者在zookeeper上注册服务，消费者去zookeeper上调用服务，以此实现rpc框架。</p><h2 id="实验过程"><a href="#实验过程" class="headerlink" title="实验过程"></a>实验过程</h2><p>项目实现功能：生产者实现方法能够在页面上显示自己喜欢看的书：武侠小说，消费者通过远程调用生产者的接口，在页面上查看生产者喜欢看的书籍。</p><h3 id="安装配置zookeeper"><a href="#安装配置zookeeper" class="headerlink" title="安装配置zookeeper"></a>安装配置zookeeper</h3><p>在<a href="http://zookeeper.apache.org/" target="_blank" rel="external nofollow noopener noreferrer">官网</a>选择一个版本下载，我下载的是zookeeper-3.5.7，解压并修改配置文件，配置文件中dataDir我我使用的默认路径/bmp/zookeeper。切换至bin目录下，使用命令./zkServer.sh start，启动服务</p><p>出现如下字样说明服务启动成功：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /Users/Local/Resource/zookeeper/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br></pre></td></tr></table></figure><h3 id="新建生产者provider项目"><a href="#新建生产者provider项目" class="headerlink" title="新建生产者provider项目"></a>新建生产者provider项目</h3><p>（1）提供接口</p><p>在provider项目中新建client模块，其中定义IBook接口，该接口就是要注册到zookeeper中心。供远程消费者调用。</p><p>IBook接口：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">IBook</span> </span>&#123;</span><br><span class="line">    <span class="function">String <span class="title">getBookName</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br><span class="line">接口方法的具体实现，方法内返回生产者喜欢看的书籍名称：</span><br><span class="line"><span class="meta">@Service</span>(<span class="string">"book"</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BookImp</span> <span class="keyword">implements</span> <span class="title">IBook</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getBookName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"武侠小说"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>服务器端口设置为：server.port=8082</p><p>（2）引用服务</p><p>生产者配置文件中添加服务引用，需要配置dubbo依赖和zookeeper依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--dubbo依赖--&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>dubbo-spring-boot-starter<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.2.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba.spring.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>dubbo-spring-boot-starter<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">       <span class="comment">&lt;!--zkclient依赖--&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.101tec<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>zkclient<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.10<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>（3）生产者在zookeeper上注册</p><p>在项目内新建配置文件provider.xml，指定当前服务的名字，指定注册中心的地址，这里默认注册中心在本地，指定通信规则是dubbo，通信端口为20880，声明需要暴露的服务接口，同时指向容器中的接口实现对象。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dubbo:application</span> <span class="attr">name</span>=<span class="string">"user-provider"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dubbo:registry</span> <span class="attr">address</span>=<span class="string">"zookeeper://localhost:2181"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dubbo:protocol</span> <span class="attr">name</span>=<span class="string">"dubbo"</span> <span class="attr">port</span>=<span class="string">"20880"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dubbo:service</span> <span class="attr">ref</span>=<span class="string">"book"</span> <span class="attr">interface</span>=<span class="string">"com.cl.client.inter.IBook"</span>/&gt;</span></span><br></pre></td></tr></table></figure><p>使用zkui-master查看注册中心上已注册的生产者节点如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507205643.png" alt></p><p>可以看到在bubbo节点下的providers生产者节点中包含了当前提供服务的节点的url地址、接口名称和接口内方法。</p><p>在本地对provider项目测试，输入<a href="http://localhost:8082/provider/sayBook：" target="_blank" rel="external nofollow noopener noreferrer">http://localhost:8082/provider/sayBook：</a></p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507205656.png" alt></p><p>测试结果显示正确在页面上显示了书籍的名称，说明provider项目运行结果无误。</p><p>测试结束后，单独将接口进行编译，导出对应jar包，名称为client.jar。</p><h3 id="新建消费者consumer项目"><a href="#新建消费者consumer项目" class="headerlink" title="新建消费者consumer项目"></a>新建消费者consumer项目</h3><p>（1）消费者在zookeeper上调用</p><p>在项目内新建配置文件consumer.xml，指定当前服务的名字，指定注册中心的地址，指定需要调用的服务的接口名称，以便调用。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dubbo:application</span> <span class="attr">name</span>=<span class="string">"user-consumer"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dubbo:registry</span> <span class="attr">address</span>=<span class="string">"zookeeper://localhost:2181"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dubbo:reference</span> <span class="attr">id</span>=<span class="string">"book"</span> <span class="attr">interface</span>=<span class="string">"com.cl.client.inter.IBook"</span>/&gt;</span></span><br></pre></td></tr></table></figure><p>（2）实现远程调用</p><p>将client.jar包导入consumer项目中，同时将IBook接口实例化，编写测试代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="meta">@RequestMapping</span>(<span class="string">"/consumer"</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ConsumerController</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> IBook book;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@RequestMapping</span>(<span class="string">"/getBook"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getBook</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"生产者喜欢的书是："</span>+book.getBookName();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>先启动provider端，然后在启动consumer端，打开网页，显示如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507205705.png" alt></p><p>启动后，provider端通过dubbo服务将自己的url地址注册到zookeeper上，consumer端通过调用dubbo服务，根据配置的注册中心以及接口名称到zookeeper上查找对应的provieder端中的接口实现，从而实现了远程调用。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;什么是zookeeper&quot;&gt;&lt;a href=&quot;#什么是zookeeper&quot; class=&quot;headerlink&quot; title=&quot;什么是zookeeper&quot;&gt;&lt;/a&gt;什么是zookeeper&lt;/h2&gt;&lt;p&gt;zookeeper是一个分布式服务框架，是Apache Hadoop 的一个子项目，它主要是用来解决分布式应用中经常遇到的一些数据管理问题，如：统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等。简单来说zookeeper是文件系统加上监听通知机制.&lt;/p&gt;&lt;p&gt;zookeeper维护着文件系统的数据结构，里面包含了很多目录节点，例如NameService、GroupMember等，这些子目录想可以增加删除以及存储数据。客户端注册监听关心的目录节点，当目录节点发生变化，zookeeper会通知客户端。&lt;/p&gt;
    
    </summary>
    
    
      <category term="java" scheme="https://www.xiapf.com/categories/java/"/>
    
    
      <category term="java" scheme="https://www.xiapf.com/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>字节码增强技术-javassist</title>
    <link href="https://www.xiapf.com/blogs/javassist/"/>
    <id>https://www.xiapf.com/blogs/javassist/</id>
    <published>2020-04-28T12:38:02.000Z</published>
    <updated>2020-05-07T13:43:10.156Z</updated>
    
    <content type="html"><![CDATA[<p>javassist动态修改字节码</p><p>直接上代码，修改Hello.say方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//ClassPool classPool = ClassPool.getDefault();</span></span><br><span class="line">ClassPool classPool = <span class="keyword">new</span> ClassPool(<span class="keyword">true</span>);</span><br><span class="line">CtClass ctClass = classPool.get(<span class="string">"com.xpf.study.javassit.Hello"</span>);</span><br><span class="line"><span class="comment">//CtClass ctClass = classPool.get(aClass.getName());</span></span><br><span class="line">CtMethod ctMethod = ctClass.getDeclaredMethod(<span class="string">"say"</span>);</span><br><span class="line">ctMethod.setBody(<span class="string">"System.out.println(\"nihao:\" + $1);"</span>);</span><br><span class="line"><span class="comment">//ctClass.writeFile("./aaaa/");</span></span><br><span class="line">Object instance = ctClass.toClass().newInstance();</span><br><span class="line">Method say = instance.getClass().getDeclaredMethod(<span class="string">"say"</span>, String<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">say.setAccessible(<span class="keyword">true</span>);</span><br><span class="line">say.invoke(instance, <span class="string">"java2"</span>);</span><br></pre></td></tr></table></figure><a id="more"></a><p>注意：ctClass.toClass()，ctClass.writeFile等操作后，会冻结字节码，不在允许修改字节码，所以要解冻，可以使用ctClass.defrost();</p><p>如果在使用Hello之前，jvm中已经加载了Hello</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Hello hello = new Hello();</span><br><span class="line">//修改字节码。。。。。</span><br></pre></td></tr></table></figure><p>会报错，因为同一个类加载器不能同时加载两个相同的class</p><p>所以，做如下修改，增加一个加载器，从另一个加载器中加载</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//ClassPool classPool = ClassPool.getDefault();</span></span><br><span class="line">ClassPool classPool = <span class="keyword">new</span> ClassPool(<span class="keyword">true</span>);</span><br><span class="line">Loader loader = <span class="keyword">new</span> Loader(classPool);</span><br><span class="line">Class&lt;?&gt; clazz = loader.loadClass(<span class="string">"com.xpf.study.javassit.Hello"</span>);</span><br><span class="line">Object instance3 = clazz.newInstance();</span><br><span class="line">Method say3 = instance3.getClass().getDeclaredMethod(<span class="string">"say"</span>, String<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">say3.setAccessible(<span class="keyword">true</span>);</span><br><span class="line">say3.invoke(instance3, <span class="string">"java"</span>);</span><br></pre></td></tr></table></figure><p>但是，注意，此时loader获取的class不再是CtClass类型，而是普通的Class类型，所以不再可以使用javassis的api</p><p>同时原来的Hello代码不变。此种方式取出来的instance虽然表面是Hello类型，但是却不 instance of Hello</p><p>另一个变通方法，修改class名称</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CtClass ctClass2 = classPool.getAndRename(<span class="string">"com.xpf.study.javassit.Hello"</span>, <span class="string">"com.xpf.study.javassit.HelloCopy"</span>);</span><br></pre></td></tr></table></figure><p>此时，可以返回类型还是CtClass，可以自由使用相应api</p><p>注意，使用此方式获取CtClass后，如果增加方法，jdk和其他jvm中已经加载的类库不能直接使用，需要引用，或者使用全路径的方式，如List，需要使用java.util.List，因为此加载器中，并没有加载其他类库。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;javassist动态修改字节码&lt;/p&gt;&lt;p&gt;直接上代码，修改Hello.say方法&lt;/p&gt;&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;//ClassPool classPool = ClassPool.getDefault();&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;ClassPool classPool = &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; ClassPool(&lt;span class=&quot;keyword&quot;&gt;true&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;CtClass ctClass = classPool.get(&lt;span class=&quot;string&quot;&gt;&quot;com.xpf.study.javassit.Hello&quot;&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;//CtClass ctClass = classPool.get(aClass.getName());&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;CtMethod ctMethod = ctClass.getDeclaredMethod(&lt;span class=&quot;string&quot;&gt;&quot;say&quot;&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;ctMethod.setBody(&lt;span class=&quot;string&quot;&gt;&quot;System.out.println(\&quot;nihao:\&quot; + $1);&quot;&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;//ctClass.writeFile(&quot;./aaaa/&quot;);&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Object instance = ctClass.toClass().newInstance();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Method say = instance.getClass().getDeclaredMethod(&lt;span class=&quot;string&quot;&gt;&quot;say&quot;&lt;/span&gt;, String&lt;span class=&quot;class&quot;&gt;.&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt;)&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;say.setAccessible(&lt;span class=&quot;keyword&quot;&gt;true&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;say.invoke(instance, &lt;span class=&quot;string&quot;&gt;&quot;java2&quot;&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="java" scheme="https://www.xiapf.com/categories/java/"/>
    
    
      <category term="java" scheme="https://www.xiapf.com/tags/java/"/>
    
      <category term="字节码" scheme="https://www.xiapf.com/tags/%E5%AD%97%E8%8A%82%E7%A0%81/"/>
    
  </entry>
  
  <entry>
    <title>监督学习笔记（五）——利用AdaBoost元算法提高分类性能</title>
    <link href="https://www.xiapf.com/blogs/adaboost/"/>
    <id>https://www.xiapf.com/blogs/adaboost/</id>
    <published>2020-04-26T14:08:08.000Z</published>
    <updated>2020-08-06T15:02:54.492Z</updated>
    
    <content type="html"><![CDATA[<p>元算法或者集成算法是将不同的分类器组合起来，组合的方式可以为：将不同算法组合起来，或者将数据集不同部分给不同的算法，同一种算法在不同设置下进行集成。简言之，元算法是将分类器进行重新集成，提高分类效果，就像生活中决定一件事需要听多方面的意见。</p><h2 id="构建分类器方法"><a href="#构建分类器方法" class="headerlink" title="构建分类器方法"></a>构建分类器方法</h2><h3 id="bagging"><a href="#bagging" class="headerlink" title="bagging"></a>bagging</h3><p>在源数据集中选择S次，得到S个数据集（可能会有重复的），选择某种算法和这S个数据集训练得到S个分类器，最终的结果由各分类器投票结果产生，如随机森林算法。</p><a id="more"></a><h3 id="boosting"><a href="#boosting" class="headerlink" title="boosting"></a>boosting</h3><p>boosting是通过分类器加权求和得到结果，选择n个分类器，初始状态权重相同，每个根据分类器分错的结果调整权重，权重代表上一个分类器迭代的成功度。</p><h2 id="AdaBoost算法"><a href="#AdaBoost算法" class="headerlink" title="AdaBoost算法"></a>AdaBoost算法</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>通过多个弱分类器构造一个强分类器，在训练集上训练一个弱分类器，并得出其错误率，在第二次训练的时候，对第一次分错的样本提高其权重，最终将所有分类器的结果加权得到最终的结果。</p><p>每个分类器的权值调整alpha<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200426164427.png" alt>,其中ε是当前分类器的错误率</p><p>当样本被正确分类，样本的权重被更新为<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200426164638.png" alt></p><p>当样本被错误分类，样本的权重被更新为<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200426164745.png" alt></p><h3 id="构建弱分类器"><a href="#构建弱分类器" class="headerlink" title="构建弱分类器"></a>构建弱分类器</h3><p>单层决策树：这里的基分类器选择了单层决策树，将分类器结果设置为1，当不满足阈值条件时，赋值为-1</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#单层决策树生成函数</span></span><br><span class="line"><span class="comment">#通过阈值比较来划分数据</span></span><br><span class="line"><span class="comment">#输入</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stumpClassify</span><span class="params">(dataMat,dime,threVal,threInq)</span>:</span></span><br><span class="line">m=shape(dataMat)[<span class="number">0</span>] <span class="comment">#m行数据</span></span><br><span class="line">returnMat=ones((m,<span class="number">1</span>))</span><br><span class="line"><span class="comment">#print(dime)</span></span><br><span class="line"><span class="comment">#不满足等式要求的设置为-1</span></span><br><span class="line"><span class="keyword">if</span> threInq==<span class="string">"lt"</span>: </span><br><span class="line">returnMat[dataMat[:,dime]&lt;=threVal]=<span class="number">-1.0</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">returnMat[dataMat[:,dime]&gt;threVal]=<span class="number">-1.0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> returnMat</span><br></pre></td></tr></table></figure><p>找到最佳单层决策树：</p><p>（1）在所有输入的数据特征上进行遍历，根据每个样本的最大值和最小值，以及步长得出阈值，从而得到当前的单层决策树即弱分类器</p><p>（2）每个弱分类器设置一个错误率矩阵，当当前预测的正确，则设置为0，将分类器的权重乘以错误率，得到新的权重矩阵（样本的）</p><p>（3）每次比较当前分类器的样本的错误率，找到最小的错误率即最佳单层决策树分类器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.遍历每个特征，根据最大值和最小值，得出遍历的步长</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">minVal=min(dataMat[:,i])</span><br><span class="line">maxVal=max(dataMat[:,i])</span><br><span class="line">stepSize=(maxVal-minVal)/numSize</span><br><span class="line"><span class="comment">#枚举步数，从而确定阈值</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(numSize+<span class="number">1</span>):</span><br><span class="line"><span class="keyword">for</span> inq <span class="keyword">in</span> [<span class="string">"lt"</span>,<span class="string">"gt"</span>]:</span><br><span class="line">threVal=minVal+stepSize*j</span><br><span class="line"><span class="comment">#2.调用阈值分类结果</span></span><br><span class="line">predictVal=stumpClassify(dataMat,i,threVal,inq)</span><br><span class="line">errorMat=mat(ones((m,<span class="number">1</span>)))</span><br><span class="line">errorMat[predictVal==classMat]=<span class="number">0</span></span><br><span class="line">       </span><br><span class="line"><span class="comment">#3.得出错误分类的向量 adaboost和分类器交互的地方</span></span><br><span class="line">error=D.T*errorMat</span><br><span class="line"></span><br><span class="line"><span class="comment">#因为是“单层决策树”，即只分裂一次就可直接得到分类结果了。所以直接用分类错误的比率来衡量决策树的好坏就可</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#print("split:dim: %d\t,threshVal: %f\t,threshVal ineq: %s\t,error: %f" % (i,threVal,inq,error))</span></span><br><span class="line"><span class="keyword">if</span>(error&lt;minError):</span><br><span class="line">minError=error</span><br><span class="line">bestClass=predictVal.copy()</span><br><span class="line"><span class="comment">#4.得出最佳决策树的信息</span></span><br><span class="line">bestStump[<span class="string">'dim'</span>]=i</span><br><span class="line">bestStump[<span class="string">'thresh'</span>]=threVal</span><br><span class="line">bestStump[<span class="string">'ineq'</span>]=inq</span><br></pre></td></tr></table></figure><h3 id="调整权重得到最终结果"><a href="#调整权重得到最终结果" class="headerlink" title="调整权重得到最终结果"></a>调整权重得到最终结果</h3><p>（1）假设迭代numiter次（numiter个弱分类器），每次迭代都得到一个最佳分类器的错误率，根据错误率，计算出分类器的权重并进行保存</p><p>（2）根据分类器权重调整每个样本的权重</p><p>（3）根据分类器权重得出结果，每次将错误率累积（即用分类器权重得出的结果和真实值比较，当满足条件时则输出结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#初始化数据权重</span></span><br><span class="line">D=mat(ones((m,<span class="number">1</span>))/m)</span><br><span class="line">weakAdaList=[]</span><br><span class="line">adaClassEst=mat(zeros((m,<span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(numIter):</span><br><span class="line"><span class="comment">#1.得出当前弱分类器</span></span><br><span class="line">bestStump,error,bestClass=buildStump(dataMat,classLabels,D)</span><br><span class="line"><span class="comment">#print(D.T)</span></span><br><span class="line"><span class="comment">#2.弱分类器权重</span></span><br><span class="line">alpha=float(<span class="number">0.5</span>*log((<span class="number">1</span>-error)/max(error,<span class="number">1e-16</span>))) <span class="comment">#1e-6是为了防止溢出alpha=1.2*ln(1-error/error)</span></span><br><span class="line">bestStump[<span class="string">'alpha'</span>]=alpha</span><br><span class="line"><span class="comment">#更新弱分类器集合</span></span><br><span class="line">weakAdaList.append(bestStump)</span><br><span class="line"><span class="comment">#print("bestClass:",bestClass.T)</span></span><br><span class="line"><span class="comment">#3.更新数据权重,将现在的和原来的对比</span></span><br><span class="line">expon=multiply(<span class="number">-1</span>*alpha*mat(classLabels).T,bestClass)</span><br><span class="line">D=multiply(D,exp(expon))</span><br><span class="line">D=D/sum(D)</span><br><span class="line"></span><br><span class="line"><span class="comment">#3.更新结果</span></span><br><span class="line"><span class="comment">#print(alpha)</span></span><br><span class="line">adaClassEst+=alpha*bestClass</span><br><span class="line"><span class="comment">#print("adaClassEst:",adaClassEst.T)</span></span><br><span class="line"><span class="comment">#通过二值分类器，判断是否相同，不同的则增加错误率</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#sign(x)函数表示x&gt;0,返回0;x=0,返回0;x&lt;0;返回-1</span></span><br><span class="line"><span class="comment">#sign(aggClassEst) != mat(classLabels) 返回bool矩阵,对应元素相等则为True,不等则为False</span></span><br><span class="line"><span class="comment">#multiply(),False相当于0，True相当于1，表示累加分类错误个数</span></span><br><span class="line"> </span><br><span class="line">adaError=multiply(sign(adaClassEst)!=mat(classLabels).T,ones((m,<span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">errorRate=sum(adaError)/m</span><br><span class="line">print(<span class="string">"total error rate:"</span>,errorRate)</span><br><span class="line"><span class="keyword">if</span>(errorRate==<span class="number">0</span>):</span><br><span class="line"><span class="keyword">break</span>;</span><br></pre></td></tr></table></figure><h2 id="应用案例"><a href="#应用案例" class="headerlink" title="应用案例"></a>应用案例</h2><p>测试基于adaboost的分类</p><p>利用上述构造得到的分类器，应用单层决策树得到没饿分类器的类别值，并进行累加得到最后的结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#利用得出的多个弱分类器对数据进行分类</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adaClassify</span><span class="params">(dataToClass,classfier)</span>:</span></span><br><span class="line">dataToClass=mat(dataToClass)</span><br><span class="line">m,n=shape(dataToClass)</span><br><span class="line"><span class="comment">#累加分类的结果</span></span><br><span class="line">adaClassEst=mat(zeros((m,<span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment">#遍历所有的弱分类器  得到一个类别的估计值</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(classfier)):</span><br><span class="line"><span class="comment">#用单层决策树得出类别估计值</span></span><br><span class="line">classEst=stumpClassify(dataToClass,classfier[i][<span class="string">'dim'</span>],classfier[i][<span class="string">'thresh'</span>],classfier[i][<span class="string">'ineq'</span>])</span><br><span class="line">adaClassEst+=classfier[i][<span class="string">'alpha'</span>]*classEst</span><br><span class="line"><span class="comment">#print(adaClassEst)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> sign(adaClassEst)</span><br></pre></td></tr></table></figure><p>导入病马疝气病数据集</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200426210006.png" alt></p><p>选择10个分类器，将x的值输入，得到预测值，将预测值和实际值相等的设置为0，求和得出错误预测得样本数量为16个。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">weakAdaList=adaboost.adaBoostTrainDS(dataSet,classLabels,<span class="number">10</span>)</span><br><span class="line">test,ctest=adaboost.loadDtaSet(<span class="string">"horseColicTest2.txt"</span>)</span><br><span class="line">predict10=adaboost.adaClassify(test,weakAdaList)</span><br><span class="line"></span><br><span class="line">ctest=mat(ctest).T</span><br><span class="line">m,n=ctest.shape</span><br><span class="line">error=mat(ones((m,n)))</span><br><span class="line">error[predict10==ctest]=<span class="number">0</span></span><br><span class="line"></span><br><span class="line">print(sum(error))</span><br></pre></td></tr></table></figure><p>10个分类器的错误率为</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200426210138.png" alt></p><h2 id="如何衡量分类的性能"><a href="#如何衡量分类的性能" class="headerlink" title="如何衡量分类的性能"></a>如何衡量分类的性能</h2><p>之前的算法都是用错误率来衡量算法，但是不全面，一般衡量分类的性能使用混淆矩阵</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200426211129.png" alt></p><h3 id="正确率和召回率"><a href="#正确率和召回率" class="headerlink" title="正确率和召回率"></a>正确率和召回率</h3><p>正确率：TP/(TP+FP) 代表预测结果为真的占总样本的比例</p><p>召回率：TP/(TP+FN）代表预测结果为真占总样本中所有为真的样本的比例。召回率大的分类算法中，真的样本判错的少</p><p>一般好的分类是正确率高，召回率也高，但是两者存在制约关系</p><h3 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h3><p>ROC曲线横轴代表假阳率：FP/(FP+TN)，纵轴代表真阳率：TP/(TP+FN)，左下角代表把所有样本判断为反例，右上角代表把所有样本判断为真例，较好的分类器集中在左上角，即真阳率高，假阳率低</p><p>（1）划定起始点，确定x轴步长（分类为假的个数），y轴步长（分类为真的个数）</p><p>（2）将分类样本按照预测强度排序，从低到高，从排名最低的阳例开始，（所有排名更低的阳例都被判为反例，排名更高的样例判为正例，-&gt;排名高的样例真阳率高假阳率低，所以从（1，1）开始，先把低强度的样例判断）如果样例为真，对真阳率修改，如果样例为假，对假阳率修改</p><p>（）当遇到真值向下走，即沿着纵轴走，遇到假值向左走，即沿着横轴走，将变动的坐标画下来</p><p>（4）划定总体范围，进行图像显示</p><p>AUC：中间过程对曲线下的面积进行累计，这个给出了分类器的平均性能</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#ROC曲线的绘制</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotROC</span><span class="params">(predStrength,classLabels)</span>:</span></span><br><span class="line"><span class="comment">#光标起始点</span></span><br><span class="line">cur=(<span class="number">1.0</span>,<span class="number">1.0</span>)</span><br><span class="line"><span class="comment">#AUC面积值</span></span><br><span class="line">ySum=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#x轴y轴步长</span></span><br><span class="line">numPos=sum(array(classLabels)==<span class="number">1.0</span>)</span><br><span class="line">ystep=<span class="number">1</span>/numPos</span><br><span class="line">xstep=<span class="number">1</span>/(len(classLabels)-numPos)</span><br><span class="line">sortPred=predStrength.argsort()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#画图</span></span><br><span class="line"><span class="comment">#fig=plt.figure()</span></span><br><span class="line">ax=plt.subplot(<span class="number">111</span>)</span><br><span class="line">print(sortPred)</span><br><span class="line">print(sortPred.tolist())</span><br><span class="line">print(sortPred.tolist()[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#对真值则向下走，对假值向左边走  光标从1，1开始</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> sortPred.tolist()[<span class="number">0</span>]:</span><br><span class="line"><span class="keyword">if</span>(classLabels[i]==<span class="number">1</span>):</span><br><span class="line">detX=<span class="number">0</span></span><br><span class="line">detY=ystep</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">detX=xstep</span><br><span class="line">detY=<span class="number">0</span></span><br><span class="line">ySum+=cur[<span class="number">1</span>]</span><br><span class="line"><span class="comment">#x从哪里变到哪里，y从哪里变到哪里</span></span><br><span class="line">ax.plot((cur[<span class="number">0</span>],cur[<span class="number">0</span>]-detX),(cur[<span class="number">1</span>],cur[<span class="number">1</span>]-detY),<span class="string">"b"</span>)</span><br><span class="line">cur=(cur[<span class="number">0</span>]-detX,cur[<span class="number">1</span>]-detY)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"the average of roc:"</span>+str(ySum*xstep))</span><br><span class="line">ax.plot((<span class="number">0</span>,<span class="number">1</span>),(<span class="number">0</span>,<span class="number">1</span>),<span class="string">"b--"</span>)</span><br><span class="line">plt.title(<span class="string">"roc"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"false postive"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"true postive"</span>)</span><br><span class="line">ax.axis([<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>]) <span class="comment">#x,y轴范围</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>结果如下图</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200426213015.png" alt></p><p>计算平均性能时，每次对高度累加，矩形的宽度是xstep,最终两者相乘得到面积</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200426220255.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;元算法或者集成算法是将不同的分类器组合起来，组合的方式可以为：将不同算法组合起来，或者将数据集不同部分给不同的算法，同一种算法在不同设置下进行集成。简言之，元算法是将分类器进行重新集成，提高分类效果，就像生活中决定一件事需要听多方面的意见。&lt;/p&gt;&lt;h2 id=&quot;构建分类器方法&quot;&gt;&lt;a href=&quot;#构建分类器方法&quot; class=&quot;headerlink&quot; title=&quot;构建分类器方法&quot;&gt;&lt;/a&gt;构建分类器方法&lt;/h2&gt;&lt;h3 id=&quot;bagging&quot;&gt;&lt;a href=&quot;#bagging&quot; class=&quot;headerlink&quot; title=&quot;bagging&quot;&gt;&lt;/a&gt;bagging&lt;/h3&gt;&lt;p&gt;在源数据集中选择S次，得到S个数据集（可能会有重复的），选择某种算法和这S个数据集训练得到S个分类器，最终的结果由各分类器投票结果产生，如随机森林算法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="集成学习" scheme="https://www.xiapf.com/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>神经网络和深度学习——第三周编程作业</title>
    <link href="https://www.xiapf.com/blogs/NNet3/"/>
    <id>https://www.xiapf.com/blogs/NNet3/</id>
    <published>2020-04-26T14:07:22.000Z</published>
    <updated>2020-10-27T06:03:48.219Z</updated>
    
    <content type="html"><![CDATA[<p>主要学习构建一个浅层神经网络</p><p>logistic回归可以理解为是一个单层的网络，这里构造一个隐藏层一个输出层的网络，即两层网络（网络层数计算不加入输入层）</p><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>浅层神经网络有输入层，一个隐藏层，一个输出层</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200427142410.png" alt></p><p>约定：ai代表第i层，ai[j]代表第i层第j个神经元</p><h2 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h2><p>网络搭建的时候需要初始化权值和偏置量</p><a id="more"></a><p>（1）权值不能初始化全为0</p><p>全都初始化为0，在第一个隐藏层每个神经元执行相同的运算，即使后面进行梯度下降操作，每个神经元都会与其他神经元的输出一样</p><p>（2）初始化的数据需要是很小的数</p><p>当激活函数选择sigmoid或者tanh的时候，当初始权重取值很大，那么乘上x得到的z也会很大，此时激活函数的值趋近与1或者0，这时候梯度变化很小，会导致算法学习起来很慢</p><h2 id="前向传播和反向传播"><a href="#前向传播和反向传播" class="headerlink" title="前向传播和反向传播"></a>前向传播和反向传播</h2><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200427142845.png" alt></p><p>其中输入x,w[1],b[1]，隐藏层计算z[1],a[1]，输出层计算z[2],a[2]（这里需要根据w[2],b[2]计算）</p><p>黑色箭头代表前向传播，红色箭头代表反向传播</p><p>将X样本全部都竖向排列，即每一列代表一个样本，X是（nx,m)维向量，其中nx代表输入特征的个数，m代表样本的总数量。</p><p>前向传播的公式为：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200427144106.png" alt></p><p>反向传播的为：</p><h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200427144504.png" alt></p><p>其中dz[l]=dA[l]*g^’(dz[l])，又因为dA[l-1]=W[l]T * dz[l]，即dz[l]=W[l+1]T  dz[l+1] * g^’(dz[l])，所以得出了隐藏层中dz[1]</p><h2 id="激活函数的选择"><a href="#激活函数的选择" class="headerlink" title="激活函数的选择"></a>激活函数的选择</h2><p>网络每层的激活函数可以不同，根据每层特点来选择</p><h3 id="不同的激活函数"><a href="#不同的激活函数" class="headerlink" title="不同的激活函数"></a>不同的激活函数</h3><p>（1）sigmoid函数</p><p>当需要输出的是二分类问题时，使用sigmoid函数，最终输出两个值0或者1，当z的值大于0.5输出1，反之输出0。</p><p>一般用在最后的输出层，激活函数的导数为a（1-a）</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200427142719.png" alt></p><p>（2）tanh函数</p><p>该函数用在隐藏层比sigmoid函数好，因为其输出的数据值平均为0。</p><p>一般用在隐藏层，激活函数的导数为1-a**2</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200427145057.png" alt></p><p>（3）ReLu函数</p><p>斜率变化很快，算法学习速度快。</p><p>激活函数的导数为，当z&lt;0，导数为0，反之为1</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200427145121.png" alt></p><h3 id="为什么需要非线性激活函数"><a href="#为什么需要非线性激活函数" class="headerlink" title="为什么需要非线性激活函数"></a>为什么需要非线性激活函数</h3><p>如果用线性激活函数，相当于是把输入重新线性组合了，隐藏层根本没用</p><h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><p>搭建一个浅层网络，对导入的数据进行分类</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>数据集使用planar_utils中的load_planar_dataset，将数据打印出来可知，输入的样本x是2 * 400的向量，每一列代表其坐标，标签列y是0和1</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200427145525.png" alt></p><p>小技巧：将不同标签的点按照不同颜色打印出来在scatter中用cmap=plt.cm.spectral</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(X[<span class="number">0</span>,:],X[<span class="number">1</span>,:],c=squeeze(Y),cmap=plt.cm.Spectral)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>squeeze是将数据变为1维，scatter不能显示二维的数据</p><h3 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h3><p>用sklearn自带的logistic回归测试分类效果</p><p>首先得到分类器，对X,Y进行训练，打印出决策边界，根据得到的预测值和真实值相乘可以得到错误率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#2.测试使用logistic回归的准确度，并画出决策边界</span></span><br><span class="line">clt=linear_model.LogisticRegressionCV()</span><br><span class="line">clt.fit(X.T,Y.T)</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x:clt.predict(x),X,Y)</span><br><span class="line">m=shape(Y)[<span class="number">1</span>]</span><br><span class="line">rate=(dot(Y,clt.predict(X.T))+dot((<span class="number">1</span>-Y),(<span class="number">1</span>-clt.predict(X.T))))/m</span><br><span class="line">print(<span class="string">"准确率为："</span>+str(rate))</span><br></pre></td></tr></table></figure><p>因为是二分类问题，预测值中为1的和真实值为1的相乘，得到判断为1的数据，同理得到判断为0的数据，除以总数得到预测得准确率为47%</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200427145626.png" alt></p><p>附：决策边界</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_decision_boundary</span><span class="params">(model, X, y)</span>:</span></span><br><span class="line">    <span class="comment"># Set min and max values and give it some padding</span></span><br><span class="line">    x_min, x_max = X[<span class="number">0</span>, :].min() - <span class="number">1</span>, X[<span class="number">0</span>, :].max() + <span class="number">1</span></span><br><span class="line">    y_min, y_max = X[<span class="number">1</span>, :].min() - <span class="number">1</span>, X[<span class="number">1</span>, :].max() + <span class="number">1</span></span><br><span class="line">    h = <span class="number">0.01</span></span><br><span class="line">    <span class="comment"># Generate a grid of points with distance h between them</span></span><br><span class="line">    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))</span><br><span class="line">    <span class="comment"># Predict the function value for the whole grid</span></span><br><span class="line">    Z = model(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line">    Z = Z.reshape(xx.shape)</span><br><span class="line">    <span class="comment"># Plot the contour and training examples</span></span><br><span class="line">    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)</span><br><span class="line">    plt.ylabel(<span class="string">'x2'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'x1'</span>)</span><br><span class="line">    plt.scatter(X[<span class="number">0</span>, :], X[<span class="number">1</span>, :], c=np.squeeze(y), cmap=plt.cm.Spectral)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>最终利用logistic得出的分类为</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200427145550.png" alt></p><p>由结果可以看出，logistic分类效果不好，因为给出的数据是线性不可分的，所以分类不理想。</p><p>下面使用神经网络进行分类</p><h3 id="搭建网络结构"><a href="#搭建网络结构" class="headerlink" title="搭建网络结构"></a>搭建网络结构</h3><p>设定输入层，隐藏层，输出层的神经元个数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.初始化网络结构</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initial_layer_size</span><span class="params">(X,Y,n_h)</span>:</span></span><br><span class="line"><span class="comment">#输入样本的特质数量</span></span><br><span class="line">m=shape(X)[<span class="number">0</span>]</span><br><span class="line">n=shape(Y)[<span class="number">0</span>]</span><br><span class="line">n_x=m</span><br><span class="line">n_h=n_h</span><br><span class="line">n_y=n</span><br><span class="line"><span class="keyword">return</span> n_x,n_h,n_y</span><br></pre></td></tr></table></figure><h3 id="初始化参数"><a href="#初始化参数" class="headerlink" title="初始化参数"></a>初始化参数</h3><p>初始化各层的w和b的初始值，这里的w用正态分布随机数乘以一个很小的数来初始化，以此保证训练的速度（很小的数，梯度变化快），这里将每层的w,b作为parameters保存下来，便于前向传播、反向传播、更新参数中使用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#2.初始化参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initial_parameters</span><span class="params">(n_x,n_h,n_y)</span>:</span></span><br><span class="line"><span class="comment">#初始化w1,b1,w2,b2</span></span><br><span class="line"><span class="comment">#初始化成一个很小的数</span></span><br><span class="line">random.seed(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">W1=random.randn(n_h,n_x)*<span class="number">0.01</span></span><br><span class="line">b1=zeros((n_h,<span class="number">1</span>))</span><br><span class="line">W2=random.randn(n_y,n_h)*<span class="number">0.01</span></span><br><span class="line">b2=zeros((n_y,<span class="number">1</span>))</span><br><span class="line">parameters=&#123;<span class="string">"W1"</span>:W1,<span class="string">"W2"</span>:W2,<span class="string">"b1"</span>:b1,<span class="string">"b2"</span>:b2&#125;</span><br><span class="line"><span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><p>根据公式，计算每层Z，A的值，并进行保存在cache中，便于反向传播中计算导数使用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#3.前向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_progation</span><span class="params">(X,parameters)</span>:</span></span><br><span class="line">W1=parameters[<span class="string">"W1"</span>]</span><br><span class="line">W2=parameters[<span class="string">"W2"</span>]</span><br><span class="line">b1=parameters[<span class="string">"b1"</span>]</span><br><span class="line">b2=parameters[<span class="string">"b2"</span>]</span><br><span class="line">Z1=dot(W1,X)+b1</span><br><span class="line">A1=tanh(Z1)</span><br><span class="line">Z2=dot(W2,A1)+b2</span><br><span class="line">A2=sigmoid(Z2)</span><br><span class="line">cache=&#123;<span class="string">"Z1"</span>:Z1,<span class="string">"Z2"</span>:Z2,<span class="string">"A1"</span>:A1,<span class="string">"A2"</span>:A2&#125;</span><br><span class="line"><span class="keyword">return</span> A2,cache</span><br></pre></td></tr></table></figure><h3 id="计算损失"><a href="#计算损失" class="headerlink" title="计算损失"></a>计算损失</h3><p>损失函数和logistic中一样，使用交叉熵函数，这里需要用到最后一层中的输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#4.计算损失函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(Y,yhat)</span>:</span></span><br><span class="line"><span class="comment">#yhat=cache["A2"]</span></span><br><span class="line">m=shape(Y)[<span class="number">1</span>]</span><br><span class="line">multiplyc=multiply(Y,log(yhat))+multiply((<span class="number">1</span>-Y),log(<span class="number">1</span>-yhat))</span><br><span class="line">cost=-sum(multiplyc)/m</span><br><span class="line"><span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><h3 id="反向传播-1"><a href="#反向传播-1" class="headerlink" title="反向传播"></a>反向传播</h3><p>根据公式，得到每层的dz,dw,db，并将梯度变化用grad保存下来，方便更新参数中使用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#5.计算反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">back_progation</span><span class="params">(X,Y,parameters,cache)</span>:</span></span><br><span class="line">m=shape(X)[<span class="number">1</span>]</span><br><span class="line"><span class="comment">#计算梯度</span></span><br><span class="line">A2=cache[<span class="string">"A2"</span>]</span><br><span class="line">Z2=cache[<span class="string">"Z2"</span>]</span><br><span class="line">A1=cache[<span class="string">"A1"</span>]</span><br><span class="line">Z1=cache[<span class="string">"Z1"</span>]</span><br><span class="line">W1=parameters[<span class="string">"W1"</span>]</span><br><span class="line">W2=parameters[<span class="string">"W2"</span>]</span><br><span class="line"></span><br><span class="line">dZ2=A2-Y</span><br><span class="line">dW2=(<span class="number">1</span>/m)*dot(dZ2,A1.T)</span><br><span class="line">db2=(<span class="number">1</span>/m)*sum(dZ2,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">dZ1=multiply(dot(W2.T,dZ2),(<span class="number">1</span>-A1**<span class="number">2</span>))</span><br><span class="line">dW1=(<span class="number">1</span>/m)*dot(dZ1,X.T)</span><br><span class="line">db1=(<span class="number">1</span>/m)*sum(dZ1,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">grads=&#123;<span class="string">"dW1"</span>:dW1,<span class="string">"dW2"</span>:dW2,<span class="string">"db1"</span>:db1,<span class="string">"db2"</span>:db2&#125;</span><br></pre></td></tr></table></figure><p>附：dot是点乘（数字相乘），最终得到是一个数字，multiply是各个位置相乘（矩阵相乘），最终得到一个矩阵</p><h3 id="更新参数"><a href="#更新参数" class="headerlink" title="更新参数"></a>更新参数</h3><p>根据梯度计算每次更新的w,b，并在parameters中保存，每次根据新的w和b重新前向传播计算损失</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#6.更新参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters,grads,alpha)</span>:</span></span><br><span class="line">W1=parameters[<span class="string">"W1"</span>]</span><br><span class="line">W2=parameters[<span class="string">"W2"</span>]</span><br><span class="line">b1=parameters[<span class="string">"b1"</span>]</span><br><span class="line">b2=parameters[<span class="string">"b2"</span>]</span><br><span class="line"></span><br><span class="line">dW1=grads[<span class="string">"dW1"</span>]</span><br><span class="line">dW2=grads[<span class="string">"dW2"</span>]</span><br><span class="line">db1=grads[<span class="string">"db1"</span>]</span><br><span class="line">db2=grads[<span class="string">"db2"</span>]</span><br><span class="line"></span><br><span class="line">W1=W1-alpha*dW1</span><br><span class="line">W2=W2-alpha*dW2</span><br><span class="line">b1=b1-alpha*db1</span><br><span class="line">b2=b2-alpha*db2</span><br><span class="line">parameters=&#123;<span class="string">"W1"</span>:W1,<span class="string">"W2"</span>:W2,<span class="string">"b1"</span>:b1,<span class="string">"b2"</span>:b2&#125;</span><br><span class="line"><span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h3 id="生成模型"><a href="#生成模型" class="headerlink" title="生成模型"></a>生成模型</h3><p>（1）初始化网络结构</p><p>（2）初始化参数</p><p>（3）每次循环中</p><p>1°先正向传播，根据初始化的参数（parameters），得到cache（保存了每层的A,Z）</p><p>°再根据输出层结果得出损失函数</p><p>3°接着，进行反向传播，根据cache得出每层的梯度保存在grad中</p><p>°最后进行参数更新，按照梯度，利用w=w-alpha * dw（b同理）得出更新的参数</p><p>按照以上步骤进行循环，每1000次输出损失，最终得到训练出的模型，即parameters</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#7.模型整合 训练模型，按照次数输出损失函数的值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X,Y,n_h,numIter,alpha)</span>:</span></span><br><span class="line">random.seed(<span class="number">3</span>)</span><br><span class="line">n_x,n_h,n_y=initial_layer_size(X,Y,n_h)</span><br><span class="line">parameters=initial_parameters(n_x,n_h,n_y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(numIter):</span><br><span class="line">A2,cache=forward_progation(X,parameters)</span><br><span class="line">cost=compute_cost(Y,A2)</span><br><span class="line">grads=back_progation(X,Y,parameters,cache)</span><br><span class="line">parameters=update_parameters(parameters,grads,alpha)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(i%<span class="number">1000</span>==<span class="number">0</span>):</span><br><span class="line">print(<span class="string">"第"</span>+str(i)+<span class="string">"次,损失是："</span>+str(cost))</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h4 id="模型预测"><a href="#模型预测" class="headerlink" title="模型预测"></a>模型预测</h4><p>根据得到的模型，进行一次前向传播得到输出层的值，即为预测值，进行四舍五入得到模型预测值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#8.模型预测</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_predict</span><span class="params">(X,parameters)</span>:</span></span><br><span class="line">A2,cache=forward_progation(X,parameters)</span><br><span class="line"><span class="comment"># prediction=cache["A2"]</span></span><br><span class="line"><span class="comment"># print(prediction)</span></span><br><span class="line">prediction=numpy.round(A2)</span><br><span class="line"><span class="keyword">return</span> prediction</span><br></pre></td></tr></table></figure><p>最终，先训练出模型，再对模型预测，得到最终结果，画出决策边界（决策边界中输入的参数是预测函数模型，即将预测得值输入，和真实值比较，从而画出边界）</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200427153130.png" alt></p><p>可以看出效果比logistic好很多，基本都分类正确。</p><p>最终分类正确率为90.5%</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200427153234.png" alt></p><p>注：这里的隐藏层神经元个数选择了4个，可以选择不同的神经元查看效果。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;主要学习构建一个浅层神经网络&lt;/p&gt;&lt;p&gt;logistic回归可以理解为是一个单层的网络，这里构造一个隐藏层一个输出层的网络，即两层网络（网络层数计算不加入输入层）&lt;/p&gt;&lt;h2 id=&quot;网络结构&quot;&gt;&lt;a href=&quot;#网络结构&quot; class=&quot;headerlink&quot; title=&quot;网络结构&quot;&gt;&lt;/a&gt;网络结构&lt;/h2&gt;&lt;p&gt;浅层神经网络有输入层，一个隐藏层，一个输出层&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200427142410.png&quot; alt&gt;&lt;/p&gt;&lt;p&gt;约定：ai代表第i层，ai[j]代表第i层第j个神经元&lt;/p&gt;&lt;h2 id=&quot;参数初始化&quot;&gt;&lt;a href=&quot;#参数初始化&quot; class=&quot;headerlink&quot; title=&quot;参数初始化&quot;&gt;&lt;/a&gt;参数初始化&lt;/h2&gt;&lt;p&gt;网络搭建的时候需要初始化权值和偏置量&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="浅层网络" scheme="https://www.xiapf.com/tags/%E6%B5%85%E5%B1%82%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>神经网络和深度学习——第二周编程作业</title>
    <link href="https://www.xiapf.com/blogs/NNet2/"/>
    <id>https://www.xiapf.com/blogs/NNet2/</id>
    <published>2020-04-17T16:03:55.000Z</published>
    <updated>2020-10-27T06:04:10.952Z</updated>
    
    <content type="html"><![CDATA[<h2 id="神经网络中的设定"><a href="#神经网络中的设定" class="headerlink" title="神经网络中的设定"></a>神经网络中的设定</h2><p>设定(x,y)是单独的一个样本，m个训练样本为{ ((x(1),y(1)), ((x(12),y(2)), ….((x(m),y(m))}</p><p>设定输入的样本为</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200417221326.png" alt></p><p>即将X中所有输入的样本都纵向排列（不使用横向排列的方式），每一列代表一条数据，每一行代表一个输入特征，如原输入样本x1中的特征为a,b,c，a,b,c分别对应了数值，输入样本x2中也有这三个特征，但是数值可能不一样，所以X是nx维向量，有m个列（代表m条数据）</p><a id="more"></a><p>Y则将所有样本输出依次排列成m维行向量</p><h2 id="logistic原理"><a href="#logistic原理" class="headerlink" title="logistic原理"></a>logistic原理</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>logistic是用一条直线去拟合数据点，即需要得到y=wx+b这条直线，当已知输入样本为{ ((x(1),y(1)), ((x(12),y(2)), ….((x(m),y(m))}，通过不断调整w和b的值，让预测输出yhat近似等于实际输出y。</p><p>其中w对应每个输入特征，即需要构建的w是个nx维的列向量，则需要得到的直线为y=w.T*X+b</p><h3 id="模型公式"><a href="#模型公式" class="headerlink" title="模型公式"></a>模型公式</h3><p>y=w.T*X+b</p><p>由于logistic是个二分类器，需要分类函数，这里选择sigmoid，即a=sigmoid(z)，其中z=y</p><p>同时，衡量一个算法运行情况使用误差函数，常用的误差函数为均方差函数，即L=1/2(yhat-y)2，但是该函数不是凸函数，logistic中使用梯度下降法，找到最优的w和b，适合用凸函数，只有一个最优解，均方差函数有多个局部最优解，不适合该求解方法，这里选择的损失函数为L=-(ylogyhat+(1-y)log(1-yhat))，这是训练一个样本时算法的表现，要衡量全部样本的表现需要用到成本函数J=1/m*∑L=-1/m∑-(ylogyhat+(1-y)log(1-yhat))，即通过J的值判断找到的w,b是否合适。J是衡量算法好坏的依据</p><p>综上，logistic模型公式为</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200417225111.png" alt></p><p>注：选择该成本函数的原因</p><p>由上面两个公式，约定yhat=p(y=1|x),即给定训练样本x,y取1的概率</p><p>当y=0时，p(y|x)=1-yhat；当y=1时，p(y|x)=yhat，则可以将该表达式进行合并为p(y|x)=yhat^(y)+(1-yhat)^(1-y)，根据极大似然函数取对数得到第三个式子，为了进行缩放加上了1/m，最终得出了成本函数</p><h3 id="正向传播和反向传播"><a href="#正向传播和反向传播" class="headerlink" title="正向传播和反向传播"></a>正向传播和反向传播</h3><p>logistic算法采用梯度下降算法：每次都朝着下降的快的反向移动，因此需要计算梯度，沿着梯度放下修正w和b的值</p><p>正向传播中，根据输入的x1,x2…,w,b的值，计算z，由z再计算出a，根据a的值得出损失函数L</p><p>反向传播中</p><p>根据得出的L的值，反向得出各个梯度值，最终得出dw,db，即需要沿着这个梯度变化。首先从损失函数L得出a的梯度da=-y/a+(1-y)/(1-a)，再由a得到dz，dz=dL/dz=dL/da * da/dz（链式法则）=a-y，同理得出dw=x * dz，db=dz。</p><p>根据梯度下降公式w:=w-alpha * dw   b:=b-alpha * db每次来修改w和b的值</p><p>反向传播过程图</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200417231506.png" alt></p><h3 id="提升算法性能"><a href="#提升算法性能" class="headerlink" title="提升算法性能"></a>提升算法性能</h3><p>（1）不使用显示的for循环改用向量化</p><p>根据上述公式，为了求出z，需要对每个x(i)，求出z(i)，再依次得出yhat(i),L(i)，当有m个样本时，就需要循环m次，数据集很大的情况下效率很低，因此将该for循环实现的功能向量化，只需要把x用向量表示，即将每个样本纵向排列得到一个m * nx的矩阵，这样直接通过dot(w.T,X)，即可以计算出z的值，省去了遍历数据集的循环</p><p>（2）利用numpy中的库函数的广播功能</p><p>当m * n的矩阵和1 * n的矩阵相加时，后面的矩阵自动扩展成m * n进行计算，如上述式子中的常数b，在计算过程中会自动扩展</p><h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><p>利用logistic回归搭建一个可以识别猫的神经网络</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><h3 id="处理数据"><a href="#处理数据" class="headerlink" title="处理数据"></a>处理数据</h3><p>这里猫的图片数据保存在h5文件中，以64*64的三通道数据保存</p><p>（1）首先，从h5文件中读入训练数据和测试数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.处理数据</span></span><br><span class="line">train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes=load_dataset()</span><br></pre></td></tr></table></figure><p>以训练数据位列，查看数据纬度，发现纬度为（290，64，64，3）代表有290个图像，每个图像都是64 * 64的，里面包含了红绿蓝三个通道数字，因此需要将单个图片中红绿蓝矩阵合并为一个列向量，即</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200417232700.png" alt></p><p>（2）因此，第二步需要进行数据降维</p><p>需要将每个图片数据变为64 * 64 * 3=12288维的列向量，或者也可以不指定列数，直接给个-1，让程序自己计算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_set_x=train_set_x_orig.reshape(train_set_x_orig.shape[<span class="number">0</span>],<span class="number">-1</span>).T <span class="comment">#将每个图片竖着排放</span></span><br><span class="line">test_set_x=test_set_x_orig.reshape(test_set_x_orig.shape[<span class="number">0</span>],<span class="number">-1</span>).T</span><br></pre></td></tr></table></figure><p>（3）数据标准化</p><p>图像中的像素点事0~255之间，只需要把输入的数据除以255即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_set_x=train_set_x/<span class="number">255</span></span><br><span class="line">test_set_x=test_set_x/<span class="number">255</span></span><br></pre></td></tr></table></figure><h3 id="搭建神经网络"><a href="#搭建神经网络" class="headerlink" title="搭建神经网络"></a>搭建神经网络</h3><p>（0）确定分类函数</p><p>因为logistic是二值分类器，这里选择sigmoid函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">sig=<span class="number">1</span>/(<span class="number">1</span>+exp(-z))</span><br><span class="line"><span class="keyword">return</span> sig</span><br></pre></td></tr></table></figure><p>（1）初始化w,b</p><p>根据输入数据的特征纬度，建立w矩阵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inital_wPara_bPara</span><span class="params">(dim)</span>:</span></span><br><span class="line">w=zeros((dim,<span class="number">1</span>))<span class="comment">#初始化为列向量</span></span><br><span class="line">b=<span class="number">0</span></span><br><span class="line"><span class="keyword">return</span> w,b</span><br></pre></td></tr></table></figure><p>（2）搭建模型</p><p>对每次循环</p><p>1°正向传播</p><p>根据输入的值，计算出当前的z，a，J，从而得出dw,db的梯度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">progation</span><span class="params">(w,b,x,y)</span>:</span></span><br><span class="line"><span class="comment">#logictic公式</span></span><br><span class="line">z=dot(w.T,x)+b</span><br><span class="line">a=sigmoid(z)</span><br><span class="line">m=x.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">loss=(<span class="number">-1</span>/m)*sum((y*log(a)+(<span class="number">1</span>-y)*log(<span class="number">1</span>-a)))</span><br><span class="line">dz=a-y</span><br><span class="line">dw=<span class="number">1</span>/m*dot(x,dz.T)</span><br><span class="line">db=<span class="number">1</span>/m*sum(dz)</span><br><span class="line"></span><br><span class="line">grad=&#123;<span class="string">"dw"</span>:dw,<span class="string">"db"</span>:db&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> grad,loss</span><br></pre></td></tr></table></figure><p>2°反向传播</p><p>根据dw,db的梯度，计算新的w，b的值</p><p>3°记录损失和最终找出的最合适的w,b的值</p><p>每次根据迭代次数记录损失</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span><span class="params">(w,b,x,y,alpha,numIter,print_cost=False)</span>:</span></span><br><span class="line"></span><br><span class="line">costs=[]</span><br><span class="line"><span class="comment">#遍历numiter次</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(numIter):</span><br><span class="line"><span class="comment">#得出每次的梯度，进行修正</span></span><br><span class="line">grad,cost=progation(w,b,x,y)</span><br><span class="line">dw=grad[<span class="string">'dw'</span>]</span><br><span class="line">db=grad[<span class="string">'db'</span>]</span><br><span class="line">w=w-alpha*dw</span><br><span class="line">b=b-alpha*db</span><br><span class="line"><span class="keyword">if</span>(i%<span class="number">100</span>==<span class="number">0</span>):</span><br><span class="line">costs.append(cost)</span><br><span class="line"><span class="keyword">if</span>(print_cost) <span class="keyword">and</span> (i%<span class="number">100</span>==<span class="number">0</span>):</span><br><span class="line">print(<span class="string">"i:"</span>+str(i)+<span class="string">"  误差是："</span>+str(cost))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">params=&#123;<span class="string">"w"</span>:w,<span class="string">"b"</span>:b&#125;</span><br><span class="line">grads=&#123;<span class="string">"dw"</span>:dw,<span class="string">"db"</span>:db&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> params,grads,costs</span><br></pre></td></tr></table></figure><p>4°对模型结果进行预测</p><p>根据训练得出的模型w,b,计算z=w*x+b，求出z的值，则得出a=sigmoid(z)，根据a的值记录最终的输出是1还是0（a&gt;0.5是1，反之是0）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(w,b,x)</span>:</span></span><br><span class="line">a=sigmoid(dot(w.T,x)+b)</span><br><span class="line">m=a.shape[<span class="number">1</span>]</span><br><span class="line">yPredict=zeros((<span class="number">1</span>,m))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line"><span class="keyword">if</span>(a[:,i]&gt;<span class="number">0.5</span>):</span><br><span class="line">yPredict[<span class="number">0</span>,i]=<span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">yPredict[<span class="number">0</span>,i]=<span class="number">0</span></span><br><span class="line"><span class="keyword">return</span> yPredict</span><br></pre></td></tr></table></figure><p>（3）最终形成模型，对训练集和测试集上的数据查看准确率</p><p>对得出的预测值减去实际的输出并取绝对值，当预测相同，则对应位置为0，不同则为1，所有数相加起来除以总数，即对所有元素取平均，则是误差率，用1减去这个数，则是正确率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(trainX,trainY,testX,testY,alpha=<span class="number">0.005</span>,numIter=<span class="number">2000</span>,print_cost=True)</span>:</span></span><br><span class="line"><span class="comment">#初始化</span></span><br><span class="line"><span class="comment">#行向量是多少数据特征，列向量是多少条数据，w的个数对应数据特征的个数</span></span><br><span class="line">m=trainX.shape[<span class="number">0</span>]</span><br><span class="line">w,b=inital_wPara_bPara(m)</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练模型</span></span><br><span class="line">params,grads,costs=optimize(w,b,trainX,trainY,alpha,numIter,print_cost)</span><br><span class="line"></span><br><span class="line">w=params[<span class="string">"w"</span>]</span><br><span class="line">b=params[<span class="string">"b"</span>]</span><br><span class="line">trainPredict=predict(w,b,trainX)</span><br><span class="line">testPredict=predict(w,b,testX)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"训练集准确率："</span>,<span class="number">100</span>-mean(abs(trainPredict-trainY))*<span class="number">100</span>)</span><br><span class="line">print(<span class="string">"测试集准确率："</span>,<span class="number">100</span>-mean(abs(testPredict-testY))*<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">d=&#123;<span class="string">"costs"</span>:costs,<span class="string">"trainPredict:"</span>:trainPredict,<span class="string">"testPredict:"</span>:testPredict,<span class="string">"w"</span>:w,<span class="string">"b"</span>:b,<span class="string">"alpha"</span>:alpha,<span class="string">"numIter"</span>:numIter&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> d</span><br></pre></td></tr></table></figure><h3 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h3><p>（1）固定学习率</p><p>读入costs的值，进行显示，得出每一百次costs误差的变化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">d=model(train_set_x,train_set_y_orig,test_set_x,test_set_y_orig)</span><br><span class="line">costs=d[<span class="string">"costs"</span>]</span><br><span class="line">plt.plot(costs)</span><br><span class="line">plt.xlabel(<span class="string">"iter"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"costs"</span>)</span><br><span class="line">plt.title(<span class="string">"alpha="</span>+str(d[<span class="string">"alpha"</span>]))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>在迭代次数2000，学习率为0.005下误差的变化</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200417234704.png" alt></p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200417234727.png" alt></p><p>可以看出训练集准确率很高，但是测试集准确率有所降低，说明训练的模型存在过拟合的现象</p><p>（2）不同学习率</p><p>输入不同的学习率，查看误差的变化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">alpha=[<span class="number">0.01</span>,<span class="number">0.001</span>,<span class="number">0.0001</span>]</span><br><span class="line">m=shape(alpha)[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">print(<span class="string">"alpha="</span>+str(alpha[i]))</span><br><span class="line">d=model(train_set_x,train_set_y_orig,test_set_x,test_set_y_orig,alpha[i])</span><br><span class="line">costs=d[<span class="string">"costs"</span>]</span><br><span class="line">plt.plot(costs,label=str(d[<span class="string">"alpha"</span>]))</span><br><span class="line">print(<span class="string">"------------------------------------------------"</span>)</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">"iter"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"costs"</span>)</span><br><span class="line">legend = plt.legend(loc=<span class="string">'upper center'</span>, shadow=<span class="literal">True</span>)</span><br><span class="line">frame = legend.get_frame()</span><br><span class="line">frame.set_facecolor(<span class="string">'0.90'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>结果为</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200417235032.png" alt></p><p>从结果可以看出，当学习率很小的时候，误差基本不怎么变化，即一直没到嘴有点，当学习率很大的时候，前面由于学习率大，所有误差变化快，出现陡降，当走到最优解之后，误差趋向平缓</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;神经网络中的设定&quot;&gt;&lt;a href=&quot;#神经网络中的设定&quot; class=&quot;headerlink&quot; title=&quot;神经网络中的设定&quot;&gt;&lt;/a&gt;神经网络中的设定&lt;/h2&gt;&lt;p&gt;设定(x,y)是单独的一个样本，m个训练样本为{ ((x(1),y(1)), ((x(12),y(2)), ….((x(m),y(m))}&lt;/p&gt;&lt;p&gt;设定输入的样本为&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200417221326.png&quot; alt&gt;&lt;/p&gt;&lt;p&gt;即将X中所有输入的样本都纵向排列（不使用横向排列的方式），每一列代表一条数据，每一行代表一个输入特征，如原输入样本x1中的特征为a,b,c，a,b,c分别对应了数值，输入样本x2中也有这三个特征，但是数值可能不一样，所以X是nx维向量，有m个列（代表m条数据）&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="回归" scheme="https://www.xiapf.com/tags/%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>Leecode学习笔记（六）——系统设计题</title>
    <link href="https://www.xiapf.com/blogs/sysDesignNote/"/>
    <id>https://www.xiapf.com/blogs/sysDesignNote/</id>
    <published>2020-04-13T07:35:46.000Z</published>
    <updated>2020-04-13T07:38:00.472Z</updated>
    
    <content type="html"><![CDATA[<h2 id="缓存机制"><a href="#缓存机制" class="headerlink" title="缓存机制"></a>缓存机制</h2><p>（1）LRU</p><p>least recently use:最近最少使用</p><p>删除缓存依据：按照时间计算，每次删除最久未使用的数据。</p><p>（2）LFU</p><p>least frequently use：最近最不常使用</p><p>删除缓存依据：按照时间和使用频率计算，以使用频率为主，每次删除数据时，查看使用频率最小的数据进行删除，当使用频率相同时，根据最久未使用即时间来删除。</p><a id="more"></a><p>总体思路：</p><p>缓存主要是为了获取数据和插入数据，因此需要对缓存数据进行节点设计，根据其特性设置key.value用于根据key查找值，同时需要设置使用时间time,使用频率cnt.</p><p>为了方便查找可以使用hash表存储key以及node,可以方便的在hash表内通过node-&gt;key查找到node，但是哈希表无序，因此还需要存储节点的使用时间/频率，这时候有两种思路：</p><p>第一种是利用set容器，c++内置的set是使用红黑树排序，即会形成平衡二叉树，使用运算符重载，将set内部根据时间和频率来进行排序，使用频率最少或者时间最少的会排在前面，以set记录时间/频率，但是搜索时间复杂度是O（logn）。</p><p>第二种思路是建立双链表list存储节点，哈希表中存储key映射到list节点的迭代器，当数据使用是，将其从原来位置删除，插入最前面，当需要删除数据的时候，从末位删除，链表操作的时间复杂度是O（1）。</p><p>获取数据（根据密钥获得值）：</p><p>根据哈希表中的key查找节点，并进行缓存数据更新。</p><p>（2）插入数据（将key,value插入）：</p><p>判断key是否存储过了，当存储过之后更新数据值，反之需要插入数据，此时需要判断是否已经达到容量顶端，达到了，需要删除最不常使用的数据。，并进行缓存数据更新。</p><p>注：每次操作过后都需要进行缓存数据更新，即在哈希表中更新数据节点以及set/双向链表list进行更新。</p><h2 id="设计简单推特"><a href="#设计简单推特" class="headerlink" title="设计简单推特"></a>设计简单推特</h2><p>题目：355. 设计推特设计一个简化版的推特(Twitter)，可以让用户实现发送推文，关注/取消关注其他用户，能够看见关注人（包括自己）的最近十条推文。你的设计需要支持以下的几个功能：postTweet(userId, tweetId): 创建一条新的推文,getNewsFeed(userId): 检索最近的十条推文。每个推文都必须是由此用户关注的人或者是用户自己发出的。推文必须按照时间顺序由最近的开始排序。,follow(followerId, followeeId): 关注一个用户,unfollow(followerId, followeeId): 取消关注一个用户</p><p>思路：</p><p>（1）初始化两个hash表</p><p>题目要求中功能3，4是需要能够关注和取消关注一个用户，那么就需要存储当前用户到自己关注的人之间的映射，需要用到hash表。</p><p>题目要求中的功能1，2是需要对推文进行操作，需要存储当前用户在某个时刻写的推文（这里的推文用数字代替），为了方便通过id查找推文，后面的数据用pair形式存储时间和推文。这里需要一个全局变量来计算发推的时间。初始化时间为0，离现在越近，时间越大。</p><p>存储当前用户到自己关注的人之间的映射 map1&lt;int,set<int>&gt;</int></p><p>存储当前用户在某个时刻写的推文的映射 map2&lt;int,vector&lt;pair&lt;int,int&gt;&gt; ，为了方便通过id查找推文，后面的数据用pair形式存储时间和推文</p><p>（2）关注和取消关注</p><p>直接操作map1，加入当前用户关注的id和删除取消关注的id</p><p>（3）创建推文</p><p>直接操作map2，加入当前用户，在当前时间，写下的推文</p><p>（4）检索推文</p><p>需要注意的是搜索推文的时候需要把自己也加上。检索推文的时候需要进行比较，取前10条，这里可以建立优先队列，根据时间进行自动排序。</p><p>建立优先级队列即大顶堆priori_queue&lt;pair&lt;int,int&gt;&gt;，这里会根据pair中第一个元素进行降序排列，当第一个元素相等会根据第二个元素排序，但由于时间肯定是不一样的，这里默认是对时间进行自动排序，离现在越近，时间越大。</p><p>首先将自己加入自己关注的用户列表中，通过当前用户的id查到所有关注的人（这里需要包括自己），再通过查到关注的用户的id，找到他们在某个时间发的推文，将其插入队列中。这时候输出堆顶前10个就是最近的推文。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;缓存机制&quot;&gt;&lt;a href=&quot;#缓存机制&quot; class=&quot;headerlink&quot; title=&quot;缓存机制&quot;&gt;&lt;/a&gt;缓存机制&lt;/h2&gt;&lt;p&gt;（1）LRU&lt;/p&gt;&lt;p&gt;least recently use:最近最少使用&lt;/p&gt;&lt;p&gt;删除缓存依据：按照时间计算，每次删除最久未使用的数据。&lt;/p&gt;&lt;p&gt;（2）LFU&lt;/p&gt;&lt;p&gt;least frequently use：最近最不常使用&lt;/p&gt;&lt;p&gt;删除缓存依据：按照时间和使用频率计算，以使用频率为主，每次删除数据时，查看使用频率最小的数据进行删除，当使用频率相同时，根据最久未使用即时间来删除。&lt;/p&gt;
    
    </summary>
    
    
      <category term="算法" scheme="https://www.xiapf.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="https://www.xiapf.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="系统设计" scheme="https://www.xiapf.com/tags/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>mac下使用nginx实现负载均衡</title>
    <link href="https://www.xiapf.com/blogs/nginx/"/>
    <id>https://www.xiapf.com/blogs/nginx/</id>
    <published>2020-04-10T06:38:52.000Z</published>
    <updated>2020-04-10T06:42:10.471Z</updated>
    
    <content type="html"><![CDATA[<h2 id="安装流程"><a href="#安装流程" class="headerlink" title="安装流程"></a>安装流程</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install nginx</span><br></pre></td></tr></table></figure><h3 id="查看是否安装成功"><a href="#查看是否安装成功" class="headerlink" title="查看是否安装成功"></a>查看是否安装成功</h3><p>nginx默认的端口号为8080，默认的启动页面为index.html，初始页面的默认路径为/usr/local/var/www/index.html，在命令行内输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nginx</span><br></pre></td></tr></table></figure><p>这时已启动nginx服务，打开浏览器输入localhose:8080，显示如下页面说明安装成功。</p><a id="more"></a><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200410142258.png" alt></p><p>配置文件默认的路径为/usr/local/ect/nginx/nginx.conf，对nginx的操作主要是修改配置文件。</p><h2 id="实现负载均衡"><a href="#实现负载均衡" class="headerlink" title="实现负载均衡"></a>实现负载均衡</h2><p>nginx可以抽象的理解为时一个代理服务器，它通过接收网页端的请求，并将其按照一定策略转发到服务器中，实现负载均衡。这里策略主要是按照权重设置每台服务器，权重越大说明转发给其的请求的可能性越大，同时还可以设置备用服务器，平时不给备用服务器发送请求，一旦其他服务器宕机，将会使用备用服务器，来保证服务器中服务的连续性。</p><h3 id="设置服务器"><a href="#设置服务器" class="headerlink" title="设置服务器"></a>设置服务器</h3><p>在同一个项目中新建三个启动项，作为三个服务器，并分别设置端口号为9091，9092，9093，将端口号分别保存为application.properties，application.properties2，application.properties3，并在启动环境变量中将每个服务器对应的端口号进行加载。</p><p>选择edit configurations</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200410141413.png" alt></p><p>分别设置每个服务器的端口</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200410141518.png" alt></p><p>这样三个服务端就设置好了，当调用哪个服务器时会在控制台显示其端口。</p><h3 id="设置nginx"><a href="#设置nginx" class="headerlink" title="设置nginx"></a>设置nginx</h3><p>主要是通过设置配置文件，负载均衡是通过关键字upstream设置，打开nginx.conf配置文件，在http项目下增加upstream，把之前设置的服务器的端口号加入，并设置每个服务器的权值和备用服务器情况，这里设置第一台服务器时主服务器，其他两台服务器时备用服务器。</p><p>（1）upstream后面跟着的是当前分发请求的入口名称，需要在location中增加该名称</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200410142229.png" alt></p><p>（2）找到server项目下的location，nginx接收请求之后会从location中找返回的页面，因为nginx默认监听的端口是8080，启动入口是index.html，为了让nginx从设置的webpoots中找服务器，需要把原来的端口设置为80,原来的入口页面注释掉，并增加设置的接收请求的服务器的名称</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200410142558.png" alt></p><p>这里proxy_pass后面的名称需要和upstream后设置的名称要一致，不然nginx找不到接收请求的入口服务器</p><p>注：upstream后面跟的服务器入口名称不能有下划线，不然会报错。</p><p>（3）修改完nginx配置文件后，输入命令，让配置起作用</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nginx -s reload</span><br></pre></td></tr></table></figure><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>（1）我这里服务器实现的功能是在网页端输入对应手机名称，返回手机的信息，被调用的服务器会在控制台显示自己的端口号。打开浏览器输入localhost/xiaomi，网页显示：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200410143131.png" alt></p><p>（2）这是只有第一台服务器会在控制台显示自己的端口号，点击刷新，其他两个服务器不会显示端口号，即没有将请求转发给备用服务器。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200410143329.png" alt></p><p>（3）当将第一台服务器停止运行后，网页端仍不会报错，这时候调用了另外两台服务器，并按权值接收分发过来的请求。</p><p>暂停第一个服务器</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200410143527.png" alt></p><p>其他两个服务器起作用</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200410143539.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;安装流程&quot;&gt;&lt;a href=&quot;#安装流程&quot; class=&quot;headerlink&quot; title=&quot;安装流程&quot;&gt;&lt;/a&gt;安装流程&lt;/h2&gt;&lt;h3 id=&quot;安装&quot;&gt;&lt;a href=&quot;#安装&quot; class=&quot;headerlink&quot; title=&quot;安装&quot;&gt;&lt;/a&gt;安装&lt;/h3&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;brew install nginx&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;h3 id=&quot;查看是否安装成功&quot;&gt;&lt;a href=&quot;#查看是否安装成功&quot; class=&quot;headerlink&quot; title=&quot;查看是否安装成功&quot;&gt;&lt;/a&gt;查看是否安装成功&lt;/h3&gt;&lt;p&gt;nginx默认的端口号为8080，默认的启动页面为index.html，初始页面的默认路径为/usr/local/var/www/index.html，在命令行内输入&lt;/p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;nginx&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;这时已启动nginx服务，打开浏览器输入localhose:8080，显示如下页面说明安装成功。&lt;/p&gt;
    
    </summary>
    
    
      <category term="负载均衡" scheme="https://www.xiapf.com/categories/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"/>
    
    
      <category term="负载均衡" scheme="https://www.xiapf.com/tags/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"/>
    
      <category term="nginx" scheme="https://www.xiapf.com/tags/nginx/"/>
    
  </entry>
  
  <entry>
    <title>非监督学习笔记（二）——使用Aprior算法进行关联分析</title>
    <link href="https://www.xiapf.com/blogs/apriori/"/>
    <id>https://www.xiapf.com/blogs/apriori/</id>
    <published>2020-04-09T10:50:13.000Z</published>
    <updated>2020-04-09T10:58:40.508Z</updated>
    
    <content type="html"><![CDATA[<h2 id="算法目的"><a href="#算法目的" class="headerlink" title="算法目的"></a>算法目的</h2><p>从数据中找到各属性之间的隐含关系，Aprior算法就是在数据集中构造出频繁项集，从其中找出关联规则。</p><p>这里的频繁项集是指经常出现在一起的数据属性，通过出现的频率来衡量频繁项集。</p><p>关联规则是指两个集合之间存在的联系，如p-&gt;q就是一种关联规则，关联规则通过置信度衡量，两个集合之间的置信度=p和q并集的支持度/p的支持度。</p><a id="more"></a><p>因此Aprior算法主要目的就是从数据中找到频繁集合，从频繁集合中找出关联规则。用户输入最小支持度得出频繁集合，根据设定的最小置信度得出想要的关联规则</p><h2 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h2><h3 id="构造频繁集"><a href="#构造频繁集" class="headerlink" title="构造频繁集"></a>构造频繁集</h3><p>总体思路：</p><p>从集合元素个数为1开始构造频繁集，非频繁集的单个元素不加入后续构造频繁集中，因为非频繁集的超集也是非频繁集。</p><p>当集合个数大于等于2个时，每次取前k-2个集合进行合并，得出候选集，再根据最小置信度求出频繁集合</p><p>1.构造单个元素的集合</p><p>输入数值为整个数据集合</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.对每一个列表数据</span></span><br><span class="line"><span class="keyword">for</span> tran <span class="keyword">in</span> dataSet:</span><br><span class="line"><span class="comment">#2.对每个单个数据项</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> tran:</span><br><span class="line"><span class="keyword">if</span> [item] <span class="keyword">not</span> <span class="keyword">in</span> C1:</span><br><span class="line">C1.append([item])</span><br></pre></td></tr></table></figure><p>2.计算各个数据集合的出现概率，并把大于最小支持度的集合保存，作为频繁集</p><p>2.1计算各个数据集合的出现概率</p><p>输入候选集合，对每个候选集合判断是否是当前集合的子集，如果是就需要增加其出现的次数，如果是首次出现则设置为1</p><p>注：A.issubset(B):判断a是否是b的子集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.计算所有数据项在原始数据集中出现的次数</span></span><br><span class="line">sSet=&#123;&#125;</span><br><span class="line"><span class="keyword">for</span> tran <span class="keyword">in</span> D:</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> C1:</span><br><span class="line"><span class="comment">#判断item是否是tran的子集</span></span><br><span class="line"><span class="keyword">if</span> item.issubset(tran):</span><br><span class="line"><span class="keyword">if</span> item <span class="keyword">not</span> <span class="keyword">in</span> sSet:<span class="comment">#初始字典中没有这个键</span></span><br><span class="line">sSet[item]=<span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">sSet[item]+=<span class="number">1</span></span><br></pre></td></tr></table></figure><p>2.2根据最小支持度保存频繁集合</p><p>将每个候选集合出现的次数除以集合总数，得出出现的概率，再和最小支持度比较</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#2.计算每个数据项的支持度，并把小于最小支持度的数据项删除</span></span><br><span class="line">numLen=len(D)</span><br><span class="line">retList=[]</span><br><span class="line">supportData=&#123;&#125;</span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> sSet:</span><br><span class="line">support=sSet[key]/numLen</span><br><span class="line"><span class="keyword">if</span>(support&gt;=minSupport):</span><br><span class="line"><span class="comment">#在索引为0的位置之前插入</span></span><br><span class="line">retList.insert(<span class="number">0</span>,key)</span><br><span class="line">supportData[key]=support</span><br></pre></td></tr></table></figure><p>3.apriori算法得出频繁集合</p><p>注：当一个集合不是频繁集，那么他的超集也不是频繁集，以这个原则来减少需要构造的集合数。所以每次从一个元素的集合开始寻找频繁集，非频繁集的元素集合不保存，依次作为初始构造起点，非频繁集的元素自然不会加入到后续筛选多个元素的集合中去。</p><p>3.1 从长度为1的集合开始找到频繁集合</p><p>（1）构造数据 从原始数据集中得出不重复的总的数据集合</p><p>（2）构造初始频繁集合</p><p>得出单个数据集中的频繁集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.构造数据</span></span><br><span class="line">D=list(map(set,dataSet))</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.构造初始数据项</span></span><br><span class="line">C1=createC1(dataSet)</span><br><span class="line">L1,supportData=scanD(D,C1,minSupport)</span><br><span class="line">L=[L1]</span><br></pre></td></tr></table></figure><p>（3）构造元素数多于2个的频繁集合</p><p>令k=2,假设初始频繁集为L=[L1],当L[K-2]即前一个元素数量有频繁集合，那么就可以循环查看当前长度的元素数量是否也有频繁集合（假设前一个元素数量是1，频繁集合中元素数为1），从L[K-2]中构造当前的候选集合，并进行支持度判断</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#3.构造长度为2开始的频繁数据集</span></span><br><span class="line">k=<span class="number">2</span></span><br><span class="line"><span class="keyword">while</span>(len(L[k<span class="number">-2</span>])&gt;<span class="number">0</span>):</span><br><span class="line"><span class="comment">#3.1得到构造的频繁数据集</span></span><br><span class="line">Ck=aprioriGen(L[k<span class="number">-2</span>],k)</span><br><span class="line"><span class="comment">#3.2判断是否是频繁项</span></span><br><span class="line">Lk,Suk=scanD(D,Ck,minSupport)</span><br><span class="line"></span><br><span class="line">L.append(Lk)</span><br><span class="line"><span class="comment">#字典插入/更新数值</span></span><br><span class="line">supportData.update(Suk)</span><br><span class="line">k=k+<span class="number">1</span>;</span><br></pre></td></tr></table></figure><p>3.2 构造长度大于2的频繁集合</p><p>注：这里有个小技巧：取前k-2个集合，当长度相同时合并则为该数量下集合构成的候选集，如k=2时，此时输入的L[k-2]={1},{2},{3},取前k-2个，则是{1},{2},合并为{1,2}，每次i从0开始取值，下一层循环j从i+1开始取值，则得到k=2时的候选集为{1,2},{1,3},{2,3}</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(L)):</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>,len(L)):</span><br><span class="line"><span class="comment">#取前k-2个作为集合，如果相同则合并，即为上一层得出下一层的集合</span></span><br><span class="line">L1=list(L[i])[:k<span class="number">-2</span>]<span class="comment">#取出两个集合前k-1个元素list[:]从开始到末尾位置</span></span><br><span class="line">L2=list(L[j])[:k<span class="number">-2</span>]</span><br><span class="line">L1.sort()</span><br><span class="line">L2.sort()</span><br><span class="line"><span class="keyword">if</span>(L1==L2):</span><br><span class="line">retList.append(L[i]|L[j])</span><br></pre></td></tr></table></figure><h3 id="挖掘关联规则"><a href="#挖掘关联规则" class="headerlink" title="挖掘关联规则"></a>挖掘关联规则</h3><p>根据p的置信度=supprot (p|q) /support (p)求出频繁集的置信度</p><p>总体思路：</p><p>输入的频繁集合，支持度</p><p>（1）单元素的频繁集没有关联规则，所以从L[1]开始遍历</p><p>（2）对L[i]中的每个单项，构造出不重复的单个元素集合组成的列表H</p><p>当只有两个元素时，直接得到关联规则</p><p>当有两个以上的元素时，不断构造2个元素及以上的不重复子集来得出关联规则</p><p>1.构造2个元素的频繁集合的置信度</p><p>输入频繁项集和只有单个元素的列表H</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> freq <span class="keyword">in</span> H:</span><br><span class="line">conf=supportData[freqSet]/supportData[freqSet-freq]</span><br><span class="line"><span class="keyword">if</span>(conf&gt;minConf):</span><br><span class="line">print(freqSet-freq,<span class="string">"-&gt;"</span>,freq,<span class="string">":"</span>,conf)</span><br><span class="line">br1.append((freqSet-freq,freq,conf))</span><br><span class="line">cbr1.append(freq)</span><br></pre></td></tr></table></figure><p>2.构造2个元素以上的频繁集合的置信度</p><p>输入单元素的频繁集合，在此基础上不断根据2个元素以上的频繁集合合并，得出不重复的集合元素，并得出关联规则</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">m=len(H[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">if</span>(len(freqSet)&gt;(m+<span class="number">1</span>)):</span><br><span class="line">hmq=aprioriGen(H,m+<span class="number">1</span>) <span class="comment">#得出2个元素以上的频繁集合</span></span><br><span class="line">hmq=calConf(freqSet,hmq,supportData,br1,minConf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(len(hmq)&gt;<span class="number">1</span>):</span><br><span class="line">ruleFromConf(freqSet,hmq,supportData,br1,minConf)</span><br></pre></td></tr></table></figure><p>3.将单个元素得出的关联规则和多个元素的频繁集得出的关联规则合并</p><p>根据输入的频繁集合，支持度中，L[1]是包含2个元素的频繁集，L[2]是包含3个元素的频繁集</p><p>当时2个元素的频繁集合时，直接根据公式得出置信度，即关联规则，当时多个元素的频繁集合时，需要将子集合并得出关联规则</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,len(L)):</span><br><span class="line"><span class="keyword">for</span> freq <span class="keyword">in</span> L[i]:</span><br><span class="line">H1=[frozenset([item]) <span class="keyword">for</span> item <span class="keyword">in</span> freq]</span><br><span class="line">     <span class="comment">#多个元素的频繁集合</span></span><br><span class="line"><span class="keyword">if</span>(i&gt;<span class="number">1</span>):</span><br><span class="line">ruleFromConf(freq,H1,supportData,bigRulesList,minConf)</span><br><span class="line">     <span class="comment">#单个元素的频繁集合</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">calConf(freq,H1,supportData,bigRulesList,minConf)</span><br></pre></td></tr></table></figure><h2 id="应用实例"><a href="#应用实例" class="headerlink" title="应用实例"></a>应用实例</h2><p>问题：发现毒蘑菇的相似特征</p><p>数据集：原始数据集在UCI上，使用关联规则需要把样本数据转换为特征集合，有人做了解析，直接使用该数据集</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200409183650.png" alt="毒蘑菇数据"></p><p>第一列1代表无毒，2代表有毒。</p><p>利用apriori算法得出频繁集合，查看有毒特征一般和哪些特征一起出现，这样就可以通过其他特征分辨出毒蘑菇。</p><p>根据得出的频繁项集，找到长度为2的频繁项集中有毒蘑菇特征的集合为</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200409184446.png" alt="毒蘑菇频繁集"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">data=apriori.loadMushroomData(<span class="string">"mushroom.dat"</span>)</span><br><span class="line">L,supportData=apriori.apriori(data,<span class="number">0.3</span>)</span><br><span class="line">print(L)</span><br><span class="line">print(supportData)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> L[<span class="number">1</span>]:</span><br><span class="line"><span class="keyword">if</span> &#123;<span class="number">2</span>&#125;.issubset(item):</span><br><span class="line">print(item)</span><br></pre></td></tr></table></figure><p>也可以选择其他长度的频繁项集查看</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;算法目的&quot;&gt;&lt;a href=&quot;#算法目的&quot; class=&quot;headerlink&quot; title=&quot;算法目的&quot;&gt;&lt;/a&gt;算法目的&lt;/h2&gt;&lt;p&gt;从数据中找到各属性之间的隐含关系，Aprior算法就是在数据集中构造出频繁项集，从其中找出关联规则。&lt;/p&gt;&lt;p&gt;这里的频繁项集是指经常出现在一起的数据属性，通过出现的频率来衡量频繁项集。&lt;/p&gt;&lt;p&gt;关联规则是指两个集合之间存在的联系，如p-&amp;gt;q就是一种关联规则，关联规则通过置信度衡量，两个集合之间的置信度=p和q并集的支持度/p的支持度。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="关联分析" scheme="https://www.xiapf.com/tags/%E5%85%B3%E8%81%94%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>监督学习笔记（四）——利用回归预测数值型数值</title>
    <link href="https://www.xiapf.com/blogs/regression/"/>
    <id>https://www.xiapf.com/blogs/regression/</id>
    <published>2020-04-02T10:44:05.000Z</published>
    <updated>2020-08-06T15:03:31.498Z</updated>
    
    <content type="html"><![CDATA[<h2 id="算法作用"><a href="#算法作用" class="headerlink" title="算法作用"></a>算法作用</h2><h3 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h3><p>之前的监督学习方法如：贝叶斯分类、logistic回归、svm支持向量机都是将数据点分成对应类，这里是利用回归对连续型数值预测，即给出一个数据能预测其输出。</p><p>这里主要讨论线性回归，通过找到线性回归中的系数构造回归方程来预测数值。算法核心：求回归系数。</p><h3 id="“回归”的含义"><a href="#“回归”的含义" class="headerlink" title="“回归”的含义"></a>“回归”的含义</h3><p>回归是用来预测目标值的输出，例如给定一个x，它满足方程y=a*x+b，那么我们就能通过公式得出x的输出。用回归来预测也类似于这个过程，上面提到的方程是回归中的回顾方程，主要是需要确定回归系数a、b，这里记为w，将所有特征乘以回归系数即可以得到预测值。</p><a id="more"></a><h2 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h2><p>回归的目的是为确定回归系数w，即找到误差最小的w，误差是指实际值y减去预测值yhat，为了使得误差中的正值和负值抵消掉，一般采用平方误差来找到最优w，平方误差定义为：∑（y-w * x)^2，对该式子求导化简可得w=(x^T * x)^-1 * x^T * y，因此可以通过输入的x和y的值求得w，这样就能建立出回归方程即学习模型。</p><p>w的等式中需要对x^T * x求逆矩阵，因此在计算之前需要判断该矩阵是否可逆，可以通过行列式的值判断。</p><h3 id="标准线性回归"><a href="#标准线性回归" class="headerlink" title="标准线性回归"></a>标准线性回归</h3><p>（1）应用场景及原理</p><p>应用场景：几乎所有数据都可以用线性回归来拟合数据</p><p>原理：使用w=(x^T * x)^-1 * x^T * y求解回归系数</p><p>（2）代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">standRegres</span><span class="params">(x,y)</span>:</span></span><br><span class="line">  xMat=mat(x)</span><br><span class="line">  yMat=mat(y).transpose()</span><br><span class="line">  m,n=shape(xMat)</span><br><span class="line">  xTx=xMat.T*xMat</span><br><span class="line">  </span><br><span class="line">  <span class="comment">#判断是否可逆</span></span><br><span class="line">  <span class="keyword">if</span>(linalg.det(xTx)==<span class="number">0</span>):</span><br><span class="line">    print(<span class="string">"行列式不可逆"</span>)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    inv=linalg.inv(xTx)</span><br><span class="line">    ws=inv*xMat.T*yMat</span><br><span class="line">    <span class="keyword">return</span> ws</span><br></pre></td></tr></table></figure><p>导入如图所示的数据</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200401155534.png" alt="数据"></p><p>数据集中前两列为输入的特征，最后一列是预测值，其中第0列是手动设置为1，为了得出回归方程中的常数项，使用xMat*ws即可得到预测值，在图像中对第一列数据画出拟合的直线：plot(xMat[:,1],yhat)</p><p>结果如图：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200401161414.png" alt="线性回归"></p><p>判断模型的好坏可以通过相关系数，使用corrcoef(yMat,yHat)，可得</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200401161238.png" alt="相关系数"></p><p>可以看出模型大致贴合数据。</p><h3 id="局部加权线性回归"><a href="#局部加权线性回归" class="headerlink" title="局部加权线性回归"></a>局部加权线性回归</h3><p>（1）应用场景及原理</p><p>应用场景：由标准线性回归效果可以看出模型存在欠拟合，数据点和直线只能大致匹配，因此需要使用局部加权线性回归，对每个数据点引入权重，通过核函数对周围的点赋权重w(i,i)=exp(|x(i)-x)|/-2k^2，这里的k是由用户各处需要给周围的点赋予多大的权重。</p><p>原理：回归系数w=(x^T * w * x)^-1 * w *x^T * y</p><p>（2）代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#局部加权线性回归</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lwlr</span><span class="params">(testPoint,x,y,k)</span>:</span></span><br><span class="line">  xMat=mat(x)</span><br><span class="line">  yMat=mat(y).transpose()</span><br><span class="line">  m,n=shape(xMat)</span><br><span class="line">  <span class="comment">#创建权重对角矩阵</span></span><br><span class="line">  w=mat(eye(m))</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> m:</span><br><span class="line">    diff=testPoint-xMat[i,:]</span><br><span class="line">    <span class="comment">#通过高斯核赋予权重，越靠近点(i，i),权重越大</span></span><br><span class="line">    w[i,i]=exp(diff*diff.T)/(<span class="number">-2</span>*k**<span class="number">2</span>)</span><br><span class="line">  xTx=xMat.T*w*xMat</span><br><span class="line">  </span><br><span class="line">  <span class="comment">#判断是否可逆</span></span><br><span class="line">  <span class="keyword">if</span>(linalg.det(xTx)==<span class="number">0</span>):</span><br><span class="line">    print(<span class="string">"行列式不可逆"</span>)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    inv=linalg.inv(xTx)</span><br><span class="line">    ws=inv*w*xMat.T*yMat</span><br><span class="line">    <span class="keyword">return</span> testPoint*ws <span class="comment">#返回对该点的预测值，遍历数据集xmat，则可以得到所有数的预测输出</span></span><br></pre></td></tr></table></figure><p>画图时需要将数据重新排序</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sid=argsort(xmat[:,<span class="number">1</span>],axis=<span class="number">0</span>)<span class="comment">#第一列数据按序排列获得位置</span></span><br><span class="line">xcopy=xmat[sid][:,<span class="number">0</span>,:]<span class="comment">#取第一列和第二列</span></span><br><span class="line">plot(xcopy[:,<span class="number">1</span>],yhat[sid])</span><br></pre></td></tr></table></figure><p>运行效果如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200401170052.png" alt="局部回归"></p><p>对uci上鲍鱼年龄进行测试，当使用不同的参数运行可得，当参数越大，在测试集上误差越大，但是当参数越大，在训练集上误差越小，因此需要根据测试集误差选择合适模型。</p><p>这里参数选择0.1、1、10：<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200401170423.png" alt="不同参数"></p><h3 id="缩减系数"><a href="#缩减系数" class="headerlink" title="缩减系数"></a>缩减系数</h3><p>以下两种算法属于缩减系数，即通过将一些不重要的特征缩减至0，来降低模型复杂度，减小误差。因为当模型越复杂，误差就会越大。缩减系数也需要根据具体数据集选择误差小的模型。</p><h3 id="岭回归"><a href="#岭回归" class="headerlink" title="岭回归"></a>岭回归</h3><p>（1）应用场景及原理</p><p>应用场景：当数据特征多余数据点个数时，求解w的公式中xtx会是奇异矩阵；或者当出现数据点特征过多，需要缩减特征的时候也可以使用岭回归。岭回归因为引入了对角矩阵I，其中对角矩阵为n*n矩阵，其中n=数据集中特征的数量，对角线上均是1，形似岭，所以称为岭回归。当引入单位矩阵I之后，可以保证矩阵可逆。</p><p>原理：在单位矩阵前需要乘以一个系数λ，这里的系数需要用户给出。</p><p>回归系数w=(x^T * x+λI)^-1  *x^T * y</p><p>（2）代码</p><p>整体过程类似于标准线性回归，虽然加入了对角矩阵也需要判断x^T * x+λI是否可逆，因为给出的系数可能为0，因为根据输入的λ不同，ws的结果会不同，因此，测试时可以输入30组λ，根据λ取值选择误差最小的回归系数ws。</p><p>（3）注意点——数据标准化与还原</p><p>因为岭回归可以用来缩减数据特征，当标准化之后所有数据特征同等重要，每个数据特征在最终回归方程中的大小会不同，小的数据说明该对应特征相比于其他更不重要，如果需要丢弃特征，可以从小的数据开始。</p><p>所以，岭回归中需要将所有数据进行标准化，最终求得ws之后，需要进行数据还原才能得出最终的回归矩阵。</p><p>用标准化数据进行模型建立：x=(x-xmean)/xvar，y=y-ymean</p><p>当得出ws时，回归方程应为y=((x-xmean) / xvar) * ws+ymean，其中(x-xmean) /xvar * ws是使用标准化后的x得出预测y值（标准化后的值），加上ymean就是实际值。</p><p>（4）代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">standRidge</span><span class="params">(xMat,yMat,lam)</span>:</span></span><br><span class="line">  <span class="comment">#xMat=mat(x)</span></span><br><span class="line">  <span class="comment">#yMat=mat(y).transpose()</span></span><br><span class="line">  m,n=shape(xMat)</span><br><span class="line">  xTx=xMat.T*xMat+lam*eye(n)</span><br><span class="line">  </span><br><span class="line">  <span class="comment">#判断是否可逆</span></span><br><span class="line">  <span class="keyword">if</span>(linalg.det(xTx)==<span class="number">0</span>):</span><br><span class="line">    print(<span class="string">"行列式不可逆"</span>)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    inv=linalg.inv(xTx)</span><br><span class="line">    ws=inv*xMat.T*yMat</span><br><span class="line">    <span class="keyword">return</span> ws</span><br><span class="line">  </span><br><span class="line"><span class="comment">#得到30个系数中ws的变化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ridgeTest</span><span class="params">(x,y)</span>:</span></span><br><span class="line">  numIter=<span class="number">30</span></span><br><span class="line">  xMat=mat(x)</span><br><span class="line">  yMat=mat(y).transpose()</span><br><span class="line">  m,n=shape(xMat)</span><br><span class="line">  xMean=mean(xMat,axis=<span class="number">0</span>)</span><br><span class="line">  xVar=var(xMat,axis=<span class="number">0</span>)</span><br><span class="line">  yMean=mean(yMat,axis=<span class="number">0</span>)</span><br><span class="line">  xTest=(xMat-xMean)/xVar</span><br><span class="line">  yTest=yMat-yMean</span><br><span class="line">  <span class="comment">#将30次系数保存下来</span></span><br><span class="line">  returnMat=zeros((numIter,n))</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(numIter):</span><br><span class="line">    ws=standRidge(xTest,yTest,exp(i<span class="number">-10</span>))</span><br><span class="line">    returnMat[i,:]=ws</span><br><span class="line">   <span class="keyword">return</span> returnMat</span><br></pre></td></tr></table></figure><p>最终得出logλ和回归系数之间的关系</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200402165626.png" alt="logλ和回归系数关系图"></p><p>实际应用中可以通过对wsMat矩阵求预测值，根据预测值-真实值的平方，再求和得出平法误差，找到使得平法误差最小的回归系数矩阵对应的λ则为最佳系数。</p><h3 id="前向逐步回归"><a href="#前向逐步回归" class="headerlink" title="前向逐步回归"></a>前向逐步回归</h3><p>（1）应用场景及原理</p><p>应用场景：该算法属于贪心算法，每次都选择正方向和负方向变化很小的步长，来计算回归系数，每次选择误差最小的回归系数，前向逐步回归也用于缩减特征，当不重要的特征在迭代次数很小的时候不会对数据产生影响即对于特征向量在回归方程中系数等于0，因为需要缩减特征，观察特征的重要性，数据也需要进行标准化。</p><p>（2）代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">standWise</span><span class="params">(x,y,eps=<span class="number">0.001</span>,numIter=<span class="number">300</span>)</span>:</span></span><br><span class="line">  xMat=mat(x)</span><br><span class="line">  yMat=mat(y).transpose()</span><br><span class="line">  m,n=shape(xMat)</span><br><span class="line">  xMean=mean(xMat,axis=<span class="number">0</span>)</span><br><span class="line">  xVar=var(xMat,axis=<span class="number">0</span>)</span><br><span class="line">  yMean=mean(yMat,axis=<span class="number">0</span>)</span><br><span class="line">  xMat=(xMat-xMean)/xVar</span><br><span class="line">  yMat=yMat-yMean</span><br><span class="line">  <span class="comment">#回归系数初始化</span></span><br><span class="line">  ws=zeros((n,<span class="number">1</span>))</span><br><span class="line">  wsMax=ws.copy()</span><br><span class="line">  returnMat=zeros((numIter))</span><br><span class="line">  <span class="comment">#在迭代次数中进行移动ws方向</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(numIter):</span><br><span class="line">    lossErr=inf</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(m):</span><br><span class="line">      <span class="keyword">for</span> sign <span class="keyword">in</span> [<span class="number">-1</span>,<span class="number">1</span>]:</span><br><span class="line">        wsTest=ws.copy()</span><br><span class="line">        wsTest[j]+=eps*sign</span><br><span class="line">        yHat=xMat*ws</span><br><span class="line">        err=rssRrr(yMat.A,yHat.A)</span><br><span class="line">        <span class="keyword">if</span>(err&lt;lossErr):</span><br><span class="line">          lossErr=err</span><br><span class="line">          wsMax=ws</span><br><span class="line">      ws=wsMax</span><br><span class="line">      returnMat[i,:]=ws.T</span><br><span class="line">    <span class="keyword">return</span> returnMat</span><br></pre></td></tr></table></figure><p>得出在鲍鱼数据集上，步长0.001，迭代300次后回归系数和迭代次数的效果图：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200402173125.png" alt="回归系数和迭代次数关系图"></p><h3 id="衡量误差"><a href="#衡量误差" class="headerlink" title="衡量误差"></a>衡量误差</h3><p>判断一个模型的好坏，需要通过预测值和真实值之间的差值进行计算，为了抵消正负值，这里采用平方和形式，假设真实值为ymat，预测值为yhat，则误差error=sum(ymat-yhat)**2，这里需要注意是对应位置相加，所以真实值和预测值都需要转换为数组形式进行计算。</p><p>ymat和yhat需要格式一致，假设ymat为矩阵，则二者需要转换为ymat.A，yhat.A；如果ymat是列表，则则二者需要转换为array(ymat)，yhat.T.A</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rssRrr</span><span class="params">(ymat,yhat)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> sum((ymat-yhat)**<span class="number">2</span>)<span class="comment">#ymat和yhat都需要转换为数组形式进行计算</span></span><br></pre></td></tr></table></figure><h2 id="应用实例"><a href="#应用实例" class="headerlink" title="应用实例"></a>应用实例</h2><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>预测乐高玩具的价格：根据给出的乐高销售网页中不同年份的乐高套装的价格建立回归模型，从而预测玩具价格。</p><h3 id="实现结果"><a href="#实现结果" class="headerlink" title="实现结果"></a>实现结果</h3><p>（1）网页数据获取</p><p>导入bs4中的BeautifulSoup，该模块可以用来获取网页中的数据，主要是读取html文件，find_all可以得出各个属性模块中的值，因为乐高销售网页中的玩具都是放在table中，可以读取每个table，获取玩具的标题，是否全新，是否售出（表格第三列），售出的价格（表格第4列）等。将玩具的年份、套装数量、是否全新、价格存到列表中，作为模型数据集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">获取数据</span><br><span class="line"><span class="comment">#从页面读取数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scrapePage</span><span class="params">(retX,retY,fileName,yr,numPce,OriPrice)</span>:</span></span><br><span class="line"><span class="comment">#1.读取网页</span></span><br><span class="line">fr=open(fileName)</span><br><span class="line"><span class="comment">#2.用beautifulSoup处理,把读取的网页转换为数据</span></span><br><span class="line">soup=BeautifulSoup(fr)</span><br><span class="line">i=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#soup数据全部以列表形式返回</span></span><br><span class="line"><span class="comment">#3.读取table的长度，当还有table即还有商品的时候循环</span></span><br><span class="line">tableCur=soup.find_all(<span class="string">'table'</span>,r=i)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span>(len(tableCur)&gt;<span class="number">0</span>):</span><br><span class="line"><span class="comment">#读取商品每次，根据有无售出来获取价格</span></span><br><span class="line">title=tableCur[<span class="number">0</span>].find_all(<span class="string">'a'</span>)[<span class="number">1</span>].text.lower()<span class="comment">#a标签中第一个有名称,获取文本并转换为小写</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#3.1判断是否全新</span></span><br><span class="line"><span class="keyword">if</span>(title.find(<span class="string">'new'</span>)!=<span class="number">-1</span>) <span class="keyword">or</span> (title.find(<span class="string">'nstd'</span>)!=<span class="number">-1</span>):</span><br><span class="line">newFlag=<span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">newFlag=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#3.2判断是否卖出</span></span><br><span class="line"><span class="comment">#读取td</span></span><br><span class="line">sellFlag=tableCur[<span class="number">0</span>].find_all(<span class="string">'td'</span>)[<span class="number">3</span>].find_all(<span class="string">"span"</span>)</span><br><span class="line"><span class="keyword">if</span>(len(sellFlag)==<span class="number">0</span>):<span class="comment">#此时没有卖出</span></span><br><span class="line">print(<span class="string">"the item"</span>+str(i)+<span class="string">" do not sell out"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">soldPrice=tableCur[<span class="number">0</span>].find_all(<span class="string">'td'</span>)[<span class="number">4</span>]</span><br><span class="line">price=soldPrice.text</span><br><span class="line">price=price.replace(<span class="string">'$'</span>,<span class="string">''</span>)</span><br><span class="line">price=price.replace(<span class="string">','</span>,<span class="string">''</span>)</span><br><span class="line"><span class="keyword">if</span>(len(soldPrice)&gt;<span class="number">1</span>):</span><br><span class="line">price=price.replace(<span class="string">'Free shipping'</span>,<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line">sellPrice=float(price)</span><br><span class="line"></span><br><span class="line"><span class="comment">#3.3去掉不完整的价格</span></span><br><span class="line"><span class="keyword">if</span>(sellPrice&gt;OriPrice*<span class="number">0.5</span>):</span><br><span class="line">retX.append([yr,numPce,newFlag,OriPrice])</span><br><span class="line"><span class="comment">#print(str(price)+"\t"+str(newFlag)+"\t"+title)</span></span><br><span class="line">retY.append(sellPrice)</span><br><span class="line"></span><br><span class="line">i=i+<span class="number">1</span></span><br><span class="line">tableCur=soup.find_all(<span class="string">'table'</span>,r=i)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> retX,retY</span><br></pre></td></tr></table></figure><p>（2）算法选择</p><p>可以采用十折交叉验证：可以建立随机数据randomList=list(range(m))，使用random.shuffle打乱序列中的元素，90%作为训练集，10%作为测试集。</p><p>a）标准线性回归</p><p>为了得出回归方程中的常数项，需要在原有数据上插入常数1，新建一个全为1的矩阵，除了第一列之外其余列用原有数据填充。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m,n=shape(lgX)</span><br><span class="line">retX=mat(ones((m,n+<span class="number">1</span>)))</span><br><span class="line">retX[:,<span class="number">1</span>:<span class="number">5</span>]=mat(lgX)</span><br></pre></td></tr></table></figure><p>根据standRegres得出回归系数，回归系数每一列对应方程中的系数值，以乐高数据中4列数据为例，y=ws[0]+ws[1] * 年份+ws[1] * 套装数量+ws[2] * 是否全新+ws[3] * 价格，得出方程为：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200402180915.png" alt="标准线性回归方程"></p><p>可以看出标准线性回归得出的回归系数有些不合理的地方，当部件数量多，价格会更低，因此选择岭回归查看效果。</p><p>b）岭回归</p><p>因为岭回归中含有参数，根据测试集得出误差最小的参数，对误差集求平均，得出最小的误差所在的行即为最佳回归系数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#最小的误差下的回归系数</span></span><br><span class="line">meanError=mean(testErr,axis=<span class="number">0</span>)</span><br><span class="line">minErr=min(meanError)</span><br><span class="line">minWs=wsMat[nonzero(meanError=minErr)]</span><br></pre></td></tr></table></figure><p>得出回归系数后需要得出回归方程，因为是用标准化数据进行模型建立：x=(x-xmean)/xvar，y=y-ymean。</p><p>当得出ws时，回归方程应为y=((x-xmean) / xvar) * ws+ymean，其中(x-xmean) /xvar * ws是使用标准化后的x得出预测y值（标准化后的值），加上ymean就是实际值。</p><p>所有回归方程可以写成：y=x * ws/xvar -(xmean/xvar) * ws+ymean，后面-(xmean/xvar) * ws+ymean是方程得常数项。记ureg=ws/xvar，以乐高数据中4列数据为例，y=常数项+ureg[0,0] * 年份+ureg[0,1] * 套装数量+ureg[0,2] * 是否全新+ureg[0,3] * 价格，得出方程为：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200402182822.png" alt="岭回归"></p><p>因为测试集随机选取，每次结果略有不同。可以看出岭回归的方程比标准回归方程拟合效果要好，更符合实际。</p><p>这是ws缩减系数中第一次得出的回归系数：</p><blockquote><p>[[-1.07635546e+02]<br> [-1.54119215e+04]<br> [-1.42998055e+01]<br> [ 4.32941501e+04]]</p></blockquote><p>可以看出第4个系数最大，说明其最重要，当只能选择一个特征时，可以选择第4个特征，如果要选择两个特征，可以选择第2个和第4个特征。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;算法作用&quot;&gt;&lt;a href=&quot;#算法作用&quot; class=&quot;headerlink&quot; title=&quot;算法作用&quot;&gt;&lt;/a&gt;算法作用&lt;/h2&gt;&lt;h3 id=&quot;应用场景&quot;&gt;&lt;a href=&quot;#应用场景&quot; class=&quot;headerlink&quot; title=&quot;应用场景&quot;&gt;&lt;/a&gt;应用场景&lt;/h3&gt;&lt;p&gt;之前的监督学习方法如：贝叶斯分类、logistic回归、svm支持向量机都是将数据点分成对应类，这里是利用回归对连续型数值预测，即给出一个数据能预测其输出。&lt;/p&gt;&lt;p&gt;这里主要讨论线性回归，通过找到线性回归中的系数构造回归方程来预测数值。算法核心：求回归系数。&lt;/p&gt;&lt;h3 id=&quot;“回归”的含义&quot;&gt;&lt;a href=&quot;#“回归”的含义&quot; class=&quot;headerlink&quot; title=&quot;“回归”的含义&quot;&gt;&lt;/a&gt;“回归”的含义&lt;/h3&gt;&lt;p&gt;回归是用来预测目标值的输出，例如给定一个x，它满足方程y=a*x+b，那么我们就能通过公式得出x的输出。用回归来预测也类似于这个过程，上面提到的方程是回归中的回顾方程，主要是需要确定回归系数a、b，这里记为w，将所有特征乘以回归系数即可以得到预测值。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="回归" scheme="https://www.xiapf.com/tags/%E5%9B%9E%E5%BD%92/"/>
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>监督学习笔记（三）——logistic回归</title>
    <link href="https://www.xiapf.com/blogs/logistic/"/>
    <id>https://www.xiapf.com/blogs/logistic/</id>
    <published>2020-03-24T09:43:06.000Z</published>
    <updated>2020-08-06T15:03:04.282Z</updated>
    
    <content type="html"><![CDATA[<h2 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h2><h3 id="原理分析"><a href="#原理分析" class="headerlink" title="原理分析"></a>原理分析</h3><p>（1）数据拟合</p><p>给出一些数据，用一条直线来拟合这些数据称为logictic回归</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200324153824.png" alt="直线拟合"></p><p>用logistic回归来进行数据分类是通过给出的所有数据点拟合出分类边界的直线，确定这条直线的系数就称为回归系数，当用回归系数乘以当前特征则得出该条直线：</p><p>z=w0 * x0+w1 * x1+…+wn * xn，其中w0 - wn是回归系数，x0 - xn是各个特征，得出的z是当前特征下的预测值。</p><a id="more"></a><p>为了表示直线上下的浮动，需要将x0设置为1，这时的w0就相当于直线方程中的偏置b。</p><p>（2）分类函数</p><p>当logistic回归用作二值分类器的时候，可以用sigmoid函数进行类别判定，sigmoid（x)=1/1+e-x，图像如下</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200324154722.png" alt="sigmoid"></p><p>当z&gt;0.5时，函数值趋近与1，当z&lt;0.5时，函数趋近于0。可以利用这个特性，对将每个特征乘以回归系数得到的输出类别进行判定。</p><p>因此logistic回归就是要求最佳的回归系数，这是求解最优问题，可以使用梯度上升的方法。</p><p>（3）寻求最优</p><p>梯度上升算法是通过每次都向梯度变化最佳的的地方走，最终找到最优值，迭代公式为：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200324155903.png" alt="迭代公式"></p><p>其中w是回归系数，alpha是每次沿着梯度方向变化的步长，f(w)是梯度变化函数，即损失函数。这里明确一点，求最佳的回归系数的过程中沿着梯度变化最佳的方向变化，这里的最佳可以理解为是误差最小，也就是w0 * x0+w1 * x1+…+wn * xn得出的预测值f和实际值之间的误差最小，那么就是最佳的方向，即对所有特征来说 f(w)=∑（y-f)xi，这是对公式的直观理解，<a href="https://blog.csdn.net/CharlieLincy/article/details/70767791" target="_blank" rel="external nofollow noopener noreferrer">这里也有对人做了对整个过程的详细推导</a>。到这里为止，logistic回归的核心思想很明确了，就是在一个循环中，对所有的向量乘以当前的回归系数，根据得出的误差调整回归系数，不断循环，直至回归系数稳定。</p><h3 id="和SVM的区别"><a href="#和SVM的区别" class="headerlink" title="和SVM的区别"></a>和SVM的区别</h3><p>相同点：二者都是分类器</p><p>不同点：</p><p>logistic回归用到了所有数据点，svm只用了靠近分类平面的点即支持向量。</p><p>logistic回归利用极大似然估计的思想求解参数，svm通过最大化几何间隔求得最优平面</p><h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><h3 id="梯度上升"><a href="#梯度上升" class="headerlink" title="梯度上升"></a>梯度上升</h3><p>批处理的方式，将所有数据点一次输入处理</p><p>（1）初始化回归系数，以及每次向梯度方向变化的步长</p><p>根据每个特征的个数，将回归系数初始化为1，假设有3个特征，则回归系数被初始化为3*1的列向量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m,n=shape(dataMat)</span><br><span class="line">weights=ones((n,<span class="number">1</span>))</span><br><span class="line">alpha=<span class="number">0.01</span></span><br></pre></td></tr></table></figure><p>（2）不断迭代，直至回归系数稳定</p><p>一般来说，回归系数稳定很难判定，因此通过控制迭代次数来修正回归系数</p><p>这里的dataMat格式为<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200324163550.png" alt="dataMat">每个数据点都有三个特征</p><p>weight的格式为<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200324164025.png" alt="weights">dataMat和weight两者相乘，拿第一行来说得到x01w0+x11w1+x21*w0，每行都得出这个值，将每行的结果通过sigmoid函数预测即可得到预测得值，用实际值减去该值，则为误差值。因为这个结果时100 *1 的矩阵，所以实际标签矩阵也应该是100 * 1的矩阵，因此，输入的labelMat需要进行转置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#对输入的数据进行处理</span></span><br><span class="line">labelMat=labelMat.transpose()</span><br><span class="line">numIter=<span class="number">500</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(numIter):</span><br><span class="line">  h=sigmoid(dataMat*weight)</span><br><span class="line">  error=y-h</span><br><span class="line">  weights=weights+alpha*dataMat.transpose()*error <span class="comment">#这里error是100 *1 的矩阵，矩阵乘法要求行列相同，因此x应该转换为3*100的矩阵进行计算</span></span><br></pre></td></tr></table></figure><h3 id="随机梯度上升"><a href="#随机梯度上升" class="headerlink" title="随机梯度上升"></a>随机梯度上升</h3><p>上面的批处理方式的梯度上升方法每次都要遍历整个数据集，当数据量大的时候就比较费时，因此引入一种在线学习方式的梯度上升，每次随机选择一个样本对回归系数进行修正</p><p>（1）初始化回归系数</p><p>因为每次是对一个数据点处理，因此，只需要初始化一个长度为n的数组，n是数据集中数据特征的个数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">m,n=shape(dataMat)</span><br><span class="line">weights=ones(n)</span><br></pre></td></tr></table></figure><p>（2）每次的步长都更新</p><p>当刚开始的时候离最优值很远，可以步长大点，快速向最优值逼近，当到后面的时候，需要减小步长，慢慢逼近，不然可能会错过最优值。</p><p>j控制迭代次数，i控制每个样本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alpha=<span class="number">4</span>/(<span class="number">1</span>+i+j)+<span class="number">0.01</span></span><br></pre></td></tr></table></figure><p>（3）随机选择样本修正回归系数</p><p>设置一个dataIndex里面存储0 - m-1的数字，每次通过random.uniform生成一个在0 - len(dataIndex)之间的随机数，代表此次选择这行的样本进行回归系数的修正，每次将该次选择的随机数在dataIndex中删除，防止下次重复选择</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#为了实现对应位置相乘dataMat需要转换为数组</span></span><br><span class="line">dataArr=array(dataMat)</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(numIter):</span><br><span class="line">  dataIndex=list(range(m))</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">    alpha=<span class="number">4</span>/(<span class="number">1</span>+i+j)+<span class="number">0.01</span></span><br><span class="line">    </span><br><span class="line">    randomIndex=int(random.uniform(<span class="number">0</span>,len(dataIndex)))</span><br><span class="line">    chooseIndex=dataIndex[randomIndex]</span><br><span class="line">    h=sigmoid(dataArr[chooseIndex]*weights) <span class="comment">#datamat[i]是1*3数组，weights是1*3数组，对应位置相乘</span></span><br><span class="line">    error=labelMat[chooseIndex]-h</span><br><span class="line">    weights=weights+alpha*error*dataMat[]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">del</span>(dataIndex[randomIndex])</span><br></pre></td></tr></table></figure><p>该方法不需要对矩阵进行转置，运算简洁，同时每次迭代就修改回归系数m次，而批处理的方法每次只能修改一次，这种方法只需要迭代很少的次数，回归系数就趋于稳定了。</p><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>因为数据集本身只有两列，即两个数据特征，因此x0设置为1，w0作为偏置，最终得出的拟和直线为y=w0+w1x1+w2x2，因为使用的是sigmoid函数，分割的直线的y值应该是0，因为sigmoid(0)=0.5，直线转换为w0+w1x1+w2x2=0 =&gt; x2=-w0-w1x1/w2，用plt.plot(x,x2)，这里的x可以在x的取值范围内，取一定步长进行显示（利用arange），可以将图像画出来，如下图</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200324170500.png" alt="效果图"></p><h2 id="应用实例"><a href="#应用实例" class="headerlink" title="应用实例"></a>应用实例</h2><p>问题：用logistic回归预测得疝气病的病马死亡率</p><h3 id="数据集描述"><a href="#数据集描述" class="headerlink" title="数据集描述"></a>数据集描述</h3><p>数据集是从UCI上下载的，里面包含22列属性，其中前21列是马的一些生理特征，最后一列是是否存活，属于类别标签。该案例是从前21列的特征中的出回归系数，得到拟合的回归直线，对马是否存活进行分类。</p><p>该数据集中部分数据（约30%）丢失，对特征丢失的情况进行处理：由于logistic回归中z=w0x0+w1x1+…，其中当特征等于0的时候不影响等式的结果，同时因为使用的判定类别的函数为sigmoid函数，当取值为0，即simoid(0)=0.5，不影响分类，所有丢失的特质均设置为0；对类别标签丢失的情况进行处理：当类别标签丢失的时候无法判断马的存活情况，因此将该条数据丢弃。</p><h3 id="应用描述"><a href="#应用描述" class="headerlink" title="应用描述"></a>应用描述</h3><p>（1）类别判定</p><p>对输入的向量乘以回归系数求和，如果大于0.5，则分类为1，反之分类为0</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">prob=sigmoid(sum(inX*weights))</span><br><span class="line"><span class="keyword">if</span> prob&gt;<span class="number">0.5</span>:</span><br><span class="line">  <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure><p>（2）建立logistic回归模型并进行测试</p><p>训练集建立模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">trainingFile=open(<span class="string">"horseColicTraining.txt"</span>)</span><br><span class="line">trainingMat=[]</span><br><span class="line">trainingLabels=[]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> curLine <span class="keyword">in</span> trainingFile.readlines():</span><br><span class="line">pstLine=curLine.strip().split(<span class="string">"\t"</span>)</span><br><span class="line">dataList=[]</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">21</span>):</span><br><span class="line">dataList.append(float(pstLine[j]))</span><br><span class="line">trainingMat.append(dataList)</span><br><span class="line">trainingLabels.append(float(pstLine[<span class="number">21</span>]))</span><br><span class="line"></span><br><span class="line">weights=stoGradAscent1(trainingMat,trainingLabels,<span class="number">500</span>)</span><br></pre></td></tr></table></figure><p>测试次测试错误率：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">testFile=open(<span class="string">"horseColicTest.txt"</span>)</span><br><span class="line"></span><br><span class="line">testMat=[]</span><br><span class="line">testLabels=[]</span><br><span class="line"><span class="keyword">for</span> curLine <span class="keyword">in</span> testFile.readlines():</span><br><span class="line">pstLine=curLine.strip().split(<span class="string">"\t"</span>)</span><br><span class="line">dataList=[]</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">21</span>):</span><br><span class="line">dataList.append(float(pstLine[j]))</span><br><span class="line">testMat.append(dataList)</span><br><span class="line">testLabels.append(float(pstLine[<span class="number">21</span>]))</span><br><span class="line"></span><br><span class="line">error=<span class="number">0</span></span><br><span class="line">n=shape(testMat)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">predictData=classfyVec(testMat[i],weights)</span><br><span class="line"><span class="keyword">if</span>(predictData!=testLabels[i]):</span><br><span class="line">error+=<span class="number">1</span></span><br><span class="line">errorRate=float(error)/n</span><br><span class="line">print(<span class="string">"error rate is:"</span>+str(errorRate))</span><br></pre></td></tr></table></figure><p>（3）取平均数</p><p>一次数据可能存在偶然性，因此运行10次取平均值，作为该模型的分类正确率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">numTest=<span class="number">10</span></span><br><span class="line">errorCount=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(numTest):</span><br><span class="line">errorCount+=colicTest()</span><br><span class="line"></span><br><span class="line">allError=float(errorCount/<span class="number">10</span>)</span><br><span class="line">print(<span class="string">"the average of ten times error is:"</span>)</span><br><span class="line">print(allError)</span><br></pre></td></tr></table></figure><p>最终运行10次的结果为：</p><blockquote><p>error rate is:0.6268656716417911<br>error rate is:0.3582089552238806<br>error rate is:0.26865671641791045<br>error rate is:0.2537313432835821<br>error rate is:0.44776119402985076<br>error rate is:0.29850746268656714<br>error rate is:0.47761194029850745<br>error rate is:0.26865671641791045<br>error rate is:0.26865671641791045<br>error rate is:0.5522388059701493<br>the average of ten times error is:<br>25.6</p></blockquote><p>平均错误率25%左右，因为数据集有30%的数据丢失，因此错误率相对较高。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;算法原理&quot;&gt;&lt;a href=&quot;#算法原理&quot; class=&quot;headerlink&quot; title=&quot;算法原理&quot;&gt;&lt;/a&gt;算法原理&lt;/h2&gt;&lt;h3 id=&quot;原理分析&quot;&gt;&lt;a href=&quot;#原理分析&quot; class=&quot;headerlink&quot; title=&quot;原理分析&quot;&gt;&lt;/a&gt;原理分析&lt;/h3&gt;&lt;p&gt;（1）数据拟合&lt;/p&gt;&lt;p&gt;给出一些数据，用一条直线来拟合这些数据称为logictic回归&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200324153824.png&quot; alt=&quot;直线拟合&quot;&gt;&lt;/p&gt;&lt;p&gt;用logistic回归来进行数据分类是通过给出的所有数据点拟合出分类边界的直线，确定这条直线的系数就称为回归系数，当用回归系数乘以当前特征则得出该条直线：&lt;/p&gt;&lt;p&gt;z=w0 * x0+w1 * x1+…+wn * xn，其中w0 - wn是回归系数，x0 - xn是各个特征，得出的z是当前特征下的预测值。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="分类" scheme="https://www.xiapf.com/tags/%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>Leecode算法学习笔记（五）面试高频题——topk问题</title>
    <link href="https://www.xiapf.com/blogs/topK/"/>
    <id>https://www.xiapf.com/blogs/topK/</id>
    <published>2020-03-23T10:56:18.000Z</published>
    <updated>2020-03-23T10:57:39.102Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>给出一个数组，求数组中前k个最大/最小的数或者求数组中第k个最大/最小的数。</p><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><h3 id="暴力法"><a href="#暴力法" class="headerlink" title="暴力法"></a>暴力法</h3><p>（1）思路：调用sort函数将数组排序，选择前k个或者第k个输出</p><p>（2）复杂度分析：</p><p>时间复杂度：主要是在比较排序上了，该方法对整个数组排序，c++中的sort使用的是快速排序，时间复杂度为O(nlogn)</p><p>空间复杂度：没有用到额外的空间，复杂度为O（1）</p><a id="more"></a><p>（3）代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sort(num.<span class="built_in">begin</span>(),num.<span class="built_in">end</span>())</span><br></pre></td></tr></table></figure><h3 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h3><p>（1）思路：</p><p>维护一个k个元素的堆，将数组中元素和队首元素比较，形成最终稳定的堆：</p><p>求前k个最大的数即需要维护一个小顶堆，队首是最小的元素，当读入的数据大于等于队首元素，则加入堆中，保证其余元素都大于队首的元素，每次仅需比较队首元素即可，当遍历到最后，堆中剩下前k个最大的数；</p><p>当求前k个最小的数即需要维护一个大顶堆，队首是最大的元素，当读入的数据小于等于队首元素，则加入堆中，保证其他元素都小于队首的元素，最终，堆中剩下前k个最小的数。</p><p>（2）复杂度分析：</p><p>时间复杂度：每次维护一个k个元素的堆，相当于建立一个二叉树，深度是logk，最差的情况是n个数都需要排序比较，则复杂度是O（nlogk）</p><p>空间复杂度：用到了k大小的堆，复杂度为O（k）</p><p>（3）代码：</p><p>以求前k个最大的元素为例</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//1.建小顶堆 后两个元素缺省，自动建立大顶堆</span></span><br><span class="line">prioriy_queue&lt;<span class="keyword">int</span>,<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;,greater&lt;<span class="keyword">int</span>&gt;&gt; p;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;k;i++)</span><br><span class="line">  p.push(num[i])</span><br><span class="line">  </span><br><span class="line"> <span class="comment">//2.队首元素比较</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=k;i&lt;num.<span class="built_in">size</span>();i++)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="comment">//队首元素小，则把新元素加入</span></span><br><span class="line">  <span class="keyword">if</span>(p.top()&lt;num[i])</span><br><span class="line">  &#123;</span><br><span class="line">    p.pop();</span><br><span class="line">    p.push(num[i]);</span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h3><p>（1）思路：快速排序是以一个基准点进行排序，当在基准点的左边都是小于该数的，右边都是大于该数的。从这个思路出发可以想到，求前k个最小的数可以转换为在第k个位置进行排序，左边是小于的数，右边是大于的数，求前k个最小的数就是前k个数。因为快速排序时每次排序可以得到当前基准点的位置，以此判断是否在第k个位置。这种方法不用遍历整个数组，排序的次数减少了。快速排序处理两边的数，这边只需要对一边的数进行排序。</p><p>当求前k个最大的数时可以进行转换：首先第k个最大的数，如果6个数，从小到大排列，则索引是6-k，即求第6-k个最小的数，前k个最大的数，就是从该位置到数组末尾。</p><p>eg.假设有6个数字2，5，7，1，3，9，排序之后为1，2，3，5，7，9，求第3大的数字即5，从左往右数，元素5的索引为3，3=6-3（k的值），所以第k个最大的数=第6-k个最小的数</p><p>（2）复杂度分析：</p><p>时间复杂度：相比较sort方法来说，减少了排序的次数，获取每次排序的位置，当该位置小于k，则说明下一次排序需要在当前位置和right位置之间寻找第k个位置，反之需要在left和当前位置之间寻找。每次减少一半需要排序的数字，则为n+n/2+…=O（n）</p><p>空间复杂度：递归调用栈，复杂度为O（n）</p><p>（3）代码：</p><p>以求前k个最大的元素为例</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//主函数</span></span><br><span class="line"><span class="keyword">int</span> pos=num.<span class="built_in">size</span>()-k <span class="comment">//进行位置转换</span></span><br><span class="line">quickSort(num,<span class="number">0</span>,num.<span class="built_in">size</span>(),pos)</span><br><span class="line"></span><br><span class="line"><span class="comment">//快速排序函数</span></span><br><span class="line"><span class="keyword">void</span> quickSort(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;num,<span class="keyword">int</span> lef,<span class="keyword">int</span> right,<span class="keyword">int</span> k)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">int</span> p=partition(num,left,right);</span><br><span class="line">  <span class="keyword">if</span>(p==k)</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">if</span>(p&gt;k)</span><br><span class="line">      quickSort(num,left,p<span class="number">-1</span>,k)</span><br><span class="line">     <span class="keyword">else</span></span><br><span class="line">       quickSort(num,p+<span class="number">1</span>,right,k)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//排序位置函数</span></span><br><span class="line"><span class="keyword">int</span> partition(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;num,<span class="keyword">int</span> left,<span class="keyword">int</span> right)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">int</span> i=left;</span><br><span class="line">  <span class="keyword">int</span> j=right;</span><br><span class="line">  <span class="keyword">int</span> base=num[i];</span><br><span class="line">  <span class="comment">//找到基准值的位置</span></span><br><span class="line">  <span class="keyword">while</span>(i&lt;j)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="comment">//找到比基准值小的</span></span><br><span class="line">    <span class="keyword">while</span>(i&lt;j&amp;&amp;num[j]&gt;=base)</span><br><span class="line">      j--;</span><br><span class="line">    <span class="comment">//找到比基准值大的</span></span><br><span class="line">    <span class="keyword">while</span>(i&lt;j&amp;&amp;num[i]&lt;=base)</span><br><span class="line">      i++;</span><br><span class="line">    <span class="comment">//交换</span></span><br><span class="line">    <span class="keyword">if</span>(i&lt;j)</span><br><span class="line">      swap(num[i],num[j]);</span><br><span class="line">  &#125;</span><br><span class="line">  num[left]=num[i];</span><br><span class="line">  num[i]=base;</span><br><span class="line">  <span class="keyword">return</span> i;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;问题描述&lt;/h2&gt;&lt;p&gt;给出一个数组，求数组中前k个最大/最小的数或者求数组中第k个最大/最小的数。&lt;/p&gt;&lt;h2 id=&quot;解题思路&quot;&gt;&lt;a href=&quot;#解题思路&quot; class=&quot;headerlink&quot; title=&quot;解题思路&quot;&gt;&lt;/a&gt;解题思路&lt;/h2&gt;&lt;h3 id=&quot;暴力法&quot;&gt;&lt;a href=&quot;#暴力法&quot; class=&quot;headerlink&quot; title=&quot;暴力法&quot;&gt;&lt;/a&gt;暴力法&lt;/h3&gt;&lt;p&gt;（1）思路：调用sort函数将数组排序，选择前k个或者第k个输出&lt;/p&gt;&lt;p&gt;（2）复杂度分析：&lt;/p&gt;&lt;p&gt;时间复杂度：主要是在比较排序上了，该方法对整个数组排序，c++中的sort使用的是快速排序，时间复杂度为O(nlogn)&lt;/p&gt;&lt;p&gt;空间复杂度：没有用到额外的空间，复杂度为O（1）&lt;/p&gt;
    
    </summary>
    
    
      <category term="算法" scheme="https://www.xiapf.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="https://www.xiapf.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="topK" scheme="https://www.xiapf.com/tags/topK/"/>
    
  </entry>
  
  <entry>
    <title>非监督学习笔记（一）——K均值聚类</title>
    <link href="https://www.xiapf.com/blogs/kMeans/"/>
    <id>https://www.xiapf.com/blogs/kMeans/</id>
    <published>2020-03-19T07:46:28.000Z</published>
    <updated>2020-08-06T15:03:23.297Z</updated>
    
    <content type="html"><![CDATA[<p>非监督学习和监督学习不同，监督学习知道需要寻找的内容，即目标变量，会通过数据训练出能得出目标变量学习模型，而监督学习不知道目标变量，非监督学习中有一种聚类方法，把相似的点放在一个簇（类）中，类似于全自动分类，度量数据的相似有很多函数，包括欧式距离，球面距离等。</p><h2 id="含义"><a href="#含义" class="headerlink" title="含义"></a>含义</h2><p>k均值聚类属于一种聚类算法，通过给定的数值k，将数据点分为k类，分类依据为相似的数据点放到一个簇里，这里簇的中心称为质心，一般可以用距离公式来度量数据点之间的相似度。假设以欧式距离作为相似度评价依据，则点到各个质心中的最小距离的那个簇是该数据点所在的位置，最终可以将数据分为k类。</p><a id="more"></a><h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><p>注：因为度量数据之间相似度的函数通常都需要将数据转换为矩阵进行处理，所以程序中需要注意数据的存储形式，如果是列表形式需要转换为矩阵进行处理。</p><p>当有赋值操作的时候，mat类型不能直接赋值，需要转换为list类型。例如，matop=matop.tolist()[0]</p><p>（1）读入数据</p><p>根据输入的文件名读入数据，根据相似度计算方法，需要把数据转换为float数值类型，同时转换为矩阵</p><p>（2）确定相似度计算方法</p><p>以欧式距离为例：计算两个矩阵对应位置相减平方求和，最后再开根号作为两个矩阵之间的距离</p><p>sqrt(sum(power)(veca-vecb))   列表或者矩阵之间求次方用power函数</p><p>（3）确定初始质心选择方法</p><p>设定输入的数据形式为（x,y）</p><p>一般初始质心使用随机选择的方法，为了将随机矩阵的值控制在数据范围内，根据每列的最大值和最小值，生成k*n（n此时为2）个随机数据，以此作为初始的k个质心。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#获取数据点的列数</span></span><br><span class="line">n=shape(data)[<span class="number">1</span>]</span><br><span class="line"><span class="comment">#随机数组</span></span><br><span class="line">centroids=mat(zeros((k,n)))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">  minI=min(data[:,i])</span><br><span class="line">  maxI=max(data[:,i])</span><br><span class="line">  rangeI=float(maxI-minI)</span><br><span class="line">  centroids[:,i]=mat(minI+rangeI*rand(k,<span class="number">1</span>))<span class="comment">#返回k*1个0~1之间的随机数</span></span><br></pre></td></tr></table></figure><p>（4）根据相似度将数据分类</p><p>a）初始化质心：以随机选择的方式</p><p>b）当簇仍在发生变化</p><p>对每个数据点，计算当前点到各个质心的距离，选择最近的距离，将该点放入</p><p>将簇内所有数据点取均值，重新计算当前的质心</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">n=shape(data)[<span class="number">0</span>]</span><br><span class="line">clusterAssment=mat(zeros((n,<span class="number">2</span>)))<span class="comment">#存储对应行的点属于哪个簇，以及误差</span></span><br><span class="line"><span class="keyword">while</span>(簇在变化):</span><br><span class="line">  flag=false<span class="comment">#标记簇是否在变化</span></span><br><span class="line">  <span class="comment">#1.对每个数据点</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">    minDist=inf</span><br><span class="line">    minIndex=<span class="number">-1</span></span><br><span class="line">    <span class="comment">#计算该点到各个质心的距离</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(k):</span><br><span class="line">      pstCurret=distmes(centroids[k,:],data[i,:])<span class="comment">#distmes为相似度计算方法</span></span><br><span class="line">      <span class="keyword">if</span>(pstCurret&lt;minDist):</span><br><span class="line">        最小值交换</span><br><span class="line">    <span class="keyword">if</span>(clusterAssment[i,<span class="number">0</span>]!=minIndex):</span><br><span class="line">      flag=true<span class="comment">#说明簇还在变化</span></span><br><span class="line">     <span class="comment">#2.将点加入到簇中</span></span><br><span class="line">    clusterAssment[i,:]=minIndex,minDist**<span class="number">2</span></span><br><span class="line">  <span class="comment">#3.对每个簇（即质心），对簇内数据点取均值</span></span><br><span class="line">  <span class="keyword">for</span> cent <span class="keyword">in</span> range(k):</span><br><span class="line">    pstCurret=data[nonzero(clusterAssment[:,<span class="number">0</span>].A[<span class="number">0</span>]==cent)]</span><br><span class="line">    centroids[cent,:]=mean(pstCurret,axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h2 id="算法改进"><a href="#算法改进" class="headerlink" title="算法改进"></a>算法改进</h2><p>k均值聚类算法依赖与用户给定的数值k，此时无法确定根据k得出的簇是最优的。评价算法的优劣是根据误差平方和（称SSE），即数据真实值为y，预测值为y1，则误差平方和为（y-y1）2，k均值聚类算法的改进是根据SSE最小时，将数据分为k个类。</p><p>初始时将所有数据点作为一个簇，每次将簇一分为二，选择哪个簇进行划分要看能否最大化减少sse，当划分之后的sse比原来的小，则保存当前的质心，最终得到的簇误差平方和最小，改进的算法也称二分k均值聚类。</p><p>（1）将所有数据作为一个簇，取均值得到初始的质心</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#所有数据取均值</span></span><br><span class="line">centroids0=mean(data,axis=<span class="number">0</span>).tolist[<span class="number">0</span>]</span><br><span class="line"><span class="comment">#转换为矩阵形式</span></span><br><span class="line">centroids=mat(centroids0)</span><br><span class="line"><span class="comment">#求初始质心到各点的距离</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">  clusterAssment[i,<span class="number">1</span>]=distmes(centroids,data[i,:])**<span class="number">2</span></span><br></pre></td></tr></table></figure><p>（2）当簇的长度小于k</p><p>a）对所有簇</p><p>将簇一分为二，得到此时的误差，找到一分为二之后误差最小的簇</p><p>b）误差最小的簇，将其保存</p><p>注：当有赋值操作的时候，mat类型不能直接赋值，需要转换为list类型。例如，matop=matop.tolist()[0]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将每个质心放入一个大列表中进行操作</span></span><br><span class="line">centroList=[centroids0]</span><br><span class="line"><span class="keyword">while</span>(len(centroList)&lt;k):</span><br><span class="line">  lossSSE=inf</span><br><span class="line">  <span class="comment">#1.对所有簇，找误差最小的</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(len(centroList)):</span><br><span class="line">    <span class="comment">#当前簇的所有数据点 clusterAssment第0列</span></span><br><span class="line">    pstCurrent=data[nonzero(clusterAssment[:,<span class="number">0</span>].A==i)[<span class="number">0</span>],:]</span><br><span class="line">    <span class="comment">#二分簇</span></span><br><span class="line">    centroids,splitCluster=kmeans(pstCurrent,<span class="number">2</span>)</span><br><span class="line">    <span class="comment">#求误差</span></span><br><span class="line">    sseSplit=sum(splitCluster[:,<span class="number">1</span>])</span><br><span class="line">    sseNotSplit=sum(clusterAssment[nonzero(clusterAssment[:,<span class="number">0</span>].A!=i)[<span class="number">0</span>],<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">if</span>(sseSplit+sseNotSplit&lt;lossSSE):</span><br><span class="line">      bestCent=i</span><br><span class="line">      bestCluster=splitCluster.copy()</span><br><span class="line">      bestCentroid=centroids</span><br><span class="line">      lossSSE=sseSplit+sseNotSplit</span><br><span class="line">   <span class="comment">#2.更新簇的结果，一个取代成为第i个簇，另一个成为第m+1个簇，原来一共有m个簇</span></span><br><span class="line">  bestCluster[nonzero(bestCluster[:,<span class="number">0</span>].A==<span class="number">0</span>)[<span class="number">0</span>],<span class="number">0</span>]=bestCent</span><br><span class="line">  bestCluster[nonzero(bestCluster[:,<span class="number">0</span>].A==<span class="number">1</span>)[<span class="number">0</span>],<span class="number">0</span>]=len(centroList)</span><br><span class="line">  </span><br><span class="line">  <span class="comment">#3.将最佳分类的质心加入簇列表 二分类所以质心是两个</span></span><br><span class="line">  centroList[bestCent]=bestCentroid[<span class="number">0</span>,:].tolist()[<span class="number">0</span>]</span><br><span class="line">  centroList.append(bestCentroid[<span class="number">1</span>,:].tolist()[<span class="number">0</span>])</span><br><span class="line">  </span><br><span class="line">  <span class="comment">#4.将簇分类的结果加入列表中</span></span><br><span class="line">  clusterAssment[nonzero(clusterAssment[:,<span class="number">0</span>].A==bestCent)[<span class="number">0</span>],:]=bestCluster</span><br></pre></td></tr></table></figure><h2 id="应用实例"><a href="#应用实例" class="headerlink" title="应用实例"></a>应用实例</h2><p>给出地图上一些点的经度和纬度，将其分为合适的类，使得通过中心点能很快到达其余点。同时观察不同的簇数目k的分类效果。</p><p>（1）数据来源</p><p>从谷歌api上获取一些club的经度和纬度保存为place.txt，其中每一行代表一个club的位置，最后两行保存了对应地点的纬度和经度</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200319144017.png" alt="数据点"></p><p>（2）运行kmeans.clubsCluster(“place.txt”,5)</p><p>结果如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200319150954.png" alt="效果图"></p><p>可以看出数据点被分成了5个簇，每个簇的中心用“+”号标记出来了，此时从簇的中心到其余点是最近的。</p><p>（3）观察不同k下的运行结果</p><p>当分的簇越多，簇的误差就越小，但是数量越多不符合实际，因此需要找一个折中点。</p><p>（4）程序：</p><p>a）相似度计算方法使用球面距离公式：</p><p>设所求点A纬度角β1，经度角α1， 点B 纬度角β2， 经度角α2  R*arccos[cosβ1cosβ2cos（α1-α2）+sinβ1sinβ2] </p><p>b）聚类：</p><p>1.读入文件中最后两行数据，并转换为数值类型的mat类型矩阵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fr=open(filename)</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">  <span class="comment">#以\t分割每行数据，并且去掉每行最后的回车符</span></span><br><span class="line">  lineList=line.rstrip(<span class="string">"\n"</span>).split(<span class="string">"\t"</span>)</span><br><span class="line">  dataList.append((float(lineList[<span class="number">4</span>],float(lineList[<span class="number">5</span>])))</span><br><span class="line">dataList=mat(dataList)</span><br></pre></td></tr></table></figure><p>2.使用二分k均值分类，得出最佳分类簇centroids</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">centroids,clusterAssment=biKmeans(dataList,k,distMes=球面距离公式)</span><br></pre></td></tr></table></figure><p>3.画图</p><p>将得到的分类结果展示出来，使用matplotlib库中的pyplot进行画图</p><p>画图准备</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">fig=plt.figure()</span><br><span class="line"><span class="comment">#设置显示图形的位置和比例</span></span><br><span class="line">ret=[<span class="number">0.1</span>,<span class="number">0.1</span>,<span class="number">0.8</span>,<span class="number">0.8</span>]</span><br><span class="line">.....</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>画背景</p><p>读入背景图片，并显示</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">img=plt.imread(背景图片)</span><br><span class="line"><span class="comment">#按照横纵坐标数据比列设置坐标轴,并会去掉预设坐标数字</span></span><br><span class="line">axpos=dict[xticks[],yticks[]]</span><br><span class="line"><span class="comment">#为显示的图像设置底层区域</span></span><br><span class="line">axis0=fig.add_axis(ret,label=<span class="string">""</span>,**axpos)</span><br><span class="line">axis0.imshow(img)</span><br></pre></td></tr></table></figure><p>加上**axpos效果，此时按照数据点数值大小设置坐标轴</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200319150954.png" alt="加上**axpos"></p><p>去掉**axpos效果，发现坐标轴数字出现重叠</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200319151338.png" alt="去掉**axpos"></p><p>画数据点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#设置图案标记</span></span><br><span class="line">markerList=[<span class="string">'s'</span>,....]</span><br><span class="line"><span class="comment">#为显示的图像增加子区域,frameon设置是否覆盖下面的区域</span></span><br><span class="line">axis1=fig.add_axis(ret,label=<span class="string">""</span>,frameon=false)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">  <span class="comment">#获取第i个簇的数据点</span></span><br><span class="line">  pstCurrent=dataList[nonzero(clusterAssment[:,<span class="number">0</span>].A==i)[<span class="number">0</span>],:]</span><br><span class="line">  pstMarker=markerList[i/len]</span><br><span class="line">  <span class="comment">#二维转换为一维显示</span></span><br><span class="line"> axis1.scatter(pstCurrent[:,<span class="number">0</span>].flattern().A[<span class="number">0</span>],pstCurrent[:,<span class="number">1</span>].flattern().A[<span class="number">0</span>],marker=pstMarker,s=<span class="number">90</span>)</span><br></pre></td></tr></table></figure><p>注：</p><p>1.subplot，add_axis的区别</p><p>subplot是设置一整个区域中的子图，不存在重叠：subplot(221)表示将显示图像的区域分为2*2个，当前图案显示在第一个位置。</p><p>add_axis是设置同一个图像中的区域，存在重叠。</p><p>2.scatter，plot的区别</p><p>scatter是绘制数据点，属于离散型图案</p><p>plot是将数据点连接起来，属于连续型图案</p><p>画每个簇的中心</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(k):  </span><br><span class="line">  axis1.scatter(centroids[i,<span class="number">0</span>],centroids[i,<span class="number">1</span>],marker=<span class="string">"+"</span>,s=<span class="number">300</span>)<span class="comment">#每个质心颜色会不一样</span></span><br><span class="line">  或者 axis1.scatter(centroids[:,<span class="number">0</span>].flattern().A[<span class="number">0</span>],centroids[:,<span class="number">1</span>].flattern().A[<span class="number">0</span>],marker=<span class="string">"+"</span>,s=<span class="number">300</span>)<span class="comment">#每个质心颜色一样</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;非监督学习和监督学习不同，监督学习知道需要寻找的内容，即目标变量，会通过数据训练出能得出目标变量学习模型，而监督学习不知道目标变量，非监督学习中有一种聚类方法，把相似的点放在一个簇（类）中，类似于全自动分类，度量数据的相似有很多函数，包括欧式距离，球面距离等。&lt;/p&gt;&lt;h2 id=&quot;含义&quot;&gt;&lt;a href=&quot;#含义&quot; class=&quot;headerlink&quot; title=&quot;含义&quot;&gt;&lt;/a&gt;含义&lt;/h2&gt;&lt;p&gt;k均值聚类属于一种聚类算法，通过给定的数值k，将数据点分为k类，分类依据为相似的数据点放到一个簇里，这里簇的中心称为质心，一般可以用距离公式来度量数据点之间的相似度。假设以欧式距离作为相似度评价依据，则点到各个质心中的最小距离的那个簇是该数据点所在的位置，最终可以将数据分为k类。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>监督学习笔记（二）——朴素贝叶斯分类器</title>
    <link href="https://www.xiapf.com/blogs/bayes/"/>
    <id>https://www.xiapf.com/blogs/bayes/</id>
    <published>2020-03-12T11:25:59.000Z</published>
    <updated>2020-08-06T15:02:58.569Z</updated>
    
    <content type="html"><![CDATA[<h2 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h2><p>（1）问题原理：朴素贝叶斯分类的依据是概率，假设需要分为两类c1和c2，则某个数据点属于哪一类，需要计算p(c1|x)和p{c2|x)，即计算数据点x来自c1和来自c2中的概率哪个大，如果p(c1|x)&gt;p{c2|x)，则x被分到c1类别中，反之被分到c2类别中。</p><p>（2）计算原理：上述概率属于条件概率，根据公式p(c|x)=p(x|c) * p(c)/ p(x)可以得到，当求（x,y）来自哪个类别时，即用（x,y）替换x，即求p(c|x,y),代入条件概率公式中得，p(c|x,y)=p(x,y|c) * p(c)/p(x,y)，因此问题转换为求p(x,y|c) * p(c)/p(x,y)中三个概率的值。</p><a id="more"></a><p>求p(c)：对于所有数据点来说，p(c)是出现某类别的数据/总数据</p><p>求p(x,y|c)：因为朴素贝叶斯的假设是所有特征是独立的，那么p(x,y|c)=p(x1,y1|c) * p(x2,y2|c) *… * p(xn,yn|c)，p(xi,yi|c)是求在某个类别下，每个特征的数据占当前类别总数据的概率，每个特征的概率求解出后连乘即可得到该概率</p><p>求p(x,y)：因为对于所有类别来说p(x,y)都是相同的，所以只要比较上面两个概率乘积的大小即可</p><h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><p>以二分类为例子：</p><p>（1）载入数据集</p><p>1.读取文件路径，并将读取的文件按照单词进行划分，划分使用正则表达式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">regex=re.compile(<span class="string">"\\W+"</span>)</span><br><span class="line">listToken=regex.split(file)</span><br></pre></td></tr></table></figure><p>2.将数据进行存储，同时存储数据标签</p><p>注：因为此时存储的数据集是文本，在测试和训练的时候需要转换为单词向量的形式</p><p>输入词汇表和需要变为单词向量的文本，在词汇表中出现的单词的位置，在单词向量中对应设置为1，其余为0，这样就能构造出单词向量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> inputWord:</span><br><span class="line">  <span class="keyword">if</span> word <span class="keyword">in</span> vocabList:</span><br><span class="line">    returnVec[vocaList.index(word)]+=<span class="number">1</span><span class="comment">#在词汇表中单词的位置的，对应的单词向量的位置设置为1</span></span><br></pre></td></tr></table></figure><p>（ * ）可增加的部分：删除高频词</p><p>因为有些词例如a,about,the等这些常用词在很多地方都会出现，这些高频出现的词，可能会对分类的概率产生影响，可以对所有数据进行遍历，找到出现频率最高的单词在词汇表中删除（可以取前20或30等，根据数据量决定）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcTopWord</span><span class="params">(fullText,vocabList)</span>:</span></span><br><span class="line">  //存储出现概率及单词，用到键值对</span><br><span class="line">  wordList=&#123;&#125;</span><br><span class="line">  <span class="keyword">for</span> word <span class="keyword">in</span> fullText:</span><br><span class="line">    wordlist[word]=fullText.(word)<span class="comment">#在整篇文档计算单词出现的次数，count</span></span><br><span class="line">  //从大到小排序,用值排序</span><br><span class="line">  rankList=sorted(wordList.items(),key=operator.itemgetter(<span class="number">1</span>),reverse=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>（2）得出词汇表</p><p>找到数据集中不重复的单词作为词汇表，为后续统计每个单词出现的频率（即每个特征的概率）做准备</p><p>用set得到不重复的单词</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> dataWord <span class="keyword">in</span> dataSet:</span><br><span class="line">vocaList=vocaList|set(dataWord)</span><br></pre></td></tr></table></figure><p>（3）随机选择一部分作为测试集和训练集</p><p>将训练集设置为总数据集数量，测试集用随机数设置（random.uniform）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">trainSet=list(range(lenData))</span><br><span class="line"><span class="comment">#假设随机选择5个测试集</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">  <span class="comment">#得出位置 int类型</span></span><br><span class="line">  testIndex=(int)(random.uniform(<span class="number">0</span>,len(trainSet)))</span><br><span class="line">  <span class="comment">#测试集的数据位置</span></span><br><span class="line">  testSet.appen(trainSet[testIndex])</span><br><span class="line">  <span class="comment">#训练集的数据位置</span></span><br><span class="line">  <span class="keyword">del</span>(trainSet[testIndex])</span><br></pre></td></tr></table></figure><p>（4）根据训练集训练贝叶斯分类器</p><p>1.计算p(c)、p(x,y|c)</p><p>计算p(c)：因为是两个类别，只需要计算p(c)，用1-p(c)可以得出另一个类别出现的概率。p(c)通过对类别标记中1出现的次数求和即为1类别出现的次数，再除以文档总数，即为p(c)</p><p>计算p(x,y|c)：对所有数据进行遍历，分布对0类别和1类别下，所有单词出现的概率进行计算</p><p>注：为防止出现很多很小的数：计算概率时用log，即p(x,y|c)用logp(x,y|c)表示</p><p>为防止各个特征的概率相乘出现0：初始设置p(x,y|c)=1，p(c)=2</p><p>2.训练的结果就是得出训练集下数据属于不同类别的概率</p><p>以两个类别为例，即最终得出在0类别和1类别下所有特征的出现的概率p(x,y|c0)，p(x,y|c1)和0类别、1类别数据出现的概率p(c)，得出这两个概率后，用测试集数据乘以不同类别下的p(ci|x,y)（即用数据 *p(x,y|ci) * p(c)），比较p(x,y|c0)和p(x,y|c1)得出分属的类别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#已求出p0V,p1V,pAuixs</span></span><br><span class="line"><span class="comment">#属于两个类别的概率，进行比较即可</span></span><br><span class="line">p1=sum(inputMatrix*p1V)+log(pAuixs)</span><br><span class="line">p0=sum(inputMatrix*p0V)+log(<span class="number">1</span>-pAuixs)</span><br></pre></td></tr></table></figure><p>（5）在测试集上，使用贝叶斯分类器，分类结果和实际标记对比，得出总体错误率</p><p>1.计算条件概率，即比较两个类别的概率，确定分类的结果</p><p>2.与实际结果比对，分类错误则进行标记</p><h2 id="应用案例"><a href="#应用案例" class="headerlink" title="应用案例"></a>应用案例</h2><p>广告内容倾向分类</p><p>该案例使用feedparse获取网页的rss源，对网页所有目录的列表读取其中的概要，输入两个网页，根据两个网页中单词出现的概率来进行分类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#获得不重复的单词列表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createVocabList</span><span class="params">(dataSet)</span>:</span></span><br><span class="line"><span class="comment">#set返回的是不重复的数据</span></span><br><span class="line">vocaList=set([])</span><br><span class="line"><span class="keyword">for</span> dataWord <span class="keyword">in</span> dataSet:</span><br><span class="line">vocaList=vocaList|set(dataWord)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> list(vocaList)<span class="comment">#返回列表</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2.0词袋模型，每个词可以出现多次</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bagOfWords2Vec</span><span class="params">(vocaList,inputWord)</span>:</span></span><br><span class="line">returnVec=[<span class="number">0</span>]*len(vocaList)</span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> inputWord:</span><br><span class="line"><span class="keyword">if</span>(word <span class="keyword">in</span> vocaList):</span><br><span class="line">returnVec[vocaList.index(word)]+=<span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">print(<span class="string">"the word:"</span>+str(word)+<span class="string">" is not in the vocaList"</span>)</span><br><span class="line"><span class="keyword">return</span> returnVec</span><br><span class="line"></span><br><span class="line"><span class="comment">#利用朴素贝叶斯进行文档分类</span></span><br><span class="line"><span class="comment">#主要是进行概率比较，条件概率定义为p(c|w)=p(w|c)*p(c)/p(w),对所有类别来说，p(w)都是相同的，因此可以不计算</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainNB0</span><span class="params">(trainMatrix,trainCatalog)</span>:</span></span><br><span class="line"><span class="comment">#0.计算不同类别下的文档概率即p(c)</span></span><br><span class="line"><span class="comment">#此处是二值分类，另一个概率可以用1-p(c)来计算</span></span><br><span class="line"><span class="comment">#0.0文档的个数</span></span><br><span class="line">numDocu=len(trainMatrix)</span><br><span class="line"><span class="comment">#0.1侮辱性文档的概率</span></span><br><span class="line"></span><br><span class="line">pAuixs=sum(trainCatalog)/numDocu</span><br><span class="line"><span class="comment">#0.2初始化累加值</span></span><br><span class="line"><span class="comment">#每行词汇的长度</span></span><br><span class="line">numWords=len(trainMatrix[<span class="number">0</span>])</span><br><span class="line"><span class="comment">#因为是独立的概率，如果其中一个为0，整个为0，避免这种情况，初始化为1</span></span><br><span class="line"><span class="comment">#p1num=zeros((numWords))</span></span><br><span class="line"><span class="comment">#p0num=zeros(numWords)</span></span><br><span class="line"><span class="comment">#p1Denom=0</span></span><br><span class="line"><span class="comment">#p0Denom=0</span></span><br><span class="line">p1num=ones((numWords))</span><br><span class="line">p0num=ones(numWords)</span><br><span class="line">p1Denom=<span class="number">2</span></span><br><span class="line">p0Denom=<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#1.按照不同类别计算p(w|c)即计算不同类别下词向量出现的次数</span></span><br><span class="line"><span class="comment">#根据当前类别下每个词出现的次数除以在该类别下所有单词出现的次数</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(numDocu):</span><br><span class="line"><span class="comment">#1.0 侮辱类别</span></span><br><span class="line"><span class="keyword">if</span>(trainCatalog[i]==<span class="number">1</span>):</span><br><span class="line">p1num+=trainMatrix[i]</span><br><span class="line">p1Denom+=sum(trainMatrix[i])</span><br><span class="line"><span class="comment">#1.1 非侮辱类别</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">p0num+=trainMatrix[i]</span><br><span class="line">p0Denom+=sum(trainMatrix[i])</span><br><span class="line"></span><br><span class="line"><span class="comment">#为了避免出现过小的数,出现下溢出，使用log</span></span><br><span class="line">p1Vec=log(p1num/p1Denom)</span><br><span class="line">p0Vec=log(p0num/p0Denom)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> p0Vec,p1Vec,pAuixs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#朴素贝叶斯分类器</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classfyNB</span><span class="params">(wordVec,p0Vec,p1Vec,pAuixs)</span>:</span></span><br><span class="line"><span class="comment">#比较单词向量乘以p(w|c)*p(c),因为p(w|c)使用了log处理，所以，这边的pauixs也需要加上log,log相加代表相乘</span></span><br><span class="line"><span class="comment">#因为是独立的，所以计算每个单词出现的概率p(wi|c)，因为做了log处理，这里sum等同于去掉log后概率相乘</span></span><br><span class="line">p1=sum(wordVec*p1Vec)+log(pAuixs)</span><br><span class="line">p0=sum(wordVec*p0Vec)+log(<span class="number">1</span>-pAuixs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(p1&gt;p0):</span><br><span class="line"><span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#测试分类器</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testingNB</span><span class="params">()</span>:</span></span><br><span class="line"><span class="comment">#0.载入实验样本</span></span><br><span class="line">listPosts,listLabels=loadDataSet();</span><br><span class="line"><span class="comment">#1.生成词汇表</span></span><br><span class="line">vocabList=createVocabList(listPosts)</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.文本转换为单词向量，方法中是对每行文本进行转换</span></span><br><span class="line">m=len(listPosts)</span><br><span class="line">wordVec=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">wordVec.append(setOfWords2Vec(vocabList,listPosts[i]))</span><br><span class="line"></span><br><span class="line"><span class="comment">#3.构造贝叶斯分类器</span></span><br><span class="line">p0Vec,p1Vec,pAuixs=trainNB0(wordVec,listLabels)</span><br><span class="line"></span><br><span class="line"><span class="comment">#4.测试数据进行测试</span></span><br><span class="line"><span class="comment">#4.0构造测试数据</span></span><br><span class="line">testList=[<span class="string">'love'</span>, <span class="string">'my'</span>, <span class="string">'dalmation'</span>]</span><br><span class="line"><span class="comment">#4.1转换为单词向量</span></span><br><span class="line">testVec=setOfWords2Vec(vocabList,testList)</span><br><span class="line"><span class="comment">#4.2计算概率</span></span><br><span class="line">testP=classfyNB(testVec,p0Vec,p1Vec,pAuixs)</span><br><span class="line">print(str(testList)+<span class="string">" is:"</span>+str(testP))</span><br><span class="line"></span><br><span class="line"><span class="comment">#4.测试数据进行测试</span></span><br><span class="line"><span class="comment">#4.0构造测试数据</span></span><br><span class="line">testList=[<span class="string">'stupid'</span>, <span class="string">'garbage'</span>]</span><br><span class="line"><span class="comment">#4.1转换为单词向量</span></span><br><span class="line">testVec=setOfWords2Vec(vocabList,testList)</span><br><span class="line"><span class="comment">#4.2计算概率</span></span><br><span class="line">testP=classfyNB(testVec,p0Vec,p1Vec,pAuixs)</span><br><span class="line">print(str(testList)+<span class="string">" is:"</span>+str(testP))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#利用贝叶斯分类器进行垃圾邮件过滤</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#读取文件内的文本，对长度小于两个的单词进行过滤</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textParse</span><span class="params">(file)</span>:</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="comment">#0.正则表达式进行分割</span></span><br><span class="line">regex=re.compile(<span class="string">"\\W+"</span>)</span><br><span class="line"><span class="comment">#1.对读入的问津进行分割</span></span><br><span class="line"><span class="comment">#file=file.encode('utf-8')</span></span><br><span class="line"></span><br><span class="line">listTokens=regex.split(file)</span><br><span class="line"><span class="keyword">return</span> [token.lower() <span class="keyword">for</span> token <span class="keyword">in</span> listTokens <span class="keyword">if</span> len(token)&gt;<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#用朴素贝叶斯分类器对个人广告区域进行倾向分类</span></span><br><span class="line"><span class="comment">#从rss中获取文本</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#将文本中出现频率前30的去掉</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcMostFreq</span><span class="params">(vocabList,fullList)</span>:</span></span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"><span class="comment">#代表键值对</span></span><br><span class="line">wordFreq=&#123;&#125;</span><br><span class="line"><span class="comment">#对整篇文档计算单词表中单词出现的频率</span></span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> vocabList:</span><br><span class="line">wordFreq[token]=fullList.count(token)</span><br><span class="line"><span class="comment">#按照字典中值进行排序（即出现的次数），以第几个域排序，按照降序排列</span></span><br><span class="line"><span class="comment">#此时变为了列表，可以返回前30个</span></span><br><span class="line">freqList=sorted(wordFreq.items(),key=operator.itemgetter(<span class="number">1</span>),reverse=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">return</span> freqList[:<span class="number">30</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#输入的参数为：从feedparse中获取的网页的rss源，feed0是0网页，里面的内容标记为0，feed1是1网页，里面的内容标记为1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loacWords</span><span class="params">(feed0,feed1)</span>:</span></span><br><span class="line"><span class="keyword">import</span> feedparser</span><br><span class="line"><span class="comment">#0.获取文本</span></span><br><span class="line">docList=[]</span><br><span class="line">classList=[]</span><br><span class="line">wordList=[]</span><br><span class="line">fullList=[]</span><br><span class="line"><span class="comment">#feed0['entries']是获取所有条目的列表</span></span><br><span class="line">minlen=min(len(feed0[<span class="string">'entries'</span>]),len(feed1[<span class="string">'entries'</span>]))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(minlen):</span><br><span class="line"><span class="comment">#对第i个条目的概要提取内容，并进行单词划分</span></span><br><span class="line">wordList=textParse(feed0[<span class="string">'entries'</span>][i][<span class="string">'summary'</span>])</span><br><span class="line">docList.append(wordList)</span><br><span class="line">classList.append(<span class="string">"0"</span>)</span><br><span class="line"><span class="comment">#把所有单词加入到一个列表方便计数</span></span><br><span class="line">fullList.extend(wordList)</span><br><span class="line"></span><br><span class="line">wordList=textParse(feed1[<span class="string">'entries'</span>][i][<span class="string">'summary'</span>])</span><br><span class="line">docList.append(wordList)</span><br><span class="line">classList.append(<span class="string">"1"</span>)</span><br><span class="line"><span class="comment">#把所有单词加入到一个列表方便计数</span></span><br><span class="line">fullList.extend(wordList)</span><br><span class="line"></span><br><span class="line"><span class="comment">#1.构建词汇表</span></span><br><span class="line">vocabList=createVocabList(docList);</span><br><span class="line"></span><br><span class="line"><span class="comment">##0000.增加的部分，将文本中高频出现的前30个单词从词汇表中去除</span></span><br><span class="line">mostFreq=calcMostFreq(vocabList,fullList)</span><br><span class="line"></span><br><span class="line"><span class="comment">#freq是键值对，[0]第0个代表单词，[1]第一个代表出现的次数</span></span><br><span class="line"><span class="keyword">for</span> freq <span class="keyword">in</span> mostFreq:</span><br><span class="line"><span class="keyword">if</span> freq[<span class="number">0</span>] <span class="keyword">in</span> vocabList:</span><br><span class="line">vocabList.remove(freq[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.文本转换为单词向量</span></span><br><span class="line"><span class="comment">#inputWord=[]</span></span><br><span class="line"><span class="comment">#for i in range(2*minlen):</span></span><br><span class="line"><span class="comment">#inputWord[i]=bagOfWords2Vec(vocabList,docList[i])</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#3.随机选取20个作为测试集</span></span><br><span class="line">trainingSet=list(range(<span class="number">2</span>*minlen))</span><br><span class="line">testingSet=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line"><span class="comment">#生成随机浮点数 random.uniform</span></span><br><span class="line">testIndex=(int)(random.uniform(<span class="number">0</span>,len(trainingSet)))</span><br><span class="line">testingSet.append(trainingSet[testIndex])</span><br><span class="line"><span class="keyword">del</span>(trainingSet[testIndex])</span><br><span class="line"></span><br><span class="line"><span class="comment">#将字符串转换为数字</span></span><br><span class="line">classList=[int(x) <span class="keyword">for</span> x <span class="keyword">in</span> classList]</span><br><span class="line"><span class="comment">#4.剩余的作为训练集，计算分类器的概率，用于分类</span></span><br><span class="line">traingData=[]</span><br><span class="line">traingLabels=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> trainingSet:</span><br><span class="line"><span class="comment">#2.文本转换为单词向量</span></span><br><span class="line">traingData.append(bagOfWords2Vec(vocabList,docList[i]))</span><br><span class="line">traingLabels.append(classList[i])</span><br><span class="line">p0Vec,p1Vec,pAuixs=trainNB0(traingData,traingLabels)</span><br><span class="line"><span class="comment">#5.对测试集进行测试</span></span><br><span class="line">error=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> testingSet:</span><br><span class="line">testingData=bagOfWords2Vec(vocabList,docList[i])</span><br><span class="line">testResult=classfyNB(testingData,p0Vec,p1Vec,pAuixs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(testResult!=classList[i]):</span><br><span class="line">error+=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"the error rate is:"</span>+str((float)(error)/<span class="number">20</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> vocabList,p0Vec,p1Vec</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;算法原理&quot;&gt;&lt;a href=&quot;#算法原理&quot; class=&quot;headerlink&quot; title=&quot;算法原理&quot;&gt;&lt;/a&gt;算法原理&lt;/h2&gt;&lt;p&gt;（1）问题原理：朴素贝叶斯分类的依据是概率，假设需要分为两类c1和c2，则某个数据点属于哪一类，需要计算p(c1|x)和p{c2|x)，即计算数据点x来自c1和来自c2中的概率哪个大，如果p(c1|x)&amp;gt;p{c2|x)，则x被分到c1类别中，反之被分到c2类别中。&lt;/p&gt;&lt;p&gt;（2）计算原理：上述概率属于条件概率，根据公式p(c|x)=p(x|c) * p(c)/ p(x)可以得到，当求（x,y）来自哪个类别时，即用（x,y）替换x，即求p(c|x,y),代入条件概率公式中得，p(c|x,y)=p(x,y|c) * p(c)/p(x,y)，因此问题转换为求p(x,y|c) * p(c)/p(x,y)中三个概率的值。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="分类" scheme="https://www.xiapf.com/tags/%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>监督学习笔记（一）——支持向量机</title>
    <link href="https://www.xiapf.com/blogs/svm/"/>
    <id>https://www.xiapf.com/blogs/svm/</id>
    <published>2020-03-06T08:20:10.000Z</published>
    <updated>2020-08-06T15:03:28.352Z</updated>
    
    <content type="html"><![CDATA[<h2 id="含义"><a href="#含义" class="headerlink" title="含义"></a>含义</h2><p>（1）支持向量：支持向量机用于解决分类问题，当给出一组线性可分的数据时，此时可以得出一条直线将数据分隔开，要求这条直线即求出了分类的依据，当根据距离分隔线最近的点，取其距离的最大值就能得到最优分割的直线。其中距离分隔线最近的点称为支持向量。</p><p>（2）机：机是指该方法是一个分类器，会产生二值决策机。</p><p>（3）优点：支持向量机方法只使用支持向量，并没有用全部的数据点，所以内存方面优于knn。</p><a id="more"></a><p>（4）缺点：支持向量机一般用于线性可分的数据，当数据线性不可分时无法使用。对于复杂数据需要借助核函数，将复杂数据映射到高维空间进行处理</p><h2 id="求解的理论依据"><a href="#求解的理论依据" class="headerlink" title="求解的理论依据"></a>求解的理论依据</h2><p>（1）确定分隔平面和输出函数</p><p>分隔平面：wT*x+b，其中w和b描述了所给数据的分隔平面</p><p>因为是二值分类器，输出值是-1和+1，所以使用signmoid函数，当点到直线的距离f(wT*x+b)&gt;0时属于 +1类，反之属于-1类。</p><p>（2）求距离：</p><p>数据点到分隔平面的距离记为<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200306150346.png" alt="点到平面的几何距离"></p><p>要求得最佳分隔平面就要找到距离平面最佳的点的距离将其最大化，则为最佳分类，所以即求<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200306150600.png" alt="求解目标"></p><p>但是该目标函数求解困难，因此使用拉格朗日乘子法进行转换带约束调节的目标函数：<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200306150813.png" alt="转换函数"></p><p>其中约束条件为：<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200306151016.png" alt="约束条件"></p><p>因为实际数据不可能存在100%线性可分所以此时加上一个松弛变量，可以允许一些点在分隔平面的另一边，此时前一个约束条件变为0&lt;=alpha&lt;=C，这里的常数C确保点距离平面的最大间隔和所有数据点的函数间隔小于1.0（因为是支持向量）</p><p>（3）问题转换</p><p>根据（2）中转换后的目标函数和约束条件可以看出，SVM（支持向量机，以下均简称为SVM）的主要求解目标转换为求alpha的值，通过alpha可以表示分隔平面。</p><h2 id="使用的算法"><a href="#使用的算法" class="headerlink" title="使用的算法"></a>使用的算法</h2><p>由求解的理论依据中可知，SVM需要求解的目标为带有约束条件的函数，一般采用二次规划求解方法，但是计算复杂，这里使用platt SMO算法求解。</p><p>（1）算法思路：每次选择一对alpha进行优化，当找到合适的两个alpha后，需要增大一个减小另一个，因为需要满足约束条件：<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200306151016.png" alt="约束条件">，合适是指两个alpha值要在间隔边界之外，同时没有被区间化处理或者不在边界上</p><p>（2）基本算法流程：以简单smo算法为例</p><blockquote><p>#0.初始化alpha向量为0</p><p>#1.当迭代次数小于最大迭代次数（外循环）<br>    #2.对数据集中每个向量（内循环）<br>        #3.如果该向量可以被优化<br>            #4.随机找另一个向量<br>            #5.优化这两个向量<br>            #6.如果这两个向量不能被优化，跳出内循环</p><p>#7.如果所有向量都不能被优化，增加迭代次数，进行下一次的循环</p></blockquote><p>（3）完整platt smo 算法</p><p>platt smo算法在简化smo算法基础上提升时间，对数据量大的数据集执行时间大大减少。</p><p>相比于传统的二次规划求解方法，每次选取两个alpha进行优化，时间效率大幅提高。</p><p>算法流程：</p><blockquote><p>#外循环 使用两种方式交替得到第一个alpha的值</p><p>#1.0 对所有数据点进行遍历扫描</p><p>#2.0 对非边界值即0&lt;alpha并且alpha&gt;c的alpha进行扫描</p><p>#内循环 采用最大化步长的方式得到第二个alpha的值</p></blockquote><h2 id="核函数的引入"><a href="#核函数的引入" class="headerlink" title="核函数的引入"></a>核函数的引入</h2><p>当面对非线性数据的时候，需要使用核函数，将复杂数据映射到高维空间，此时分类器易于理解，即可使用SVM方法。</p><p>主要使用径向基函数的高斯版本：<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200306155004.png" alt="径向基函数"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kernelTrans</span><span class="params">(X,A,kTup=<span class="params">()</span>)</span>:</span></span><br><span class="line">  m=shape(X)[<span class="number">0</span>]</span><br><span class="line">  K=zeros((m,<span class="number">1</span>))</span><br><span class="line">  <span class="comment">#对核函数的第一个参数进行不同情况的讨论</span></span><br><span class="line">  <span class="comment">#1.线性核</span></span><br><span class="line">  <span class="keyword">if</span>(kTup[<span class="number">0</span>]=<span class="string">"lin"</span>):</span><br><span class="line">    K=X*A.T</span><br><span class="line">  <span class="comment">#2.径向基核</span></span><br><span class="line">  <span class="keyword">elif</span>(kTup[<span class="number">0</span>]=<span class="string">"rbf"</span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">      delta=X[i,:]-A</span><br><span class="line">      K[i]=delta*delta.T</span><br><span class="line">     K=exp(K/(<span class="number">-1</span>*kTup[<span class="number">1</span>]**<span class="number">2</span>))      </span><br><span class="line">  <span class="comment">#3.抛出异常</span></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">raise</span> NameError()</span><br><span class="line">  <span class="keyword">return</span></span><br></pre></td></tr></table></figure><h2 id="应用实例——手写数字识别"><a href="#应用实例——手写数字识别" class="headerlink" title="应用实例——手写数字识别"></a>应用实例——手写数字识别</h2><p>识别手写字——使用svm比knn效率高，占内存小，因为svm只需用到支持向量来进行分类</p><p>（1）将图像转换为向量</p><p>（2）读取文件夹列表中各个图像文件</p><p>（3）处理分类问题</p><p>为方便处理，只保留了1和9两个数字，当是数字1时，分类为+1，当是数字9时分类为-1</p><blockquote><p>#1.读入图像转换后的数据向量和标签属性</p><p>#2.调用platt smo算法得出alpha和b的值</p><p>#3.根据支持向量大于0的特性得出其中alpha&gt;0的支持向量，并得出支持向量的数据点和标签数据</p><p>#4.在训练集上得出分类结果</p><p>#5.将分类的结果应用于测试集</p></blockquote><p>完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#plat smo可以处理数据量大的，上面simplesmo只能处理数据量较小的数据集</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#platsmo的支持函数</span></span><br><span class="line"><span class="comment">#建立一个对象</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">optStruct</span>:</span></span><br><span class="line"><span class="comment">#各种变量初始化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,dataMat,lableMat,C,toler,kTup)</span>:</span><span class="comment">#增加核函数初始化参数</span></span><br><span class="line">self.dataMat=dataMat</span><br><span class="line">self.lableMat=lableMat</span><br><span class="line">self.C=C</span><br><span class="line">self.toler=toler</span><br><span class="line">self.m=shape(dataMat)[<span class="number">0</span>]</span><br><span class="line">self.alpha=mat(zeros((self.m,<span class="number">1</span>)))</span><br><span class="line">self.b=<span class="number">0</span></span><br><span class="line">self.echache=mat(zeros((self.m,<span class="number">2</span>)))<span class="comment">#第一列表示cache是否有效，第二列是实际的E（误差）值</span></span><br><span class="line"><span class="comment">#构建k</span></span><br><span class="line">self.k=mat(zeros((self.m,self.m)))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(self.m):</span><br><span class="line"><span class="comment">#调用函数，填充k</span></span><br><span class="line">self.k[:,i]=kernelTras(self.dataMat,self.dataMat[i,:],kTup)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#保存误差缓存</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcEK</span><span class="params">(oS,ke)</span>:</span></span><br><span class="line">fk=float(multiply(oS.alpha,oS.lableMat).T*oS.k[:,ke]+oS.b)</span><br><span class="line">EK=fk-float(oS.lableMat[ke])</span><br><span class="line"><span class="keyword">return</span> EK</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#计算误差，并且存入缓存</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateEK</span><span class="params">(oS,k)</span>:</span></span><br><span class="line">EK=calcEK(oS,k)</span><br><span class="line">oS.echache[k]=[<span class="number">1</span>,EK]</span><br><span class="line"></span><br><span class="line"><span class="comment">#内循环中的启发方式</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#选择第二个alpha值，保证每次优化都是最大的步长，如果是第一次进入循环就先随机选择一个alpha[j],反之进入循环，根据最大步长选择alpha[j]的值</span></span><br><span class="line"><span class="comment">#alphaj的值和前一个alphai的下标和误差Ei有关</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">selectJ</span><span class="params">(i,oS,Ei)</span>:</span></span><br><span class="line">maxk=<span class="number">-1</span></span><br><span class="line">maxDeltaE=<span class="number">0</span></span><br><span class="line">Ej=<span class="number">0</span></span><br><span class="line">oS.echache[i]=[<span class="number">1</span>,Ei]</span><br><span class="line"><span class="comment">#输入列表为目标的列表值，nozero返回不为空的值即非零alpha值</span></span><br><span class="line">validDeltaList=nonzero(oS.echache[:,<span class="number">0</span>].A)[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">if</span> (len(validDeltaList))&gt;<span class="number">1</span>:</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> validDeltaList:</span><br><span class="line"><span class="keyword">if</span>(k==i):</span><br><span class="line"><span class="keyword">continue</span>;</span><br><span class="line">Ek=calcEK(oS,k)</span><br><span class="line">delta=abs(Ek-Ei)</span><br><span class="line"><span class="keyword">if</span>(delta&gt;maxDeltaE):</span><br><span class="line">maxk=k</span><br><span class="line">maxDeltaE=delta</span><br><span class="line">Ej=Ek</span><br><span class="line"><span class="keyword">return</span> maxk,Ej</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">j=selectRandom(i,oS.m)</span><br><span class="line">Ej=calcEK(oS,j)</span><br><span class="line"><span class="keyword">return</span> j,Ej</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">innerI</span><span class="params">(oS,i)</span>:</span></span><br><span class="line">exi=calcEK(oS,i)</span><br><span class="line"><span class="keyword">if</span> ((oS.lableMat[i]*exi&lt;-oS.toler) <span class="keyword">and</span> (oS.alpha[i]&lt;oS.C)) <span class="keyword">or</span> ((oS.lableMat[i]*exi&gt;oS.toler) <span class="keyword">and</span> (oS.alpha[i]&gt;<span class="number">0</span>)):</span><br><span class="line"><span class="comment">#1.3启发式找另一个向量</span></span><br><span class="line">j,exj=selectJ(i,oS,exi)</span><br><span class="line"></span><br><span class="line"><span class="comment">#1.4优化这两个向量</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#记录旧的，看优化之后有无变换</span></span><br><span class="line">alphaJold=oS.alpha[j].copy()</span><br><span class="line">alphaIold=oS.alpha[i].copy()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(oS.lableMat[i]!=oS.lableMat[j]):</span><br><span class="line">L=max(<span class="number">0</span>,oS.alpha[j]-oS.alpha[i])</span><br><span class="line">H=min(oS.C,oS.C+oS.alpha[j]-oS.alpha[i])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">L=max(<span class="number">0</span>,oS.alpha[j]+oS.alpha[i]-oS.C)</span><br><span class="line">H=min(oS.C,oS.C+oS.alpha[j]+oS.alpha[i]-oS.C)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(L==H):</span><br><span class="line">print(<span class="string">"L==H"</span>)</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#最优修改量</span></span><br><span class="line"><span class="comment">#eta=2.0*oS.dataMat[i,:]*oS.dataMat[j,:].T-oS.dataMat[i,:]*oS.dataMat[i,:].T-oS.dataMat[j,:]*oS.dataMat[j,:].T</span></span><br><span class="line"><span class="comment">#使用核函数之后，修正eta的值</span></span><br><span class="line">eta=<span class="number">2.0</span>*oS.k[i,j]-oS.k[i,i]-oS.k[j,j]</span><br><span class="line"><span class="keyword">if</span>(eta&gt;=<span class="number">0</span>):</span><br><span class="line">print(<span class="string">"eta&gt;=0"</span>)</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">oS.alpha[j]-=oS.lableMat[j]*(exi-exj)/eta</span><br><span class="line">oS.alpha[j]=adjustBig(oS.alpha[j],H,L)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将j的更新存到缓存</span></span><br><span class="line">updateEK(oS,j)</span><br><span class="line"></span><br><span class="line"><span class="comment">#优化j之后无变换</span></span><br><span class="line"><span class="keyword">if</span> abs(alphaJold-oS.alpha[j])&lt;<span class="number">0.00001</span>:</span><br><span class="line">print(<span class="string">"alphaJold-alpha[j])&lt;0.00001"</span>)</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#优化j有变化，接着优化i</span></span><br><span class="line">oS.alpha[i]+=oS.lableMat[j]*oS.lableMat[i]*(alphaJold-oS.alpha[j])</span><br><span class="line"></span><br><span class="line"><span class="comment">#将i的更新存到缓存</span></span><br><span class="line">updateEK(oS,i)</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置偏置量</span></span><br><span class="line"><span class="comment">#b1=oS.b-exi-oS.lableMat[i]*(oS.alpha[i]-alphaIold)*oS.dataMat[i,:]*oS.dataMat[i,:].T-oS.lableMat[j]*(oS.alpha[j]-alphaJold)*oS.dataMat[i,:]*oS.dataMat[j,:].T</span></span><br><span class="line"><span class="comment">#b2=oS.b-exj-oS.lableMat[i]*(oS.alpha[i]-alphaIold)*oS.dataMat[i,:]*oS.dataMat[j,:].T-oS.lableMat[j]*(oS.alpha[j]-alphaJold)*oS.dataMat[j,:]*oS.dataMat[j,:].T</span></span><br><span class="line"><span class="comment">#使用核函数之后，修正b1,b2的值</span></span><br><span class="line">b1=oS.b-exi-oS.lableMat[i]*(oS.alpha[i]-alphaIold)*oS.k[i,i]-oS.lableMat[j]*(oS.alpha[j]-alphaJold)*oS.k[i,j]</span><br><span class="line">b2=oS.b-exj-oS.lableMat[i]*(oS.alpha[i]-alphaIold)*oS.k[i,j]-oS.lableMat[j]*(oS.alpha[j]-alphaJold)*oS.k[j,j]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> oS.alpha[i]&gt;<span class="number">0</span> <span class="keyword">and</span> oS.alpha[i]&lt;oS.C:</span><br><span class="line">oS.b=b1</span><br><span class="line"><span class="keyword">elif</span> oS.alpha[j]&gt;<span class="number">0</span> <span class="keyword">and</span> oS.alpha[j]&lt;oS.C:</span><br><span class="line">oS.b=b2</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">oS.b=(b1+b2)/<span class="number">2.0</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#完整的platt smo 外循环</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">smoP</span><span class="params">(dataMatIn,labelArr,C,toler,maxIter,kTup=<span class="params">(<span class="string">"lin"</span>,<span class="number">0</span>)</span>)</span>:</span></span><br><span class="line"><span class="comment">#0.相关变量初始化</span></span><br><span class="line">oS=optStruct(mat(dataMatIn),mat(labelArr).transpose(),C,toler,kTup)</span><br><span class="line"></span><br><span class="line">entireSet=<span class="literal">True</span></span><br><span class="line">alphaChange=<span class="number">0</span></span><br><span class="line">iter=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#1.主循环</span></span><br><span class="line"><span class="keyword">while</span> (iter&lt;maxIter) <span class="keyword">and</span> ((alphaChange&gt;<span class="number">0</span>) <span class="keyword">or</span> (entireSet)):</span><br><span class="line">alphaChange=<span class="number">0</span></span><br><span class="line"><span class="comment">#1.选择第一个alpha的两种循环</span></span><br><span class="line"><span class="comment">#1.0对所有数据集进行遍历</span></span><br><span class="line"><span class="keyword">if</span>(entireSet):</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(oS.m):</span><br><span class="line">alphaChange+=innerI(oS,i)</span><br><span class="line">print(<span class="string">"fullset: iter:"</span>+str(iter)+<span class="string">" i:"</span>+str(i)+<span class="string">" alphaChange:"</span>+str(alphaChange))</span><br><span class="line">iter+=<span class="number">1</span></span><br><span class="line"><span class="comment">#1.1对非边界值进行遍历</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">nonBoundIs=nonzero((oS.alpha.A&gt;<span class="number">0</span>)*(oS.alpha.A&lt;C))[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> nonBoundIs:</span><br><span class="line">alphaChange+=innerI(oS,i)</span><br><span class="line">print(<span class="string">"nonbound: iter:"</span>+str(iter)+<span class="string">" i:"</span>+str(i)+<span class="string">" alphaChange:"</span>+str(alphaChange))</span><br><span class="line"></span><br><span class="line">iter+=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(entireSet):</span><br><span class="line">entireSet=<span class="literal">False</span></span><br><span class="line"><span class="keyword">elif</span>(alphaChange==<span class="number">0</span>):</span><br><span class="line">entireSet=<span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> oS.alpha,oS.b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#利用求得的alpha值求得分类的超平面，其中可以求出w的值</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcW</span><span class="params">(alpha,dataMatIn,labelArr)</span>:</span></span><br><span class="line">dataMat=mat(dataMatIn)</span><br><span class="line">lableMat=mat(labelArr).transpose()</span><br><span class="line">m,n=shape(dataMat)</span><br><span class="line">w=zeros((n,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line"><span class="comment">#其中起作用的只有支持向量，即不为0的alpha</span></span><br><span class="line">w+=multiply((alpha[i]*lableMat[i]),dataMat[i,:].T)</span><br><span class="line"><span class="keyword">return</span> w</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#在复杂数据上使用核函数</span></span><br><span class="line"><span class="comment">#将数据从一个特征空间映射到另一个空间</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kernelTras</span><span class="params">(X,A,kTup)</span>:</span></span><br><span class="line">m,n=shape(X)</span><br><span class="line">k=zeros((m,<span class="number">1</span>))</span><br><span class="line"><span class="comment">#根据ktup的第一个参数来求核函数的值</span></span><br><span class="line"><span class="keyword">if</span>(kTup[<span class="number">0</span>]==<span class="string">"lin"</span>):</span><br><span class="line">k=X*A.T</span><br><span class="line"><span class="keyword">elif</span>(kTup[<span class="number">0</span>]==<span class="string">"rbf"</span>):</span><br><span class="line"><span class="comment">#对每个元素计算高斯值</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(m):</span><br><span class="line">deltaRow=X[j,:]-A</span><br><span class="line">k[j]=deltaRow*deltaRow.T</span><br><span class="line">k=exp(k/(<span class="number">-1</span>*kTup[<span class="number">1</span>]**<span class="number">2</span>))</span><br><span class="line"><span class="comment">#若元祖无法识别，则抛出异常</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">raise</span> NameError(<span class="string">"the keneral name wrong"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> k</span><br><span class="line"></span><br><span class="line"><span class="comment">#识别手写字——使用svm比knn效率高，占内存小，因为svm只需用到支持向量来进行分类</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#1.将图像转换为向量——将32*32的图像转换为1*1024的向量</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">img2vector</span><span class="params">(filename)</span>:</span></span><br><span class="line"><span class="comment">#0.构造转换后返回的向量</span></span><br><span class="line">returnVect=zeros((<span class="number">1</span>,<span class="number">1024</span>))</span><br><span class="line"><span class="comment">#1.处理图像</span></span><br><span class="line">fr=open(filename)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">32</span>):<span class="comment">#因为图像是32行的</span></span><br><span class="line">lineArr=fr.readline() <span class="comment">#每次读入一行</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">32</span>):<span class="comment">#因为图像是32列的</span></span><br><span class="line">returnVect[<span class="number">0</span>,<span class="number">32</span>*i+j]=int(lineArr[j])</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> returnVect</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.从文件夹中读入所有图片信息并存储为向量和标签——,为方便，只保留了1和9两个数字，出现1的标签记为1，出现9的标签记为-1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadImage</span><span class="params">(dirname)</span>:</span></span><br><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> listdir</span><br><span class="line"><span class="comment">#0.用于存储标签</span></span><br><span class="line">hwLabels=[]</span><br><span class="line"></span><br><span class="line"><span class="comment">#1.用于存储数据向量</span></span><br><span class="line"><span class="comment">#得到文件夹下图像名称的列表 例：1_20.txt</span></span><br><span class="line">trainDirList=listdir(dirname)</span><br><span class="line">m=len(trainDirList)</span><br><span class="line"><span class="comment">#用于存储数据向量</span></span><br><span class="line">trainMat=zeros((m,<span class="number">1024</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.处理图像文件中的内容</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line"><span class="comment">#图像文件完整名称</span></span><br><span class="line">fullName=trainDirList[i]</span><br><span class="line"><span class="comment">#图像文件去除后缀后的名称</span></span><br><span class="line">fileName=fullName.split(<span class="string">"."</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="comment">#图像文件去除下划线后的名称</span></span><br><span class="line">className=int(fileName.split(<span class="string">"_"</span>)[<span class="number">0</span>])</span><br><span class="line"><span class="comment">#为方便，只保留了1和9两个数字，出现1的标签记为1，出现9的标签记为-1</span></span><br><span class="line"><span class="keyword">if</span>(className==<span class="number">1</span>):</span><br><span class="line">hwLabels.append(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">hwLabels.append(<span class="number">-1</span>)</span><br><span class="line"><span class="comment">#将图像文件中的内容转换为向量进行存储</span></span><br><span class="line">trainMat[i,:]=img2vector(<span class="string">"%s/%s"</span>%(dirname,fullName))</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> trainMat,hwLabels</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#3.使用核函数进行分类测试的径向基函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testDigits</span><span class="params">(kTup=<span class="params">(<span class="string">"rbf"</span>,<span class="number">10</span>)</span>)</span>:</span></span><br><span class="line">dataMatIn,labelArr=loadImage(<span class="string">"trainingDigits"</span>)</span><br><span class="line">dataMat=mat(dataMatIn)</span><br><span class="line">lableMat=mat(labelArr).transpose()</span><br><span class="line">m,n=shape(dataMat)</span><br><span class="line"><span class="comment">#根据platt smo算法得到alpha,b的值</span></span><br><span class="line">alpha,b=smoP(dataMatIn,labelArr,<span class="number">200</span>,<span class="number">0.0001</span>,<span class="number">10000</span>,kTup)</span><br><span class="line">print(alpha[alpha&gt;<span class="number">0</span>])</span><br><span class="line">print(b)</span><br><span class="line"><span class="comment">#得到支持向量</span></span><br><span class="line">sVInd=nonzero(alpha.A&gt;<span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line">sVs=dataMat[sVInd]</span><br><span class="line">sVlabel=lableMat[sVInd]</span><br><span class="line">print(<span class="string">"tere are "</span>+str(shape(sVs)[<span class="number">0</span>])+<span class="string">" support vectors"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#在训练集上使用核函数</span></span><br><span class="line">error=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">kernel=kernelTras(sVs,dataMat[i,:],kTup)</span><br><span class="line">precdit=kernel.T*multiply(sVlabel,alpha[sVInd])+b</span><br><span class="line"><span class="keyword">if</span>(sign(precdit)!=sign(lableMat[i])):</span><br><span class="line">error+=<span class="number">1</span></span><br><span class="line">print(<span class="string">"train error rate: "</span>+str(float(error)/m)+<span class="string">"——————"</span>+str(error))</span><br><span class="line"></span><br><span class="line"><span class="comment">#在测试集上使用核函数</span></span><br><span class="line">dataMatIn,labelArr=loadImage(<span class="string">"testDigits"</span>)</span><br><span class="line">dataMat=mat(dataMatIn)</span><br><span class="line">lableMat=mat(labelArr).transpose()</span><br><span class="line">m=shape(dataMat)[<span class="number">0</span>]</span><br><span class="line">error=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">kernel=kernelTras(sVs,dataMat[i,:],kTup)</span><br><span class="line">precdit=kernel.T*multiply(sVlabel,alpha[sVInd])+b</span><br><span class="line"><span class="keyword">if</span>(sign(precdit)!=sign(lableMat[i])):</span><br><span class="line">error+=<span class="number">1</span></span><br><span class="line">print(<span class="string">"test error rate: "</span>+str(float(error)/m)+<span class="string">"——————"</span>+str(error))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;含义&quot;&gt;&lt;a href=&quot;#含义&quot; class=&quot;headerlink&quot; title=&quot;含义&quot;&gt;&lt;/a&gt;含义&lt;/h2&gt;&lt;p&gt;（1）支持向量：支持向量机用于解决分类问题，当给出一组线性可分的数据时，此时可以得出一条直线将数据分隔开，要求这条直线即求出了分类的依据，当根据距离分隔线最近的点，取其距离的最大值就能得到最优分割的直线。其中距离分隔线最近的点称为支持向量。&lt;/p&gt;&lt;p&gt;（2）机：机是指该方法是一个分类器，会产生二值决策机。&lt;/p&gt;&lt;p&gt;（3）优点：支持向量机方法只使用支持向量，并没有用全部的数据点，所以内存方面优于knn。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="分类" scheme="https://www.xiapf.com/tags/%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>Leecode算法学习笔记（四）——动态规划</title>
    <link href="https://www.xiapf.com/blogs/movStateNote/"/>
    <id>https://www.xiapf.com/blogs/movStateNote/</id>
    <published>2020-03-05T03:22:57.000Z</published>
    <updated>2020-04-13T07:33:32.893Z</updated>
    
    <content type="html"><![CDATA[<h2 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h2><h3 id="一般思路"><a href="#一般思路" class="headerlink" title="一般思路"></a>一般思路</h3><p>（1）明确状态含义</p><p>首先要明确dp[i]的含义，例如在求分割回文字符串的次数中，dp[i]表示i个回文字符串的分割次数</p><p>（2）确定初始状态</p><p>可以考虑最多…的时候dp的状态</p><p>例如求分割回文字符串的次数，初始状态即为i个字符串最多分割i次</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//求分割回文字符串的次数</span></span><br><span class="line"><span class="comment">//初始状态</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;len;i++)</span><br><span class="line">  dp[i]=i;</span><br><span class="line"><span class="comment">//i个字符串，最多分割i次</span></span><br></pre></td></tr></table></figure><a id="more"></a><p>（3）确定状态转移方程</p><p>假设已知dp[i]的状态，列出dp[i+1]和dp[i]之间的关系式，即为状态转移方程</p><p>明确目标：要求dp{i]的值</p><p>例如求分割回文字符串的次数，其中状态转移方程如下：</p><p>当i从1变化到len之间，求dp[i]。当0~i本身是回文时，不需要分割，即dp[i]=0。该题dp[i+1]和dp[i]之间没有明确联系，可以从分割边界考虑。假设分割边界为j，已知dp[j]，求dp[i]，就需要分两种情况考虑</p><p>a）j+1~i之间是回文，那么dp[j]再分割一次就可以得到j之前的字符串和后面的回文，即dp[i]=dp[j]+1，但dp[i]有可能更小，所以dp[i]在dp[i]和dp[j]+1之间取小的</p><p>b）j+1~i之间不是回文，继续移动j，找到符合回文的子串</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;len;i++)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">if</span>(<span class="number">0</span>~i是回文)</span><br><span class="line">    dp[i]=<span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;i;j++)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">if</span>(j+<span class="number">1</span>~i是回文)</span><br><span class="line">      dp[i]=<span class="built_in">min</span>(dp[i],dp[j]+<span class="number">1</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>（4）最终输出</p><p>一般输出为dp[len-1]，例如在求分割回文字符串的次数中，最终是求len个字符串的分割次数，即求dp[len-1]</p><h3 id="股票问题"><a href="#股票问题" class="headerlink" title="股票问题"></a>股票问题</h3><p>（1）明确状态含义</p><p>股票有三种状态：买入，卖出，休息（不买入也不卖出）    最终状态为持有股票设置为0，不持有股票1</p><p>转换关系如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200304222109.png" alt="stock"></p><p>设置dp[i] [k] [s]表示第天股票的持有情况，k表示至今最多进行k笔交易，整个数组表示可以得到的最大利润</p><p>（2）确定状态转移方程</p><p>转换关系中就两种状态，因此分别考虑两种情况下的转移方程：</p><p>a）今天持有股票，即求dp[i] [k] [1]</p><p>持有股票有两种情况，一种是前一天本来就有股票，并且今天休息，即dp[i-1] [k] [1]；另一种是前一天没有股票，今天购入了，此时前一天进行了k-1笔交易，买入股票，说明利润减少，需要减掉买入的价格,即dp[i-1] [k-1] [0]-price[i]</p><p>综上，求利润最大，即求两种情况的最大者，dp[i] [k] [1]=max(dp[i-1] [k] [1]，dp[i-1] [k-1] [0]-price[i])</p><p>b）今天不持有股票，即求dp[i] [k] [0]</p><p>不持有股票也有两种情况，一种是前一天本来就没有股票，并且今天休息，即dp[i-1] [k] [0]；另一种是前一天有股票，今天卖出了，卖出股票，说明利润增加，需要加上卖出的价格,即dp[i-1] [k] [1]+price[i]</p><p>综上，求利润最大，即求两种情况的最大者，dp[i] [k] [0]=max(dp[i-1] [k] [0]]，dp[i-1] [k] [1]+price[i])</p><p>综合a和b，可以得出状态转移方程</p><p>（3）确定初始状态</p><p>因为状态转移方程中只有两个最终状态，所以对两个最终状态的初始情况进行讨论：</p><p>a）刚开始没有持有股票时，没有利润，即dp[0] [0]=0</p><p>b）刚开四持有股票，之前没有利润，亏损买入的价格，即dp[0] [1]=-price[0]</p><p>（4）最终输出</p><p>最后一天利润是最大的，此时不持有股票的利润肯定大于持有的，即dp[len-1] [k] [0]&gt;dp[len-1] [k] [1]，所以选择dp[len-1] [k] [1]</p><p>（5）实际应用</p><p>a）给定一个数组，它的第 i 个元素是一支给定股票第 i 天的价格。如果你最多只允许完成一笔交易（即买入和卖出一支股票），设计一个算法来计算你所能获取的最大利润。注意你不能在买入股票前卖出股票。</p><p>此时k=1，代入方程得</p><p>dp[i] [1] [1]=max(dp[i-1] [1] [1]，dp[i-1] [0] [0]-price[i]) 其中dp[i-1] [0] [0]=0</p><p>dp[i] [1] [0]=max(dp[i-1] [1] [0]，dp[i-1] [1] [1]+price[i])</p><p>可以看出，方程中k=1对转换无影响，所以省去k</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//初始状态</span></span><br><span class="line">dp[<span class="number">0</span>][<span class="number">0</span>]=<span class="number">0</span>;</span><br><span class="line">dp[<span class="number">0</span>][<span class="number">1</span>]=-price[<span class="number">0</span>];</span><br><span class="line"><span class="comment">//转移方程</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;len;i++)</span><br><span class="line">&#123;</span><br><span class="line">  dp[i][<span class="number">1</span>]=<span class="built_in">max</span>(dp[i<span class="number">-1</span>][<span class="number">1</span>]，-price[i]);</span><br><span class="line">  dp[i][<span class="number">0</span>]=<span class="built_in">max</span>(dp[i<span class="number">-1</span>][<span class="number">0</span>]，dp[i<span class="number">-1</span>][<span class="number">1</span>]+price[i]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>b）给定一个数组，它的第 i 个元素是一支给定股票第 i 天的价格。设计一个算法来计算你所能获取的最大利润。你可以尽可能地完成更多的交易（多次买卖一支股票）。注意：你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。</p><p>此时k=无穷，那么k的值和k-1的值近似一样，因此k同样可以省略，此时的方程为</p><p>dp[i] [1]=max(dp[i-1] [1]，dp[i-1] [0]-price[i]) 其中dp[i-1] [0]不可以省略，因为其值已经不等于0</p><p>dp[i] [0]=max(dp[i-1] [0]，dp[i-1]  [1]+price[i])</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//初始状态</span></span><br><span class="line">dp[<span class="number">0</span>][<span class="number">0</span>]=<span class="number">0</span>;</span><br><span class="line">dp[<span class="number">0</span>][<span class="number">1</span>]=-price[<span class="number">0</span>];</span><br><span class="line"><span class="comment">//转移方程</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;len;i++)</span><br><span class="line">&#123;</span><br><span class="line">  dp[i][<span class="number">1</span>]=<span class="built_in">max</span>(dp[i<span class="number">-1</span>][<span class="number">1</span>]，dp[i<span class="number">-1</span>][<span class="number">0</span>]-price[i]);</span><br><span class="line">  dp[i][<span class="number">0</span>]=<span class="built_in">max</span>(dp[i<span class="number">-1</span>][<span class="number">0</span>]，dp[i<span class="number">-1</span>][<span class="number">1</span>]+price[i]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>c）给定一个数组，它的第 i 个元素是一支给定的股票在第 i 天的价格。设计一个算法来计算你所能获取的最大利润。你最多可以完成 两笔 交易。注意: 你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。</p><p>此时k从1变换到2，不为0是因为要有利润至少会有一笔交易，此时代码中将多一重循环</p><p>dp[i] [k] [1]=max(dp[i-1] [k] [1]，dp[i-1] [k-1] [0]-price[i]) 其中dp[i-1] [0] [0]=0</p><p>dp[i] [k] [0]=max(dp[i-1] [k] [0]，dp[i-1] [k] [1]+price[i])</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//转移方程</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;len;i++)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> k=<span class="number">1</span>;k&lt;=<span class="number">2</span>;k++)</span><br><span class="line">  &#123;</span><br><span class="line">     <span class="comment">//定义初始状态</span></span><br><span class="line">    <span class="keyword">if</span>(i==<span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="comment">//刚开始时，不持有股票，利润为0</span></span><br><span class="line">      dp[i][k][<span class="number">0</span>]=<span class="number">0</span>;</span><br><span class="line">      <span class="comment">//刚开始时，持有股票，利润为亏损买入对应的股票</span></span><br><span class="line">      dp[i][k][<span class="number">1</span>]=-price[i];</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    dp[i][k][<span class="number">1</span>]=<span class="built_in">max</span>(dp[i<span class="number">-1</span>][k][<span class="number">1</span>]，dp[i<span class="number">-1</span>][k<span class="number">-1</span>][<span class="number">0</span>]-price[i]);</span><br><span class="line">    dp[i][k][<span class="number">0</span>]=<span class="built_in">max</span>(dp[i<span class="number">-1</span>][k][<span class="number">0</span>]，dp[i<span class="number">-1</span>][k][<span class="number">1</span>]+price[i]);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h3><p>动态规划题：</p><p>一般都是当前状态和之前的状态有关，或者和其他的位置有关，这时候需要考虑动态规划。常见于字符串问题。</p><p>1：221. 最大正方形</p><p>（0）确定状态方程的含义</p><p>dp[i] [j]代表当前位置最大正方形的边长，明确一点当前位置的边长取决于左边，上边和斜对角，可以理解当前位置为右下角位置，这三个位置的最小值再加上1就是当前位置的边长。加上这三个位置的值一样，那么直接加1，也无需比较，但是假设有一个比较小，就说明有一边缺一个角，所以这时候需要取最小值。</p><p>初始边长设为-1，每次与当前位置边长比较，找到最大的边长</p><p>（1）初始状态：</p><p>第一行和第一列无法形成三角形，即自身的值就是当前正方形的边长</p><p>因为这里存储的是字符，所以需要通过“-’0‘”取得实际数值</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; dp(m,<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;(n));</span><br><span class="line">dp[<span class="number">0</span>][<span class="number">0</span>]=matrix[<span class="number">0</span>][<span class="number">0</span>]-<span class="string">'0'</span>;</span><br><span class="line">res=dp[<span class="number">0</span>][<span class="number">0</span>];</span><br><span class="line"><span class="comment">//第0行</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;i++)</span><br><span class="line">&#123;</span><br><span class="line">    dp[<span class="number">0</span>][i]=matrix[<span class="number">0</span>][i]-<span class="string">'0'</span>;</span><br><span class="line">    res=<span class="built_in">max</span>(res, dp[<span class="number">0</span>][i]);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//第0列</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;m;i++)</span><br><span class="line">&#123;</span><br><span class="line">    dp[i][<span class="number">0</span>]=matrix[i][<span class="number">0</span>]-<span class="string">'0'</span>;</span><br><span class="line">    res=<span class="built_in">max</span>(res, dp[i][<span class="number">0</span>]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>（2）状态转移方程</p><p>只有当前位置是1才更新边长，否则如果是’0‘，说明当前位置不可能有正方形形成</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//2.状态转移方程</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;m;i++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">1</span>;j&lt;n;j++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//注意输入的字符，不是数字</span></span><br><span class="line">        <span class="comment">//1.dp[i][j]表示右下角元素的最大正方形，当当前位置是1时，可以更新正方形大小,找上左斜对角的最小值再加一（因为当前位置是1）</span></span><br><span class="line">        <span class="keyword">if</span>(matrix[i][j]==<span class="string">'1'</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            dp[i][j]=<span class="built_in">min</span>(dp[i<span class="number">-1</span>][j],<span class="built_in">min</span>(dp[i<span class="number">-1</span>][j<span class="number">-1</span>], dp[i][j<span class="number">-1</span>]))+<span class="number">1</span>;</span><br><span class="line">            res=<span class="built_in">max</span>(res, dp[i][j]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//2.当前位置为0，那正方形大小为0</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            dp[i][j]=<span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;动态规划&quot;&gt;&lt;a href=&quot;#动态规划&quot; class=&quot;headerlink&quot; title=&quot;动态规划&quot;&gt;&lt;/a&gt;动态规划&lt;/h2&gt;&lt;h3 id=&quot;一般思路&quot;&gt;&lt;a href=&quot;#一般思路&quot; class=&quot;headerlink&quot; title=&quot;一般思路&quot;&gt;&lt;/a&gt;一般思路&lt;/h3&gt;&lt;p&gt;（1）明确状态含义&lt;/p&gt;&lt;p&gt;首先要明确dp[i]的含义，例如在求分割回文字符串的次数中，dp[i]表示i个回文字符串的分割次数&lt;/p&gt;&lt;p&gt;（2）确定初始状态&lt;/p&gt;&lt;p&gt;可以考虑最多…的时候dp的状态&lt;/p&gt;&lt;p&gt;例如求分割回文字符串的次数，初始状态即为i个字符串最多分割i次&lt;/p&gt;&lt;figure class=&quot;highlight c++&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;//求分割回文字符串的次数&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;//初始状态&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt;(&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; i=&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;;i&amp;lt;len;i++)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  dp[i]=i;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;//i个字符串，最多分割i次&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="算法" scheme="https://www.xiapf.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="https://www.xiapf.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="动态规划" scheme="https://www.xiapf.com/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    
  </entry>
  
  <entry>
    <title>mac下搭建和使用mve</title>
    <link href="https://www.xiapf.com/blogs/mve/"/>
    <id>https://www.xiapf.com/blogs/mve/</id>
    <published>2020-03-04T03:47:55.000Z</published>
    <updated>2020-09-09T04:34:49.957Z</updated>
    
    <content type="html"><![CDATA[<p>mve可以将多角度的图片序列重构成三维点云</p><h2 id="一、搭建mve"><a href="#一、搭建mve" class="headerlink" title="一、搭建mve"></a>一、搭建mve</h2><p>搭建过程参照官方文档<a href="https://github.com/simonfuhrmann/mve" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/simonfuhrmann/mve</a></p><p>以下均在终端进行操作</p><p>1.从github上下载代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/simonfuhrmann/mve.git</span><br></pre></td></tr></table></figure><p>下载之后会在根目录建立mve文件夹</p><a id="more"></a><p>2.进入mve文件夹</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd mve</span><br></pre></td></tr></table></figure><p>进行编辑</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make -j8</span><br></pre></td></tr></table></figure><p>3.添加依赖库</p><p>编译过程中报错，是因为缺少依赖库，需要手动安装。主要是对图片处理的函数库。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">brew install libjpeg</span><br><span class="line">brew install libpng</span><br><span class="line">brew install libtiff</span><br></pre></td></tr></table></figure><p>此时就能成功编译，打开mve下的apps文件夹，当所有的文件夹下生成了”.o”后缀的文件，说明环境已经搭建好。</p><h2 id="二、简单使用"><a href="#二、简单使用" class="headerlink" title="二、简单使用"></a>二、简单使用</h2><p>1.在官网下载数据集</p><p><a href="http://download.hrz.tu-darmstadt.de/media/FB20/GCC/mve_datasets/" target="_blank" rel="external nofollow noopener noreferrer">http://download.hrz.tu-darmstadt.de/media/FB20/GCC/mve_datasets/</a></p><p>我选择的是der_hass-20140923数据集，里面包含雕像不同角度的79张图像，下载后进行解压</p><p>2.设置环境变量</p><p>生成点云过程中是使用apps下中的子文件夹实现，所以需要设置环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#进入根目录</span><br><span class="line">cd ~</span><br><span class="line">#给任意一个文件添加环境变量</span><br><span class="line">vim ~/.bash_profile</span><br></pre></td></tr></table></figure><p>输入vim命令后，进入输入环境变量页面，输入“O”，进入写模式，将apps下所有文件夹路径加入环境变量中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export PATH=/Users/mve/apps/bundle2pset:$PATH</span><br><span class="line">#以下省略</span><br></pre></td></tr></table></figure><p>输入结束后，按esc，在文末输入”:wq”，退出并保存环境变量</p><p>生效环境变量，将环境变量应用到下载好的数据集下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bash_profile</span><br></pre></td></tr></table></figure><p>查看环境变量是否生效，在数据集文件夹下打开终端，输入：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo $PATH</span><br></pre></td></tr></table></figure><p>当环境变量中包含了apps下文件夹的路径，说明环境变量设置成功</p><p>3.重建三维生成点云</p><p>der_hass-20140923是下载的数据集，der_hass是生成数据所在的文件夹</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">makescene -i der_hass-20140923 der_hass</span><br></pre></td></tr></table></figure><p>按以下命令一个个运行，der_hass是生成的散文图所在的文件夹</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sfmrecon der_hass</span><br><span class="line">dmrecon -s2 der_hass</span><br><span class="line">scene2pset -F2 der_hass der_hass/pset-L2.ply</span><br><span class="line">fssrecon der_hass/pset-L2.ply der_hass/surface-L2.ply</span><br><span class="line">meshclean -t10 der_hass/surface-L2.ply der_hass/surface-L2-clean.ply</span><br></pre></td></tr></table></figure><p>运行过程比较慢，需要耐心等待，大约需3小时左右，最终生成的点云文件为surface-L2-clean.ply</p><p>4.效果展示</p><p>使用meshlab打开点云文件，效果如下</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200120193729.png" alt="效果图"></p><p>可以看出生成的点云完整，不存在不连续的部分，实现了从图像-&gt;稀疏点云-&gt;稠密点云-&gt;点云表面重建的全过程。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;mve可以将多角度的图片序列重构成三维点云&lt;/p&gt;&lt;h2 id=&quot;一、搭建mve&quot;&gt;&lt;a href=&quot;#一、搭建mve&quot; class=&quot;headerlink&quot; title=&quot;一、搭建mve&quot;&gt;&lt;/a&gt;一、搭建mve&lt;/h2&gt;&lt;p&gt;搭建过程参照官方文档&lt;a href=&quot;https://github.com/simonfuhrmann/mve&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;https://github.com/simonfuhrmann/mve&lt;/a&gt;&lt;/p&gt;&lt;p&gt;以下均在终端进行操作&lt;/p&gt;&lt;p&gt;1.从github上下载代码&lt;/p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;git clone https://github.com/simonfuhrmann/mve.git&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;下载之后会在根目录建立mve文件夹&lt;/p&gt;
    
    </summary>
    
    
      <category term="点云" scheme="https://www.xiapf.com/categories/%E7%82%B9%E4%BA%91/"/>
    
    
      <category term="mve" scheme="https://www.xiapf.com/tags/mve/"/>
    
      <category term="点云" scheme="https://www.xiapf.com/tags/%E7%82%B9%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>Leecode算法学习笔记（三）</title>
    <link href="https://www.xiapf.com/blogs/listbNote/"/>
    <id>https://www.xiapf.com/blogs/listbNote/</id>
    <published>2020-03-03T07:18:41.000Z</published>
    <updated>2020-03-03T07:20:20.057Z</updated>
    
    <content type="html"><![CDATA[<h2 id="关于链表的算法"><a href="#关于链表的算法" class="headerlink" title="关于链表的算法"></a>关于链表的算法</h2><p>链表节点的定义</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">ListNode</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">  <span class="keyword">int</span> val;</span><br><span class="line">  ListNode* next;</span><br><span class="line">  ListNode(<span class="keyword">int</span> x):val(x),next(<span class="literal">NULL</span>)&#123;&#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>链表常用算法思想包括：快慢指针（对空间复杂度有要求，可以考虑使用快慢指针）</p><a id="more"></a><h3 id="快慢指针"><a href="#快慢指针" class="headerlink" title="快慢指针"></a>快慢指针</h3><p>新建两个指针，快指针走两步，慢指针走一步</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//快慢指针法只使用两个指针，空间复杂度很小，只有O(1)</span></span><br><span class="line">ListNode* fast=head;<span class="comment">//快指针</span></span><br><span class="line">ListNode* slow=head;<span class="comment">//慢指针</span></span><br><span class="line"><span class="keyword">while</span>(fast!=<span class="literal">NULL</span>&amp;&amp;fast-&gt;next!=<span class="literal">NULL</span>)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="comment">//慢指针走一步</span></span><br><span class="line">  slow=slow-&gt;next;</span><br><span class="line">  <span class="comment">//快指针走两步</span></span><br><span class="line">  fast=fast-&gt;next-&gt;next;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>应用场景：</p><p>环形链表（判断链表中是否存在环）：</p><p>（1）判断有无环：可以利用快慢指针，如果存在环，那么在某一个链表节点快指针指向的节点就等于慢指针指向的节点（是节点整体相等，包括val和next，不能只用val判断），反之，则不存在环，整体问题抽象成追赶问题，因为存在环，所以一定能追赶上。</p><p>（2）求环的入口节点：当需要求环形入口节点的时候，首先将判断有无环，在相遇的时候break，此时假设链表中无环部分长度为a，环长度b，相遇时快指针比慢指针走了n个环长度，f=s+nb，又因为f=2s，所以s=nb，慢指针要走到环入口，需要长度为a+nb，所以慢指针需要再走a个长度，此时将fast设置为head，从head走到环入口长度正好为a，当两个指针指向相同链节点的时候，就到达了环的入口</p><h2 id="关于括号的算法"><a href="#关于括号的算法" class="headerlink" title="关于括号的算法"></a>关于括号的算法</h2><h3 id="栈"><a href="#栈" class="headerlink" title="栈"></a>栈</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">[<span class="comment">//用map存储括号的键值对</span></span><br><span class="line">unorder_map&lt;<span class="keyword">char</span>&gt; <span class="built_in">map</span>&#123;&#123;<span class="string">')'</span>,<span class="string">'('</span>&#125;,&#123;&#125;&#125;;</span><br><span class="line">]</span><br><span class="line"><span class="comment">//用栈存储读入的字符</span></span><br><span class="line"><span class="built_in">stack</span>&lt;<span class="keyword">char</span>&gt; s;</span><br><span class="line"><span class="keyword">while</span>(字符串不为空)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">if</span>(当字符是左半边括号时)</span><br><span class="line">    左括号入栈；</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">  &#123;</span><br><span class="line">    ...</span><br><span class="line">    [</span><br><span class="line">    <span class="comment">//取栈顶元素</span></span><br><span class="line">    q=s.top();</span><br><span class="line">    <span class="keyword">if</span>(栈顶元素==<span class="built_in">map</span>[q])</span><br><span class="line">      出栈</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">      未能匹配上</span><br><span class="line">    ]</span><br><span class="line">    [</span><br><span class="line">      <span class="comment">//先出栈，当出栈后栈为空，说明当前右括号前面没有匹配的左括号，此时右括号入栈作为一个位置标记，计算后面的长度，如果在else里出栈，那么会面临出栈后，s.top()不存在的现象</span></span><br><span class="line">      s.pop();</span><br><span class="line">      <span class="comment">//栈为空，存入当前括号</span></span><br><span class="line">      <span class="keyword">if</span>(s.empty())</span><br><span class="line">      s.push(i)</span><br><span class="line">      <span class="comment">//栈不为空，获得长度</span></span><br><span class="line">      <span class="keyword">else</span></span><br><span class="line">        length=<span class="built_in">max</span>(length,i-s.top())</span><br><span class="line">    ]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>应用场景：</p><p>有效的括号（给定一个只包括 <code>&#39;(&#39;</code>，<code>&#39;)&#39;</code>，<code>&#39;{&#39;</code>，<code>&#39;}&#39;</code>，<code>&#39;[&#39;</code>，<code>&#39;]&#39;</code> 的字符串，判断字符串是否有效。）：在map中存储括号的键值对，利用map[值]=键，每次读入字符串的字符时，如果是左边括号就入栈，否则就是右边括号，需要进行检测是否配对，取这时栈顶的括号，如果键值对匹配上，则出栈，检测下一个，否则是没有匹配上。</p><p>最长有效括号（给定一个只包含 <code>&#39;(&#39;</code> 和 <code>&#39;)&#39;</code> 的字符串，找出最长的包含有效括号的子串的长度。）：</p><p>（1）初始化：用栈来存储括号，栈里保存括号位置的下标，为了保证获取长度，初始化栈为-1。</p><p>（2）当扫描到左括号的时候，入栈，当扫描到右括号的时候，此时将栈顶元素出栈，分为两种情况，一种是当栈为空的时候，说明之前没有匹配的左括号，把当前括号入栈，另一种栈不为空，说明有匹配的左括号，用当前位置减去栈顶的位置，并更新长度</p><h3 id="深度遍历-剪枝"><a href="#深度遍历-剪枝" class="headerlink" title="深度遍历+剪枝"></a>深度遍历+剪枝</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//输入参数为当前由括号组成的路径，左子树括号的个数，右子树括号的个数，以及最终结果</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="built_in">string</span> str,<span class="keyword">int</span> left,<span class="keyword">int</span> right,<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&gt;&gt; res)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="comment">//结束的条件,此时出现了一条满足条件的路径:左右子树的括号个数都为0</span></span><br><span class="line">  <span class="keyword">if</span>(left==<span class="number">0</span>&amp;&amp;right==<span class="number">0</span>)</span><br><span class="line">    res.push(str)</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  <span class="comment">//剪枝：左子树的括号个数大于右子树的括号个数,出现不匹配</span></span><br><span class="line">  <span class="keyword">if</span>(left&gt;right)</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  <span class="comment">//左边括号多，先生成左括号</span></span><br><span class="line">  <span class="keyword">if</span>(left&gt;<span class="number">0</span>)</span><br><span class="line">    dfs(str+<span class="string">'('</span>,left<span class="number">-1</span>,right,res)</span><br><span class="line">  <span class="comment">//右边括号多，再生成右括号</span></span><br><span class="line">  <span class="keyword">if</span>(right&gt;<span class="number">0</span>)</span><br><span class="line">    dfs(str+<span class="string">'）'</span>,left<span class="number">-1</span>,right,res)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>应用场景：</p><p>括号的生成（给出 n 代表生成括号的对数，写出一个函数，使其能够生成所有可能的并且有效的括号组合）：实质是对括号做减法，当左括号数量大于0，就不断生成左括号，右括号数量大于0，生成右括号，但是存在不满足括号匹配的条件，当左括号数量大于右括号，此时需要剪枝（加一个判断条件，return）,当左右括号数量都为0，说明找到了一个合适的路径，将其存储下来。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;关于链表的算法&quot;&gt;&lt;a href=&quot;#关于链表的算法&quot; class=&quot;headerlink&quot; title=&quot;关于链表的算法&quot;&gt;&lt;/a&gt;关于链表的算法&lt;/h2&gt;&lt;p&gt;链表节点的定义&lt;/p&gt;&lt;figure class=&quot;highlight c++&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;ListNode&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&amp;#123;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; val;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  ListNode* next;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  ListNode(&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; x):val(x),next(&lt;span class=&quot;literal&quot;&gt;NULL&lt;/span&gt;)&amp;#123;&amp;#125;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;链表常用算法思想包括：快慢指针（对空间复杂度有要求，可以考虑使用快慢指针）&lt;/p&gt;
    
    </summary>
    
    
      <category term="算法" scheme="https://www.xiapf.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="https://www.xiapf.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="链表" scheme="https://www.xiapf.com/tags/%E9%93%BE%E8%A1%A8/"/>
    
      <category term="括号" scheme="https://www.xiapf.com/tags/%E6%8B%AC%E5%8F%B7/"/>
    
  </entry>
  
  <entry>
    <title>Leecode算法学习笔记（二）</title>
    <link href="https://www.xiapf.com/blogs/backTrackNote/"/>
    <id>https://www.xiapf.com/blogs/backTrackNote/</id>
    <published>2020-02-25T07:08:13.000Z</published>
    <updated>2020-03-04T04:56:30.496Z</updated>
    
    <content type="html"><![CDATA[<h2 id="回溯法"><a href="#回溯法" class="headerlink" title="回溯法"></a>回溯法</h2><p>回溯一般用于找到满足要求的的所有解，即相当于构造一棵树，从跟节点出发，找到满足条件的所有路径，并进行保存，当目前的路径不符合条件的时候，需要回溯，即剪枝，删除不符合的分支，当到达结束的条件时，即找到了一个满足条件的解，再继续构造下一个符合条件的路径。</p><p>常用于解决树种求得所有解的问题和排列组合中求解问题，或者问题可以转换为求树中一条路径的问题，可以画图。</p><a id="more"></a><h3 id="关于树的回溯"><a href="#关于树的回溯" class="headerlink" title="关于树的回溯"></a>关于树的回溯</h3><p>伪代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//result存储最终结果，res存储中间结果</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(TreeNode* root,<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&gt; result,<span class="built_in">vector</span> res，….)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   <span class="comment">//处理当前节点</span></span><br><span class="line">   res.push(root-&gt;val)</span><br><span class="line"></span><br><span class="line">   …</span><br><span class="line"></span><br><span class="line">  <span class="comment">//结束的条件</span></span><br><span class="line">  <span class="keyword">if</span>(满足题目要求（一般是值相等）并且到达根节点)</span><br><span class="line">  result.push(res)</span><br><span class="line">  <span class="keyword">if</span>(root-&gt;left)</span><br><span class="line">  &#123;</span><br><span class="line">    doTree(root-&gt;left,其他参数)</span><br><span class="line">    <span class="comment">//不满足条件，需要回溯</span></span><br><span class="line">    res.pop()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span>(root-&gt;right)</span><br><span class="line">  &#123;</span><br><span class="line">    doTree(root-&gt;right,其他参数)</span><br><span class="line">    <span class="comment">//不满足条件，需要回溯</span></span><br><span class="line">    res.pop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>应用场景：</p><p>找到所有满足目标节点的路径：每次把当前节点加入，并将目标值减去当前节点的值，判断当前叶子节点值（左右子树为空）与0是否相等，相等，则找到一个满足的结果，否则，继续对左右子树递归，此时若存在不满足条件的，需要回溯（pop）</p><h3 id="排列组合"><a href="#排列组合" class="headerlink" title="排列组合"></a>排列组合</h3><p>排列组合求所有符合问题的解，基本框架：</p><p>组合问题按顺序读取，不需要设置used数组，排列问题则需要设置</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="keyword">int</span> start,<span class="keyword">int</span> target)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="comment">//结束的条件：找到满足要求的一个解</span></span><br><span class="line">  <span class="keyword">if</span>(满足值为<span class="number">0</span>或者其他条件)</span><br><span class="line">  &#123;</span><br><span class="line">    result.push(res);</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//循环的条件</span></span><br><span class="line">  <span class="keyword">for</span>(i&lt;数组长度&amp;&amp;target-num[i]&gt;=<span class="number">0</span>)</span><br><span class="line">  &#123;</span><br><span class="line">    [</span><br><span class="line">      <span class="comment">//避免重复元素得出重复组合</span></span><br><span class="line">      <span class="keyword">if</span>(i&gt;start&amp;&amp;num[i]==num[i<span class="number">-1</span>])</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">    ]</span><br><span class="line">    [</span><br><span class="line">      <span class="comment">//全排列中不能使用上一层使用的元素</span></span><br><span class="line">      <span class="comment">//设置标志位或者其他判断条件</span></span><br><span class="line">      <span class="keyword">if</span>(!use[i])&#123;&#125;</span><br><span class="line">    ]</span><br><span class="line">    <span class="comment">//当时排列问题，并且有重复数字的时候，需要将上面两个条件结合</span></span><br><span class="line">    <span class="comment">//选择当前节点</span></span><br><span class="line">    res.push(num[i]);</span><br><span class="line">    [</span><br><span class="line">      <span class="comment">//选择了该节点，需要设置标志位</span></span><br><span class="line">      used[i]=<span class="literal">true</span>;</span><br><span class="line">    ]</span><br><span class="line">    <span class="comment">//[可以重复使用]</span></span><br><span class="line">    dfs(i,target-num[i]);</span><br><span class="line">    [<span class="comment">//不可以重复使用</span></span><br><span class="line">    [dfs(i+<span class="number">1</span>,target-num[i]);</span><br><span class="line">    ]</span><br><span class="line">    <span class="comment">//回溯，取消当前选择，返回上一层</span></span><br><span class="line">    res.pop();</span><br><span class="line">    [</span><br><span class="line">      <span class="comment">//撤销选择，需要还原标志位</span></span><br><span class="line">      used[i]=<span class="literal">true</span>;</span><br><span class="line">    ]</span><br><span class="line">      </span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p> 应用场景：</p><p>组合求和（数组元素内部不重复，在数组中找到和为目标值的组合）：首先对数组排序，排序能更方便去重。在dfs函数中，每次传入数组中数据的位置和目标值，当剩余值大于等于0，进入循环，将当前数组值存入路径中，再继续递归，做下一次决策，由于可重复，所以下一次可以仍然从i开始，当满足条件时，将当前路径存入结果，此时回溯到上一层，继续寻找合适的路径。</p><p>组合求和（数组元素内部可能有重复，在数组中找到和为目标值的组合，此时不能重复使用数组的元素，解集不能包含重复的组合）：思路同上，因为不能重复使用数组元素，所以在for循环中dfs函数传值只鞥从i+1开始。与此同时，由于解集不能包含重复的组合，所以在排序后数组，当前一个数组元素和当前数组元素相同，那么当前元素得出的路径和上一个元素必定相同，因此这时需要跳过当前元素，对下一个元素处理。</p><p>全排列（给定一个没有重复数字的序列，返回其所有可能的全排列）：排列题是上一次选择的节点，这次不能再选择。每一层都可以选择一个未出现的元素，需要对数组元素设置标志位，当使用过，就将标志位置为1，此时也需要在for循环中加一层判断，查看当前数组是否被使用，当使用过时，需要跳过该元素，下一层继续选择，找到一个合适的结果后存储下来，接着撤销本次选择，返回上一层查看是否有其他结果。</p><p>全排列（给定一个可包含重复数字的序列，返回所有不重复的全排列）：思路同上，此时需要额外加个判断，看是否有重复的数字，有重复的数字则跳过。具体为：本次节点与前一个相同，并且前一个已使用过，说明已有相同的排列，则本次排列跳过。</p><p>n皇后问题：转换为全排列问题，在每一个的n个位置中选一个作为当前选择，n行做n个选择，并且是做做不重复的选择，正好构成一个排列，此时的排列满足不在同一行不在同一列，只要对当前的排列需要再满足题设条件即可，即不在主对角线，不在副对角线，vector<bool> master1;//副对角线和相同，都为i+j i为行，j为列，两者相加和是当前行，vector<bool> master2;//主对角线差相同，为i-j+n-1 i为行，j为列，再设置两个标志位进行判断，最后将得到的排列用题目中格式打印出来作为结果。</bool></bool></p><p>子集（给定一组不含重复元素的整数数组 nums，返回该数组所有可能的子集（幂集））：对不同子集个数进行深度遍历，此时需要在主函数中加一个循环</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;nums.<span class="built_in">size</span>()+<span class="number">1</span>;i++)<span class="comment">//i的长度为0~nums.size(),所以需要加一</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="comment">//----通过i控制子集大小</span></span><br><span class="line">  dfs(start,i);</span><br><span class="line">  <span class="comment">//start是从数组中取元素的开始的位置，i是深度，即子集中元素的个数</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>分割字符串（给定一个字符串 <em>s</em>，将 <em>s</em> 分割成一些子串，使每个子串都是回文串）：每次判断当前的起始至末尾位置是否是回文，如果是就将结果压入res，继续递归。通过不断变换分割的位置，来构造决策树。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=start;i&lt;len;i++)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">if</span>(start~i之间不是回文)</span><br><span class="line">    继续找下一个回文的位置</span><br><span class="line">    </span><br><span class="line">   <span class="comment">//start~i之间是回文，压入结果内</span></span><br><span class="line">    res.push(s.substr(start,start+i+<span class="number">1</span>))</span><br><span class="line">    <span class="comment">//继续看下一个分割位置</span></span><br><span class="line">    dfs(i+<span class="number">1</span>,depth)</span><br><span class="line">    <span class="comment">//撤销选择，回到上一层决策</span></span><br><span class="line">    res.pop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;回溯法&quot;&gt;&lt;a href=&quot;#回溯法&quot; class=&quot;headerlink&quot; title=&quot;回溯法&quot;&gt;&lt;/a&gt;回溯法&lt;/h2&gt;&lt;p&gt;回溯一般用于找到满足要求的的所有解，即相当于构造一棵树，从跟节点出发，找到满足条件的所有路径，并进行保存，当目前的路径不符合条件的时候，需要回溯，即剪枝，删除不符合的分支，当到达结束的条件时，即找到了一个满足条件的解，再继续构造下一个符合条件的路径。&lt;/p&gt;&lt;p&gt;常用于解决树种求得所有解的问题和排列组合中求解问题，或者问题可以转换为求树中一条路径的问题，可以画图。&lt;/p&gt;
    
    </summary>
    
    
      <category term="算法" scheme="https://www.xiapf.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="https://www.xiapf.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="回溯" scheme="https://www.xiapf.com/tags/%E5%9B%9E%E6%BA%AF/"/>
    
  </entry>
  
  <entry>
    <title>Leecode算法学习笔记（一）</title>
    <link href="https://www.xiapf.com/blogs/treeNote/"/>
    <id>https://www.xiapf.com/blogs/treeNote/</id>
    <published>2020-02-24T08:56:00.000Z</published>
    <updated>2020-03-10T09:46:57.962Z</updated>
    
    <content type="html"><![CDATA[<h2 id="关于树的算法"><a href="#关于树的算法" class="headerlink" title="关于树的算法"></a>关于树的算法</h2><p>除了基本的遍历，其余关于树的处理基本都是依赖递归算法，总体思路，是对当前节点进行处理，再将剩余的交给左子树和右子树分别递归处理。</p><h3 id="层次遍历"><a href="#层次遍历" class="headerlink" title="层次遍历"></a>层次遍历</h3><p>队列：每次把同一层的节点入队，根据先进先出的特点，依次读取队首节点，再把当前节点的左右子树入队，作为下一层需要遍历的节点</p><p>伪代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//队列存储根节点</span></span><br><span class="line">p.push(root)</span><br><span class="line"><span class="keyword">while</span>(队列不为空)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="comment">//获取该层节点个数</span></span><br><span class="line">  <span class="built_in">width</span>=p.<span class="built_in">size</span>()</span><br><span class="line">  <span class="comment">//读取该层所有节点，并存储下一层节 点</span></span><br><span class="line">  <span class="keyword">for</span>(i&lt;<span class="built_in">width</span>)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="comment">//读取队首节点</span></span><br><span class="line">    r=p.front()</span><br><span class="line">    出队</span><br><span class="line">    <span class="keyword">if</span>(r的左子树存在) 左子树入队</span><br><span class="line">    <span class="keyword">if</span>(r的右子树存在) 右子树入队</span><br><span class="line">  &#125;</span><br><span class="line">  存储每层的结果</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><a id="more"></a><p>应用场景：</p><p>锯齿形层次遍历：需要把偶数行反序排列</p><p>求最大深度：每层入队时，增加一个层数</p><p>求最小深度（从根节点到最近叶子节点的最短路径上的节点数量）：每层遇到左右节点均为空的节点时，输出此时的层数</p><h3 id="前、中、后序遍历"><a href="#前、中、后序遍历" class="headerlink" title="前、中、后序遍历"></a>前、中、后序遍历</h3><p>a.前序 遍历特点：根-左-右</p><p>栈：每次先访问当前节点，并依次将右子树压入栈，再循环遍历左子树，根据先进后出的特点，循环结束后，下次将从最左边子树的右子树开始循环</p><p>伪代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//新建一个树节点</span></span><br><span class="line"></span><br><span class="line">p=root</span><br><span class="line"><span class="comment">//p不为空说明还有未遍历到的节点，s不为空说明栈里还有存储的子树</span></span><br><span class="line"><span class="keyword">while</span>(p不为空||s不为空)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">while</span>(p不为空)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="comment">//访问当前节点</span></span><br><span class="line">    res.push(p-&gt;val)</span><br><span class="line">    <span class="comment">//右子树入栈</span></span><br><span class="line">    s.push(p-&gt;right)</span><br><span class="line">    <span class="comment">//不断遍历左子树</span></span><br><span class="line">    p=p-&gt;left</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//处理下一个节点</span></span><br><span class="line">  p=s.top();</span><br><span class="line">  出栈</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>b.后序 遍历特点：左-右-根</p><p>栈：因为先对根节点访问较为简单，原理类似前序遍历，因此，可以先访问当前根节点，将左子树压入栈，再循环遍历右子树。循环结束，下一次是对最右边的左子树开始循环。形成结果是根-右-左，最后将结果逆序即可</p><p>伪代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//新建一个树节点</span></span><br><span class="line">p=root</span><br><span class="line"><span class="comment">//p不为空说明还有未遍历到的节点，s不为空说明栈里还有存储的子树</span></span><br><span class="line"><span class="keyword">while</span>(p不为空||s不为空)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">while</span>(p不为空)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="comment">//访问当前根节点</span></span><br><span class="line">    res.push(p-&gt;val)</span><br><span class="line">    <span class="comment">//左子树入栈</span></span><br><span class="line">    s.push(p-&gt;left)</span><br><span class="line">    <span class="comment">//遍历右子树</span></span><br><span class="line">    p=p-&gt;right</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//处理下一个节点</span></span><br><span class="line">  p=s.top;</span><br><span class="line">  出栈</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//将最终结果逆序</span></span><br><span class="line">reverse(res.<span class="built_in">begin</span>(),res.<span class="built_in">end</span>())</span><br></pre></td></tr></table></figure><p>c.中序 遍历特点：左-根-右</p><p>栈：中序处理方法和上两种有所不同，需要不断循环先找到最左边叶子节点，在此过程中需要不断把当前遍历到的根节点入栈，栈中存储根节点和左子树节点，当循环结束时，从栈尾中取出节点，再不断对该节点的右子树循环，</p><p>伪代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//新建一个树节点</span></span><br><span class="line">p=root</span><br><span class="line"><span class="comment">//p不为空说明还有未遍历到的节点，s不为空说明栈里还有存储的子树</span></span><br><span class="line"><span class="keyword">while</span>(p不为空||s不为空)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="comment">//循环过程中不断将根节点和左子树节点入栈</span></span><br><span class="line">  <span class="keyword">while</span>(p不为空)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="comment">//根节点入栈</span></span><br><span class="line">    s.push(p)</span><br><span class="line">    <span class="comment">//找到最左边的子树节点</span></span><br><span class="line">    p=p-&gt;left</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//访问当前栈尾节点，</span></span><br><span class="line">  p=s,top();</span><br><span class="line">  res.push(p-&gt;val)</span><br><span class="line">  <span class="comment">//从栈中取出根节点</span></span><br><span class="line">  出栈</span><br><span class="line">  <span class="comment">//循环右子树</span></span><br><span class="line">  p=p-&gt;right</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>应用场景：</p><p>验证一棵树是否是二叉搜索树（二叉搜索树的特点是左子树小于根节点，根节点小于右子树）：可以通过中序遍历比较节点值来判断</p><h3 id="递归"><a href="#递归" class="headerlink" title="递归"></a>递归</h3><p>总体框架：明确一个节点需要做的事，剩余的分别扔给左子树和右子树处理</p><p>伪代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">TreeNode* <span class="title">doTree</span><span class="params">(TreeNode* root)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="comment">//递归结束的条件：一个节点需要做的事（可能有多个结束条件）</span></span><br><span class="line">  <span class="keyword">if</span>(到达树的末尾)</span><br><span class="line">  <span class="keyword">do</span>….</span><br><span class="line">  <span class="comment">//递归循环的条件剩余的交给左右子树处理</span></span><br><span class="line">  doTree(root-&gt;left,其他参数)</span><br><span class="line">  doTree(root-&gt;right,其他参数)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​        </p><p>具体而言，可以分为以下几类：</p><p>根据前序、中序遍历或者中序、后序遍历构造二叉树：根据前序根节点在首位，可以在中序遍历确定左右子树；根据后序根节点在尾部，可以在中序遍历确定左右子树</p><p> 伪代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">TreeNode* <span class="title">getTree</span><span class="params">(<span class="built_in">vector</span> p, <span class="keyword">int</span> pl,<span class="keyword">int</span> pr, <span class="built_in">vector</span> i, <span class="keyword">int</span> il,<span class="keyword">int</span> ir)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="comment">//结束的条件</span></span><br><span class="line">  <span class="keyword">if</span>(左位置大于右位置)</span><br><span class="line">  <span class="keyword">return</span> null</span><br><span class="line">  <span class="comment">//循环的条件</span></span><br><span class="line">  <span class="comment">//根据前序或者后序遍历的特点找到根节点</span></span><br><span class="line">  pivot=p[pl]</span><br><span class="line">  <span class="comment">//根据根节点生成树节点</span></span><br><span class="line">  TreeNode root=<span class="keyword">new</span> TreeNode(pivot)</span><br><span class="line">  <span class="comment">//在中序遍历中找到根节点的位置</span></span><br><span class="line">  pivotindex=il</span><br><span class="line">  <span class="keyword">while</span>(i[pivotindex]!=pivot)</span><br><span class="line">  &#123;</span><br><span class="line">  pivotindex++</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//确定左子树</span></span><br><span class="line">  root-&gt;left= getTree(p,I,左子树的位置)</span><br><span class="line">  <span class="comment">//确定右子树</span></span><br><span class="line">  root-&gt;right= getTree(p,I,右子树的位置)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>伪代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">TreeNode* <span class="title">doTree</span><span class="params">(TreeNode* root)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">if</span>(递归到树的最后，即某一结束的条件)</span><br><span class="line">  ….</span><br><span class="line">  <span class="keyword">if</span>(满足题目要求（达到某一个值）&amp;&amp; doTree（root-&gt;left）&amp;&amp; doTree（root-&gt;right）(左右子树分别满足条件，此时递归，位置也可放到<span class="keyword">return</span>后面))</span><br><span class="line">    …</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>应用场景：</p><p>判断树的结构和值是否相同，即是否是相同的树：一个节点判断值是否相同，且左右子树的值和结构是否一致（调用函数，即递归）</p><p>对称二叉树：克隆一个数，和原来的树比较结构是否一致，思路同上</p><p>平衡二叉树（每个节点的左右子树的高度差绝对值小于1）：判断当前节点的高度，且左右子树的高度是否满足高度差小于1（调用函数，即递归）  注：里面包含求树的高度，可以找出左右子树高的那边再加1（也是使用递归）</p><p>判断一个树的路径和是否和目标值相同：每次判断当前叶子节点值（左右子树为空）与目标值是否相等，否则，将目标值减去当前节点值继续对左右子树递归</p><p>转换为二叉搜索树：需要利用中点，将原有数据进行排序,每次需要确定构造的数据的左右位置</p><p>伪代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">transBTS</span><span class="params">(<span class="built_in">vector</span> num，<span class="keyword">int</span> left,<span class="keyword">int</span> right)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="comment">//结束的条件</span></span><br><span class="line">  <span class="keyword">if</span>(left&gt;right)</span><br><span class="line">  <span class="keyword">return</span> null</span><br><span class="line">  <span class="comment">//找到中点,构造根节点</span></span><br><span class="line">  <span class="comment">//(1)数组可以直接利用索引的1/2</span></span><br><span class="line">  <span class="comment">//(2)链表需要利用快慢指针，快指针走两步，慢指针走一步，快指针到末尾的时候，慢指针正好到中点</span></span><br><span class="line">  mid=…</span><br><span class="line">  TreeNode* root=<span class="keyword">new</span> TreeNode(num(mid))</span><br><span class="line">  <span class="comment">//递归构造左子树</span></span><br><span class="line">  root-&gt;left= transBTS(root-&gt;left,left,mid<span class="number">-1</span>)</span><br><span class="line">  <span class="comment">//递归构造右子树</span></span><br><span class="line">  root-&gt;right= transBTS(root-&gt;right, mid<span class="number">-1</span>,right)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>关于二叉搜索树的框架：利用左小右大的特点（类似二分查找）</p><p>​    伪代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">TreeNode* <span class="title">BTS</span><span class="params">(TreeNode* root,<span class="keyword">int</span> num)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="comment">//结束条件</span></span><br><span class="line">  <span class="keyword">if</span>(root-&gt;val==num)</span><br><span class="line">  <span class="keyword">do</span>..(可以做增删改之类的操作)</span><br><span class="line">  <span class="comment">//循环条件</span></span><br><span class="line">  <span class="comment">//大于：对左子树</span></span><br><span class="line">  <span class="keyword">if</span>(root-&gt;val&gt;num)</span><br><span class="line">  <span class="keyword">return</span> BTS(root-&gt;left,num)</span><br><span class="line">  <span class="comment">//小于：对右子树</span></span><br><span class="line">  <span class="keyword">if</span>(root-&gt;val&lt;num)</span><br><span class="line">  <span class="keyword">return</span> BTS(root-&gt;right,num)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="求“树的高度”类型题"><a href="#求“树的高度”类型题" class="headerlink" title="求“树的高度”类型题"></a>求“树的高度”类型题</h3><p>1.求树的高度</p><p>采用递归，自底向上</p><p>找到最底层节点，每次返回当前节点的左子树高度和右子树高度的较大值，则当前节点的高度为max(left,right)+1，一直向上递归。如图所示的二叉树：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20191226110659.png" alt="二叉树"></p><p>从根节点递归，一直找到最左边的节点1，此时它的左右子树均为空，left=0,right=0,所以它的高度为1，接着返回上一层节点7的右子树4的高度，同理，节点4的高度为1，因此7的高度为max(depth(1),depth(4))+1，以此类推，一直向上找到根节点的高度即为树的高度。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">depth</span><span class="params">(TreeNode* root)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">if</span>(root==null)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//当前节点的左子树高度</span></span><br><span class="line">  <span class="keyword">int</span> l=depth(root-&gt;left);</span><br><span class="line">  <span class="comment">//当前节点的右子树高度</span></span><br><span class="line">  <span class="keyword">int</span> r=depth(root-&gt;left);</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//返回当前节点的高度为两者中大的那个再加一</span></span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">max</span>(l,r)+<span class="number">1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>2.求二叉树直径，即任意两个结点路径长度中的最大值</p><p>思路同求树的高度：任意两个节点之间的距离是当前节点左子树加上右子树的高度，因为是求最大路径和，则最大不可能出现在中间节点，只可能是任意两个叶子节点之间的距离，该距离可以通过两个叶子节点之间连线经过的根节点的左右子树的高度相加来求解，如图二叉树</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200310165729.jpg" alt="二叉树"></p><p>节点1和节点2之间的路径为3，即为经过的根节点3的左右子树的高度和，同理，姐弟啊1和节点9之间的距离为5，即为即为经过的根节点6的左右子树的高度和.</p><p>所以问题转换为：求二叉树上所有节点的左右子树高度和的最大值，因此，可以将“求二叉树高度”的代码稍微进行改动即可。</p><p>注：因为每次要比较各个节点的高度和，所以需要传入一个maxvalue的值(传入的时候要加上取地址符号)，每次进行比较，一直更新，最终得出最大路径和。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxTrack</span><span class="params">(TreeNode* root,<span class="keyword">int</span>&amp; maxValue)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">if</span>(root==null)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//求左右子树的高度</span></span><br><span class="line">  <span class="keyword">int</span> l=maxTrack(root-&gt;left,maxValue);</span><br><span class="line">  <span class="keyword">int</span> r=maxTrack(root-&gt;right,maxValue);</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//对当前的高度和即路径和进行比较</span></span><br><span class="line">  maxValue=<span class="built_in">max</span>(maxValue,l+r);</span><br><span class="line"></span><br><span class="line">  <span class="comment">//返回左右子树高度较大的那个</span></span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">max</span>(l,r)+<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最终得出maxValue为最大路径和，maxTrack函数的最终返回值是根节点左右子树的高度和。</p><p>3.求二叉树的最大路径和，即从任意一个节点出发，到另一个节点之间的路径和的最大值。</p><p>思路同求二叉树的高度：从根节点开始递归，一直找到最左边的节点，计算此时的节点的左右子树较大的路径和，将值传递给当前父节点，父节点将自身值加上该值，继续传值给上一层节点，上一层的父节点也是在左右子树的较大值之间进行选择。保证每次都是在当前节点的左右子树中选择大的值进行回传，这样的保证路径和往大的方向走。</p><p>如图二叉树</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20191226110659.png" alt="二叉树"></p><p>节点1的路径和是1,此时maxPath=0,更新为1，节点4的路径和是4，此时maxPath=1,更新为4，当求到节点7的路径和时，在它的左子树（节点1）和右子树（节点4）的路径和之间选择大的，即选择节点4，此时路径为7+4=11，此时maxPath=4,更新为11。再对节点3处理，左子树（11），右子树（2），选择左子树，此时路径和是11+3,此时maxPath=11,更新为14，以此类推，最终得到maxPath。</p><p>注：当当前左右子树加上当前节点值是负数，说明该条路径出现了“负贡献”，此时路径归0</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxPath</span><span class="params">(TreeNode* root,<span class="keyword">int</span>&amp; maxValue)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">if</span>(root==null)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//求左右子树的路径和</span></span><br><span class="line">  <span class="keyword">int</span> l=maxPath(root-&gt;left,maxValue);</span><br><span class="line">  <span class="keyword">int</span> r=maxPath(root-&gt;right,maxValue);</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//更新当前最大路径</span></span><br><span class="line">  maxValue=<span class="built_in">max</span>(maxValue,l+r+root-&gt;value);</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//判断路径有无出现负贡献</span></span><br><span class="line">  <span class="keyword">int</span> temp=<span class="built_in">max</span>(l,r)+root-&gt;val;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//每次选择左右子树中大的和自身相加！！！！</span></span><br><span class="line">  <span class="comment">//返回当前节点的路径和，作为上一次的子树值</span></span><br><span class="line">  <span class="keyword">return</span> temp&lt;<span class="number">0</span>?<span class="number">0</span>:temp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最终得到maxValue是最大路径和，maxPath返回值是根节点的左右子树加上自身的路径和。</p><h3 id="常规思路——bfs和dfs"><a href="#常规思路——bfs和dfs" class="headerlink" title="常规思路——bfs和dfs"></a>常规思路——bfs和dfs</h3><p>二叉树的题目大多数是遍历的变形题，往两个方向考虑：使用bfs——层次遍历还是dfs——深度遍历/回溯、递归，当使用非递归，即层次遍历的时候注意叶子节点的判断。</p><p>例如求二叉树从根节点出发的路径和，可以使用层次遍历，使用两个栈，一个用来存储每层的节点，另一个用来存储每层相加的和，当出现叶子节点的时候就把相加的和累加上去。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span>(栈不为空)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">int</span> <span class="built_in">width</span>=p.<span class="built_in">size</span>();</span><br><span class="line">  <span class="comment">//存储每层累加的和</span></span><br><span class="line">  <span class="keyword">int</span> val=<span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="built_in">width</span>;i++)</span><br><span class="line">  &#123;</span><br><span class="line">    r=p.front();</span><br><span class="line">    <span class="comment">//把当前节点的值加上上一次的和的10倍</span></span><br><span class="line">    val=numQue.front()*<span class="number">10</span>+r-&gt;val;<span class="comment">//numQue中存储每层节点累加的和</span></span><br><span class="line">    p.pop();</span><br><span class="line">    numQue.pop();</span><br><span class="line">    <span class="keyword">if</span>(叶子节点)</span><br><span class="line">      res+=val;</span><br><span class="line">    <span class="keyword">if</span>(左子树存在)</span><br><span class="line">    &#123;</span><br><span class="line">      p.push(左子树)</span><br><span class="line">      <span class="comment">//之前累加的和入栈</span></span><br><span class="line">      numQue.push(val);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(右子树存在)</span><br><span class="line">    &#123;</span><br><span class="line">      p.push(右<span class="number">2</span>子树)</span><br><span class="line">      <span class="comment">//之前累加的和入栈</span></span><br><span class="line">      numQue.push(val);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;关于树的算法&quot;&gt;&lt;a href=&quot;#关于树的算法&quot; class=&quot;headerlink&quot; title=&quot;关于树的算法&quot;&gt;&lt;/a&gt;关于树的算法&lt;/h2&gt;&lt;p&gt;除了基本的遍历，其余关于树的处理基本都是依赖递归算法，总体思路，是对当前节点进行处理，再将剩余的交给左子树和右子树分别递归处理。&lt;/p&gt;&lt;h3 id=&quot;层次遍历&quot;&gt;&lt;a href=&quot;#层次遍历&quot; class=&quot;headerlink&quot; title=&quot;层次遍历&quot;&gt;&lt;/a&gt;层次遍历&lt;/h3&gt;&lt;p&gt;队列：每次把同一层的节点入队，根据先进先出的特点，依次读取队首节点，再把当前节点的左右子树入队，作为下一层需要遍历的节点&lt;/p&gt;&lt;p&gt;伪代码：&lt;/p&gt;&lt;figure class=&quot;highlight c++&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;//队列存储根节点&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;p.push(root)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;while&lt;/span&gt;(队列不为空)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;comment&quot;&gt;//获取该层节点个数&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;built_in&quot;&gt;width&lt;/span&gt;=p.&lt;span class=&quot;built_in&quot;&gt;size&lt;/span&gt;()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;comment&quot;&gt;//读取该层所有节点，并存储下一层节 点&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt;(i&amp;lt;&lt;span class=&quot;built_in&quot;&gt;width&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;//读取队首节点&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    r=p.front()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    出队&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt;(r的左子树存在) 左子树入队&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt;(r的右子树存在) 右子树入队&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  存储每层的结果&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="算法" scheme="https://www.xiapf.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="https://www.xiapf.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="树" scheme="https://www.xiapf.com/tags/%E6%A0%91/"/>
    
  </entry>
  
  <entry>
    <title>排序算法</title>
    <link href="https://www.xiapf.com/blogs/sort/"/>
    <id>https://www.xiapf.com/blogs/sort/</id>
    <published>2019-12-26T06:27:48.000Z</published>
    <updated>2020-01-20T12:17:50.705Z</updated>
    
    <content type="html"><![CDATA[<p>几种排序算法的比较，对原始序列：6，3，5，7，2，9，8，1，4  进行排序</p><h6 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h6><p>第一次：从第一个数字开始，依次相邻比较，前面大于后面则调换位置，比到最后一位，则选出了最大值放在最后</p><p>​        第一次排序后：3，5，6，2，7，8，1，4，9</p><p>​        第二次排序后：3，5，2，6，7，1，4，8，9</p><a id="more"></a><p>​        。。。。。。。</p><p>时间复杂度：O(n^2)</p><h6 id="简单选择排序"><a href="#简单选择排序" class="headerlink" title="简单选择排序"></a>简单选择排序</h6><p>第一次：首先排第一个位置，先设min=0记录最小值位置，默认第一个位置，将6与3比较，3比6小，则min=1(代表3的位置)；再将3与5比较，3比5小，不改变min值；依次将min位置的数值依次与后续数字比较，比min小则更改min为更小值的位置，直到最后一个数字比完。此时min=7(1的位置)，将6与7对换，则第一次排序完成。</p><p>​        第一次排序后顺序是：1，3，5，7，2，9，8，6，4</p><p>第二次：排第二个位置，方法同上，设min=1代表默认第二个位置，即数值3默认最小值，将3依次与后续比较，找出最小值，min=最小值的位置，交换第二个位置的值3与min位置的值2，完成第二次排序。</p><p>​        第二次排序后：1，2，5，7，3，9，8，6，4</p><p>​        第三次排序后：1，2，3，7，5，9，8，6，4</p><p>​        。。。。。。。</p><p>时间复杂度：O(n^2)</p><p>​        第五次排序后顺序为：1，2，3，4，5，9，8，6，7</p><p>​        第六次排序后顺序为：1，2，3，4，5，6，8，9，7</p><p>​        第七次排序后顺序为：1，2，3，4，5，6，7，9，8</p><p>​        第八次排序后顺序为：1，2，3，4，5，6，7，8，9</p><h6 id="直接插入排序"><a href="#直接插入排序" class="headerlink" title="直接插入排序"></a>直接插入排序</h6><p>​        第一次排序后：6</p><p>​        第二次排序后：3，6</p><p>​        第三次排序后：3，5，6</p><p>​        。。。。。。。</p><p>时间复杂度：O(n^2)</p><h6 id="折半插入排序"><a href="#折半插入排序" class="headerlink" title="折半插入排序"></a>折半插入排序</h6><p>同直接插入排序，但是在比较的时候，是折半比较，思想同折半查找</p><p>时间复杂度：O(n^2)</p><h6 id="希尔排序"><a href="#希尔排序" class="headerlink" title="希尔排序"></a>希尔排序</h6><p>第一次：设步长d=5，则6和9；3和8；5和1；7和4；直接插入排序</p><p>​        第一次排序后：6，3，1，4，2，9，8，5，7</p><p>第二次：d = d/2 = 2，则6，1，2，8，7；3，4，9，5；直接插入排序</p><p>​        第二次排序后：1，3，2，4，6，5，7，9，8</p><p>第三次：d = d/2 = 1，则所有数据直接插入排序</p><p>​        第三次排序后：1，2，3，4，5，6，7，8，9</p><p>时间复杂度：O(nlog2^n)</p><h6 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h6><p>第一次：选取第一个数6为轴，将第一个6与最后一个数字4比较（从后往前比），如果大于比较的数字，则调换，然后与原位置后面开始比（从前往后比），以此类推，第一次排完后，轴6的左边都小于6，右边都大于6。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20191226160921.png" alt="快速排序i,j变化"></p><p>​        第一次排序后：4，3，5，1，2，6，8，9，7</p><p>第二次：则将上个比较后的轴的左右依次做快速排序，直到所有子表的表长不超过1</p><p>时间复杂度：O(nlog2^n)</p><h6 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h6><p>首先建立初始堆，按照顺序方式建立完全二叉树</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20191226110659.png" alt="二叉树"></p><p>然后建立大顶堆，分别将有子节点的7，3，5，6从底层开始，分别与子节点比较，选大的作为父节点，即可得到最大的根节点的二叉树，即大顶堆</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20191226140326.png" alt="大顶堆"></p><p>然后，将堆顶拿下来，将叶子节点放上去，再次按照大顶堆方法，得到根节点，以此类推，最终得到有序数列</p><p>时间复杂度：O(nlog2^n)</p><h6 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h6><p>对数组递归折半分割，直到分割成单个，然后递归合并比较，直到合并为整个数组</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20191226142419.png" alt="归并排序"></p><p>时间复杂度：O(nlog2^n)</p><p>总结：时间复杂度是nlog2^n的是：快些(希)归队(堆)，不稳定的是：快些(希)选队(堆)</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;几种排序算法的比较，对原始序列：6，3，5，7，2，9，8，1，4  进行排序&lt;/p&gt;&lt;h6 id=&quot;冒泡排序&quot;&gt;&lt;a href=&quot;#冒泡排序&quot; class=&quot;headerlink&quot; title=&quot;冒泡排序&quot;&gt;&lt;/a&gt;冒泡排序&lt;/h6&gt;&lt;p&gt;第一次：从第一个数字开始，依次相邻比较，前面大于后面则调换位置，比到最后一位，则选出了最大值放在最后&lt;/p&gt;&lt;p&gt;​        第一次排序后：3，5，6，2，7，8，1，4，9&lt;/p&gt;&lt;p&gt;​        第二次排序后：3，5，2，6，7，1，4，8，9&lt;/p&gt;
    
    </summary>
    
    
      <category term="算法" scheme="https://www.xiapf.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="https://www.xiapf.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="排序" scheme="https://www.xiapf.com/tags/%E6%8E%92%E5%BA%8F/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://www.xiapf.com/blogs/hello-world/"/>
    <id>https://www.xiapf.com/blogs/hello-world/</id>
    <published>2019-12-13T12:52:51.111Z</published>
    <updated>2019-12-13T12:52:51.111Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external nofollow noopener noreferrer">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external nofollow noopener noreferrer">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external nofollow noopener noreferrer">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external nofollow noopener noreferrer">GitHub</a>.</p><a id="more"></a><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external nofollow noopener noreferrer">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external nofollow noopener noreferrer">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external nofollow noopener noreferrer">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="external nofollow noopener noreferrer">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;documentation&lt;/a&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;a href=&quot;https://hexo.io/docs/troubleshooting.html&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;troubleshooting&lt;/a&gt; or you can ask me on &lt;a href=&quot;https://github.com/hexojs/hexo/issues&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>hexo部署时用户名密码设置</title>
    <link href="https://www.xiapf.com/blogs/hexo-deploy/"/>
    <id>https://www.xiapf.com/blogs/hexo-deploy/</id>
    <published>2019-12-13T06:48:37.000Z</published>
    <updated>2019-12-13T12:52:51.112Z</updated>
    
    <content type="html"><![CDATA[<p>hexo部署时用户名密码问题：</p><p>记住用户名密码，不用每次都输入用户名密码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --global credential.helper store</span><br></pre></td></tr></table></figure><p>清除用户名密码，防止用户名密码输入错误</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --system --<span class="built_in">unset</span> credential.helper</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;hexo部署时用户名密码问题：&lt;/p&gt;&lt;p&gt;记住用户名密码，不用每次都输入用户名密码&lt;/p&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/spa
      
    
    </summary>
    
    
      <category term="hexo" scheme="https://www.xiapf.com/categories/hexo/"/>
    
    
      <category term="hexo" scheme="https://www.xiapf.com/tags/hexo/"/>
    
      <category term="git" scheme="https://www.xiapf.com/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>docker简易教程</title>
    <link href="https://www.xiapf.com/blogs/docker/"/>
    <id>https://www.xiapf.com/blogs/docker/</id>
    <published>2019-12-12T05:44:29.000Z</published>
    <updated>2019-12-13T12:52:51.111Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Docker：虚拟化容器技术"><a href="#Docker：虚拟化容器技术" class="headerlink" title="Docker：虚拟化容器技术"></a>Docker：虚拟化容器技术</h2><p>三大组件：镜像，容器，仓库</p><pre><code>镜像：想象成系统iso或者ghost镜像容器：想象成一个系统环境仓库：想象成GitHub</code></pre><p>优点：隔离性   便捷性–移植和集群     轻量级   云支持</p><h2 id="安装Docker"><a href="#安装Docker" class="headerlink" title="安装Docker"></a>安装Docker</h2><p>Cenos7下安装docker</p><p>1.把yum包更新到最新</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum update</span><br></pre></td></tr></table></figure><a id="more"></a><p>2.安装需要的软件包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y yum-utils device-mapper-persistent-data lvm2</span><br></pre></td></tr></table></figure><p>3.设置yum源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo</span><br></pre></td></tr></table></figure><p>4.可以查看所有仓库中所有docker版本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum list docker-ce --showduplicates | sort -r</span><br></pre></td></tr></table></figure><p>5.安装Docker</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install docker-ce-17.12.1.ce</span><br></pre></td></tr></table></figure><p>6.启动Docker，加入开机启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl start docker</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> docker</span><br></pre></td></tr></table></figure><p>7.验证安装是否成功</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker version</span><br></pre></td></tr></table></figure><h2 id="编辑容器"><a href="#编辑容器" class="headerlink" title="编辑容器"></a>编辑容器</h2><p>从仓库中拉去一个centos7的镜像 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">doucker pull centos:7</span><br></pre></td></tr></table></figure><p>启动镜像生成容器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -i -t &lt;IMAGE ID&gt; bash</span><br></pre></td></tr></table></figure><p>进入容器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it &lt;CONTAINER ID&gt; bash</span><br></pre></td></tr></table></figure><p>此时就是进入了centos7的linux系统，可以在里安装jdk,tomcat等，然后部署war后，执行</p><p><code>curl localhost:端口号</code>    看看是否可以访问。</p><p>为了可以启动容器的时候，自动启动tomcat，可以写一个脚本，启动容器的时候，启动此脚本。<br>例如我在最根目录创建了runtomcat.sh的脚本<br>脚本内容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/soft/jdk1.5.0_22</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$PATH</span></span><br><span class="line">sh /soft/apache-tomcat-5.5.25/bin/catalina.sh run</span><br></pre></td></tr></table></figure><h2 id="制作镜像"><a href="#制作镜像" class="headerlink" title="制作镜像"></a>制作镜像</h2><p>将原来的容器制成镜像–docker commit containerid new_image:tag</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker commit centos7_base centos:7.1</span><br></pre></td></tr></table></figure><p>使用新的镜像制成容器–docker run -d -p 主机端口:容器端口/tcp –name 容器名 镜像名:tag  </p><p>并执行docker内脚本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -p 18080:8080 --name centos7_1 centos:7.1 /runtomcat.sh</span><br></pre></td></tr></table></figure><p>宿主机端口是18080映射了容器内的8080tomcat端口，现在直接访问宿主机的ip:18080/cvbs就可以访问容器的web服务了。<br>查看docker运行情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker ps -a</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20191212144010.png" alt="查看docker运行情况"></p><p>可以看到此时新的容器已经启动了</p><p>再次制成一个容器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -p 28080:8080 --name centos7_2 centos:7.1 /runtomcat.sh</span><br></pre></td></tr></table></figure><p>查看docker运行情况–<code>docker ps –a</code></p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20191212144100.png" alt="查看docker运行情况"></p><p>此时可以看到已经部署了两个服务了，分别映射在宿主机的18080和28080端口</p><p>外部浏览器访问：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20191212144117.png" alt="浏览器双端口访问"></p><p>可以看到两个端口都可以访问了</p><p>因为我是在虚拟机里安装，所以局域网中其他机器无法访问到此ip，</p><p>所以为了省略配置虚拟机对外暴露的ip等步骤，直接在主机中通过配置nginx做代理，配置如下</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20191212144141.png" alt="nginx配置"></p><p>此时外部访问即可<code>http://10.45.12.120/cvbs</code>（我的局域网内ip）即可访问到docker中部署两个的应用。</p><p>注意：因为有些项目没有做分布式session处理，所以简单使用ip_hash策略解决session问题</p><h2 id="IDEA中配置docker"><a href="#IDEA中配置docker" class="headerlink" title="IDEA中配置docker"></a>IDEA中配置docker</h2><p>1.在根目录写Dockerfile文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#指定基础镜像，在其上进行定制FROM java:8</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#这里的 /tmp 目录就会在运行时自动挂载为匿名卷，任何向 /data 中写入的信息都不会记录进容器存储层</span></span><br><span class="line">VOLUME /tmp</span><br><span class="line"></span><br><span class="line"><span class="comment">#复制上下文目录下的target/demo-1.0.0.jar 到容器里</span></span><br><span class="line">ADD target/spring-boot-0.0.1-SNAPSHOT.jar test.jar</span><br><span class="line"></span><br><span class="line"><span class="comment">#bash方式执行，使test.jar可访问</span></span><br><span class="line"><span class="comment">#RUN新建立一层，在其上执行这些命令，执行结束后， commit 这一层的修改，构成新的镜像。</span></span><br><span class="line">RUN bash -c <span class="string">"touch /test.jar"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#声明运行时容器提供服务端口，这只是一个声明，在运行时并不会因为这个声明应用就会开启这个端口的服务</span></span><br><span class="line">EXPOSE 8088</span><br><span class="line"></span><br><span class="line"><span class="comment">#指定容器启动程序及参数   &lt;ENTRYPOINT&gt; "&lt;CMD&gt;"</span></span><br><span class="line">ENTRYPOINT [<span class="string">"java"</span>,<span class="string">"-jar"</span>,<span class="string">"test.jar"</span>]</span><br></pre></td></tr></table></figure><p>2.在启动配置中配置dockerfile,image,container,port(宿主机和docker的端口映射)</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20191212144221.png" alt="配置dockerfile"></p><p>3.Maven打包</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20191212144237.png" alt="maven打包"></p><p>4.Docker部署</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20191212144250.png" alt="docker部署"></p><p>5.启动Docker</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20191212144304.png" alt="启动docker"></p><p>6.查看docker运行情况</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20191212144319.png" alt="查看docker运行情况3"></p><p>7.浏览器访问成功</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20191212144335.png" alt="浏览器访问"></p><h2 id="k8s"><a href="#k8s" class="headerlink" title="k8s"></a>k8s</h2><p>k8s是一个开源的容器集群管理系统，可以实现容器集群的自动化部署、自动扩缩容、维护等功能。</p><p>可以对各种虚拟化容器技术做统一的管理</p><p>实际项目中大量用到docker时，可以在研究k8s的实际使用。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Docker：虚拟化容器技术&quot;&gt;&lt;a href=&quot;#Docker：虚拟化容器技术&quot; class=&quot;headerlink&quot; title=&quot;Docker：虚拟化容器技术&quot;&gt;&lt;/a&gt;Docker：虚拟化容器技术&lt;/h2&gt;&lt;p&gt;三大组件：镜像，容器，仓库&lt;/p&gt;&lt;pre&gt;&lt;code&gt;镜像：想象成系统iso或者ghost镜像

容器：想象成一个系统环境

仓库：想象成GitHub&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;优点：隔离性   便捷性–移植和集群     轻量级   云支持&lt;/p&gt;&lt;h2 id=&quot;安装Docker&quot;&gt;&lt;a href=&quot;#安装Docker&quot; class=&quot;headerlink&quot; title=&quot;安装Docker&quot;&gt;&lt;/a&gt;安装Docker&lt;/h2&gt;&lt;p&gt;Cenos7下安装docker&lt;/p&gt;&lt;p&gt;1.把yum包更新到最新&lt;/p&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;yum update&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="docker" scheme="https://www.xiapf.com/categories/docker/"/>
    
    
      <category term="docker" scheme="https://www.xiapf.com/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>future与futuretask用法区别</title>
    <link href="https://www.xiapf.com/blogs/future-futuretask/"/>
    <id>https://www.xiapf.com/blogs/future-futuretask/</id>
    <published>2019-12-12T05:32:37.000Z</published>
    <updated>2019-12-13T12:52:51.111Z</updated>
    
    <content type="html"><![CDATA[<h6 id="Future和FutureTask方法区别，主要在于获取返回结果上。"><a href="#Future和FutureTask方法区别，主要在于获取返回结果上。" class="headerlink" title="Future和FutureTask方法区别，主要在于获取返回结果上。"></a>Future和FutureTask方法区别，主要在于获取返回结果上。</h6><p>Future方式：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.concurrent.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test7</span> <span class="keyword">implements</span> <span class="title">Callable</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"1"</span>);</span><br><span class="line">        ExecutorService executor = <span class="keyword">new</span> ThreadPoolExecutor(<span class="number">3</span>,<span class="number">3</span>,<span class="number">0</span>,TimeUnit.SECONDS,<span class="keyword">new</span> LinkedBlockingQueue&lt;Runnable&gt;());</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Future&lt;?&gt; submit = executor.submit(<span class="keyword">new</span> Test7());</span><br><span class="line">            Object o = submit.get();</span><br><span class="line">            System.out.println(o);</span><br><span class="line">        &#125;<span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            System.out.println(<span class="number">3</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        executor.shutdown();</span><br><span class="line">        System.out.println(<span class="string">"4"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">test</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span> i = <span class="number">1</span>/<span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">call</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        test();</span><br><span class="line">        System.out.println(<span class="number">5</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Integer(<span class="number">6</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><a id="more"></a><p>submit里放的是实现callable接口的类，通过返回值submit.get()，获取返回结果6，如果test抛出异常，此时，主线程会捕获，打印3，不调用submit.get()，将不会捕获异常。</p><p>FutureTask方式：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.concurrent.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test8</span> <span class="keyword">implements</span> <span class="title">Callable</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"1"</span>);</span><br><span class="line">        FutureTask ft = <span class="keyword">new</span> FutureTask(<span class="keyword">new</span> Test8());</span><br><span class="line">        ExecutorService executor = <span class="keyword">new</span> ThreadPoolExecutor(<span class="number">3</span>,<span class="number">3</span>,<span class="number">0</span>,TimeUnit.SECONDS,<span class="keyword">new</span> LinkedBlockingQueue&lt;Runnable&gt;());</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Future&lt;?&gt; submit = executor.submit(ft);</span><br><span class="line">            Object o = submit.get();</span><br><span class="line">            System.out.println(o);</span><br><span class="line">            Object o1 = ft.get();</span><br><span class="line">            System.out.println(o1);</span><br><span class="line">        &#125;<span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            System.out.println(<span class="number">3</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        executor.shutdown();</span><br><span class="line">        System.out.println(<span class="string">"4"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">test</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span> i = <span class="number">1</span>/<span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">call</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        test();</span><br><span class="line">        System.out.println(<span class="number">5</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Integer(<span class="number">6</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>executor里放的是futuretask，futuretask包装了一层callable对象，此时要获取返回结果需要使用futuretask的get()方法，不能使用submit.get(),submit.get()会返回null;只用ft.get()会捕获异常。</p>]]></content>
    
    <summary type="html">
    
      &lt;h6 id=&quot;Future和FutureTask方法区别，主要在于获取返回结果上。&quot;&gt;&lt;a href=&quot;#Future和FutureTask方法区别，主要在于获取返回结果上。&quot; class=&quot;headerlink&quot; title=&quot;Future和FutureTask方法区别，主要在于获取返回结果上。&quot;&gt;&lt;/a&gt;Future和FutureTask方法区别，主要在于获取返回结果上。&lt;/h6&gt;&lt;p&gt;Future方式：&lt;/p&gt;&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; java.util.concurrent.*;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Test7&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;implements&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Callable&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(String[] args)&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        System.out.println(&lt;span class=&quot;string&quot;&gt;&quot;1&quot;&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ExecutorService executor = &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; ThreadPoolExecutor(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;,TimeUnit.SECONDS,&lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; LinkedBlockingQueue&amp;lt;Runnable&amp;gt;());&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;try&lt;/span&gt; &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            Future&amp;lt;?&amp;gt; submit = executor.submit(&lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; Test7());&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            Object o = submit.get();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            System.out.println(o);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &amp;#125;&lt;span class=&quot;keyword&quot;&gt;catch&lt;/span&gt; (Exception e) &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            System.out.println(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        executor.shutdown();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        System.out.println(&lt;span class=&quot;string&quot;&gt;&quot;4&quot;&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt;&lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; i = &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;/&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; Object &lt;span class=&quot;title&quot;&gt;call&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;throws&lt;/span&gt; Exception &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        test();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        System.out.println(&lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; Integer(&lt;span class=&quot;number&quot;&gt;6&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="java" scheme="https://www.xiapf.com/categories/java/"/>
    
    
      <category term="java" scheme="https://www.xiapf.com/tags/java/"/>
    
      <category term="多线程" scheme="https://www.xiapf.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"/>
    
      <category term="future" scheme="https://www.xiapf.com/tags/future/"/>
    
  </entry>
  
  <entry>
    <title>用bp网络预测绿萝叶片面积</title>
    <link href="https://www.xiapf.com/blogs/bp-net/"/>
    <id>https://www.xiapf.com/blogs/bp-net/</id>
    <published>2019-12-11T02:46:04.000Z</published>
    <updated>2020-10-27T06:06:05.642Z</updated>
    
    <content type="html"><![CDATA[<h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><blockquote><p>测量绿萝叶片使用kinect相机对图像做标定，将图像映射到坐标系中，使用matlab程序计算叶片整体占图像的像素从而计算面积，对绿萝每层的叶片都需要分次进行标定求解过程较为复杂，因此针对这种情况，采用bp网络来预测绿萝叶片面积。</p></blockquote><hr><h1 id="数据采集"><a href="#数据采集" class="headerlink" title="数据采集"></a>数据采集</h1><p><em>测量9盆绿萝叶片，实测数据共557片。训练集1-7盆，共449片，测试集8-9盆，共108片。<br>数据集中共四个属性，分别是宽度为w，长边为L1,短边为L2，面积area。</em><br><strong>其中宽度，长边，短边为手动实测，面积根据叶片像素点占图片整体像素点计算得出。</strong></p><a id="more"></a><ul><li>计算面积的MATLAB程序<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">%% 分割</span><br><span class="line">k=graythresh(I);              %得到最优阈值</span><br><span class="line">j=im2bw(I,k);                  %转换成二值图，k为分割阈值</span><br><span class="line">%imshow(j); </span><br><span class="line">f = bwmorph(j,&apos;open&apos;);  %开运算</span><br><span class="line">figure, imshow(f)</span><br><span class="line">%% 像素点统计</span><br><span class="line">[m,n]=size(f);</span><br><span class="line">k=0;</span><br><span class="line">for i=1:1:m</span><br><span class="line">    for j=1:1:n</span><br><span class="line">        if f(i,j)==0</span><br><span class="line">            k=k+1;</span><br><span class="line">        end</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line">p=k/(m*n);</span><br><span class="line">s=21*29.7*p;</span><br></pre></td></tr></table></figure></li></ul><hr><h1 id="bp网络设计"><a href="#bp网络设计" class="headerlink" title="bp网络设计"></a>bp网络设计</h1><p>bp网络示意图及各变量含义<br><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/bp%E7%BD%91%E7%BB%9C%E7%A4%BA%E6%84%8F%E5%9B%BE%E5%8F%8A%E5%90%84%E5%8F%98%E9%87%8F%E5%90%AB%E4%B9%89.png" alt="bp结构图片"></p><ol><li><p>结构选取<br>a.隐含层数量选取：单隐层结构选择，当仅有一层隐含层时，测试数据正确率已达100%，为降低网络复杂度，所以选择单隐层结构。<br>b.隐含层神经元个数选取</p><table><thead><tr><th align="left">隐含层神经元个数</th><th align="left">达到最小误差时需要迭代的次数</th></tr></thead><tbody><tr><td align="left">5</td><td align="left">1873</td></tr><tr><td align="left">10</td><td align="left">2009</td></tr><tr><td align="left">15</td><td align="left">2259</td></tr></tbody></table><p>根据程序运行来看，当隐藏层神经元个数选择过小（小于5）收敛速度很慢，当到达最大迭代次数时，误差仍很大，当个数选择过大（大于15），网络出现振荡，因此，考虑隐藏层神经元个数在5-15之间，因为本实验数据集属性少，因此选择隐藏层神经元个数为5个，减少复杂度。<br> c.误差函数选择均方差公式：$[    {E_{k} } =1/2*( \hat{y}-y) ]$<br> d.结束条件：<br> 训练结束条件：当误差函数值小于1e-3或者迭代次数大于50000次时结束训练<br> 测试误差判断：当测试集输出面积和实际面积误差函数值大于1e-5时，预测错误，反之预测正确。</p></li><li><p>初始值设置<br>a.权值矩阵的初始值<br>产生0-1之间的随机数作为权值的初始值<br>代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">srand(time(NULL));//设置随机数种子，使每次产生的随机序列不同</span><br><span class="line">for (int i = 0; i &lt; n; i++)</span><br><span class="line">    w[i] = rand() % (N + 1) / (float)(N + 1);//N为设置的精度</span><br></pre></td></tr></table></figure><p>b.学习率的初始值<br>学习率控制着算法每一轮的迭代的更新步长，若太大则容易振荡，太小则收敛速度又过慢因此需要选择适合的初始值。<br><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/bp%E5%AD%A6%E4%B9%A0%E7%8E%87%E5%8F%98%E5%8C%96.png" alt="学习率变化图片"><br>由图可以看出，当学习率小于1时，网络训练次数多集中在1000以上，但当学习率大于5时，测试集数据会出现错误，因此选择训练次数少并且误差低的学习率=5.</p></li><li><p>数据归一化<br>前三列属性取值∈[2,9]，因此采用对数函数y=log10(x) 以10为底的对数函数转换。<br>最后一列属性取值∈[10,100]，因此采用反余弦函数y=atan(x)*2/PI，保证输入的数据在0-1之间，让网络更快的收敛。</p></li><li><p>前向传播和反向传播<br>a.前向传播<br>激活函数选择sigmod函数，第i个输入层神经元到第h个隐藏层神经元的权值为Vih,第h个隐藏层神经元输入为α=∑Vih<em>Xi，输出为bh;第h个输入层神经元到第j个隐藏层神经元的权值为Whj,第j个输出层神经元的输入β=∑Whj</em>bh,输出为<br>求出激活值，代入sigmod函数中，求得输出值<br>代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">for(int j=0;j&lt;hideSum;j++)</span><br><span class="line">&#123;</span><br><span class="line">   o1[j]=0.0;</span><br><span class="line">   for(int i=0;i&lt;inSum;i++)</span><br><span class="line">   //激活值</span><br><span class="line">   o1[j]=o1[j]+w[i][j]*x[i];</span><br><span class="line">   //实际输出</span><br><span class="line">   x1[j]=1.0/(1+exp(-o1[j]-b1[j])); //b1为偏置量</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>b.反向传播<br>网络在（Xk,Yk)上的均方差为，BP算法基于梯度下降的策略，以目标的负梯度方向进行调整在学习率下，采用链式法则对权值进行更新：<br>从输出层到隐藏层，有:<br>从隐藏层到输入层，有:</p></li></ol><ul><li>算法描述:<blockquote><p>在（0，1）范围内初始化权值和学习率<br>REPEAT<br>FOR all（Xk,Yk)：<br>输入正向传播公式计算输出<br>计算输出层需修改的梯度项gj<br>计算隐藏层层需修改的梯度项eh<br>根据公式更新权值wjh,vij<br>END FOR<br>UNTIL 达到停止条件</p></blockquote></li></ul><p>标准BP算法：上述算法是对每个样本更新权值，属于标准BP算法，参数更新的频繁<br>累积BP算法：当读取完所有样本之后才更新参数，参数更新的频率低<br>区别：累积BP算法在误差下降到一定阶段，下降回非常缓慢，所以往往标准BP算法能更快得到较好解。<br><em>使用标准BP算法：对每个样本更新权值</em></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">//输出层</span><br><span class="line">for(int k=0;k&lt;outSum;k++)</span><br><span class="line">&#123;</span><br><span class="line">    //与实际输出的偏差</span><br><span class="line">    qq[k]=(yd[k]-x2[k])*x2[k]*(1-x2[k]);</span><br><span class="line">    for (int j = 0; j&lt; hideSum; ++j)</span><br><span class="line">        //调整隐藏层到输出层之间的连接权</span><br><span class="line">        w1[j][k]+=rate_w1*qq[k]*x1[j];</span><br><span class="line">&#125;</span><br><span class="line">//隐藏层</span><br><span class="line">for(int j=0;j&lt;hideSum;j++)</span><br><span class="line">&#123;</span><br><span class="line">    pp[j]=0;</span><br><span class="line">    //隐藏层的偏差</span><br><span class="line">    for (int k = 0; k&lt; outSum; k++)</span><br><span class="line">        pp[j]=pp[j]+qq[k]*w1[j][k];  //隐藏层的偏差和后面所有连接的对应输出层都有关系</span><br><span class="line">    pp[j]=pp[j]*x1[j]*(1-x1[j]);</span><br><span class="line">   for (int i = 0; i &lt; inSum; ++i)</span><br><span class="line">        //调整输入层到隐藏层之间的连接权</span><br><span class="line">        w[i][j]+=rate_w*pp[j]*x[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><em>使用累积BP算法：遍历完所有样本再更新权值<br>增加权值修改矩阵，将误差进行累加</em></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">//输出层</span><br><span class="line">for(int k=0;k&lt;outSum;k++)</span><br><span class="line">&#123;</span><br><span class="line">    //与实际输出的偏差</span><br><span class="line">    qq[k]=(yd[k]-x2[k])*x2[k]*(1-x2[k]);</span><br><span class="line">    for (int j = 0; j&lt; hideSum; ++j)</span><br><span class="line">    &#123;</span><br><span class="line">        //调整隐藏层到输出层之间的连接权</span><br><span class="line">        chg_w2[j][k]+=rate_w1*qq[k]*x1[j];</span><br><span class="line">        chg_b2[k]+=rate_b2*qq[k];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">//隐藏层</span><br><span class="line">for(int j=0;j&lt;hideSum;j++)</span><br><span class="line">&#123;</span><br><span class="line">    pp[j]=0;</span><br><span class="line">    //隐藏层的偏差</span><br><span class="line">    for (int k = 0; k&lt; outSum; k++)</span><br><span class="line">    &#123;</span><br><span class="line">        pp[j]=pp[j]+qq[k]*w1[j][k];  //隐藏层的偏差和后面所有连接的对应输出层都有关系</span><br><span class="line">    &#125;</span><br><span class="line">    pp[j]=pp[j]*x1[j]*(1-x1[j]);</span><br><span class="line">   for (int i = 0; i &lt; inSum; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        //调整输入层到隐藏层之间的连接权</span><br><span class="line">        chg_w1[i][j]=chg_w1[i][j]+rate_w1*pp[j]*x[i];</span><br><span class="line">        chg_b1[j]+=rate_b1*pp[j];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h1 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h1><table><thead><tr><th align="left">对比</th><th align="left">标准BP算法</th><th align="left">累积BP算法</th></tr></thead><tbody><tr><td align="left">迭代次数</td><td align="left">548</td><td align="left">348</td></tr><tr><td align="left">训练平均误差</td><td align="left">小于1e-5</td><td align="left">小于1e-5</td></tr><tr><td align="left">识别结果</td><td align="left">100%</td><td align="left">100%</td></tr></tbody></table><p>使用标准BP算法和累积BP算法识别率都能达到100%，但是因数据量小，使用累积BP 算法训练时间更短。</p><hr><h1 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h1><ul><li>如果将长边短边合成一条边，效果还是一样吗？<br>将数据集中长边L1和短边L2相加得到叶子的长为L，输入的属性列变为长L,宽W,面积area<br>网络结构采取上述结构，训练效果如下图：<table><thead><tr><th align="left">对比</th><th align="left">标准BP算法</th><th align="left">累积BP算法</th></tr></thead><tbody><tr><td align="left">迭代次数</td><td align="left">566</td><td align="left">717</td></tr><tr><td align="left">训练平均误差</td><td align="left">小于1e-5</td><td align="left">小于1e-5</td></tr><tr><td align="left">识别结果</td><td align="left">94.5%</td><td align="left">97.3%</td></tr></tbody></table></li></ul><p>因实验所用的叶片有破损存在，直接用长边加上短边得到的叶片长度存在一定误差，因此使用BP算法时，识别结果略有下降.<br>使用标准BP算法和累积BP算法识别率都能达到100%，但是因数据量小，使用累积BP 算法训练时间更短。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;问题描述&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;测量绿萝叶片使用kinect相机对图像做标定，将图像映射到坐标系中，使用matlab程序计算叶片整体占图像的像素从而计算面积，对绿萝每层的叶片都需要分次进行标定求解过程较为复杂，因此针对这种情况，采用bp网络来预测绿萝叶片面积。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;&lt;h1 id=&quot;数据采集&quot;&gt;&lt;a href=&quot;#数据采集&quot; class=&quot;headerlink&quot; title=&quot;数据采集&quot;&gt;&lt;/a&gt;数据采集&lt;/h1&gt;&lt;p&gt;&lt;em&gt;测量9盆绿萝叶片，实测数据共557片。训练集1-7盆，共449片，测试集8-9盆，共108片。&lt;br&gt;数据集中共四个属性，分别是宽度为w，长边为L1,短边为L2，面积area。&lt;/em&gt;&lt;br&gt;&lt;strong&gt;其中宽度，长边，短边为手动实测，面积根据叶片像素点占图片整体像素点计算得出。&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
