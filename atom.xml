<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="https://www.w3.org/2005/Atom">
  <title>八戒大强攻</title>
  
  <subtitle>好久没吃人肉了</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.xiapf.com/"/>
  <updated>2020-07-23T03:32:14.926Z</updated>
  <id>https://www.xiapf.com/</id>
  
  <author>
    <name>Xiapf</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>使用logistic回归实现多分类</title>
    <link href="https://www.xiapf.com/blogs/tfLog2/"/>
    <id>https://www.xiapf.com/blogs/tfLog2/</id>
    <published>2020-07-23T03:31:37.000Z</published>
    <updated>2020-07-23T03:32:14.926Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>使用logistic回归识别0~9的手写数字，并测试模型精度。数据集来源于斯坦福大学机器学习课程手势识别数据集： <a href="https://www.coursera.org/course/ml" target="_blank" rel="external nofollow noopener noreferrer">https://www.coursera.org/course/ml</a> 。</p><h2 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h2><p>（1）算法使用场景：logistic回归用于解决分类问题，即用于预测离散数值的输出</p><p>（2）假设函数</p><p>和线性回归类似，logistic回归是为了将几类数据划分开，沿用线性回归的思想，线性回归是找出一条直线能够拟合出数据集中的数据，logistic回归可以看做通过给出的所有数据点拟合出分类边界的直线，这里直线的函数就称为假设函数，假设函数定义为：h(x)=θ0+θ1 *x1+θ2 * x2+…+θn * x2，其中θ0是偏置参数，θ的个数取决于数据集的特征数量n，算法的目标就是要求出θ0 ~ θn这些参数。将这些参数竖向堆叠，可以将参数表示为一个新向量θ，假设函数就可以表示为：</p><a id="more"></a><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200723103016.png" alt></p><p>logistic回归是解决分类问题，最终结果可以理解为是为了得出数据属于其中一个类的概率，所以假设函数的值需要在0 ~1之间，所以最终的假设函数需要加上g(z)函数即sigmoid函数，能将输出控制在0 ~ 1之间：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200723103355.png" alt></p><p>（3）损失函数</p><p>logistic回归可以看做是参数寻优，即找到的参数需要让预测的输出和时间输出之间误差最小。为了避免陷入局部最优，代价函数需要是凸函数，由于假设函数是非线性函数，所以不能使用平方差函数，这里采用交叉熵函数：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200720151306.png" alt></p><p>（4）梯度下降</p><p>在优化参数过程中，采用梯度下降算法，为了找到的参数使得损失函数值最小（即误差最小），则每次参数都向着损失梯度下降的方向变化：</p><p>![](/Users/xpf/Library/Application Support/typora-user-images/image-20200723104441808.png)</p><p>（5）正则化</p><p>当数据集较小时，训练的模型容易出现过拟合，即数据能很好的拟合训练集，但是在不能很好拟合测试集，出现高方差，无法将训练的模型泛化到新样本，此时就要加入正则化项。</p><p>在损失函数和梯度下降中增加对参数的惩罚系数，修正模型中的每个参数，其中正则化参数λ用来一边控制使得训练的模型拟合训练数据，一边使得模型中每个参数不要占比太大，减少过拟合的问题。</p><p>加入正则化项后的损失函数</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200720164015.png" alt></p><p>由于θ0是偏置常数项，不用进行梯度下降，加入正则化项后的梯度</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200720164029.png" alt></p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><h3 id="数据集处理"><a href="#数据集处理" class="headerlink" title="数据集处理"></a>数据集处理</h3><p>（1）导入原始数据集</p><p>数据集中包含5000个手写数字，每个数字共有500个手写图片数据，每个图片以像素为20 * 20的灰度图像存储；标签值存储对应的图片代表的数字，其中数字0以标签10进行存储。</p><p>随机展示一个手写图片：使用matshow显示像素值所代表的图片，并查看对应的标签值</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200723101314.png" alt></p><p>the img should be:8</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_image</span><span class="params">(x)</span>:</span></span><br><span class="line">size=int(np.sqrt(len(x)))</span><br><span class="line">fig,ax=plt.subplots(figsize=(<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">ax.matshow(x.reshape((size,size)),cmap=matplotlib.cm.binary)</span><br><span class="line">plt.xticks(np.array([]))<span class="comment">#x轴，y轴不显示数字</span></span><br><span class="line">plt.yticks(np.array([]))</span><br><span class="line">m=raw_x.shape[<span class="number">0</span>]</span><br><span class="line">pick=np.random.randint(<span class="number">0</span>,m)</span><br><span class="line">plot_image(raw_x[pick,:])</span><br><span class="line">plt.show()</span><br><span class="line">print(<span class="string">'the img should be:&#123;&#125;'</span>.format(raw_y[pick]))</span><br></pre></td></tr></table></figure><p>使用scipy库函数读取原始数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(path,transpose=True)</span>:</span></span><br><span class="line">data=sio.loadmat(path)</span><br><span class="line">X=data.get(<span class="string">'X'</span>)</span><br><span class="line">y=data.get(<span class="string">'y'</span>)</span><br><span class="line">y=y.reshape(y.shape[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">return</span> X,y</span><br></pre></td></tr></table></figure><p>（2）预处理原始数据</p><p>在数据特征x的第一列插入元素1，用于模型参数中的偏置项，同时将标签y转换为向量化标签，即当一个样本x代表的图片为1，即标签y为1时，将y向量化转换为[0,1,0,…]，此时的标签y为10 * 1的列向量，则5000个样本的标签最终向量化转换为（10，5000）的标签值y，向量化后能根据结果快速判断预测值属于哪一个类别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(raw_x,raw_y)</span>:</span></span><br><span class="line"><span class="comment">#在x的第一列插入元素1</span></span><br><span class="line">x=np.insert(raw_x,<span class="number">0</span>,values=np.ones(raw_x.shape[<span class="number">0</span>]),axis=<span class="number">1</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment">#将y转换为向量化标签  raw_y代表0~9数字，标签为10,1,2...9 转换为10个标签</span></span><br><span class="line">y_matrix=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">11</span>):</span><br><span class="line">y_matrix.append((raw_y==i).astype(int))</span><br><span class="line"></span><br><span class="line"><span class="comment">#10代表的0放置最前面</span></span><br><span class="line">y_matrix=[y_matrix[<span class="number">-1</span>]]+y_matrix[:<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">y=np.array(y_matrix)</span><br><span class="line"><span class="keyword">return</span> x,y</span><br></pre></td></tr></table></figure><p>（3）划分训练集和测试集</p><p>将处理后的数据集打乱，按照8：2的比例划分训练集和测试集，其中训练集用于模型训练，测试集用于测试模型精度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shuffle_data</span><span class="params">(raw_x,raw_y)</span>:</span></span><br><span class="line">indices=list(range(len(raw_x)))</span><br><span class="line">np.random.shuffle(indices)</span><br><span class="line">m,n=raw_x.shape</span><br><span class="line">train_x=np.zeros((<span class="number">4000</span>,n))</span><br><span class="line">train_y=np.zeros((<span class="number">4000</span>))</span><br><span class="line">test_x=np.zeros((<span class="number">1000</span>,n))</span><br><span class="line">test_y=np.zeros((<span class="number">1000</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4000</span>):</span><br><span class="line">train_x[i]=raw_x[indices[i]]</span><br><span class="line">train_y[i]=raw_y[indices[i]]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">test_x[i]=raw_x[indices[i+<span class="number">4000</span>]]</span><br><span class="line">test_y[i]=raw_y[indices[i+<span class="number">4000</span>]]</span><br><span class="line"><span class="keyword">return</span> train_x,train_y,test_x,test_y</span><br></pre></td></tr></table></figure><h3 id="模型搭建"><a href="#模型搭建" class="headerlink" title="模型搭建"></a>模型搭建</h3><p>（1）带正则项的损失函数</p><p>根据交叉熵损失函数公式得出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#损失函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(theta,x,y)</span>:</span></span><br><span class="line">yhat=sigmoid(np.dot(x,theta))</span><br><span class="line">cost_term=-np.mean((y*np.log(yhat)+(<span class="number">1</span>-y)*np.log(<span class="number">1</span>-yhat)))</span><br><span class="line"><span class="keyword">return</span> cost_term</span><br><span class="line"></span><br><span class="line"><span class="comment">#加上正则项</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regular_cost</span><span class="params">(theta,x,y,l=<span class="number">1</span>)</span>:</span></span><br><span class="line">theta_no1=theta[<span class="number">1</span>:]</span><br><span class="line">regular=(l/(<span class="number">2</span>*len(x)))*np.sum(np.power(theta_no1,<span class="number">2</span>))</span><br><span class="line">regular_term=cost(theta,x,y)+regular</span><br><span class="line"><span class="keyword">return</span> regular_term</span><br></pre></td></tr></table></figure><p>（2）带正则项的梯度计算</p><p>计算一次梯度下降的变化为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#梯度下降算法</span></span><br><span class="line"><span class="comment">#一次梯度变化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(theta,x,y)</span>:</span></span><br><span class="line">yhat=sigmoid(np.dot(x,theta))</span><br><span class="line">grad=(<span class="number">1</span>/len(x))*np.dot(x.T,(yhat-y))</span><br><span class="line"><span class="keyword">return</span> grad</span><br><span class="line"><span class="comment">#加入正则项</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regular_gradient</span><span class="params">(theta,x,y,l=<span class="number">1</span>)</span>:</span></span><br><span class="line">theta_no1=theta[<span class="number">1</span>:]</span><br><span class="line">regular=(l/len(x))*theta_no1</span><br><span class="line">regular=np.concatenate((np.array([<span class="number">0</span>]),regular))</span><br><span class="line">regular_grad=gradient(theta,x,y)+regular</span><br><span class="line"><span class="keyword">return</span> regular_grad</span><br></pre></td></tr></table></figure><p>（3）logistic回归模型搭建</p><p>使用牛顿截断法，求出使得损失函数的最小的参数值，其中传递给损失函数的元祖为训练数据集，辅助梯度计算采用梯度下降算法中梯度计算法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logistic_regression</span><span class="params">(x,y,l=<span class="number">1</span>)</span>:</span></span><br><span class="line">theta=np.zeros(x.shape[<span class="number">1</span>])</span><br><span class="line">res=opt.minimize(fun=regular_cost,x0=theta,args=(x,y,l),jac=regular_gradient,method=<span class="string">'TNC'</span>)<span class="comment">#使用截断牛顿算法求最小值</span></span><br><span class="line"><span class="keyword">return</span> res.x</span><br></pre></td></tr></table></figure><p>以上搭建了logistic回归的二元分类问题，多分类问题是独立分为多个二分类问题，即第一个二分类问题是将标签为数字0的设置为1，其余标签设置为0，第二个二分类问题是将标签为数字1的设置为1，其余标签设置为0，以此类推，构建10个独立二分类器，最终得到的参数进行组合。当x输入10个分类器中时，选择假设函数最大的类别作为自己的预测值。</p><p>循环输入y中的10个标签值，搭建手写数字10分类模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">theta=[logistic_regression(train_x,train_y[k]) <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">10</span>)]</span><br><span class="line">theta=np.array(theta)</span><br></pre></td></tr></table></figure><p>得到的模型最优参数维度为（10，401），每一行参数代表预测为当前数字的二分类器模型，因为有401个特征，列数为401，组合起来就是10分类模型</p><blockquote><p>[[-5.40410646e+00  0.00000000e+00  0.00000000e+00 … -1.16566197e-04<br>   7.87772670e-06  0.00000000e+00]<br> [-2.38354102e+00  0.00000000e+00  0.00000000e+00 …  1.30441649e-03<br>  -7.51759934e-10  0.00000000e+00]<br> [-3.18402772e+00  0.00000000e+00  0.00000000e+00 …  4.45897167e-03<br>  -5.08363274e-04  0.00000000e+00]<br> …<br> [-1.90573390e+00  0.00000000e+00  0.00000000e+00 … -5.27541665e-04<br>   6.62163734e-05  0.00000000e+00]<br> [-7.98711225e+00  0.00000000e+00  0.00000000e+00 … -8.94541915e-05<br>   7.21498244e-06  0.00000000e+00]<br> [-4.57362378e+00  0.00000000e+00  0.00000000e+00 … -1.33527099e-03<br>   9.98511880e-05  0.00000000e+00]]</p></blockquote><h3 id="模型预测"><a href="#模型预测" class="headerlink" title="模型预测"></a>模型预测</h3><p>（1）将测试集输入训练好的模型，得到模型预测的精度</p><p>将得到的模型求出其假设函数值，并求出最大值作为预测值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">inner=sigmoid(np.dot(test_x,theta.T))</span><br><span class="line">yhat=np.argmax(inner,axis=<span class="number">1</span>)</span><br><span class="line">y_raw=test_raw_y.copy()</span><br><span class="line">y_raw[y_raw==<span class="number">10</span>]=<span class="number">0</span></span><br><span class="line">print(classification_report(y_raw,yhat))</span><br></pre></td></tr></table></figure><p>使用sklearn库的评价包，输入真实输出和预测输出得出预测准确率、召回率、F1分数、每个数字的测试集大小如下</p><pre><code>          precision    recall  f1-score   support     0.0       0.94      0.99      0.97       103     1.0       0.88      0.96      0.92        93     2.0       0.90      0.86      0.88       102     3.0       0.94      0.84      0.89       103     4.0       0.87      0.90      0.89       108     5.0       0.83      0.89      0.86        90     6.0       0.96      0.97      0.97       101     7.0       0.90      0.93      0.92       100     8.0       0.90      0.80      0.85       100     9.0       0.84      0.83      0.83       100accuracy                           0.90      1000</code></pre><p>测试集总体预测精度为90%</p><p>（2）正则化是否使用对比</p><p>当不使用正则化时，训练集预测准确率为98%，测试集预测准确率为87%；当使用正则化时，训练集预测准确率为95%，测试集预测准确率为90%</p><p>可以看出，当未使用正则化时，训练集达98%，说明模型“很好的”拟合了数据，但是测试集准确率只有87%，说明模型没有能够泛化至新的样本，出现了过拟合。当加入了正则化项后，测试集准确率提高了达到90%，说明正则化项的加入有助于减少模型的过拟合现象。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;问题描述&lt;/h2&gt;&lt;p&gt;使用logistic回归识别0~9的手写数字，并测试模型精度。数据集来源于斯坦福大学机器学习课程手势识别数据集： &lt;a href=&quot;https://www.coursera.org/course/ml&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;https://www.coursera.org/course/ml&lt;/a&gt; 。&lt;/p&gt;&lt;h2 id=&quot;算法原理&quot;&gt;&lt;a href=&quot;#算法原理&quot; class=&quot;headerlink&quot; title=&quot;算法原理&quot;&gt;&lt;/a&gt;算法原理&lt;/h2&gt;&lt;p&gt;（1）算法使用场景：logistic回归用于解决分类问题，即用于预测离散数值的输出&lt;/p&gt;&lt;p&gt;（2）假设函数&lt;/p&gt;&lt;p&gt;和线性回归类似，logistic回归是为了将几类数据划分开，沿用线性回归的思想，线性回归是找出一条直线能够拟合出数据集中的数据，logistic回归可以看做通过给出的所有数据点拟合出分类边界的直线，这里直线的函数就称为假设函数，假设函数定义为：h(x)=θ0+θ1 *x1+θ2 * x2+…+θn * x2，其中θ0是偏置参数，θ的个数取决于数据集的特征数量n，算法的目标就是要求出θ0 ~ θn这些参数。将这些参数竖向堆叠，可以将参数表示为一个新向量θ，假设函数就可以表示为：&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>使用tensorflow搭建诗句生成器（二）</title>
    <link href="https://www.xiapf.com/blogs/tfPoem2/"/>
    <id>https://www.xiapf.com/blogs/tfPoem2/</id>
    <published>2020-07-21T01:50:13.000Z</published>
    <updated>2020-07-21T01:55:10.824Z</updated>
    
    <content type="html"><![CDATA[<h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p>从字符角度出发，生成一个模型能够自动生成文本，即能够通过已有的字符，预测下一个字符出现的概率。</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><h3 id="字符级模型"><a href="#字符级模型" class="headerlink" title="字符级模型"></a>字符级模型</h3><p>采用字符级模型，参考Andrej Karpathy写的The Unreasonable Effectiveness of Recurrent Neural Networks，这里采用lstm作为基本单元</p><a id="more"></a><p>（1）基本lstm单元</p><p>定义基本的lstm单元，使用MultiRNNCell连接多层lstm，并加上dropout</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cell_fn = tf.nn.rnn_cell.BasicLSTMCell</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> is_training <span class="keyword">and</span> self.dropout &gt; <span class="number">0</span>:</span><br><span class="line">        cells = [tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=<span class="number">1.0</span>-self.dropout) <span class="keyword">for</span> cell <span class="keyword">in</span> cells]<span class="comment">#输出部分作为下一层的输入</span></span><br><span class="line"></span><br><span class="line">    multi_cell = tf.nn.rnn_cell.MultiRNNCell(cells)</span><br></pre></td></tr></table></figure><p>（2）定义网络结构</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200720194245.png" alt></p><p>每个句子拆分成字符输入网络，每次输入的诗的大小通过num_unrollings进行设置，seq_length用于设置每次输入的单个句子的长度，经过embedding层得到字符的词嵌入向量表示，输入lstm单元，经过全连接层，最终得到预测的输出，其中经过lstm输出lstm组合单元的输出激活值值，传入到下一层。</p><p>利用static_rnn使用指定的RNN生成循环神经网络：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用指定的RNN神经元创建循环神经网络</span></span><br><span class="line">    outputs, final_state = tf.nn.static_rnn(</span><br><span class="line">            cell = multi_cell,</span><br><span class="line">            inputs = sliced_inputs,</span><br><span class="line">            initial_state=self.initial_state)</span><br><span class="line">    self.final_state = final_state</span><br></pre></td></tr></table></figure><p>网络中其他设置：</p><p>1°损失函数：稀疏softmax交叉熵函数sparse_softmax_cross_entropy_with_logits；</p><p>2°训练时的设置：防止梯度爆照，进行梯度修剪，当梯度大于最大梯度时，直接使用最大梯度作为当前梯度值；</p><blockquote><p>tvars = tf.trainable_variables()<br>grads, _ = tf.clip_by_global_norm(tf.gradients(self.mean_loss, tvars), self.max_grad_norm)</p></blockquote><p>3°梯度下降算法采用adam算法</p><h3 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h3><p>（1）训练集数据转换为词嵌入</p><p>使用Word2Vec算法训练好的词向量模型，以&lt; / s &gt;开头，按词频排列，去除低频词。</p><p>词向量模型存储在vectors_poem.bin中，使用skim-gram模型，选择一个词作为输入，在选定词的前后距离内选择一个词作为目标词来作为训练集，在当前输入的上下文中得到下一个字的概率值为</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200623162639.png" alt></p><p>（2）训练数据集构建</p><p>当前诗句为“锄禾日当午”，则输入文本为“锄禾日当午”，输出文本为“禾日当午锄”，使用前一个字推出上下文中后一个字来构建训练集。</p><p>1°根据训练的词向量模型，将所有的诗句转换为词向量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poem_ids = DataLoader.get_text_idx(poems, self.w2v.vocab_hash, self.seq_max_length)</span><br></pre></td></tr></table></figure><p>以seq_max_length作为一句诗的最大长度，当超过最大长度，文本加上结束符]，按照转换之后的文本转换为词向量，不在词向量中的字符使用&lt; unknown &gt;标识。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_text_idx</span><span class="params">(text, vocab, max_document_length)</span>:</span></span><br><span class="line">        max_document_length_without_end = max_document_length - <span class="number">1</span></span><br><span class="line">        text_array = []</span><br><span class="line">        <span class="keyword">for</span> i, x <span class="keyword">in</span> enumerate(text):</span><br><span class="line">            line = []</span><br><span class="line">            <span class="keyword">if</span> len(x) &gt; max_document_length:</span><br><span class="line">                x_parts = x[:max_document_length_without_end]</span><br><span class="line">                idx = x_parts.rfind(<span class="string">'。'</span>)</span><br><span class="line">                <span class="keyword">if</span> idx &gt; <span class="number">-1</span>:</span><br><span class="line">                    x_parts = x_parts[<span class="number">0</span>:idx + <span class="number">1</span>] + <span class="string">']'</span></span><br><span class="line">                x = x_parts</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> j, w <span class="keyword">in</span> enumerate(x):</span><br><span class="line">                <span class="keyword">if</span> (w <span class="keyword">not</span> <span class="keyword">in</span> vocab):</span><br><span class="line">                    w = <span class="string">'&lt;unknown&gt;'</span></span><br><span class="line">                line.append(vocab[w])</span><br><span class="line">            text_array.append(line)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> text_array</span><br></pre></td></tr></table></figure><p>2°按照训练集、开发集、测试集进行划分，并将数据保存在poem_id.txt中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">n_samples = len(poem_ids)</span><br><span class="line">    train_size = int(self.train_frac * n_samples)</span><br><span class="line">    valid_size = int(self.valid_frac * n_samples)</span><br><span class="line">    test_size = n_samples - train_size - valid_size</span><br><span class="line"></span><br><span class="line">    self.testingSamples = poem_ids[-test_size:]</span><br><span class="line">    self.validationSamples = poem_ids[-valid_size - test_size: -test_size]</span><br><span class="line">    self.trainingSamples = poem_ids[:train_size]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存处理过的训练数据集</span></span><br><span class="line">    poem_ids_file = os.path.join(self.data_dir, <span class="string">'poem_ids.txt'</span>)</span><br><span class="line">    self.save_dataset(poem_ids_file)</span><br></pre></td></tr></table></figure><p>3°每次训练按照batch_size读取数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">x_seqs = np.ndarray((new_sample_size, self.seq_max_length), dtype=np.int32)</span><br><span class="line">    y_seqs = np.ndarray((new_sample_size, self.seq_max_length), dtype=np.int32)</span><br><span class="line">    self.x_lengths = []</span><br><span class="line">    <span class="keyword">for</span> i, sample <span class="keyword">in</span> enumerate(samples):</span><br><span class="line">        x_lengths.append(len(sample))</span><br><span class="line">        x_seqs[i] = sample + [self.padToken] * (self.seq_max_length - len(sample))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(sample_size, new_sample_size):</span><br><span class="line">        copyi = i - sample_size</span><br><span class="line">        x_seqs[i] = x_seqs[copyi]</span><br><span class="line">        x_lengths.append(x_lengths[copyi])</span><br><span class="line"></span><br><span class="line">    y_seqs[:, :<span class="number">-1</span>] = x_seqs[:, <span class="number">1</span>:]  <span class="comment"># x的1-最后给，y的0-倒数第二个</span></span><br><span class="line">    y_seqs[:, <span class="number">-1</span>] = x_seqs[:, <span class="number">0</span>]  <span class="comment"># x的第0个给y的最后一个</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;描述&quot;&gt;&lt;a href=&quot;#描述&quot; class=&quot;headerlink&quot; title=&quot;描述&quot;&gt;&lt;/a&gt;描述&lt;/h2&gt;&lt;p&gt;从字符角度出发，生成一个模型能够自动生成文本，即能够通过已有的字符，预测下一个字符出现的概率。&lt;/p&gt;&lt;h2 id=&quot;实现&quot;&gt;&lt;a href=&quot;#实现&quot; class=&quot;headerlink&quot; title=&quot;实现&quot;&gt;&lt;/a&gt;实现&lt;/h2&gt;&lt;h3 id=&quot;字符级模型&quot;&gt;&lt;a href=&quot;#字符级模型&quot; class=&quot;headerlink&quot; title=&quot;字符级模型&quot;&gt;&lt;/a&gt;字符级模型&lt;/h3&gt;&lt;p&gt;采用字符级模型，参考Andrej Karpathy写的The Unreasonable Effectiveness of Recurrent Neural Networks，这里采用lstm作为基本单元&lt;/p&gt;
    
    </summary>
    
    
      <category term="tensorflow" scheme="https://www.xiapf.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="https://www.xiapf.com/tags/tensorflow/"/>
    
      <category term="NLP" scheme="https://www.xiapf.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>用tensorflow实现机器学习算法——logistic回归</title>
    <link href="https://www.xiapf.com/blogs/tfLog/"/>
    <id>https://www.xiapf.com/blogs/tfLog/</id>
    <published>2020-07-20T08:48:38.000Z</published>
    <updated>2020-07-20T08:52:51.554Z</updated>
    
    <content type="html"><![CDATA[<h2 id="与线性回归的区别"><a href="#与线性回归的区别" class="headerlink" title="与线性回归的区别"></a>与线性回归的区别</h2><p>1.解决问题的不同</p><p>线性回归是用于解决回归问题，即用于预测连续输出</p><p>logistic回归用于解决分类问题，即用于预测离散数值的输出</p><p>2.假设函数不同</p><p>线性回归的假设函数为h=θ^T * x</p><p>logistic回归的假设函数为h=1/(1+e^(-θ^T * x))：由于logistic回归预测的是分类问题，分类标签为0或者1，所以加上了sigmoid函数，将输出转换为0或者1，当得出的线性值θ^T * x&gt;=0，即假设函数h的值&gt;=0.5，此时输出标签为1，当得出的线性值θ^T * x&lt;0，即假设函数h的值&lt;0.5，此时输出标签为0.</p><a id="more"></a><p>3.损失函数不同</p><p>线性回归损失函数为：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200720151414.png" alt></p><p>logitstic回归损失函数为：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200720151306.png" alt></p><p>不沿用线性回归的损失函数是因为logistic回归中的假设函数为非线性函数，代入误差的平方中不是凸函数，所以重新选择了新的凸函数作为logistic回归的损失函数。</p><h2 id="二分类问题"><a href="#二分类问题" class="headerlink" title="二分类问题"></a>二分类问题</h2><h3 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h3><p>（1）利用pandas中的read_csv读入（以逗号分隔才能这么读入），在第一列插入1作为偏置的特征</p><p>当使用梯度下降就需要做归一化：加快收敛速度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">data=pd.read_csv(<span class="string">'ex2data1.txt'</span>,names=[<span class="string">'test1'</span>,<span class="string">'test2'</span>,<span class="string">'score'</span>])</span><br><span class="line"></span><br><span class="line">col=data.shape[<span class="number">1</span>]</span><br><span class="line">x=data.iloc[:,:col<span class="number">-1</span>]</span><br><span class="line">x=(x-x.mean())/x.std()</span><br><span class="line">x.insert(<span class="number">0</span>,<span class="string">'ones'</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">y=data.iloc[:,col<span class="number">-1</span>:col]</span><br></pre></td></tr></table></figure><p>（2）定义sigmoid函数，将输出转换在0 ~ 1之间</p><p>def sigmoid(z):<br>    return 1/(1+np.exp(-z))</p><p>（3）每次迭代计算误差值，并利用梯度下降公式对每个参数进行修正；根据损失函数计算当前损失</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#2.计算成本函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">computeCost</span><span class="params">(theta,x,y)</span>:</span></span><br><span class="line">theta=np.mat(theta)</span><br><span class="line">m=len(x)</span><br><span class="line">inner=sigmoid(x*theta.T)</span><br><span class="line">cost=(<span class="number">-1</span>/m)*np.sum(np.multiply(y,np.log(inner))+np.multiply((<span class="number">1</span>-y),np.log(<span class="number">1</span>-inner)))<span class="comment">#对应位置相乘</span></span><br><span class="line"><span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line"><span class="comment">#3.梯度下降</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradentDescent</span><span class="params">(x,theta,y,alpha,iters)</span>:</span></span><br><span class="line">x=np.mat(x)</span><br><span class="line">y=np.mat(y)</span><br><span class="line">theta=np.mat(theta)</span><br><span class="line">m,n=x.shape</span><br><span class="line">costs=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(iters):</span><br><span class="line">error=sigmoid(x*theta.T)-y</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">theta[:,j]=theta[:,j]-alpha*(<span class="number">1</span>/m)*np.sum(np.multiply(error,x[:,j]))</span><br><span class="line">cost=computeCost(theta,x,y)</span><br><span class="line">costs.append(cost)</span><br><span class="line"><span class="keyword">return</span> theta,costs</span><br></pre></td></tr></table></figure><p>（4）根据得到的参数进行预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#4.预测</span></span><br><span class="line"><span class="comment">#准确率</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(theta,x)</span>:</span></span><br><span class="line">yhat=sigmoid(x*theta.T)</span><br><span class="line">m,n=yhat.shape</span><br><span class="line">p=np.zeros((m,n))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line"><span class="keyword">if</span>(yhat[i]&gt;=<span class="number">0.5</span>):</span><br><span class="line">p[i]=<span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">p[i]=<span class="number">0</span></span><br><span class="line"><span class="keyword">return</span> p</span><br></pre></td></tr></table></figure><p>得到模型的参数为：[[1.26836232 3.05675087 2.82011823]]<br>准确率为：0.89</p><h3 id="使用其他优化算法——scipy-optimize-自带的fmin-tnc"><a href="#使用其他优化算法——scipy-optimize-自带的fmin-tnc" class="headerlink" title="使用其他优化算法——scipy.optimize 自带的fmin_tnc"></a>使用其他优化算法——scipy.optimize 自带的fmin_tnc</h3><p>根据给出的损失函数和导数能够直接求得最小值</p><blockquote><p>res=op.fmin_tnc(func=computeCost,x0=theta,args=(x,y),fprime=gradent)</p></blockquote><p>其中导数使用梯度下降部分的导数求解方式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradent</span><span class="params">(theta,x,y)</span>:</span></span><br><span class="line">x=np.mat(x)</span><br><span class="line">y=np.mat(y)</span><br><span class="line">theta=np.mat(theta)</span><br><span class="line">m,n=x.shape</span><br><span class="line">error=sigmoid(x*theta.T)-y</span><br><span class="line">grad=np.zeros((n))</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">grad[j]=(<span class="number">1</span>/m)*np.sum(np.multiply(error,x[:,j]))</span><br><span class="line"><span class="keyword">return</span> grad</span><br></pre></td></tr></table></figure><p>得到的第一个参数就是模型的参数(array([1.7179636 , 4.01149168, 3.7426532 ]), 13, 1)<br>准确率为：0.89</p><h3 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h3><p>从结果看，梯度下降算法和其他的高级优化算法结果相近，但是梯度下降算法需要给出迭代次数和学习率alpha。</p><h2 id="使用tensorflow"><a href="#使用tensorflow" class="headerlink" title="使用tensorflow"></a>使用tensorflow</h2><p>（1）定义占位符用于输入数据</p><p>（2）定义当前作用域下的参数，预测值的计算，损失的计算，即正向传播得到的值</p><p>（3）定义梯度下降算法</p><p>（4）运行会话，将输入数据、下降算法、损失、参数输入，最终得到损失和参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用TensorFlow实现逻辑回归</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logisticRegression</span><span class="params">(x_data,y_data,alpha,iters)</span>:</span></span><br><span class="line">x=tf.placeholder(tf.float32,x_data.shape)</span><br><span class="line">y=tf.placeholder(tf.float32,y_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'logisticRegression'</span>):</span><br><span class="line">w=tf.get_variable(<span class="string">'weight'</span>,(x.shape[<span class="number">1</span>],<span class="number">1</span>),initializer=tf.constant_initializer())</span><br><span class="line">inner=tf.matmul(x,w)</span><br><span class="line">yhat=tf.nn.softmax(inner)</span><br><span class="line"><span class="comment"># loss=-tf.reduce_mean(tf.reduce_sum(y*tf.log(yhat)+(1-y)*tf.log(1-yhat),axis=1))#reduction_indices表示按行相加</span></span><br><span class="line">loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=yhat))</span><br><span class="line">opt=tf.train.GradientDescentOptimizer(learning_rate=alpha)</span><br><span class="line">opt_gennerate=opt.minimize(loss)</span><br><span class="line">loss_all=[]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(iters):</span><br><span class="line">_,loss_val,w_val=sess.run([opt_gennerate,loss,w],feed_dict=&#123;x:x_data,y:y_data&#125;)</span><br><span class="line">loss_all.append(loss_val)</span><br><span class="line"><span class="comment"># correct_predict=tf.equal(tf.argmax(yhat,1),tf.argmax(y,1))</span></span><br><span class="line"><span class="comment"># accuarcy=tf.reduce_mean(tf.cast(correct_predict,tf.float32))</span></span><br><span class="line"><span class="comment"># print('accuarcy:',accuarcy.eval(&#123;x:,y:&#125;)</span></span><br><span class="line">parameters=&#123;<span class="string">'theta'</span>:w_val,<span class="string">'loss'</span>:loss_all&#125;</span><br></pre></td></tr></table></figure><p>使用tensorflow的准确率为0.6</p><p>当需要得到测试集的准确率，可以增加以下代码：</p><blockquote><p>correct_predict=tf.equal(tf.argmax(yhat,1),tf.argmax(y,1))<br>accuarcy=tf.reduce_mean(tf.cast(correct_predict,tf.float32))<br>print(‘accuarcy:’,accuarcy.eval({x:,y:})</p></blockquote><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><h3 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h3><p>当出现过拟合现象：即数据能很好的拟合训练集，但是在不能很好拟合测试集，出现高方差，无法将训练的模型泛化到新样本，此时就要进行正则化。</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>在损失函数和梯度下降中增加对参数的惩罚系数，修正每个模型参数，其中正则化参数λ用来一边控制使得训练的模型拟合参数，一边使得每个模型参数不要太大，减少过拟合的问题。</p><p>线性回归和logistic回归中的梯度下降正则化的式子类似，但是其中的假设函数不同。</p><p>（1）线性回归的正则化</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200720164624.png" alt></p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200720164029.png" alt></p><p>（2）正规方程的正则化</p><p>正规方程可以直接求得最小的参数θ，但是当特征很多的时候，求逆矩阵运算量大，当特征少的时候可以考虑使用。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200720163840.png" alt></p><p>（3）logistic的正则化</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200720164015.png" alt></p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200720164029.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#2.计算成本函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">computeCostReg</span><span class="params">(theta,x,y,learning_rate)</span>:</span></span><br><span class="line">theta=np.mat(theta)</span><br><span class="line">m=len(x)</span><br><span class="line">inner=sigmoid(x*theta.T)</span><br><span class="line">reg=(learning_rate/(<span class="number">2</span>*m))*np.sum(np.power(theta[:,<span class="number">1</span>:theta.shape[<span class="number">1</span>]],<span class="number">2</span>))</span><br><span class="line">cost=(<span class="number">-1</span>/m)*np.sum(np.multiply(y,np.log(inner))+np.multiply((<span class="number">1</span>-y),np.log(<span class="number">1</span>-inner)))+reg<span class="comment">#对应位置相乘</span></span><br><span class="line"><span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line"><span class="comment">#3.梯度下降</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradientDescentReg</span><span class="params">(x,theta,y,alpha,iters,learning_rate)</span>:</span></span><br><span class="line">x=np.mat(x)</span><br><span class="line">y=np.mat(y)</span><br><span class="line">theta=np.mat(theta)</span><br><span class="line">m,n=x.shape</span><br><span class="line">costs=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(iters):</span><br><span class="line">error=sigmoid(x*theta.T)-y</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line"><span class="keyword">if</span> j==<span class="number">0</span>:</span><br><span class="line">theta[:,j]=theta[:,j]-alpha*(<span class="number">1</span>/m)*np.sum(np.multiply(error,x[:,j]))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">theta[:,j]=theta[:,j]-alpha*(<span class="number">1</span>/m)*np.sum(np.multiply(error,x[:,j]))+(learning_rate/m)*theta[:,j]</span><br><span class="line">cost=computeCostReg(theta,x,y,learning_rate)</span><br><span class="line">costs.append(cost)</span><br><span class="line"><span class="keyword">return</span> theta,costs</span><br></pre></td></tr></table></figure><p>就是在无正则化的损失函数计算和梯度下降中增加了正则化项，其中梯度下降中，需要对参数分类处理（第0个参数和其他参数）。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;与线性回归的区别&quot;&gt;&lt;a href=&quot;#与线性回归的区别&quot; class=&quot;headerlink&quot; title=&quot;与线性回归的区别&quot;&gt;&lt;/a&gt;与线性回归的区别&lt;/h2&gt;&lt;p&gt;1.解决问题的不同&lt;/p&gt;&lt;p&gt;线性回归是用于解决回归问题，即用于预测连续输出&lt;/p&gt;&lt;p&gt;logistic回归用于解决分类问题，即用于预测离散数值的输出&lt;/p&gt;&lt;p&gt;2.假设函数不同&lt;/p&gt;&lt;p&gt;线性回归的假设函数为h=θ^T * x&lt;/p&gt;&lt;p&gt;logistic回归的假设函数为h=1/(1+e^(-θ^T * x))：由于logistic回归预测的是分类问题，分类标签为0或者1，所以加上了sigmoid函数，将输出转换为0或者1，当得出的线性值θ^T * x&amp;gt;=0，即假设函数h的值&amp;gt;=0.5，此时输出标签为1，当得出的线性值θ^T * x&amp;lt;0，即假设函数h的值&amp;lt;0.5，此时输出标签为0.&lt;/p&gt;
    
    </summary>
    
    
      <category term="tensorflow" scheme="https://www.xiapf.com/categories/tensorflow/"/>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="tensorflow" scheme="https://www.xiapf.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>自定义注解</title>
    <link href="https://www.xiapf.com/blogs/annotation/"/>
    <id>https://www.xiapf.com/blogs/annotation/</id>
    <published>2020-07-16T02:00:05.000Z</published>
    <updated>2020-07-20T09:15:08.156Z</updated>
    
    <content type="html"><![CDATA[<p>自定义注解说明：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Target</span>(&#123;ElementType.METHOD, ElementType.TYPE&#125;)<span class="comment">//可以作用在类上和方法上  </span></span><br><span class="line"><span class="meta">@Retention</span>(RetentionPolicy.RUNTIME)<span class="comment">//可以通过反射读取注解  </span></span><br><span class="line"><span class="meta">@Inherited</span><span class="comment">//可以被子类继承  </span></span><br><span class="line"><span class="meta">@Documented</span><span class="comment">//javadoc生成文件档时，包含本注解信息  </span></span><br><span class="line"><span class="keyword">public</span> <span class="meta">@interface</span> MyAnnotation &#123;  </span><br><span class="line">    <span class="function">String <span class="title">value</span><span class="params">()</span> <span class="keyword">default</span> ""</span>;<span class="comment">//使用时没有指定key,值默认赋给value,如：MyAnnotation("abc")  </span></span><br><span class="line">&#125;  </span><br><span class="line"></span><br><span class="line"><span class="meta">@Retention</span>(RetentionPolicy.SOURCE)  </span><br><span class="line"><span class="comment">//作用是不将注解保存在class文件中，也就是说象“//”一样在编译时被过滤掉了。  </span></span><br><span class="line">  </span><br><span class="line"><span class="meta">@Retention</span>(RetentionPolicy.CLASS)  </span><br><span class="line"><span class="comment">//作用是只将注解保存在class文件中，而使用反射读取注解时忽略这些注解。  </span></span><br><span class="line">  </span><br><span class="line"><span class="meta">@Retention</span>(RetentionPolicy.RUNTIME)  </span><br><span class="line"><span class="comment">//作用是即将注解保存在class文件中，也可以通过反射读取注解。这也是最常用的值</span></span><br></pre></td></tr></table></figure><a id="more"></a><p>获取注解：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//取得方法上的指定的注解  </span></span><br><span class="line">Method m=?  </span><br><span class="line">Annotation annotation = m.getAnnotation(MyAnnotation<span class="class">.<span class="keyword">class</span>)</span>;  </span><br><span class="line">  </span><br><span class="line"><span class="comment">//取得方法上的所有注解，包括继承的注解。  </span></span><br><span class="line">Annotation[] annotations = m.getAnnotations();  </span><br><span class="line">  </span><br><span class="line"><span class="comment">//取当前方法上的所有的注解，不包括继承的  </span></span><br><span class="line">Annotation[] annotations = m.getDeclaredAnnotations();  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//取得类上的指定的注解  </span></span><br><span class="line">Annotation annotation = 类<span class="class">.<span class="keyword">class</span>.<span class="title">getAnnotation</span>(<span class="title">MyAnnotation</span>.<span class="title">class</span>)</span>;  </span><br><span class="line">  </span><br><span class="line"><span class="comment">//取得类上的所有注解，包括继承的注解。  </span></span><br><span class="line">Annotation[] annotations = 类<span class="class">.<span class="keyword">class</span>.<span class="title">getAnnotations</span>()</span>;  </span><br><span class="line">  </span><br><span class="line"><span class="comment">//取当前类上的所有的注解，不包括继承的  </span></span><br><span class="line">Annotation[] annotations = 类<span class="class">.<span class="keyword">class</span>.<span class="title">getDeclaredAnnotations</span>()</span>;</span><br></pre></td></tr></table></figure><p>spring中获取注解</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Service</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RegistryRules</span> <span class="keyword">implements</span> <span class="title">ApplicationContextAware</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> ApplicationContext context;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setApplicationContext</span><span class="params">(ApplicationContext applicationContext)</span> <span class="keyword">throws</span> BeansException </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.context = applicationContext;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@PostConstruct</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">registry</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Map&lt;String, Object&gt; beansWithRuleType = context.getBeansWithAnnotation(RuleType<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        beansWithRuleType.entrySet().stream().forEach(e -&gt; &#123;</span><br><span class="line">            MyAnnotation annotation = e.getValue().getClass().getAnnotation(MyAnnotation<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意：如果自定义注解上还有@Transactional注解，或获取不到，因为@Transactional注解会使用代理的方式，重新生成新的代理类来调用方法，新的代理类上并没有自定义注解，解决方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//@Transactional会使自定义注解失效</span></span><br><span class="line">            <span class="comment">//MyAnnotation annotation = e.getValue().getClass().getAnnotation(RuleType.class);</span></span><br><span class="line">            MyAnnotation annotation = AnnotationUtils.findAnnotation(e.getValue().getClass(), MyAnnotation<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure><p>通过AOP方式使用注解：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="meta">@Aspect</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogRecord</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Pointcut</span>(<span class="string">"@within(com.xxx.SelLog) || @annotation(com.xxx.SelLog)"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doLog</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Around</span>(<span class="string">"doLog()"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">doAround</span><span class="params">(ProceedingJoinPoint joinPoint)</span> <span class="keyword">throws</span> throwable </span>&#123;</span><br><span class="line">        log.debug(<span class="string">"class:&#123;&#125;,method:&#123;&#125; start,params:&#123;&#125;"</span>, joinPoint.getTarget().getClass(),</span><br><span class="line">            joinPoint.getSignature().getName(), JSON.toJSONString(joinPoint.getArgs()));</span><br><span class="line">        Object result = joinPoint.proceed();</span><br><span class="line">        log.debug(<span class="string">"class:&#123;&#125;,method:&#123;&#125; end,result:&#123;&#125; "</span>, joinPoint.getTarget().getClass(),</span><br><span class="line">            joinPoint.getSignature().getName(), result);</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@AfterThrowing</span>(value = <span class="string">"doLog()"</span>, throwing = <span class="string">"e"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">doAfterThrowing</span><span class="params">(JoinPoint joinPoint, Exception e)</span> </span>&#123;</span><br><span class="line">        log.debug(<span class="string">"class:&#123;&#125;,method:&#123;&#125;,exception:&#123;&#125;"</span>, joinPoint.getTarget().getClass(),</span><br><span class="line">            joinPoint.getSignature().getName(), e);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;自定义注解说明：&lt;/p&gt;&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;@Target&lt;/span&gt;(&amp;#123;ElementType.METHOD, ElementType.TYPE&amp;#125;)&lt;span class=&quot;comment&quot;&gt;//可以作用在类上和方法上  &lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;@Retention&lt;/span&gt;(RetentionPolicy.RUNTIME)&lt;span class=&quot;comment&quot;&gt;//可以通过反射读取注解  &lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;@Inherited&lt;/span&gt;&lt;span class=&quot;comment&quot;&gt;//可以被子类继承  &lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;@Documented&lt;/span&gt;&lt;span class=&quot;comment&quot;&gt;//javadoc生成文件档时，包含本注解信息  &lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;meta&quot;&gt;@interface&lt;/span&gt; MyAnnotation &amp;#123;  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;String &lt;span class=&quot;title&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;default&lt;/span&gt; &quot;&quot;&lt;/span&gt;;&lt;span class=&quot;comment&quot;&gt;//使用时没有指定key,值默认赋给value,如：MyAnnotation(&quot;abc&quot;)  &lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;@Retention&lt;/span&gt;(RetentionPolicy.SOURCE)  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;//作用是不将注解保存在class文件中，也就是说象“//”一样在编译时被过滤掉了。  &lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;@Retention&lt;/span&gt;(RetentionPolicy.CLASS)  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;//作用是只将注解保存在class文件中，而使用反射读取注解时忽略这些注解。  &lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;@Retention&lt;/span&gt;(RetentionPolicy.RUNTIME)  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;//作用是即将注解保存在class文件中，也可以通过反射读取注解。这也是最常用的值&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="java" scheme="https://www.xiapf.com/categories/java/"/>
    
    
      <category term="java" scheme="https://www.xiapf.com/tags/java/"/>
    
      <category term="annotation" scheme="https://www.xiapf.com/tags/annotation/"/>
    
  </entry>
  
  <entry>
    <title>使用tensorflow搭建诗句生成器（一）</title>
    <link href="https://www.xiapf.com/blogs/tfPoem1/"/>
    <id>https://www.xiapf.com/blogs/tfPoem1/</id>
    <published>2020-07-14T12:09:54.000Z</published>
    <updated>2020-07-21T01:55:12.537Z</updated>
    
    <content type="html"><![CDATA[<h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p>根据生成的最佳字符级模型，使用Flask框架结合html搭建web应用显示生成的诗句。根据用户输入诗句数量，随机n句诗句；或者根据用户输入的文字，根据最佳模型采样生成对应诗句。</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><h3 id="前台显示"><a href="#前台显示" class="headerlink" title="前台显示"></a>前台显示</h3><p>1、python端使用Flask框架搭建web应用</p><p>（1）导入Flask类，并创建一个app实例</p><p>（2）通过路由route()装饰器告诉Flask触发函数的url</p><a id="more"></a><p>（3）接收数据：利用html中的post方法接收参数</p><p>（4）发送数据：利用render_template将模板名称，需要参数传递的变量名称传递给模板中的变量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1</span></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"><span class="comment">#2</span></span><br><span class="line"><span class="meta">@app.route('/',methods=['POST','GET'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_poem</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">#3</span></span><br><span class="line">    startwith=request.form.get(<span class="string">'start_with'</span>)</span><br><span class="line">    text=[]</span><br><span class="line">    start_with=startwith</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> start_with:</span><br><span class="line">      <span class="comment">#调用方法生成需要回传的变量值</span></span><br><span class="line">      text=writer.cangtou(start_with)</span><br><span class="line">    <span class="comment">#4</span></span><br><span class="line">    <span class="keyword">return</span> render_template(<span class="string">'index.html'</span>,choosetext=text)</span><br></pre></td></tr></table></figure><p>2、html中采用表单提交方式</p><p>html文件需要放在template文件夹下，方便write_poem函数查找并渲染模板</p><p>（1）将各个输入框绑定名称，可用request.form.get获取输入的数据</p><p>（2）中用于显示传回的数据</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">form</span> <span class="attr">action</span>=<span class="string">""</span> <span class="attr">method</span>=<span class="string">'POST'</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">p</span>&gt;</span>想要哪种类型的诗词<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">p</span>&gt;</span>1:自由诗(需要输入生成几句诗句）<span class="tag">&lt;<span class="name">br</span>&gt;</span>2:藏头诗<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">        请输入对应序号：<span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">class</span>=<span class="string">"txt_input"</span> <span class="attr">name</span>=<span class="string">"poem_style"</span>  <span class="attr">value</span>=<span class="string">''</span> <span class="attr">style</span>=<span class="string">"margin-top:10px;"</span>/&gt;</span><span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line">        如果选择1，请给出需要生成几句诗：<span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">class</span>=<span class="string">"txt_input"</span> <span class="attr">name</span>=<span class="string">"num_sentence"</span>  <span class="attr">value</span>=<span class="string">''</span> <span class="attr">style</span>=<span class="string">"margin-top:10px;"</span>/&gt;</span><span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line">        如果选择3，请给出若干文字用于诗句生成：<span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">class</span>=<span class="string">"txt_input"</span> <span class="attr">name</span>=<span class="string">"start_with"</span>  <span class="attr">value</span>=<span class="string">''</span> <span class="attr">style</span>=<span class="string">"margin-top:10px;"</span>/&gt;</span><span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"submit"</span> <span class="attr">value</span>=<span class="string">"完成"</span> <span class="attr">class</span>=<span class="string">"button-new"</span> <span class="attr">onclick</span>=<span class="string">"myFunction()"</span> <span class="attr">style</span>=<span class="string">"margin-top:15px;"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">p</span>&gt;</span>&#123;&#123;choosetext&#125;&#125;<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="后台采样"><a href="#后台采样" class="headerlink" title="后台采样"></a>后台采样</h3><p>（1）按照字符级模型搭建网络模型，这里设置batch_size=1</p><p>（2）根据得到的最佳模型，创建saver对象，加载参数至网络中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">self.model = CharRNNLM(is_training=<span class="literal">False</span>,w2v_model = self.w2v.model,vocab_size=w2v_vocab_size, infer=<span class="literal">True</span>, **params)</span><br><span class="line">            saver = tf.train.Saver(name=<span class="string">'model_saver'</span>)</span><br><span class="line">            saver.restore(self.sess, best_model)</span><br></pre></td></tr></table></figure><p>（3）初始化隐藏层输入为0，同时如果是藏头诗，则将输入的文本按照字典中位置得出当前字的向量表示，作为初始输入；如果是随机生成诗句，则随机在字典中选择一个位置形成向量作为初始输入</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">        <span class="keyword">if</span> start_text <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> len(start_text) &gt; <span class="number">0</span>:</span><br><span class="line">    seq = list(start_text)</span><br><span class="line">    <span class="keyword">for</span> char <span class="keyword">in</span> start_text[:<span class="number">-1</span>]:</span><br><span class="line">        x = np.array([[self.w2v_model.vocab_hash[char]]])</span><br><span class="line">    x = np.array([[self.w2v_model.vocab_hash[start_text[<span class="number">-1</span>]]]])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    x = np.array([[np.random.randint(<span class="number">0</span>, self.vocab_size)]])</span><br><span class="line">    seq = []</span><br></pre></td></tr></table></figure><p>（4）按照需要生成的诗句的长度，根据skip-gram模型，得出在当前输入的上下文中得到下一个字的概率值</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200623162639.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">state, logits = session.run([self.final_state, self.logits],</span><br><span class="line">                                        &#123;self.input_data: x, self.initial_state: state&#125;)</span><br><span class="line">unnormalized_probs = np.exp(logits[<span class="number">0</span>] - np.max(logits[<span class="number">0</span>]))</span><br><span class="line">probs = unnormalized_probs / np.sum(unnormalized_probs)</span><br></pre></td></tr></table></figure><p>按照出现的词的概率进行随机采样</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sample = np.random.choice(self.vocab_size, <span class="number">1</span>, p=probs)[<span class="number">0</span>] <span class="comment">#随机采样</span></span><br></pre></td></tr></table></figure><p>并将当前采样值作为下一次的输入</p><p>（5）两种诗句形成方式（以[作为开头是因为数据集中诗句都是以此开头）</p><p>随机生成诗句：以[作为开头生成长度60的诗句（60可调节），根据生成句子中的句号个数选择用户需要的进行返回。</p><p>藏头诗：将[加在输入的文本前面，每次读入一个文本，并以此作为开头进行采样，每次只保留第一个逗号和第一个句号两个诗句。</p><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>（1）用户输入需要生成的诗句类型，如选择自由生成诗句，需要输入诗句数量；如选择藏头诗句，需要输入藏头文字。</p><p>（2）前台将用户输入传回，后台根据输入的要求调用模型进行采样生成诗句。</p><p>（3）后台将生成的诗句传回需要渲染的模板中进行显示。</p><p>例如：输入2，选择生成藏头诗，藏头的文字为：八戒我爱你，则生成的诗句显示在页面如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200714201829.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;描述&quot;&gt;&lt;a href=&quot;#描述&quot; class=&quot;headerlink&quot; title=&quot;描述&quot;&gt;&lt;/a&gt;描述&lt;/h2&gt;&lt;p&gt;根据生成的最佳字符级模型，使用Flask框架结合html搭建web应用显示生成的诗句。根据用户输入诗句数量，随机n句诗句；或者根据用户输入的文字，根据最佳模型采样生成对应诗句。&lt;/p&gt;&lt;h2 id=&quot;实现&quot;&gt;&lt;a href=&quot;#实现&quot; class=&quot;headerlink&quot; title=&quot;实现&quot;&gt;&lt;/a&gt;实现&lt;/h2&gt;&lt;h3 id=&quot;前台显示&quot;&gt;&lt;a href=&quot;#前台显示&quot; class=&quot;headerlink&quot; title=&quot;前台显示&quot;&gt;&lt;/a&gt;前台显示&lt;/h3&gt;&lt;p&gt;1、python端使用Flask框架搭建web应用&lt;/p&gt;&lt;p&gt;（1）导入Flask类，并创建一个app实例&lt;/p&gt;&lt;p&gt;（2）通过路由route()装饰器告诉Flask触发函数的url&lt;/p&gt;
    
    </summary>
    
    
      <category term="tensorflow" scheme="https://www.xiapf.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="https://www.xiapf.com/tags/tensorflow/"/>
    
      <category term="NLP" scheme="https://www.xiapf.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>用tensorflow实现机器学习算法——线性回归</title>
    <link href="https://www.xiapf.com/blogs/tfReg/"/>
    <id>https://www.xiapf.com/blogs/tfReg/</id>
    <published>2020-07-13T09:50:46.000Z</published>
    <updated>2020-07-13T09:51:51.228Z</updated>
    
    <content type="html"><![CDATA[<h2 id="读取数据——使用pandas"><a href="#读取数据——使用pandas" class="headerlink" title="读取数据——使用pandas"></a>读取数据——使用pandas</h2><p>（1）利用pandas中的read_csv读入</p><p>raw_data=pd.read_csv(‘ex1data2.txt’,names=[‘population’,’person’,’price’])</p><p>（2）使用均值mean，方程std归一化数据</p><p>data=(raw_data-raw_data.mean())/raw_data.std()</p><a id="more"></a><p>（3）每次迭代计算误差值，并对每个参数进行修正，并计算当前损失</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#2.计算成本</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(x,y,theta)</span>:</span></span><br><span class="line">m=x.shape[<span class="number">0</span>]</span><br><span class="line">inner=np.power(((x*theta.T)-y),<span class="number">2</span>)</span><br><span class="line">cost=(<span class="number">1</span>/(<span class="number">2</span>*m))*np.sum(inner)</span><br><span class="line"><span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line"><span class="comment">#3.梯度下降</span></span><br><span class="line"><span class="comment">#theta 1*2</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_gradientdescent</span><span class="params">(x,y,theta,alpha,iters)</span>:</span></span><br><span class="line">costs=[]</span><br><span class="line">m=x.shape[<span class="number">0</span>]</span><br><span class="line">n=theta.shape[<span class="number">1</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(iters):</span><br><span class="line">error=x*theta.T-y</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">theta[:,j]=theta[:,j]-alpha*(<span class="number">1</span>/m)*np.sum(np.multiply(error,x[:,j]))</span><br><span class="line">cost=compute_cost(x,y,theta)</span><br><span class="line">costs.append(cost)</span><br><span class="line"><span class="keyword">return</span> theta,costs</span><br></pre></td></tr></table></figure><h2 id="使用tensorflow"><a href="#使用tensorflow" class="headerlink" title="使用tensorflow"></a>使用tensorflow</h2><p>（1）定义占位符用于输入数据</p><p>（2）定义当前作用域下的参数，预测值的计算，损失的计算，即正向传播得到的值</p><p>（3）定义梯度下降算法</p><p>（4）运行会话，将输入数据、下降算法、损失、参数输入，最终得到损失和参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment">#用tensorflow实现线性回归</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_regression</span><span class="params">(x_data,y_data,alpha,iters)</span>:</span></span><br><span class="line"><span class="comment">#1.占位符</span></span><br><span class="line">x=tf.placeholder(tf.float32,x_data.shape)</span><br><span class="line">y=tf.placeholder(tf.float32,y_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.定义参数和损失</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'linear_regression'</span>):</span><br><span class="line">w=tf.get_variable(<span class="string">'weight'</span>,shape=(x.shape[<span class="number">1</span>],<span class="number">1</span>),initializer=tf.constant_initializer())</span><br><span class="line">y_hat=tf.matmul(x,w)</span><br><span class="line">loss=(<span class="number">1</span>/(<span class="number">2</span>*x_data.shape[<span class="number">0</span>]))*tf.matmul((y_hat-y),(y_hat-y),transpose_a=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#3.定义梯度下降算法</span></span><br><span class="line">opt=tf.train.GradientDescentOptimizer(learning_rate=alpha)</span><br><span class="line">opt_generator=opt.minimize(loss)</span><br><span class="line"></span><br><span class="line">loss_data=[]</span><br><span class="line"><span class="comment">#3.运行会话</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(iters):</span><br><span class="line">_,loss_val,w_val=sess.run([opt_generator,loss,w],feed_dict=&#123;x:x_data,y:y_data&#125;)</span><br><span class="line">loss_data.append(loss_val[<span class="number">0</span>]) <span class="comment">#loss_val是1*1数组</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">res=&#123;<span class="string">'loss'</span>:loss_data,<span class="string">'theta'</span>:w_val&#125;</span><br><span class="line"><span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><p>得到的参数：</p><p>[[-3.2414012]<br> [ 1.1272942]]</p><p>损失图为：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200713174946.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;读取数据——使用pandas&quot;&gt;&lt;a href=&quot;#读取数据——使用pandas&quot; class=&quot;headerlink&quot; title=&quot;读取数据——使用pandas&quot;&gt;&lt;/a&gt;读取数据——使用pandas&lt;/h2&gt;&lt;p&gt;（1）利用pandas中的read_csv读入&lt;/p&gt;&lt;p&gt;raw_data=pd.read_csv(‘ex1data2.txt’,names=[‘population’,’person’,’price’])&lt;/p&gt;&lt;p&gt;（2）使用均值mean，方程std归一化数据&lt;/p&gt;&lt;p&gt;data=(raw_data-raw_data.mean())/raw_data.std()&lt;/p&gt;
    
    </summary>
    
    
      <category term="tensorflow" scheme="https://www.xiapf.com/categories/tensorflow/"/>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="tensorflow" scheme="https://www.xiapf.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>人脸识别中不同方法对比分析</title>
    <link href="https://www.xiapf.com/blogs/diversityFace/"/>
    <id>https://www.xiapf.com/blogs/diversityFace/</id>
    <published>2020-07-06T09:10:05.000Z</published>
    <updated>2020-07-06T09:11:36.427Z</updated>
    
    <content type="html"><![CDATA[<h2 id="项目设置"><a href="#项目设置" class="headerlink" title="项目设置"></a>项目设置</h2><p>（1）问题描述</p><p>对于待分类的图像，根据模式识别算法判断它和哪一个图像最相似</p><p>（2）数据集来源</p><p>实验数据集来自Biometric Ideal Test官网<a href="http://biometrics.idealtest.org/dbDetailForUser.do?id=9" target="_blank" rel="external nofollow noopener noreferrer">http://biometrics.idealtest.org/dbDetailForUser.do?id=9</a> ，保存在FaceV5文件夹下。</p><p>选择其中100个人的不同的4张图片作为数据集，选择每个人的一张图片作为测试集共100张图像，保存在used文件夹下，将剩余的图像作为训练集，共300张图像，保存在unused文件夹下。每张图片的第0 ~ 2位用数字表示，用来标识每个人的身份，最终测试样本预测的类别也是需要根据图片的名称来进行判断是否识别准确。</p><a id="more"></a><p>（3）评价标准</p><p>不同方法的评价指标采用单一实数指标：正确率accuracy=预测正确的样本数/预测样本总数。</p><p>knn分类器中的超参数K取值区间设置为[1,5]，分类的距离采用L2二阶范数、相关性系数、闵可夫斯基距离</p><h2 id="不同的算法比对"><a href="#不同的算法比对" class="headerlink" title="不同的算法比对"></a>不同的算法比对</h2><p>模式识别算法中的人脸识别分为两部分：特征提取和数据比对两个阶段</p><h3 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h3><p>（1）基于颜色像素的RGB模型</p><p>特征：将训练样本和输入样本分别计算出每个图像的颜色直方图hist，对直方图归一化后保存的数据作为图像的特征。</p><p>（2）基于统计特征脸的PCA方法</p><p>PCA主成分分析原理总体来说是通过K-L变换将高维向量变成低维向量，形成特征子空间，每个图像就通过投影到该子空间上作为自身特征向量进行后续识别。其中主成分分析法是指找到一个空间，即形成的特征子空间，在该空间上消除了数据的相关性，每个类别数据能够很好的分离。通过K-L变换求出了特征空间，将训练样本和测试样本中所有图片投影在特征空间中就能求得每个图片的特征向量。</p><p>（3）基于神经网络的模型</p><p>从官网下载VGG16的预训练模型，保留全连接层，分别将训练样本和测试样本输入网络，经过卷积层、池化层、全连接层，最终输出一个4096维列向量作为每个图片的特征向量。</p><h3 id="数据比对"><a href="#数据比对" class="headerlink" title="数据比对"></a>数据比对</h3><p>采用最近邻分类knn分类器，根据已保存的训练样本和测试样本的特征向量，找到距离测试样本最近的类别作为预测的人脸，具体计算步骤：<br>（1）定义分类器中的超参数K，表示进行投票决策的样本的数目。<br>（2）遍历所有测试样本，计算样本的特征向量和其他每个训练样本的距离，按照由近至远进行排序。<br>（3）计算离测试样本最近的K个样本，统计各个分类，将最多数量的分类代表的人脸作为当前测试样本的预测值。</p><h3 id="特征提取-数据比对"><a href="#特征提取-数据比对" class="headerlink" title="特征提取+数据比对"></a>特征提取+数据比对</h3><p>基于迁移学习的模型</p><p>基于迁移学习的识别方法将提取特征和数据比都交给网络自己完成，最终输出人脸的识别结果。基于迁移学习的方法VGG16Fc采用基于模型的迁移学习的方法，保留VGG16至全连接层的预训练模型，在模型最后增加两层新的全连接层，作为需要微调的神经元部分。</p><p>预训练部分的网络用来识别图片的轮廓、线段、人脸位置、表情等图像信息，微调部分的网络进行分类识别。根据训练样本中每个图像的特征向量和图像所代表的类别训练后两层全连接层，为防止过拟合，增加Dropout层，随机丢弃一些神经元节点，最终将测试样本的特征向量输入训练好的网络中得到预测的人脸结果。</p><h3 id="总体过程"><a href="#总体过程" class="headerlink" title="总体过程"></a>总体过程</h3><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200706151929.png" alt></p><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>（1）不同距离的度量特征向量</p><p>RGB+KNN方法中，使用相关性系数度量距离的平均识别率为55%，使用闵可夫斯基距离平均识别率为50.4%，使用L2范数平均识别率为55.4%。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200706152534.png" alt></p><p>PCA+KNN方法中，使用相关性系数度量距离的平均识别率为75.2%，使用闵可夫斯基距离平均识别率为69.8%，使用L2范数平均识别率为72.6%。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200706153119.png" alt></p><p>VGG+KNN方法中，使用相关性系数度量距离的平均识别率为89.4%，使用闵可夫斯基距离平均识别率为85.6%，使用L2范数平均识别率为89.4%。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200706153149.png" alt></p><p>分析1：从结果可以看出：不同的度量向量的方法对算法具有一定影响。当选择传统的方法基于像素和统计的识别方法时，使用L2范数和相关性系数识别准确率接近，并且识别的结果优于闵可夫斯基距离，使用闵可夫斯基距离识别最差，因为它将各个分量同等看待，没有考虑到向量之间的相关性，所以识别效果差。</p><p>当选择深度学习的方法，使用神经网络时，三种度量方式识别都很准确，因为神经网络比传统只基于颜色像素、统计的特征脸学习了更多图片的信息，因此表示每个图像特征向量更准确，同一个人的特征向量距离很近，不同的人的特征向量距离非常远，所以选择不同的距离计算方式对识别效果基本无太大影响。</p><p>分析2：针对最近邻分类器K的取值，从结果可以看出，随着K的增加，预测的准确率先升高再降低，说明增加K会提高预测准确率，但是K太大，会导致最近的样本中其他样本数量过多，而导致分类错误。由于训练集中每个人的图像数量为3张，因此当K取值为2的时候分类效果最好，即K略小于训练集中同一个人的图片数量时分类效果最好，所以需要合理选择K的值。由于这里训练集图片数量较少，K=1时也能取得较好的分类效果。</p><p>（2）不同的人脸识别的方法</p><p>按照（1）中选择L2范数作为KNN分类器中度量距离的方式。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200706153408.png" alt></p><p>分析1：从结果可以看出，基于颜色像素的方法仅考虑了人脸中的颜色，没有考虑人脸的轮廓的相似度等，所以该方法识别率最低，平均识别为55.4%，基于统计特征脸的方法考虑到了人脸的轮廓，利用特征脸，将图像投影到特征空间中，更多的考虑到了人脸的特点，所以识别效果较好，平均识别率达72.6%，但是这两种方法和使用神经网络的深度学习方法相比，识别效果都不佳，神经网络识别的方法从图像中提取CNN特征，能更好的对图像分类，平均识别率为89.4%。</p><p>分析2：将传统分类方法和神经网络分类的对比，基于神经网络的识别方法在提取出图像的特征后外接knn分类器进行识别，通过距离对图像进行分类，而基于迁移学习的方法是让网络学习图像的向量和对应人脸之间的对应关系，从而对测试图像进行识别分类，识别结果准确率86%，低于传统分类方法的识别结果，但并不完全因为最近邻分类方法由于网络学习分类的方法，基于迁移学习的方法在训练集上的精度最终能达97.78%，但是在测试集上的识别精度不佳，可能是由于训练样本集太小，只有300张图像，导致了过拟合。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;项目设置&quot;&gt;&lt;a href=&quot;#项目设置&quot; class=&quot;headerlink&quot; title=&quot;项目设置&quot;&gt;&lt;/a&gt;项目设置&lt;/h2&gt;&lt;p&gt;（1）问题描述&lt;/p&gt;&lt;p&gt;对于待分类的图像，根据模式识别算法判断它和哪一个图像最相似&lt;/p&gt;&lt;p&gt;（2）数据集来源&lt;/p&gt;&lt;p&gt;实验数据集来自Biometric Ideal Test官网&lt;a href=&quot;http://biometrics.idealtest.org/dbDetailForUser.do?id=9&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;http://biometrics.idealtest.org/dbDetailForUser.do?id=9&lt;/a&gt; ，保存在FaceV5文件夹下。&lt;/p&gt;&lt;p&gt;选择其中100个人的不同的4张图片作为数据集，选择每个人的一张图片作为测试集共100张图像，保存在used文件夹下，将剩余的图像作为训练集，共300张图像，保存在unused文件夹下。每张图片的第0 ~ 2位用数字表示，用来标识每个人的身份，最终测试样本预测的类别也是需要根据图片的名称来进行判断是否识别准确。&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="https://www.xiapf.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="模式识别" scheme="https://www.xiapf.com/tags/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>序列模型RNN——语音识别与触发字检测</title>
    <link href="https://www.xiapf.com/blogs/rnn3TD/"/>
    <id>https://www.xiapf.com/blogs/rnn3TD/</id>
    <published>2020-06-30T08:14:33.000Z</published>
    <updated>2020-06-30T09:09:08.912Z</updated>
    
    <content type="html"><![CDATA[<h2 id="语言识别"><a href="#语言识别" class="headerlink" title="语言识别"></a>语言识别</h2><h3 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h3><p>输入一段语音，输出语音中文字。</p><p>（1）预处理：将输入的语音转换为频谱图，横轴显示时间，纵轴表示当前时间步的频率</p><p>（2）搭建模型：将预处理后的频谱图传入网络得到预测文字</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>（1）使用注意力模型</p><p>根据上下文向量和隐藏层的值每次输出一个文本</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630152005.png" alt></p><p>（2）使用CTC损失函数</p><p>主要是使用RNN中多对多模型，Tx=Ty，允许输出空格和重复的字符，保证输入输出相同。</p><a id="more"></a><p>对于预测的输出，可以将空格之间的字符进行折叠得到最终的结果</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630152045.png" alt></p><h2 id="触发字检测"><a href="#触发字检测" class="headerlink" title="触发字检测"></a>触发字检测</h2><p>对输入的一段音频，当未出现触发字时标签为0，出现触发字时标签为1。</p><p>防止训练集不平衡（防止0过多）：在出现触发字的时间t后，设置连续的一段音频标签为1。</p><p>网络结构采用RNN模型</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630152559.png" alt></p><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>当音频中出现“active”触发词时，在其后面加上一段蜂鸣声</p><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p>（1）构建训练集（单一训练样本的构建）</p><p>由于语音数据较为复杂，直接获取在嘈杂背景音下的说话声音较为困难，因此选择录制没有背景音的声音（正例和反例）和背景音，再进行合成，就很容易的有了在嘈杂背景下的说话声音。所以需要随机再背景音中插入正例（说active）和反例（不说active）的声音。</p><p>使用pydub合成音频大小为10000，原始音频大小为10秒，10/10000=0.001s=1ms，10秒离散成1ms，即10秒音频有10000个时间步</p><p>原始音频大小44100，频谱图得到的时间步Tx=5511，每个时间步的频率为101</p><p>1°随机选择插入的位置</p><p>根据片段的大小，在1~10000-片段大小内随机选择位置</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#随机选取片段时间</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_random_time_segment</span><span class="params">(segment_time_ms)</span>:</span></span><br><span class="line">segment_start=np.random.randint(low=<span class="number">0</span>,high=<span class="number">10000</span>-segment_time_ms)</span><br><span class="line">segment_end=segment_start+segment_time_ms<span class="number">-1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> (segment_start,segment_end)</span><br></pre></td></tr></table></figure><p>2°判断是否和之前插入位置重叠</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#判断有无和之前时间重叠</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_overlapping</span><span class="params">(segment_time,previous_time)</span>:</span></span><br><span class="line">segment_start,segment_end=segment_time</span><br><span class="line">overlap=<span class="literal">False</span></span><br><span class="line"><span class="keyword">for</span> previous_start,previous_end <span class="keyword">in</span> previous_time:</span><br><span class="line"><span class="keyword">if</span> segment_start&lt;=previous_end <span class="keyword">and</span> segment_end&gt;=previous_start:</span><br><span class="line">overlap=<span class="literal">True</span></span><br><span class="line"><span class="keyword">return</span> overlap</span><br></pre></td></tr></table></figure><p>3°根据选择的背景音插入正例或反例</p><p>随机选择插入位置，当不重复时，将插入时间归为已插入，同时将音频按照起始位置插入到背景音乐中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#选取片段</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert_audio_clip</span><span class="params">(background,audio_clip,previous_time)</span>:</span></span><br><span class="line"><span class="comment">#随机选一个时间</span></span><br><span class="line">segment_time_ms=len(audio_clip)</span><br><span class="line">segment_time=get_random_time_segment(segment_time_ms)</span><br><span class="line"></span><br><span class="line"><span class="comment">#判断是否重叠</span></span><br><span class="line"><span class="keyword">while</span> is_overlapping(segment_time,previous_time):</span><br><span class="line">segment_time=get_random_time_segment(segment_time_ms)</span><br><span class="line"></span><br><span class="line"><span class="comment">#不重叠的时间段</span></span><br><span class="line">previous_time.append(segment_time)</span><br><span class="line"></span><br><span class="line">new_background=background.overlay(audio_clip,position=segment_time[<span class="number">0</span>])<span class="comment">#要插入的音频和位置</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> new_background,segment_time</span><br></pre></td></tr></table></figure><p>4°将插入位置的标签设置为1</p><p>当插入位置没有到最后时，将插入位置后面的49个位置全部设置为1（平衡训练集中的0）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#设置标签项 在t位置为1，后面49个时间步设置为1</span></span><br><span class="line"><span class="comment">#如果 “activate” 在时间 t步结束 , 则设置 y⟨t+1⟩=1以及后面额外的连续49个的值为 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#以ms为单位的段的结束时间</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert_ones</span><span class="params">(y,segment_time_ms)</span>:</span></span><br><span class="line">segment_end_y=int(segment_time_ms*Ty/<span class="number">10000</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(segment_end_y+<span class="number">1</span>,segment_end_y+<span class="number">51</span>):</span><br><span class="line"><span class="keyword">if</span> i&lt;Ty:</span><br><span class="line">y[<span class="number">0</span>,i]=<span class="number">1</span></span><br><span class="line"><span class="keyword">return</span> y</span><br></pre></td></tr></table></figure><p>5°生成单个训练样本</p><p>从正例或者反例中随机选择几个片段，并按照选择片段的个数，随机选择插入的片段位置。</p><p>将选择的片段随机插入，并设置好标签项。</p><p>最终将标准化后的音频导出，这是就构造了一个训练样本。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#生成训练集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_training_example</span><span class="params">(background,activates,negtives)</span>:</span></span><br><span class="line">np.random.seed(<span class="number">18</span>)</span><br><span class="line"></span><br><span class="line">background=background<span class="number">-20</span> <span class="comment">#使噪音安静些</span></span><br><span class="line"></span><br><span class="line">y=np.zeros((<span class="number">1</span>,Ty))</span><br><span class="line">previous_time=[]</span><br><span class="line"></span><br><span class="line"><span class="comment">#正例 从列表中选择0~4随机的正例</span></span><br><span class="line">num_of_activites=np.random.randint(<span class="number">0</span>,<span class="number">5</span>)</span><br><span class="line">print(num_of_activites)</span><br><span class="line">random_indies=np.random.randint(len(activates),size=num_of_activites)</span><br><span class="line">print(random_indies)</span><br><span class="line">random_activtes=[activates[i] <span class="keyword">for</span> i <span class="keyword">in</span> random_indies]</span><br><span class="line"></span><br><span class="line"><span class="comment">#对每个选取的正例片段插入背景中，并更新标签</span></span><br><span class="line"><span class="keyword">for</span> random_activte <span class="keyword">in</span> random_activtes:</span><br><span class="line">background,segment_time=insert_audio_clip(background,random_activte,previous_time)</span><br><span class="line">segment_start,segment_end=segment_time</span><br><span class="line">y=insert_ones(y,segment_end)</span><br><span class="line"></span><br><span class="line"><span class="comment">#反例</span></span><br><span class="line">num_of_negatives=np.random.randint(<span class="number">0</span>,<span class="number">3</span>)</span><br><span class="line">random_indies=np.random.randint(len(negatives),size=num_of_negatives)</span><br><span class="line">random_negatives=[negatives[i] <span class="keyword">for</span> i <span class="keyword">in</span> random_indies]</span><br><span class="line"></span><br><span class="line"><span class="comment">#对每个选取的正例片段插入背景中，并更新标签</span></span><br><span class="line"><span class="keyword">for</span> random_negative <span class="keyword">in</span> random_negatives:</span><br><span class="line">background,_=insert_audio_clip(background,random_negative,previous_time)</span><br><span class="line"></span><br><span class="line"><span class="comment">#标准化剪辑的音量</span></span><br><span class="line">background=match_target_amplitude(background,<span class="number">-20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#保存</span></span><br><span class="line">file_handle=background.export(<span class="string">'train1'</span>+<span class="string">'.wav'</span>,format=<span class="string">'wav'</span>)</span><br><span class="line">print(<span class="string">'train1.wav已保存'</span>)</span><br><span class="line"><span class="comment"># print(file_handle)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#声谱图</span></span><br><span class="line">x=graph_spectrogram(<span class="string">'train1.wav'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> x,y</span><br></pre></td></tr></table></figure><p>（2）构建模型</p><p>为节省时间，这里训练集使用已处理好的样本，开发集使用真实音频并手动标记，因为系统需要在真实音频下使用。</p><p>使用np.load导入训练集和开发集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入已处理好的训练集和开发集</span></span><br><span class="line">X=np.load(<span class="string">'./XY_train/X.npy'</span>)</span><br><span class="line">Y=np.load(<span class="string">'./XY_train/Y.npy'</span>)</span><br><span class="line">X_dev=np.load(<span class="string">'./XY_dev/X_dev.npy'</span>)</span><br><span class="line">Y_dev=np.load(<span class="string">'./XY_dev/Y_dev.npy'</span>)</span><br></pre></td></tr></table></figure><p>按照如图结构构建模型，采用GRU单元</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630155058.jpg" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用两个GRU建立模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(input_shape)</span>:</span></span><br><span class="line">X_input=Input(shape=input_shape)</span><br><span class="line">X=Conv1D(<span class="number">196</span>,<span class="number">15</span>,strides=<span class="number">4</span>)(X_input)</span><br><span class="line">X=BatchNormalization()(X)</span><br><span class="line">X=Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line">X=Dropout(<span class="number">0.8</span>)(X)</span><br><span class="line"></span><br><span class="line">X=GRU(<span class="number">128</span>,return_sequences=<span class="literal">True</span>)(X)</span><br><span class="line">X=Dropout(<span class="number">0.8</span>)(X)</span><br><span class="line">X=BatchNormalization()(X)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X=GRU(<span class="number">128</span>,return_sequences=<span class="literal">True</span>)(X)</span><br><span class="line">X=Dropout(<span class="number">0.8</span>)(X)</span><br><span class="line">X=BatchNormalization()(X)</span><br><span class="line">X=Dropout(<span class="number">0.8</span>)(X)</span><br><span class="line"></span><br><span class="line">X=TimeDistributed(Dense(<span class="number">1</span>,activation=<span class="string">'sigmoid'</span>))(X)</span><br><span class="line"></span><br><span class="line">model=Model(inputs=X_input,outputs=X)</span><br><span class="line"><span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><p>（3）训练模型</p><p>为了快速建立可用系统，可以先导入预训练的模型，再编译模型（可以自定义梯度下降函数及参数）、训练模型，这时仅训练一次即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#编译及训练模型(导入下载的模型，再编译，训练一次)</span></span><br><span class="line">input_shape=(Tx,n_freq)</span><br><span class="line">model=model(input_shape)</span><br><span class="line">model=load_model(<span class="string">'./models/tr_model.h5'</span>)</span><br><span class="line">opt=Adam(lr=<span class="number">0.0001</span>,beta_1=<span class="number">0.9</span>,beta_2=<span class="number">0.999</span>,decay=<span class="number">0.01</span>)</span><br><span class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>,optimizer=opt,metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">model.fit(X,Y,epochs=<span class="number">1</span>,batch_size=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用开发集预测</span></span><br><span class="line">loss,acc=model.evaluate(X_dev,Y_dev)</span><br></pre></td></tr></table></figure><blockquote><p>loss: 0.3650805354118347<br>acc: 0.9451636075973511</p></blockquote><p>正确率达94.5%，此时模型根据输入的音频能很好的确定active触发词的位置</p><p>（4）在active触发词后加上峰鸣声</p><p>1°对输入数据预测</p><p>将输入的音频转换为频谱图，由于频谱图输出的是（频率，时间步），需要调换坐标轴，接着使用模型预测，得到输出值，输出值打印如图：（输出值中间代表每个时间步的输出）</p><p>上面是输入音频的频谱图，下面是预测输出</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630164935.png" alt></p><p>可以看出网络预测2个地方有触发词（预测的可能值很大）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#进行预测</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">detect_triggerword</span><span class="params">(filename)</span>:</span></span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">x=graph_spectrogram(filename)</span><br><span class="line"><span class="comment">#输出n_freq,Tx</span></span><br><span class="line"></span><br><span class="line">x=x.swapaxes(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">x=np.expand_dims(x,axis=<span class="number">0</span>)</span><br><span class="line">predictions=model.predict(x) <span class="comment">#中间对应每个时间步的输出</span></span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">plt.plot(predictions[<span class="number">0</span>,:,<span class="number">0</span>])</span><br><span class="line">plt.ylabel(<span class="string">'probability'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> predictions</span><br></pre></td></tr></table></figure><p>2°找到大于阈值的预测值位置加入峰鸣声</p><p>由于active触发词后49个位置都有1，为了不重复加入峰鸣声，设置每75步加入一次峰鸣，插入的位置在当前时间步占总体时间步再乘以音频持续时间</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 当出现一次触发词就插入峰鸣声音，为了不一直有声音，设置连续步</span></span><br><span class="line">chime_file=<span class="string">'audio_examples/chime.wav'</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chime_on_activate</span><span class="params">(filename,predictions,thresold)</span>:</span></span><br><span class="line">audio_clip=AudioSegment.from_wav(filename)</span><br><span class="line">chime=AudioSegment.from_wav(chime_file)</span><br><span class="line">Ty=predictions.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">countSteps=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(Ty):</span><br><span class="line">countSteps=countSteps+<span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> predictions[<span class="number">0</span>,i,<span class="number">0</span>]&gt;thresold <span class="keyword">and</span> countSteps&gt;<span class="number">75</span>:</span><br><span class="line">audio_clip=audio_clip.overlay(chime,position=((i/Ty)*audio_clip.duration_seconds)*<span class="number">1000</span>)  <span class="comment">#1000是转换为s ms-&gt;s</span></span><br><span class="line">      countSteps=<span class="number">0</span></span><br><span class="line">audio_clip.export(<span class="string">'clime_test1111.wav'</span>,format=<span class="string">'wav'</span>)</span><br></pre></td></tr></table></figure><p>3°预处理音频为10秒</p><p>设置静音大小10000，将输入音频前10000个插入，按照训练集的音频大小设置帧速率为44100，再进行保存</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#需要修剪音频在10秒之内</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_audio</span><span class="params">(filename)</span>:</span></span><br><span class="line"><span class="comment">#设置静音10000</span></span><br><span class="line">silentaudio=AudioSegment.silent(duration=<span class="number">10000</span>)</span><br><span class="line"><span class="comment">#保留文件前10秒</span></span><br><span class="line">preaudio=AudioSegment.from_wav(filename)[:<span class="number">10000</span>]</span><br><span class="line">audio_clip=silentaudio.overlay(preaudio)</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置帧速率44100</span></span><br><span class="line">audio_clip=audio_clip.set_frame_rate(<span class="number">44100</span>)</span><br><span class="line"></span><br><span class="line">myfile=<span class="string">'myaudio111.wav'</span></span><br><span class="line">audio_clip.export(myfile,format=<span class="string">'wav'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> myfile</span><br></pre></td></tr></table></figure><p>（5）结果</p><blockquote><p>myfile=preprocess_audio(‘./test1.wav’)<br>thresold=0.5 #峰鸣声没有添加，可以根据输出图调整阈值<br>predictions=detect_triggerword(myfile)<br>chime_on_activate(myfile,predictions,thresold)</p></blockquote><p>自己录制的音频13秒：<a href="https://pan.baidu.com/s/1cLyOHCMh-osYnJ18DZWsHw" target="_blank" rel="external nofollow noopener noreferrer">https://pan.baidu.com/s/1cLyOHCMh-osYnJ18DZWsHw</a></p><p>修剪之后10秒：<a href="https://pan.baidu.com/s/109EVMpY-dZ2tCkdZZzB4qg" target="_blank" rel="external nofollow noopener noreferrer">https://pan.baidu.com/s/109EVMpY-dZ2tCkdZZzB4qg</a></p><p>加入峰鸣之后：<a href="https://pan.baidu.com/s/1L_fMtBr-bBWLgnUDp9hDfQ" target="_blank" rel="external nofollow noopener noreferrer">https://pan.baidu.com/s/1L_fMtBr-bBWLgnUDp9hDfQ</a></p><p>由网络预测的两个地方的触发词和峰鸣加入的地方比对，发现音频添加正确。（由于发音问题，录制的音频有时预测不准确）</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;语言识别&quot;&gt;&lt;a href=&quot;#语言识别&quot; class=&quot;headerlink&quot; title=&quot;语言识别&quot;&gt;&lt;/a&gt;语言识别&lt;/h2&gt;&lt;h3 id=&quot;基本原理&quot;&gt;&lt;a href=&quot;#基本原理&quot; class=&quot;headerlink&quot; title=&quot;基本原理&quot;&gt;&lt;/a&gt;基本原理&lt;/h3&gt;&lt;p&gt;输入一段语音，输出语音中文字。&lt;/p&gt;&lt;p&gt;（1）预处理：将输入的语音转换为频谱图，横轴显示时间，纵轴表示当前时间步的频率&lt;/p&gt;&lt;p&gt;（2）搭建模型：将预处理后的频谱图传入网络得到预测文字&lt;/p&gt;&lt;h3 id=&quot;方法&quot;&gt;&lt;a href=&quot;#方法&quot; class=&quot;headerlink&quot; title=&quot;方法&quot;&gt;&lt;/a&gt;方法&lt;/h3&gt;&lt;p&gt;（1）使用注意力模型&lt;/p&gt;&lt;p&gt;根据上下文向量和隐藏层的值每次输出一个文本&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630152005.png&quot; alt&gt;&lt;/p&gt;&lt;p&gt;（2）使用CTC损失函数&lt;/p&gt;&lt;p&gt;主要是使用RNN中多对多模型，Tx=Ty，允许输出空格和重复的字符，保证输入输出相同。&lt;/p&gt;
    
    </summary>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="循环神经网络" scheme="https://www.xiapf.com/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>序列模型RNN——有条件的语言模型</title>
    <link href="https://www.xiapf.com/blogs/rnn3MT/"/>
    <id>https://www.xiapf.com/blogs/rnn3MT/</id>
    <published>2020-06-30T07:03:14.000Z</published>
    <updated>2020-06-30T07:11:47.878Z</updated>
    
    <content type="html"><![CDATA[<h2 id="模型简介"><a href="#模型简介" class="headerlink" title="模型简介"></a>模型简介</h2><p>（1）sequence to sequence模型：根据给定的句子得到翻译的结果</p><p>模型由编码器和解码器构成：</p><p>编码器部分读入输入的句子中每个单词，每步不输出结果，仅在最后一步输出一个编码向量。</p><p>解码器部分接受编码部分传入的编码向量，每一步输出一个翻译单词即结果。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630110532.png" alt></p><p>（2）image sequence模型：根据给出的图片输出一个描述</p><a id="more"></a><p>模型由卷积网络和RNN模型构成：</p><p>卷积网络部分去掉最后的预测值，保留至全连接层，输入的图片经过卷积网络最终得到一个描述图片的向量。</p><p>RNN模型部分仅保留解码部分，接收卷积网络传入的描述向量，每一步输出一个翻译单词即结果。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630110603.png" alt></p><h2 id="有条件的语言模型——机器翻译模型"><a href="#有条件的语言模型——机器翻译模型" class="headerlink" title="有条件的语言模型——机器翻译模型"></a>有条件的语言模型——机器翻译模型</h2><h3 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h3><p>基本的语言模型是输入x，直接输出y，结构模型同RNN模型中国的多对多模型</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630110905.png" alt></p><h3 id="机器翻译模型"><a href="#机器翻译模型" class="headerlink" title="机器翻译模型"></a>机器翻译模型</h3><p>结构同sequence to sequence模型，由编码器和解码器构成，机器翻译会得到很多可能的翻译句子，解码器每步输出的结构都是为了让p(y|x)最大，即第一步时p(y1|x)，当输入单词是的得到y1的概率最大，第二步时由于接受第一步结果的输入p(y2|x,y1)，在输入单词和第一步输出结果的情况下概率最大，…以此类推，因此是有条件的模型。</p><p>为了找到最准确的翻译，因此，目标为：要找到一个翻译句子使得条件概率最大，使得p(y^1,y^2….y^ty|x)最大：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630111740.jpg" alt></p><p>即<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630111404.png" alt></p><h3 id="两种达到准确翻译的方法"><a href="#两种达到准确翻译的方法" class="headerlink" title="两种达到准确翻译的方法"></a>两种达到准确翻译的方法</h3><p>（1）贪心搜索</p><p>在解码器部分，每一步的输出都选择当前最大概率输出的一个单词。</p><p>但是根据概率公式看，贪心搜索得到的单词仅考虑当前最佳的一个结果，可能会因为前面的最佳而达不到后面最佳的单词，因此效果不是特别好。</p><p>（2）Beam Search（束搜索）</p><p>思路和贪心搜索类似，但是增加了束宽b，每一步输出都选概率最大的前b个单词。</p><p>每一步是复制了三个同样的网络来评估得到的单词概率，当b=1时就退化成了贪心搜索。</p><p>以b=3为例，在第一步选出的三个单词的基础上选择下一个单词时，就是复制了三个同样的网络，找到三个网络得到的单词中最大概率的前三个：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630112804.png" alt></p><h3 id="改进Beam-Search（束搜索）"><a href="#改进Beam-Search（束搜索）" class="headerlink" title="改进Beam Search（束搜索）"></a>改进Beam Search（束搜索）</h3><p>由于束搜索最终需要让概率最大，是将每步概率相乘得到，概率在0~1之间属于很小的数，很多很小的数相乘会出现数值下溢，同时当出现短句子的时候往往概率大，原概率容易出现翻译成短句子的现象，因此需要进行长度归一化。</p><p>（1）防止数值下溢：在概率之前加上log，因为log函数单调递增，当log函数增加，原函数也增加，所以无影响</p><p>（2）防止出现翻译出来大量的短句子：在最终求和的概率前面加上归一化的系数Ty^alpha，这里alpha可以进行尝试，取值为1，是进行长度归一化，取值为0，则没有归一化，这里可以取折中的0.7或者尝试其他值。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630113356.png" alt></p><p>束宽的选择：b越大，结果更好，但运行慢，b越小，结果会差，但运行快。需要合理选择</p><h3 id="误差分析"><a href="#误差分析" class="headerlink" title="误差分析"></a>误差分析</h3><p>如何判断机器翻译模型的好坏：由于机器翻译模型有RNN模型和束搜索模型同时作用，需要合理判别。机器翻译模型最终目标为：argmax   p（y|x），两者的作用：</p><p>（1）RNN模型：主要是为了能得到概率 p（y|x）</p><p>（2）束搜索模型：找到最大的概率，argmax</p><p>因此，当人工翻译概率为y * ，机器翻译为y^时:（同样可以列出表格，遍历开发集找到出错的样本进行分析）</p><p>当p(y *|x) &gt;p(y^|x)，说明束搜索没有找到更大的概率，需要改进束搜索，选择更大的束宽b；</p><p>当p(y *|x) &lt;=p(y^|x)，说明束搜索找到了大的概率，但给予人工翻译小的概率值，说明RNN模型计算概率存在问题，需要改RNN模型，增加正则化、增加更多训练数据或者尝试不同网络模型等方法。</p><h3 id="Bleu得分"><a href="#Bleu得分" class="headerlink" title="Bleu得分"></a>Bleu得分</h3><p>一个单实数评估指标，能够加速算法性能。</p><p>将机器翻译中的所有单词元组出现的概率相加，归一化后乘以惩罚项（防止过短的翻译）：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630114422.png" alt></p><p>（1）BP</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630114434.png" alt></p><p>当机器翻译长度比参考翻译长度长，则惩罚项不起作用，若短于参考翻译，则惩罚项起作用。</p><p>（2）Pn</p><p>Pn表示n元组单词的概率，n元组单词表示n个相邻的单词，概率为每个元组的单词的得分上限之和 / 所有元组单词在机器翻译中出现的次数：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630114454.png" alt></p><p>其中得分上限是指每个元组的单词在几个人工翻译句子中出现的最大次数</p><p>（3）总结</p><p>Bleu得分可以用来评价机翻的好坏，代替人工方法进行自动评估。</p><h2 id="Attention模型——更符合人的思维的机翻模型"><a href="#Attention模型——更符合人的思维的机翻模型" class="headerlink" title="Attention模型——更符合人的思维的机翻模型"></a>Attention模型——更符合人的思维的机翻模型</h2><h3 id="提出背景"><a href="#提出背景" class="headerlink" title="提出背景"></a>提出背景</h3><p>上述的机翻模型是需要编码器将所有句子读入，才进行翻译，当句子过长时，需要等的时间长，会出现考虑不到后面的句子Bleu得分低的情况，而且不符合人的思维。</p><p>人的思维是读入一部分句子就进行翻译，因此提出了注意力模型。</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>整体结构</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630121532.png" alt></p><p>说明：为了考虑前后单词的意思影响，需要采用双向RNN</p><p>1°每个节点有正向激活值和反向激活值</p><p>2°当输出第一个预测单词时，每个输入按照权重和激活值得到上下文向量</p><p>3°根据上下文向量和前一个隐藏层的值得到当前预测值</p><p>综上，实现了输入一段文字就实时输出预测的翻译单词。</p><p>（1）一个输出神经元结构</p><p>以时间t输出的y^t为例：</p><p>首先按照权重乘以激活值求得上下文向量context，然后利用前一层的隐藏层的值s^t-1得到预测值</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630121517.png" alt></p><p>（2）权重的得出</p><p>权重取值如图所示，所有节点的权重加起来需要满足为1</p><p>alpha&lt;t,t^`&gt;表示预测y^t需要放在a&lt;t^&gt;上的注意力数量，即权重，当前的权重和上一层的值和当前激活值有关，因此将二者传入网络。</p><p>求权重的方法：</p><p>构造小型的神经元进行计算，根据权重和上一个隐藏层值经过全连接层得到e的值，再连接一个全连接层求得式子的值，最后连接一个softmax激活函数，保证权重之和为1.</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630121548.png" alt></p><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>将人类描述的日期转换为机器能识别的日期，即YYYY-MM-DD 10位表达法</p><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p>（1）构建一个神经元的上下文：利用全局变量</p><p>1°定义重复层：由于需要根据上一层的隐藏层值和当前激活值求得权重，所以需要按照Tx个时间步复制上一层的值</p><p>2°定义连接层：将上一层隐藏层值和当前激活值连接</p><p>3°定义全连接层层：将连接值传入小型网络求得中间值</p><p>4°定义全连接层层：将中间值传入小型网络求得结果</p><p>5°定义softmax层：为了保证权重求和为1，最后输出层采用softmax，输入权重</p><p>6°定义点乘层：为了得到上下文context，将得到的权重和激活值相乘得到上下文向量</p><p>总结：根据前一层的隐藏层的值，重复Tx次和激活值连接，经过两个全连接层得到权重，最后和激活值相乘得到上下文向量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#对一个神经元求出其上下文</span></span><br><span class="line"><span class="comment">#设置全局变量 </span></span><br><span class="line">repeator=RepeatVector(Tx) <span class="comment">#重复</span></span><br><span class="line">concatenator=Concatenate(axis=<span class="number">-1</span>) <span class="comment">#连接</span></span><br><span class="line">densor1=Dense(<span class="number">10</span>,activation=<span class="string">'tanh'</span>)</span><br><span class="line">densor2=Dense(<span class="number">1</span>,activation=<span class="string">'relu'</span>)</span><br><span class="line">activator=Activation(softmax,name=<span class="string">"attention_weights"</span>)</span><br><span class="line">dotor=Dot(axes=<span class="number">1</span>) <span class="comment">#进行点积的轴的个数</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#上下文用到前一个隐藏层的值和当前的激活值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">one_step_attention</span><span class="params">(a,s_prev)</span>:</span></span><br><span class="line"><span class="comment">#要和Tx个a值相乘，需要重复</span></span><br><span class="line">s_prev=repeator(s_prev)</span><br><span class="line">concat=concatenator([a,s_prev])</span><br><span class="line"></span><br><span class="line"><span class="comment">#利用两个全连接层求出权重</span></span><br><span class="line">e=densor1(concat)</span><br><span class="line">energies=densor2(e)</span><br><span class="line"></span><br><span class="line"><span class="comment">#保证权重求和为1</span></span><br><span class="line">alpha=activator(energies)</span><br><span class="line"></span><br><span class="line"><span class="comment">#权重乘以激活值得到上下文</span></span><br><span class="line">context=dotor([alpha,a])</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> context</span><br></pre></td></tr></table></figure><p>（2）搭建模型：利用全局变量</p><p>这里使用LSTM单元</p><p>1°初始化网络：定义输入的X，LSTM单元的隐藏层的值s0，记忆细胞c0</p><p>2°定义双向RNN——Bidiretional：根据双向RNN网络计算出激活值</p><p>3°对于每个输出的时间步Ty，计算上下文的值</p><p>4°定义LSTM：将上下文向量、上一层的隐藏层值、记忆细胞传入LSTM单元中</p><p>5°定义Dense：将lstm的值传入全连接层得到当前时间步的输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#建立模型</span></span><br><span class="line"><span class="comment">#设置全局变量</span></span><br><span class="line">n_a=<span class="number">32</span></span><br><span class="line">n_s=<span class="number">64</span></span><br><span class="line">post_activation_LSTM_cell=LSTM(n_s,return_state=<span class="literal">True</span>)</span><br><span class="line">output_layer=Dense(len(machine_vocab),activation=softmax)</span><br><span class="line"><span class="comment">#return_sequences: 布尔值。是返回输出序列中的最后一个输出，还是全部序列。</span></span><br><span class="line"><span class="comment">#return_state: 布尔值。除了输出之外是否返回最后一个状态。</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(Tx,Ty,n_a,n_s,human_vocab_size,machine_vocab_size)</span>:</span></span><br><span class="line">X=Input(shape=(Tx,human_vocab_size))</span><br><span class="line">s0=Input(shape=(n_s,),name=<span class="string">'s0'</span>)</span><br><span class="line">c0=Input(shape=(n_s,),name=<span class="string">'c0'</span>)</span><br><span class="line">s=s0</span><br><span class="line">c=c0 <span class="comment">#c为记忆细胞</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#n_a为输出维度</span></span><br><span class="line">a=Bidirectional(LSTM(n_a,return_sequences=<span class="literal">True</span>),input_shape=(m,Tx,n_a*<span class="number">2</span>))(X)<span class="comment">#双向RNN</span></span><br><span class="line">outputs=[]</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出序列长度Ty</span></span><br><span class="line"><span class="comment">#每一步输出一个结果</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(Ty):</span><br><span class="line">context=one_step_attention(a,s)</span><br><span class="line"></span><br><span class="line"><span class="comment">#initial_state = [hidden state, cell state]</span></span><br><span class="line">s,_,c=post_activation_LSTM_cell(context,initial_state=[s,c])</span><br><span class="line">output=output_layer(s)</span><br><span class="line">outputs.append(output)</span><br><span class="line">model=Model(inputs=[X,s0,c0],outputs=outputs)</span><br><span class="line"><span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><p>（3）结果</p><p>这里设定输入的日期长度为30，即Tx=30，输入日期长度固定为10，即Ty=10</p><p>为了快速建立可用系统，可以先导入预训练的权重，再编译模型（可以自定义梯度下降函数及参数）、训练模型，最终根据输入的数据得出结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">model=model(Tx,Ty,n_a,n_s,len(human_vocab),len(machine_vocab))</span><br><span class="line"><span class="comment"># model.summary()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#编译模型</span></span><br><span class="line">model.load_weights(<span class="string">'models/model.h5'</span>)</span><br><span class="line">opt=Adam(lr=<span class="number">0.005</span>,beta_1=<span class="number">0.9</span>,beta_2=<span class="number">0.999</span>,decay=<span class="number">0.01</span>)</span><br><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,optimizer=opt,metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"><span class="comment">#输入训练集训练模型</span></span><br><span class="line">s0=np.zeros((m,n_s))</span><br><span class="line">c0=np.zeros((m,n_s))</span><br><span class="line">outputs=list(Yoh.swapaxes(<span class="number">0</span>,<span class="number">1</span>)) <span class="comment">#轴交换</span></span><br><span class="line">model.fit([Xoh,s0,c0],outputs,epochs=<span class="number">1</span>,batch_size=<span class="number">100</span>) <span class="comment">#迭代一次就能给出结果</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#输入数据进行预测</span></span><br><span class="line">Examples=[<span class="string">'3 May 1979'</span>, <span class="string">'5 April 09'</span>, <span class="string">'21th of August 2016'</span>, <span class="string">'Tue 10 Jul 2007'</span>, <span class="string">'Saturday May 9 2018'</span>, <span class="string">'March 3 2001'</span>, <span class="string">'March 3rd 2001'</span>, <span class="string">'1 March 2001'</span>]</span><br><span class="line"><span class="keyword">for</span> examples <span class="keyword">in</span> Examples:</span><br><span class="line">source=string_to_int(examples,Tx,human_vocab)</span><br><span class="line">source=np.array(list(map(<span class="keyword">lambda</span> x:to_categorical(x,num_classes=len(human_vocab)),source)))</span><br><span class="line">source=np.expand_dims(source,axis=<span class="number">0</span>) <span class="comment">#增加一个维度</span></span><br><span class="line">prediction=model.predict([source,s0,c0])</span><br><span class="line"><span class="comment"># print(prediction)</span></span><br><span class="line">prediction=np.argmax(prediction,axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">predictional=[inv_machine_vocab[int(i)] <span class="keyword">for</span> i <span class="keyword">in</span> prediction]</span><br><span class="line"></span><br><span class="line">print(<span class="string">'source:'</span>,examples)</span><br><span class="line">print(<span class="string">'predict'</span>,<span class="string">''</span>.join(predictional))</span><br></pre></td></tr></table></figure><p>结果为：</p><blockquote><p>source: 3 May 1979<br>predict 1979-05-03<br>source: 5 April 09<br>predict 2009-04-05<br>source: 21th of August 2016<br>predict 2016-08-21<br>source: Tue 10 Jul 2007<br>predict 2007-07-10<br>source: Saturday May 9 2018<br>predict 2018-05-09<br>source: March 3 2001<br>predict 2001-03-03<br>source: March 3rd 2001<br>predict 2001-03-03<br>source: 1 March 2001<br>predict 2001-03-01</p></blockquote><p>从结果看出模型很好的翻译了人类日期。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;模型简介&quot;&gt;&lt;a href=&quot;#模型简介&quot; class=&quot;headerlink&quot; title=&quot;模型简介&quot;&gt;&lt;/a&gt;模型简介&lt;/h2&gt;&lt;p&gt;（1）sequence to sequence模型：根据给定的句子得到翻译的结果&lt;/p&gt;&lt;p&gt;模型由编码器和解码器构成：&lt;/p&gt;&lt;p&gt;编码器部分读入输入的句子中每个单词，每步不输出结果，仅在最后一步输出一个编码向量。&lt;/p&gt;&lt;p&gt;解码器部分接受编码部分传入的编码向量，每一步输出一个翻译单词即结果。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200630110532.png&quot; alt&gt;&lt;/p&gt;&lt;p&gt;（2）image sequence模型：根据给出的图片输出一个描述&lt;/p&gt;
    
    </summary>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="循环神经网络" scheme="https://www.xiapf.com/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>序列模型RNN——使用词嵌入搭建NLP系统</title>
    <link href="https://www.xiapf.com/blogs/rnn2/"/>
    <id>https://www.xiapf.com/blogs/rnn2/</id>
    <published>2020-06-23T12:27:36.000Z</published>
    <updated>2020-06-23T12:32:34.998Z</updated>
    
    <content type="html"><![CDATA[<h2 id="词嵌入"><a href="#词嵌入" class="headerlink" title="词嵌入"></a>词嵌入</h2><h3 id="什么是词嵌入"><a href="#什么是词嵌入" class="headerlink" title="什么是词嵌入"></a>什么是词嵌入</h3><p>以词的特征来表示各个单词，即单词的高维度特征表示。</p><p>与one-hot表示词相比：</p><p>（1）能更好的反映词的特征，增强相关词的泛化能力（例如学习了orange juice，后面会很容易根据apple推测出apple juice，因为apple和orange的特征表示很像），而使用one-hot表示，词之前没有联系，无法泛化</p><a id="more"></a><p>（2）节约了表示词向量所占的内存，one-hot表示方法，每个词向量的大小和字典的大小相同，词嵌入方法中每个词的大小和特征维度相关。</p><p>使用t-SNE可以将高维度的特征向量映射到二维空间，可以更容易观察各个向量之间的联系，靠的更近的联系更紧密。</p><h3 id="为什么使用词嵌入"><a href="#为什么使用词嵌入" class="headerlink" title="为什么使用词嵌入"></a>为什么使用词嵌入</h3><p>词嵌入可以用在迁移学习中，目前的NLP问题中已标记的数据集较少，此时使用词嵌入可以很快速又准确的构建NLP系统。</p><p>（1）先从大量文本中学习词嵌入（或者从网上下载别人训练好的词嵌入模型）    task A</p><p>（2）将词嵌入模型迁移到具有少量标记样本的数据集中     task B</p><p>（3）微调：当自己已标记的数据量很大时，可以对已有的词嵌入模型进行微调</p><p>迁移学习原理：将在task A中学习的知识迁移到B中，此时A数据量很大，B数据量很小，训练出的模型即使在以后出现了不在B中的词的情况下也能很好的预测，因为拥有从很大数据量中学习到的知识（task A）</p><h2 id="搭建词嵌入模型的方法（Word2Vec）"><a href="#搭建词嵌入模型的方法（Word2Vec）" class="headerlink" title="搭建词嵌入模型的方法（Word2Vec）"></a>搭建词嵌入模型的方法（Word2Vec）</h2><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200623162528.png" alt></p><p>总体思路：学习词嵌入模型即学习嵌入矩阵E，将每个词使用one-hot表示为Ox，再乘以嵌入矩阵E即可得到每个词的词嵌入表示方法。</p><p>方法：通过选择上下文-目标词对来学习词嵌入。</p><p>（1）前面的4个单词-预测词</p><p>（2）前面的4个单词-预测词-后面的4个单词</p><p>（3）前面的1个单词-预测词</p><p>（4）与预测词关系最近的一个单词-预测词</p><h3 id="skip-gram模型"><a href="#skip-gram模型" class="headerlink" title="skip gram模型"></a>skip gram模型</h3><p>（1）训练集构建</p><p>上下文-目标词：任意选择一个词-选择的词的前后距离内随机再选择一个词</p><p>（2）模型</p><p>Ox-&gt;E-&gt;ec-&gt;softmax-&gt;yhat</p><p>softmax中的概率为目标词t出现在词c上下文的概率：<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200623162639.png" alt></p><p>损失函数为：L=-∑y * log(yhat)</p><p>存在问题：计算每个词的概率都需要对词汇表中的词进行求和，成本高</p><p>解决：采用分级softmax树形分类器，常用词放在顶部，非常用词放在树的深处，每个节点都是一个二分类器。</p><p>将多分类问题转换为多个二分类问题。</p><h3 id="负采样"><a href="#负采样" class="headerlink" title="负采样"></a>负采样</h3><p>（1）训练集构建</p><p>上下文-目标词：每组k+1个词，给出一组词判断是否是一对上下文-目标词对</p><p>第一组词（正样本）：当前词-在句子中前后距离内随机选择一个词，标签为1</p><p>剩余k-1个词（负样本）：当前词-在字典中随机选择一个词，标签为0</p><p>（2）模型</p><p>Ox-&gt;E-&gt;ec-&gt;二分类器-&gt;yhat</p><p>每次训练k+1个，softmax中是二分类问题。 </p><p>二分类中的概率为目标词t出现在词c上下文的概率：<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200623162745.png" alt></p><p>（3）如何选择负样本</p><p>即考虑词出现的概率又考虑词的分布，以词出现概率的3/4除以总体得到的概率来选择负样本（单一以概率，容易一直选择到the a of等词）：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200623162853.png" alt></p><h3 id="glove向量"><a href="#glove向量" class="headerlink" title="glove向量"></a>glove向量</h3><p>（1）训练集构建</p><p>上下文-目标词：用Xij表示单词i出现在单词j上下文的次数，衡量两个单词彼此接近频率的计数器</p><p>（2）模型</p><p>最小化两个单词之前的距离：<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200623162915.png" alt></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>有以上三种方法搭建词嵌入模型，得到从大量文本中学习到的词向量知识。</p><p>为了快速搭建NLP系统，可以从网上下载别人已训练好的词嵌入，用此进行系统搭建。</p><h3 id="采用词嵌入进行词的类比"><a href="#采用词嵌入进行词的类比" class="headerlink" title="采用词嵌入进行词的类比"></a>采用词嵌入进行词的类比</h3><p>描述：解决a与b类似于c与？的问题</p><p>（1）导入词嵌入</p><p>这里的词嵌入采用glove向量，导入词嵌入模型中的所有单词，及单词对应的词嵌入向量表示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.导入词嵌入中的词汇和对应表</span></span><br><span class="line"><span class="comment">#word代表字符 word_to_vec_map代表向量表</span></span><br><span class="line">word,word_to_vec_map=w2v_utils.read_glove_vecs(<span class="string">'./data/glove.6B.50d.txt'</span>)</span><br></pre></td></tr></table></figure><p>（2）定义相似度度量函数</p><p>词之间的相似度采用余弦相似度来表示，余弦值越大，当等于1时，此时角度为0，说明两个词最相似，余弦值越小两个词越不相似，根据计算余弦相似度定义函数：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200623170723.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#2.计算词之间的相似度</span></span><br><span class="line"><span class="comment">#采用余弦相似度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cosine_similarity</span><span class="params">(u,v)</span>:</span></span><br><span class="line">np_dot=np.dot(u,v)</span><br><span class="line">linalg_u=np.linalg.norm(u)</span><br><span class="line">linalg_v=np.linalg.norm(v)</span><br><span class="line">linalg_uv=np.dot(linalg_u,linalg_v)</span><br><span class="line"></span><br><span class="line">cos_sim=np.divide(np_dot,linalg_uv)</span><br><span class="line"><span class="keyword">return</span> cos_sim</span><br></pre></td></tr></table></figure><p>（3）进行词的类比</p><p>目标：ea-eb≈eb-e?，为了让等式成立就要使得等式前后的式子代表的向量越相似，即余弦值要最大</p><p>1° 将输入的三个单词转换为向量表示</p><p>2°对词汇表中所有单词进行遍历，计算ea-eb≈eb-e?</p><p>3°选择余弦值最大的即最相似时的单词作为推出的单词</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">complete_analogy</span><span class="params">(word_a,word_b,word_c,word_to_vec_map)</span>:</span></span><br><span class="line"><span class="comment">#因为词嵌入模型中单词均为小写，所以这里需要转换</span></span><br><span class="line">word_a,word_b,word_c=word_a.lower(),word_b.lower(),word_c.lower()</span><br><span class="line"><span class="comment">#将单词转换为向量进行运算</span></span><br><span class="line">e_a,e_b,e_c=word_to_vec_map[word_a],word_to_vec_map[word_b],word_to_vec_map[word_c]</span><br><span class="line"></span><br><span class="line"><span class="comment">#得到字典里所有单词</span></span><br><span class="line">words=word_to_vec_map.keys()</span><br><span class="line"></span><br><span class="line"><span class="comment">#找到最相近的距离</span></span><br><span class="line">max_cosine_sim=<span class="number">-100</span></span><br><span class="line">cosine_word=word_c</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line"><span class="keyword">if</span> word <span class="keyword">in</span> [word_a,word_b,word_c]:</span><br><span class="line"><span class="keyword">continue</span></span><br><span class="line"><span class="comment">#余弦相似度：越相似，角度越小，值越大</span></span><br><span class="line"><span class="comment">#cos值为1的时候最大，此时角度为0</span></span><br><span class="line">cosine_sim=cosine_similarity((e_a-e_b),(e_c-word_to_vec_map[word]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> cosine_sim&gt;max_cosine_sim:</span><br><span class="line">max_cosine_sim=cosine_sim</span><br><span class="line">cosine_word=word</span><br><span class="line"><span class="keyword">return</span> cosine_word</span><br></pre></td></tr></table></figure><p>结果：</p><p>输入一组词，进行类比推理：</p><blockquote><p>triads_to_try = [(‘italy’, ‘italian’, ‘spain’), (‘india’, ‘delhi’, ‘japan’), (‘man’, ‘woman’, ‘boy’), (‘happy’, ‘baby’, ‘beautiful’)]<br>for triad in triads_to_try:<br>    print (‘{} -&gt; {} &lt;====&gt; {} -&gt; {}’.format( * triad, complete_analogy( * triad,word_to_vec_map)))</p></blockquote><p>得出结果：</p><blockquote><p>italy -&gt; italian &lt;====&gt; spain -&gt; spanish<br>india -&gt; delhi &lt;====&gt; japan -&gt; tokyo<br>man -&gt; woman &lt;====&gt; boy -&gt; girl<br>happy -&gt; baby &lt;====&gt; beautiful -&gt; newborn</p></blockquote><p>可以看出类比出的词距离最相近，即相似。</p><p>（4）消除词嵌入中的偏差（选学，了解即可）</p><p>因为社会中存在年龄、性别等的歧视，所以在大量文本中学习到的词嵌入容易存在偏差，因此从以下两方面可以消除存在的偏差。</p><p>以性别歧视带来的偏差为例子</p><p>1°中和步：没有明确性别含义的词</p><p>设单词word的词嵌入为e，消除e和g之间的偏差，因为e与性别无关，即需要减少其到非性别轴上的距离差，即将e沿着g的方向归0即可</p><p>公式：<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200623171310.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#4.消除词嵌入中的偏差</span></span><br><span class="line"><span class="comment">#中和步:没有明确性别的单词</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">neutralize</span><span class="params">(word,g,word_to_vec_map)</span>:</span></span><br><span class="line">word=word.lower()</span><br><span class="line">e=word_to_vec_map[word]</span><br><span class="line">np_dot=np.dot(e,g)</span><br><span class="line"><span class="comment">#square求平方</span></span><br><span class="line">e_bias=np.multiply(np.divide(np_dot,np.square(np.linalg.norm(g))),g)</span><br><span class="line">e_disbias=e-e_bias</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> e_disbias</span><br></pre></td></tr></table></figure><p>2°均衡步：有明确性别含义的词</p><p>将具有明确性别含义的一对单词，使得他们到非性别轴的距离相等，只有性别才能将其区分。</p><p>公式：<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200623171348.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#均衡步：明确性别的单词</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">equalize</span><span class="params">(pairs,bias_axis,word_to_vec_map)</span>:</span></span><br><span class="line">w1,w2=pairs</span><br><span class="line">e_w1,e_w2=word_to_vec_map[w1],word_to_vec_map[w2]</span><br><span class="line">mu=np.divide((e_w1+e_w2),<span class="number">2</span>)</span><br><span class="line">mub=np.divide(np.multiply(np.dot(mu,bias_axis),bias_axis),np.square(np.linalg.norm(bias_axis)))</span><br><span class="line">mu_orth=mu-mub</span><br><span class="line">e_w1b=np.divide(np.multiply(np.dot(e_w1,bias_axis),bias_axis),np.square(np.linalg.norm(bias_axis)))</span><br><span class="line">e_w2b=np.divide(np.multiply(np.dot(e_w2,bias_axis),bias_axis),np.square(np.linalg.norm(bias_axis)))</span><br><span class="line">e_w1cor=np.multiply(np.sqrt(np.abs(<span class="number">1</span>-np.square(np.linalg.norm(mu_orth)))),np.divide(e_w1b-mub,np.abs((e_w1-mu_orth)-mub)))</span><br><span class="line">e_w2cor=np.multiply(np.sqrt(np.abs(<span class="number">1</span>-np.square(np.linalg.norm(mu_orth)))),np.divide(e_w2b-mub,np.abs((e_w2-mu_orth)-mub)))</span><br><span class="line">e1=e_w1cor+mu_orth</span><br><span class="line">e2=e_w2cor+mu_orth</span><br><span class="line"><span class="keyword">return</span> e1,e2</span><br></pre></td></tr></table></figure><h2 id="利用词嵌入搭建NLP系统"><a href="#利用词嵌入搭建NLP系统" class="headerlink" title="利用词嵌入搭建NLP系统"></a>利用词嵌入搭建NLP系统</h2><h3 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h3><p>利用已训练的词嵌入模型，根据输入的句子，判断句子代表的emoji表情，设定emoji标签为5个。</p><h3 id="词嵌入与数据集"><a href="#词嵌入与数据集" class="headerlink" title="词嵌入与数据集"></a>词嵌入与数据集</h3><p>（1）导入词嵌入</p><p>将单词和单词代表的golve向量作为该模型中的词嵌入，并将每个单词在单词表中的索引进行存储。num=len(word_to_vec_map[‘.’])</p><p>每个使用词嵌入的词的维度为(num,1)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#0.定义词向量</span></span><br><span class="line">word_to_index, index_to_word, word_to_vec_map=emo_utils.read_glove_vecs(<span class="string">'./data/glove.6B.50d.txt'</span>)</span><br></pre></td></tr></table></figure><p>（2）训练集</p><p>1°x：从表格中读取每行的句子构成句子数组</p><p>2°y：读入每个句子的标签值，因为emoji有五个标签，这里将y标签值转换为1 * 5的独热向量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.导入数据</span></span><br><span class="line">X_train,Y_train=emo_utils.read_csv(<span class="string">'./data/train_emoji.csv'</span>)</span><br><span class="line">X_test,Y_test=emo_utils.read_csv(<span class="string">'./data/test.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将y转换为独热向量</span></span><br><span class="line">Y_oh_train=emo_utils.convert_to_one_hot(Y_train,C=<span class="number">5</span>)</span><br><span class="line">Y_oh_test=emo_utils.convert_to_one_hot(Y_test,C=<span class="number">5</span>)</span><br></pre></td></tr></table></figure><h3 id="简单模型"><a href="#简单模型" class="headerlink" title="简单模型"></a>简单模型</h3><p>将句子中每个单词使用词嵌入表示，每个词的维度为（num,1），假设句子中有k个单词，则将这k个单词的词嵌入竖向堆叠起来，取均值，再经过softmax进行预测得到结果。</p><p>网络模型</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200623163047.png" alt></p><p>softmax网络传播</p><p>1°定义softmax层的权重和偏置量，由于输出值为（1，5）的独热向量，偏置量b的维度为（n_y,1）n_y=5，权重w的维度为（n_h,n_y），均值后avg的维度为（50,），n_h=50</p><p>w:（50，5）</p><p>b:（5，1）</p><p>2°在每次迭代中，分别对所有句子遍历，得到每个单词词嵌入的平均数，再进行正向传播，计算损失，反向传播，计算梯度</p><p>网络结构：avg-&gt;linear-&gt;a-&gt;softmax-&gt;yhat</p><p>3°得到句子中每个单词词嵌入的平均数：拆分句子中的单词，对所有单词遍历，得到每个单词的词嵌入再除以单词总数，得到平均值作为网络的输入</p><p>（1）将输入的单词转换为glove向量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#2.将输入的句子拆分为单词并用glove词向量，并对句子中每个单词取平均得到整个句子代表的向量</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sentence_to_avg</span><span class="params">(sentence,word_to_vec_map)</span>:</span></span><br><span class="line"><span class="comment">#拆分句子</span></span><br><span class="line">words=sentence.lower().split()</span><br><span class="line"></span><br><span class="line">m=len(word_to_vec_map[<span class="string">'.'</span>])</span><br><span class="line">add_sum=np.zeros((m,))</span><br><span class="line">count=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">add_sum+=word_to_vec_map[word]</span><br><span class="line">count+=<span class="number">1</span></span><br><span class="line">avg=add_sum/count</span><br><span class="line"><span class="keyword">return</span> avg</span><br></pre></td></tr></table></figure><p>（2）搭建模型，将每个句子代表的向量输入softmax单元中进行训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#3.构建模型</span></span><br><span class="line"><span class="comment">#x:(m,1)   y: (m,1)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X,Y,word_to_vec_map,learning_rate=<span class="number">0.01</span>,num_iteritions=<span class="number">400</span>)</span>:</span></span><br><span class="line"><span class="comment">#初始化参数</span></span><br><span class="line"></span><br><span class="line">m=Y.shape[<span class="number">0</span>]</span><br><span class="line">n_y=<span class="number">5</span></span><br><span class="line">n_h=len(word_to_vec_map[<span class="string">'.'</span>])</span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">W=np.random.randn(n_y,n_h)/np.sqrt(n_h)</span><br><span class="line">b=np.zeros((n_y,))</span><br><span class="line"></span><br><span class="line"><span class="comment">#y值转换为独热向量</span></span><br><span class="line">Y_oh=emo_utils.convert_to_one_hot(Y,C=n_y)</span><br><span class="line">costs=[]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#每次迭代</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(num_iteritions):</span><br><span class="line"><span class="comment">#拆分每个句子</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">avg=sentence_to_avg(X[i],word_to_vec_map)</span><br><span class="line"></span><br><span class="line"><span class="comment">#正向传播计算softmax 利用得到的均值</span></span><br><span class="line">z=np.dot(W,avg)+b</span><br><span class="line">a=emo_utils.softmax(z)</span><br><span class="line">cost=-np.sum(Y_oh[i]*np.log(a))</span><br><span class="line"></span><br><span class="line"><span class="comment">#反向传播</span></span><br><span class="line">dz=a-Y_oh[i]</span><br><span class="line">dw=np.dot(dz.reshape(n_y,<span class="number">1</span>),avg.reshape(<span class="number">1</span>,n_h))</span><br><span class="line">db=dz</span><br><span class="line"></span><br><span class="line">W=W-learning_rate*dw</span><br><span class="line">b=b-learning_rate*db</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> t%<span class="number">100</span>==<span class="number">0</span>:</span><br><span class="line">print(<span class="string">"第"</span>+str(t)+<span class="string">"次，损失为："</span>+str(cost))</span><br><span class="line">costs.append(cost)</span><br><span class="line">pred=emo_utils.predict(X,Y,W,b,word_to_vec_map)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> pred,W,b</span><br></pre></td></tr></table></figure><p>（3）结果</p><p>将训练集和词嵌入模型传入，定义学习率为0.01，迭代次数400次，最终得到训练好的网络参数（w,b），传入训练集和测试集，得到准确率为</p><blockquote><p>pred, W, b = model(X_train, Y_train, word_to_vec_map)<br>print(“训练集：”)<br>pred_train=emo_utils.predict(X_train,Y_train,W,b,word_to_vec_map)<br>print(“测试集：”)<br>pred_test=emo_utils.predict(X_test,Y_test,W,b,word_to_vec_map)</p></blockquote><blockquote><p>训练集：<br>Accuracy: 0.9772727272727273<br>测试集：<br>Accuracy: 0.8571428571428571</p></blockquote><p>选择一些句子转换为数组进行预测，得到emoji表情，发现you are not happy预测为 ❤️，可见该模型没有考虑词的顺序，只根据happy得到了预测，没有考虑前面的not，因此采用考虑词序的RNN模型对其进行改进。</p><h3 id="RNN模型"><a href="#RNN模型" class="headerlink" title="RNN模型"></a>RNN模型</h3><p>网络模型</p><p>总体思路：将每个句子乘上词嵌入矩阵得到特征向量，作为RNN的输入，每个时刻都考虑了上一个顺序的输入，最终的到输出预测值。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200623163134.png" alt></p><p>实际中网络结构稍有变化，采用两层隐藏层，为了防止过拟合，每层的输出都会经过dropout，随机丢弃一些节点，第一层隐藏层输入Tx个数据，输出Tx个数据。第二层隐藏层输入Tx个数据，输出1个数据，即最终的预测结果。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200623194624.png" alt></p><p>1°按照最大长度的句子单词数补0：由于传入网络的单词数量固定，所以需要补0</p><p>对m个句子，按照max_len大小生成（m,max_len）的补0后的单词矩阵（以单词位置作为矩阵的值）：</p><p>对每个句子，拆分其中单词，得到每个单词的索引进行存储</p><p>2°将单词矩阵乘以词嵌入矩阵E得到嵌入的特征向量表示e：每一行是一个向量表示：</p><p>根据每个单词的索引位置得到在词嵌入中的向量表示，并存储在嵌入矩阵E中；</p><p>利用keras.layer中的Embedding层得到每个单词的词嵌入表示，该层不需要训练，则设置trainable=False，输入词汇表的大小，输出词嵌入向量的大小，建立该层，并设置该层的权重为嵌入矩阵E，最终能返回每个单词的词嵌入。</p><p>注：此步骤是根据Ox * E =ec，求其中的E，而简单模型中是直接使用了词向量。</p><p>3°搭建模型：</p><p>根据输入的最大单词的长度，定义网络的输入；</p><p>根据已经训练的词向量得到词嵌入矩阵E，将网络输入传入其中，得到词嵌入，作为真正的输入LSTM单元中的数据；</p><p>定义LSTM-&gt;Dropout-&gt;LSTM-&gt;Dropout-&gt;Dense-&gt;Activation(sotfmax)，最终的到输出</p><p>（1）填充每个句子：以最大长度的句子为基准，在末尾进行补0</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#采用RNN模型构建考虑词顺序的预测网络</span></span><br><span class="line"><span class="comment">#1.将短的词按照最长的长度填充</span></span><br><span class="line"><span class="comment">#传入句子数组及其索引</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sentences_to_indices</span><span class="params">(X,word_to_index,max_len)</span>:</span></span><br><span class="line">m=X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">X_indices=np.zeros((m,max_len))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">words=X[i].lower().split()</span><br><span class="line">j=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">X_indices[i,j]=word_to_index[word]</span><br><span class="line">j=j+<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> X_indices</span><br></pre></td></tr></table></figure><p>（2）得到词嵌入矩阵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#2.构建embedding层，构建词向量矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pretrained_embedding_layer</span><span class="params">(word_to_vec_map,word_to_index)</span>:</span></span><br><span class="line">vocab_size=len(word_to_index)+<span class="number">1</span> <span class="comment">#加1是为了考虑句子没有到最大长度的0向量</span></span><br><span class="line">em_size=len(word_to_vec_map[<span class="string">'.'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#每个单词的词向量进行堆叠</span></span><br><span class="line">emb_matrix=np.zeros((vocab_size,em_size))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> word,index <span class="keyword">in</span> word_to_index.items():</span><br><span class="line">emb_matrix[index,:]=word_to_vec_map[word]</span><br><span class="line"></span><br><span class="line"><span class="comment">#输入词汇表大小，词向量维度,不用训练</span></span><br><span class="line">embedding_layer=Embedding(vocab_size,em_size,trainable=<span class="literal">False</span>)</span><br><span class="line">embedding_layer.build((<span class="literal">None</span>,))</span><br><span class="line">embedding_layer.set_weights([emb_matrix])</span><br><span class="line"><span class="keyword">return</span> embedding_layer</span><br></pre></td></tr></table></figure><p>（3）搭建模型，根据词嵌入矩阵进行两层lstm传播</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#3.搭建lstm模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Emojifier_V2</span><span class="params">(input_shape,word_to_vec_map,word_to_index)</span>:</span></span><br><span class="line">sentence_indices=Input(input_shape,dtype=<span class="string">'float32'</span>)</span><br><span class="line"></span><br><span class="line">embedding_layer=pretrained_embedding_layer(word_to_vec_map,word_to_index)</span><br><span class="line"></span><br><span class="line">embeddings=embedding_layer(sentence_indices)</span><br><span class="line"></span><br><span class="line">X=LSTM(<span class="number">128</span>,return_sequences=<span class="literal">True</span>)(embeddings)</span><br><span class="line">X=Dropout(<span class="number">0.5</span>)(X)</span><br><span class="line">X=LSTM(<span class="number">128</span>,return_sequences=<span class="literal">False</span>)(X)</span><br><span class="line">X=Dropout(<span class="number">0.5</span>)(X)</span><br><span class="line"></span><br><span class="line">X=Dense(<span class="number">5</span>)(X)</span><br><span class="line">X=Activation(activation=<span class="string">'softmax'</span>)(X)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model=Model(inputs=sentence_indices,outputs=X)</span><br><span class="line"><span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><p>（4）结果</p><p>综上，将得到的模型进行编译（compile），将输入的训练集进行补0，输入模型中（fit）（指明batch_size,epochs，则模型训练完毕，得到训练集精度接近1：</p><blockquote><p>Epoch 50/50</p><p> 32/132 [======&gt;…………………..] - ETA: 0s - loss: 0.2743 - accuracy: 0.9688<br>132/132 [==============================] - 0s 558us/step - loss: 0.1291 - accuracy: 0.9697</p><p>32/56 [================&gt;………….] - ETA: 0s<br>56/56 [==============================] - 0s 2ms/step</p></blockquote><p>输入测试集数据（evaluate），得到模型预测的损失和精度为：</p><blockquote><p>X_test_indices=sentences_to_indices(X_test,word_to_index,max_len)</p><p>loss,acc=model.evaluate(X_test_indices,Y_oh_test)</p><p>print(“test accuracy:”,acc)</p></blockquote><blockquote><p>test accuracy: 0.875</p></blockquote><p>自己写一些句子进行预测，发现not happy可以正确进行预测了，改善了上面基本模型出现的问题：预测结果i am not happy 😞，正确率也比基本模型提高了。</p><h3 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h3><p>（1）简单模型中是将训练的词向量求和得到均值作为网络的输入，而RNN模型需要将每个句子补成一样长，通过训练的词向量得到一个总的嵌入矩阵E，通过keras得到每个单词的词嵌入（一个个进行计算的），得到的结果作为网络的输入。</p><p>（2）简单模型没有考虑词序，RNN模型考虑了词序，所以预测的时候回准确些。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;词嵌入&quot;&gt;&lt;a href=&quot;#词嵌入&quot; class=&quot;headerlink&quot; title=&quot;词嵌入&quot;&gt;&lt;/a&gt;词嵌入&lt;/h2&gt;&lt;h3 id=&quot;什么是词嵌入&quot;&gt;&lt;a href=&quot;#什么是词嵌入&quot; class=&quot;headerlink&quot; title=&quot;什么是词嵌入&quot;&gt;&lt;/a&gt;什么是词嵌入&lt;/h3&gt;&lt;p&gt;以词的特征来表示各个单词，即单词的高维度特征表示。&lt;/p&gt;&lt;p&gt;与one-hot表示词相比：&lt;/p&gt;&lt;p&gt;（1）能更好的反映词的特征，增强相关词的泛化能力（例如学习了orange juice，后面会很容易根据apple推测出apple juice，因为apple和orange的特征表示很像），而使用one-hot表示，词之前没有联系，无法泛化&lt;/p&gt;
    
    </summary>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="循环神经网络" scheme="https://www.xiapf.com/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>序列模型RNN——构建即兴演奏的爵士音乐的LSTM模型</title>
    <link href="https://www.xiapf.com/blogs/rnn1Music/"/>
    <id>https://www.xiapf.com/blogs/rnn1Music/</id>
    <published>2020-06-19T06:31:48.000Z</published>
    <updated>2020-06-19T06:34:17.939Z</updated>
    
    <content type="html"><![CDATA[<p>为了解决梯度消失和获得更深层的连接，采用keras.layers.recurrent中的LSTM模型构建神经单元。</p><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>利用已有的音乐片段，搭建能够生成即兴音乐的模型。</p><h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><h3 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h3><p>导入本地的原始音乐，并从中提取音节和音调</p><p>这里的x的维度为（m,T_x,n_values），m代表样本个数，T_x代表时间步，n_values代表字典大小，每个样本被表示成n_values大小的独热向量，这里隐藏层大小设置为64，即n_a=64。</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.导入数据</span></span><br><span class="line"><span class="comment">#x.shape: (60, 30, 78)</span></span><br><span class="line"><span class="comment">#y.shape (30, 60, 78)</span></span><br><span class="line"><span class="comment">#n_values代表字典数量，indies_values代表字典</span></span><br><span class="line">X,Y,n_values,indies_values=load_music_utils()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_music_utils</span><span class="params">()</span>:</span></span><br><span class="line">    chords, abstract_grammars = get_musical_data(<span class="string">'data/original_metheny.mid'</span>)</span><br><span class="line">    corpus, tones, tones_indices, indices_tones = get_corpus_data(abstract_grammars)</span><br><span class="line">    N_tones = len(set(corpus))</span><br><span class="line">    X, Y, N_tones = data_processing(corpus, tones_indices, <span class="number">60</span>, <span class="number">30</span>)   </span><br><span class="line">    <span class="keyword">return</span> (X, Y, N_tones, indices_tones)</span><br></pre></td></tr></table></figure><p>导入数据后得到总样本、实际输出、字典的个数、具体字典的内容。</p><h3 id="利用keras搭建基本神经单元"><a href="#利用keras搭建基本神经单元" class="headerlink" title="利用keras搭建基本神经单元"></a>利用keras搭建基本神经单元</h3><p>（1）重构每个时间步的x</p><p>（2）定义单步传播的神经单元LSTM</p><p>（3）定义最终输出的预测函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#2.利用keras搭建lstm模型</span></span><br><span class="line">n_a=<span class="number">64</span></span><br><span class="line">reshapor=Reshape((<span class="number">1</span>,n_values))</span><br><span class="line">LSTM_cell=LSTM(n_a,return_state=<span class="literal">True</span>)</span><br><span class="line">densor=Dense(n_values,activation=<span class="string">'softmax'</span>)</span><br></pre></td></tr></table></figure><h3 id="搭建模型"><a href="#搭建模型" class="headerlink" title="搭建模型"></a>搭建模型</h3><p>（0）定义输入的X，当前初始的激活值的记忆细胞值</p><p>（1）在每个时间步下</p><p>（2）创建一个临时函数，提取出适当的独热向量作为keras.layers对象：x=Lambda(lambda x:X[:,t,])(X)</p><p>（3）重构当前时间步x大小</p><p>（4）调用LSTM_cell，经过一个lstm单元传播，得到激活值和记忆细胞值</p><p>（5）调用densor，得到输出值out</p><p>（6）将每个时间步的输出值进行保存</p><p>（7）最终生成模型：model=Model(inputs=[X,a0,c0],outputs=outputs)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">djmodel</span><span class="params">(T_x,n_a,n_values)</span>:</span></span><br><span class="line"><span class="comment">#定义输入数据的维度</span></span><br><span class="line">X=Input((T_x,n_values))</span><br><span class="line">a0=Input(shape=(n_a,),name=<span class="string">'a0'</span>)</span><br><span class="line">c0=Input(shape=(n_a,),name=<span class="string">'c0'</span>)</span><br><span class="line">a=a0</span><br><span class="line">c=c0</span><br><span class="line"></span><br><span class="line">outputs=[]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(T_x):</span><br><span class="line">x=Lambda(<span class="keyword">lambda</span> x:X[:,t,])(X)</span><br><span class="line">x=reshapor(x)</span><br><span class="line"></span><br><span class="line">a,_,c=LSTM_cell(x,initial_state=[a,c])</span><br><span class="line"></span><br><span class="line">out=densor(a)</span><br><span class="line"></span><br><span class="line">outputs.append(out)</span><br><span class="line">model=Model(inputs=[X,a0,c0],outputs=outputs)</span><br><span class="line"><span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><h3 id="编译并训练模型"><a href="#编译并训练模型" class="headerlink" title="编译并训练模型"></a>编译并训练模型</h3><p>（1）编译模型，指定优化算法（包含其餐宿）、损失和需要打印的准确度</p><p>（2）定义初始伪激活值，伪记忆细胞（零向量）</p><p>（3）将之前的数据集放入模型中训练，并使用ecophs指定迭代次数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#编译模型</span></span><br><span class="line">m,T_x,_=X.shape</span><br><span class="line">model=djmodel(T_x,n_a,n_values)</span><br><span class="line">opt=Adam(lr=<span class="number">0.01</span>,beta_1=<span class="number">0.9</span>,beta_2=<span class="number">0.99</span>,decay=<span class="number">0.01</span>)</span><br><span class="line">model.compile(optimizer=opt,loss=<span class="string">"categorical_crossentropy"</span>,metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">a0=np.zeros((m,n_a))</span><br><span class="line">c0=np.zeros((m,n_a))</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练模型</span></span><br><span class="line">start=time.clock()</span><br><span class="line">model.fit([X,a0,c0],list(Y),epochs=<span class="number">100</span>)</span><br><span class="line">end=time.clock()</span><br><span class="line">print(<span class="string">"模型训练时间："</span>,end-start)</span><br></pre></td></tr></table></figure><h3 id="采样和预测"><a href="#采样和预测" class="headerlink" title="采样和预测"></a>采样和预测</h3><p>采样</p><p>（1）采用已经训练好的模型的lstm单元，densor输出单元</p><p>（2）定义初始输入、伪激活值，伪记忆细胞（零向量）</p><p>（3）lstm单元：在每个时间步，根据lstm单元得出激活值和记忆细胞值</p><p>（4）densor单元：根据激活值得到输出，</p><p>（5）定义一个Lambda层，将输出转换为独热向量，作为下一个时间步的输入：</p><blockquote><p>x=Lambda(one_hot)(out)</p><p>def one_hot(x):<br>    x = K.argmax(x)<br>    x = tf.one_hot(x, 78)<br>    x = RepeatVector(1)(x)<br>    return x</p></blockquote><p>最终得到采样后的模型结果，通过输入的LSTM_cell单元，densor单元，字典长度，隐藏层大小，输出值大小得到采样的结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#采样与预测</span></span><br><span class="line"><span class="comment">#采样</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">music_inference_model</span><span class="params">(LSTM_cell,densor,n_values,n_a,T_y)</span>:</span></span><br><span class="line"><span class="comment">#T_y时间步长</span></span><br><span class="line">x0=Input((<span class="number">1</span>,n_values))</span><br><span class="line">a0=Input(shape=(n_a,),name=<span class="string">'a0'</span>)</span><br><span class="line">c0=Input(shape=(n_a,),name=<span class="string">'c0'</span>)</span><br><span class="line">a=a0</span><br><span class="line">c=c0</span><br><span class="line">x=x0</span><br><span class="line"></span><br><span class="line">outputs=[]</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(T_y):</span><br><span class="line">a,_,c=LSTM_cell(x,initial_state=[a,c])</span><br><span class="line">out=densor(a)</span><br><span class="line"><span class="comment">#上一个输出作为下一个输出入</span></span><br><span class="line">x=Lambda(one_hot)(out)</span><br><span class="line">outputs.append(out)</span><br><span class="line">inference_model=Model(inputs=[x0,a0,c0],outputs=outputs)</span><br><span class="line"><span class="keyword">return</span> inference_model</span><br><span class="line"></span><br><span class="line">inference_model=music_inference_model(LSTM_cell,densor,n_values,n_a,T_y=<span class="number">50</span>)</span><br><span class="line">x_initializer=np.zeros((<span class="number">1</span>,<span class="number">1</span>,n_values))</span><br><span class="line">a_initializer=np.zeros((<span class="number">1</span>,n_a))</span><br><span class="line">c_initializer=np.zeros((<span class="number">1</span>,n_a))</span><br></pre></td></tr></table></figure><p>预测</p><p>（1）根据采样的模型，输入的样本，激活值、记忆单元</p><p>（2）使用采样模型预测，得到输出值</p><p>（3）使用argmax取最大的索引</p><p>（4）最终将输出表示为独热向量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#预测</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_and_sanmple</span><span class="params">(inference_model,x_initializer=x_initializer,a_initializer=a_initializer,c_initializer=c_initializer)</span>:</span></span><br><span class="line"><span class="comment">#模型预测</span></span><br><span class="line">pred=inference_model.predict([x_initializer,a_initializer,c_initializer])</span><br><span class="line"><span class="comment">#取最大值</span></span><br><span class="line">pred_value=np.argmax(pred,axis=<span class="number">-1</span>)</span><br><span class="line"><span class="comment">#转换为独热向量</span></span><br><span class="line">one_hot=to_categorical(pred_value,num_classes=n_values)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> one_hot,pred_value</span><br><span class="line"></span><br><span class="line">results,indices=predict_and_sanmple(inference_model,x_initializer,a_initializer,c_initializer)</span><br><span class="line">print(<span class="string">"np.argmax(results[12]) ="</span>, np.argmax(results[<span class="number">12</span>]))</span><br><span class="line">print(<span class="string">"np.argmax(results[17]) ="</span>, np.argmax(results[<span class="number">17</span>]))</span><br><span class="line">print(<span class="string">"list(indices[12:18]) ="</span>, list(indices[<span class="number">12</span>:<span class="number">18</span>]))</span><br></pre></td></tr></table></figure><p>预测结果：</p><p>np.argmax(results[12]) = 72<br>np.argmax(results[17]) = 39<br>list(indices[12:18]) = [array([72]), array([39]), array([46]), array([57]), array([72]), array([39])]</p><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p><code>out_stream=generate_music(inference_model)</code>，利用已经生成的模型得到的采样结果在本地生成即兴的音乐，序列模型生成音乐值，采用额外库函数生成midi文件。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200619122826.png" alt></p><p>可以利用在线midi转mp3工具：<a href="https://www.ofoct.com/audio-converter/convert-midi-to-mp3-or-wav-ogg-aac-wma.html" target="_blank" rel="external nofollow noopener noreferrer">https://www.ofoct.com/audio-converter/convert-midi-to-mp3-or-wav-ogg-aac-wma.html</a> 转换文件格式，并进行试听。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;为了解决梯度消失和获得更深层的连接，采用keras.layers.recurrent中的LSTM模型构建神经单元。&lt;/p&gt;&lt;h2 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;问题描述&lt;/h2&gt;&lt;p&gt;利用已有的音乐片段，搭建能够生成即兴音乐的模型。&lt;/p&gt;&lt;h2 id=&quot;步骤&quot;&gt;&lt;a href=&quot;#步骤&quot; class=&quot;headerlink&quot; title=&quot;步骤&quot;&gt;&lt;/a&gt;步骤&lt;/h2&gt;&lt;h3 id=&quot;导入数据&quot;&gt;&lt;a href=&quot;#导入数据&quot; class=&quot;headerlink&quot; title=&quot;导入数据&quot;&gt;&lt;/a&gt;导入数据&lt;/h3&gt;&lt;p&gt;导入本地的原始音乐，并从中提取音节和音调&lt;/p&gt;&lt;p&gt;这里的x的维度为（m,T_x,n_values），m代表样本个数，T_x代表时间步，n_values代表字典大小，每个样本被表示成n_values大小的独热向量，这里隐藏层大小设置为64，即n_a=64。&lt;/p&gt;
    
    </summary>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="循环神经网络" scheme="https://www.xiapf.com/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>序列模型RNN——构建字符级语言模型</title>
    <link href="https://www.xiapf.com/blogs/rnn1DN/"/>
    <id>https://www.xiapf.com/blogs/rnn1DN/</id>
    <published>2020-06-19T03:17:25.000Z</published>
    <updated>2020-06-19T03:40:11.133Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>（1）字符级语言模型：指每个字母（字符）采用一个独热向量表示，即每个时间步中输入的是一个字符，来使用RNN构建模型。</p><p>（2）使用多对多构建一个能给恐龙岛上恐龙命名的语言模型。</p><h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><h3 id="构建字典"><a href="#构建字典" class="headerlink" title="构建字典"></a>构建字典</h3><p>（1）打开所有可以命名的名字数据集，将名字全部转换为小写字母</p><p>（2）按照字母：索引和索引：字母形式生成字典对应表，方便查找字母</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#数据集处理</span></span><br><span class="line"><span class="comment">#得到不重复的字典</span></span><br><span class="line">data=open(<span class="string">'dinos.txt'</span>,<span class="string">'r'</span>).read()</span><br><span class="line">data=data.lower()</span><br><span class="line">char=list(set(data))</span><br><span class="line">data_size,vocab_size=len(data),len(char)</span><br><span class="line">print(data_size,vocab_size)</span><br><span class="line">ix_to_char=&#123;ix:char <span class="keyword">for</span> ix,char <span class="keyword">in</span> enumerate(sorted(char))&#125;</span><br><span class="line">char_to_ix=&#123;char:ix <span class="keyword">for</span> ix,char <span class="keyword">in</span> enumerate(sorted(char))&#125;</span><br><span class="line">print(char_to_ix)</span><br><span class="line">print(ix_to_char)</span><br></pre></td></tr></table></figure><blockquote><p>19909 27<br>{‘\n’: 0, ‘a’: 1, ‘b’: 2, ‘c’: 3, ‘d’: 4, ‘e’: 5, ‘f’: 6, ‘g’: 7, ‘h’: 8, ‘i’: 9, ‘j’: 10, ‘k’: 11, ‘l’: 12, ‘m’: 13, ‘n’: 14, ‘o’: 15, ‘p’: 16, ‘q’: 17, ‘r’: 18, ‘s’: 19, ‘t’: 20, ‘u’: 21, ‘v’: 22, ‘w’: 23, ‘x’: 24, ‘y’: 25, ‘z’: 26}<br>{0: ‘\n’, 1: ‘a’, 2: ‘b’, 3: ‘c’, 4: ‘d’, 5: ‘e’, 6: ‘f’, 7: ‘g’, 8: ‘h’, 9: ‘i’, 10: ‘j’, 11: ‘k’, 12: ‘l’, 13: ‘m’, 14: ‘n’, 15: ‘o’, 16: ‘p’, 17: ‘q’, 18: ‘r’, 19: ‘s’, 20: ‘t’, 21: ‘u’, 22: ‘v’, 23: ‘w’, 24: ‘x’, 25: ‘y’, 26: ‘z’}</p></blockquote><p>得到19909个名字中27个不重复的字符，将字符按照字典顺序排序，回车键在第0个，后面字符依次排列。</p><h3 id="梯度修剪"><a href="#梯度修剪" class="headerlink" title="梯度修剪"></a>梯度修剪</h3><p>将梯度中过大或者过小的梯度直接设置为最大值和最小值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#梯度修剪</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clip</span><span class="params">(gradients,maxValue)</span>:</span></span><br><span class="line">dWax=gradients[<span class="string">"dWax"</span>]</span><br><span class="line">dWaa=gradients[<span class="string">"dWaa"</span>]</span><br><span class="line">dWya=gradients[<span class="string">"dWya"</span>]</span><br><span class="line"></span><br><span class="line">db=gradients[<span class="string">"db"</span>]</span><br><span class="line">dby=gradients[<span class="string">"dby"</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> gradient <span class="keyword">in</span> [dWax,dWaa,dWya,db,dby]:</span><br><span class="line">np.clip(gradient,-maxValue,maxValue,out=gradient)</span><br><span class="line">gradients=&#123;<span class="string">'dWax'</span>:dWax,<span class="string">'dWaa'</span>:dWaa,<span class="string">'dWya'</span>:dWya,<span class="string">'db'</span>:db,<span class="string">'dby'</span>:dby&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><h3 id="采样"><a href="#采样" class="headerlink" title="采样"></a>采样</h3><p>采样时，已经得到当前模型的训练参数，将回车键作为采样结束的标志</p><p>（1）当采样未结束的时候</p><p>（2）输入独热向量0向量和第0层的激活值，根据RNN基本单元公式，得到预测值y</p><p>（3）进行随机采样（np.random.choice）选择最大概率的索引值</p><p>（4）将选中的值转换为新的独热向量，作为下一个时间步的输入</p><p>（5）不断循环，当到末尾时得到当前的采样结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#采样</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(parameters,char_to_ix,seed)</span>:</span></span><br><span class="line">Wax=parameters[<span class="string">'Wax'</span>]</span><br><span class="line">Waa=parameters[<span class="string">'Waa'</span>]</span><br><span class="line">b=parameters[<span class="string">'b'</span>]</span><br><span class="line">Wya=parameters[<span class="string">'Wya'</span>]</span><br><span class="line">by=parameters[<span class="string">'by'</span>]</span><br><span class="line"></span><br><span class="line">vocab_size=by.shape[<span class="number">0</span>]</span><br><span class="line">n_a=Waa.shape[<span class="number">1</span>]</span><br><span class="line">a_prev=np.zeros((n_a,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#创造独热向量</span></span><br><span class="line">x=np.zeros((vocab_size,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">counter=<span class="number">0</span></span><br><span class="line"></span><br><span class="line">idx=<span class="number">-1</span></span><br><span class="line">indies=[]</span><br><span class="line">check_character=char_to_ix[<span class="string">'\n'</span>]</span><br><span class="line"><span class="keyword">while</span> idx!=check_character <span class="keyword">and</span> counter&lt;<span class="number">50</span>:</span><br><span class="line">a=np.tanh(np.dot(Waa,a_prev)+np.dot(Wax,x)+b)</span><br><span class="line">z=np.dot(Wya,a)+by</span><br><span class="line">y=cu.softmax(z)</span><br><span class="line"></span><br><span class="line"><span class="comment">#按照概率采样</span></span><br><span class="line">np.random.seed(seed+counter)</span><br><span class="line">idx=np.random.choice(list(range(vocab_size)),p=y.ravel())</span><br><span class="line">indies.append(idx)</span><br><span class="line"></span><br><span class="line"><span class="comment">#按照选择的位置生成独热向量</span></span><br><span class="line">x=np.zeros((vocab_size,<span class="number">1</span>))</span><br><span class="line">x[idx]=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#将上一个预测值直接作为下一个的输入</span></span><br><span class="line">a_prev=a</span><br><span class="line">seed+=<span class="number">1</span></span><br><span class="line">counter+=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> counter==<span class="number">50</span>:</span><br><span class="line">indies.append(char_to_ix[<span class="string">'\n'</span>])</span><br><span class="line"><span class="keyword">return</span> indies</span><br></pre></td></tr></table></figure><h3 id="搭建语言模型（一个样本）"><a href="#搭建语言模型（一个样本）" class="headerlink" title="搭建语言模型（一个样本）"></a>搭建语言模型（一个样本）</h3><p>（1）输入当前的样本的所有字符，实际输出值，上一层的激活值，参数</p><p>（2）进行一次RNN前向传播，将当前样本所有字符一一对应每个时间步，得到损失和缓存</p><p>（3）进行一次RNN反向传播，得到梯度和激活值</p><p>（4）进行一次梯度更新，得到新的梯度</p><p>（5）对得到的新梯度进行修剪，防止梯度爆炸</p><p>（6）最终返回当前样本的损失，梯度，最终的激活值输出和参数</p><p>tips：当字典中值被改变，无论在函数内还是函数外都会影响字典中的值，因此这里可以不返回parameters</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#构建语言模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span><span class="params">(x,y,a_prev,parameters,learning_rate=<span class="number">0.01</span>,maxValue=<span class="number">5</span>)</span>:</span></span><br><span class="line"><span class="comment">#前向传播</span></span><br><span class="line">loss,cache=cu.rnn_forward(x,y,a_prev,parameters,vocab_size=<span class="number">27</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#反向传播</span></span><br><span class="line">gradients,a=cu.rnn_backward(x,y,parameters,cache)</span><br><span class="line"></span><br><span class="line"><span class="comment">#修剪梯度</span></span><br><span class="line">gradients=clip(gradients,maxValue)</span><br><span class="line"></span><br><span class="line"><span class="comment">#更新参数</span></span><br><span class="line">parameters=cu.update_parameters(parameters,gradients,learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> loss,gradients,a[len(x)<span class="number">-1</span>],parameters</span><br></pre></td></tr></table></figure><h3 id="生成模型（所有样本）"><a href="#生成模型（所有样本）" class="headerlink" title="生成模型（所有样本）"></a>生成模型（所有样本）</h3><p>（1）读入当前的数据，并打乱</p><p>（2）初始化参数和损失</p><p>（3）在每次迭代中，每次随机选择一个样本进行RNN模型构建</p><p>随机选择样本：</p><p>​        index=i%len(examples)<br>​        x=[None]+[char_to_ix[ch] for ch in examples[index]]<br>​        y=x[1:]+[char_to_ix[‘\n’]]</p><p>0<br>[None, 20, 21, 18, 9, 1, 19, 1, 21, 18, 21, 19]<br>[20, 21, 18, 9, 1, 19, 1, 21, 18, 21, 19, 0]</p><p>为了保证，输入输出一样长，这里将x最前面加上空字符，输出值取实际名字并加上回车符作为结束。</p><p>（4）根据每次选择的一个样本，进行一个语言模型构建</p><p>这里采用延迟来平滑损失，加速训练</p><p>（5）每2000次进行一次采样进行模型预测，最终得到模型训练的参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#运行算法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(data,ix_to_char,char_to_ix,num_iterations=<span class="number">3500</span>,n_a=<span class="number">50</span>,vocab_size=<span class="number">27</span>,dinos_name=<span class="number">7</span>)</span>:</span></span><br><span class="line">n_x,n_y=vocab_size,vocab_size</span><br><span class="line"></span><br><span class="line"><span class="comment">#读入当前样本</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'dinos.txt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">examples=f.readlines()</span><br><span class="line">examples=[x.lower().strip() <span class="keyword">for</span> x <span class="keyword">in</span> examples]</span><br><span class="line">print(len(examples))</span><br><span class="line"></span><br><span class="line"><span class="comment">#打乱样本</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">np.random.shuffle(examples)</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化参数和损失</span></span><br><span class="line">parameters=cu.initialize_parameters(n_a,n_x,n_y)</span><br><span class="line">loss=cu.get_initial_loss(vocab_size,dinos_name)</span><br><span class="line"></span><br><span class="line">a_prev=np.zeros((n_a,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#进行迭代</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations):</span><br><span class="line">index=i%len(examples)</span><br><span class="line">x=[<span class="literal">None</span>]+[char_to_ix[ch] <span class="keyword">for</span> ch <span class="keyword">in</span> examples[index]]</span><br><span class="line">y=x[<span class="number">1</span>:]+[char_to_ix[<span class="string">'\n'</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment">#字典无论在函数内或函数外发生改变，这些改变都会作用于字典</span></span><br><span class="line">cur_loss,gradients,a_prev,parameters=optimize(x,y,a_prev,parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment">#平滑损失</span></span><br><span class="line">loss=cu.smooth(loss,cur_loss)</span><br><span class="line"></span><br><span class="line"><span class="comment">#每2000次进行一次采样</span></span><br><span class="line"><span class="keyword">if</span> i%<span class="number">2000</span>==<span class="number">0</span>:</span><br><span class="line">print(<span class="string">"第"</span>+str(i)+<span class="string">"次，损失为"</span>+str(loss))</span><br><span class="line">seed=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> name <span class="keyword">in</span> range(dinos_name):</span><br><span class="line">sample_indies=sample(parameters,char_to_ix,seed)</span><br><span class="line">cu.print_sample(sample_indies,ix_to_char)</span><br><span class="line">seed+=<span class="number">1</span></span><br><span class="line">print(<span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>（6）结果</p><p>输入字典对应表和迭代次数，进行训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(data, ix_to_char, char_to_ix, num_iterations=<span class="number">3500</span>)</span><br></pre></td></tr></table></figure><p>模型训练的结果：</p><p>得到每2000次的损失和当前预测的7个恐龙的名字</p><blockquote><p>第0次，损失为23.087336085484605<br>Nkzxwtdmfqoeyhsqwasjkjvu<br>Kneb<br>Kzxwtdmfqoeyhsqwasjkjvu<br>Neb<br>Zxwtdmfqoeyhsqwasjkjvu<br>Eb<br>Xwtdmfqoeyhsqwasjkjvu</p><p>第2000次，损失为27.884160491415773<br>Liusskeomnolxeros<br>Hmdaairus<br>Hytroligoraurus<br>Lecalosapaus<br>Xusicikoraurus<br>Abalpsamantisaurus<br>Tpraneronxeros</p></blockquote><p>使用模型训练参数进行预测：</p><p>根据需要预测的恐龙的名字数量进行迭代，输入训练好的参数和字典表得到预测索引，根据索引：字典得到对应的名字。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dinos_name=<span class="number">7</span></span><br><span class="line">seed=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> name <span class="keyword">in</span> range(dinos_name):</span><br><span class="line">sample_indies=sample(parameters,char_to_ix,seed)</span><br><span class="line">cu.print_sample(sample_indies,ix_to_char)</span><br><span class="line">seed+=<span class="number">1</span></span><br><span class="line">print(<span class="string">"\n"</span>)</span><br></pre></td></tr></table></figure><p>得到预测的恐龙的名字为：</p><blockquote><p>Livusaton<br>Hola<br>Ivushandoraunpsacrophus<br>Lecakosaunus<br>Wusnchepokupros<br>Acalpsasaurus<br>Torandor</p></blockquote><h2 id="附：构造莎士比亚风格的语言模型"><a href="#附：构造莎士比亚风格的语言模型" class="headerlink" title="附：构造莎士比亚风格的语言模型"></a>附：构造莎士比亚风格的语言模型</h2><p>大致思路：</p><p>（1）导入莎士比亚的诗歌，作为训练样本</p><p>（2）利用已搭建大莎士比亚诗歌模型，该模型上已训练1000次，只需要导入，再训练一次即可，即将数据导入到模型中使用，再根据输入的数据利用会掉函数输入模型中进行训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print_callback=LambdaCallback(on_epoch_end=on_epoch_end)</span><br><span class="line">model.fit(x,y,batch_size=<span class="number">128</span>,epochs=<span class="number">1</span>,callbacks=[print_callback])</span><br></pre></td></tr></table></figure><p>（3）该模型需要输入初始激活值，得到最终的预测结果</p><p>这里输入初始单词为：bajie，得到生成的诗歌如下：</p><blockquote><p>Write the beginning of your poem, the Shakespeare machine will complete it. Your input is: bajie</p><p>Here is your poem: </p><p>bajienst fauch is the grace,<br>the roud by sorst then every where gise,<br>gave your lover wadese bust may o ficcle,<br>and ou may but would relaved be ta niell<br>live anbsad i forse itferse mads,<br> from ho strenst my grast of thight crors,<br>and to thop hersiang thises to hesp ather,<br>and ho my to thy filens express wold’s faver,<br>do this bay of dowh i connied too,<br>my live the wherat be forse thishuror,<br>it som as wh</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;问题描述&lt;/h2&gt;&lt;p&gt;（1）字符级语言模型：指每个字母（字符）采用一个独热向量表示，即每个时间步中输入的是一个字符，来使用RNN构建模型。&lt;/p&gt;&lt;p&gt;（2）使用多对多构建一个能给恐龙岛上恐龙命名的语言模型。&lt;/p&gt;&lt;h2 id=&quot;步骤&quot;&gt;&lt;a href=&quot;#步骤&quot; class=&quot;headerlink&quot; title=&quot;步骤&quot;&gt;&lt;/a&gt;步骤&lt;/h2&gt;&lt;h3 id=&quot;构建字典&quot;&gt;&lt;a href=&quot;#构建字典&quot; class=&quot;headerlink&quot; title=&quot;构建字典&quot;&gt;&lt;/a&gt;构建字典&lt;/h3&gt;&lt;p&gt;（1）打开所有可以命名的名字数据集，将名字全部转换为小写字母&lt;/p&gt;&lt;p&gt;（2）按照字母：索引和索引：字母形式生成字典对应表，方便查找字母&lt;/p&gt;
    
    </summary>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="循环神经网络" scheme="https://www.xiapf.com/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>序列模型RNN——基本网络结构</title>
    <link href="https://www.xiapf.com/blogs/rnn1/"/>
    <id>https://www.xiapf.com/blogs/rnn1/</id>
    <published>2020-06-18T14:10:26.000Z</published>
    <updated>2020-06-18T14:13:17.534Z</updated>
    
    <content type="html"><![CDATA[<h2 id="RNN原理"><a href="#RNN原理" class="headerlink" title="RNN原理"></a>RNN原理</h2><p>（0）标记符号含义</p><p>输入的样本为（X,Y），其中X的维度为（m，T_x，n），m代表样本个数，T_x代表时间步，n代表字典长度。</p><p>RNN中每个样本采用独热向量表示，在训练集中选择不重复数据作为字典，根据字典构造每个独热向量（当前词在字典中的位置标为1，其余标为0）。</p><p>（1）网络结构</p><p>RNN网络结果如图，在t时刻输入当前时刻的x值，经过隐藏层，通过计算前一层的激活值和输入值x得到预测的y。</p><a id="more"></a><p>每个时刻的输出取决于当前时刻的输入和前一个时刻的激活值。</p><p>1° 多对多结构</p><p>很多输入对应很多输出，当Tx=Ty，即输入和输出数量相同，例如命名实体识别（根据一个句子识别出其中的主语）</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618165122.png" alt></p><p>2° 多对多结构</p><p>很多输入对应很多输出，当Tx不等于Ty，即输入和输出数量不相同，例如机器翻译。</p><p>因为机器翻译需要考虑整个句子的前后联系，所以要将x全部输入后才能进行预测。因此，前半部分按照时间步输入x，作为编码器，后半部分输出预测的y，作为解码器。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618195021.png" alt></p><p>3° 一对多结构</p><p>输入部分片段或者不输入（0向量），输出形成的完整的输出，例如生成音乐。</p><p>根据输入的x和激活值得到预测的y，上一层预测的y作为下一层的输入。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618195910.png" alt></p><p>4° 多对一结构</p><p>很多输入对应有个输出，例如情感分类问题，给一段文字，给出评分。</p><p>每次输入一个时间步的x，但不输出预测值，当所有的x均按照时间步输入之后，最后得到一个评分，该评分综合考虑了签名所有的文字。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618200114.png" alt></p><p>（2）前向传播</p><p>以多对多中，Tx=Ty为例说明RNN的前向传播。</p><p>在t=1的时间步下：</p><p>1° 有输入a&lt;0&gt;激活值（伪激活值，常设置为0），和时间步为1的x值：x&lt;1&gt;，将两者乘以对应权重，加上偏置量得到线性值，再使用激活函数（tanh/Relu），得到当前单元输出的激活值。</p><p>2° 将激活值a&lt;1&gt;乘以对应权重，加上偏置量得到线性值，再使用激活函数（softmax）,得到最终预测的时间步为1的y的预测值。</p><p>后面的神经元步骤类似：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618165220.png" alt></p><p>编码时，首先根据前向传播公式得到一个基本RNN单元处理的结果，将前一个和当前激活值，当前输入的x和参数存成缓存，将当前输出的激活值、预测值、缓存作为最终的输出。</p><p>接着，按照时间步，传入当前输入和上一层的激活值输出，得到所有时间步的激活值和预测值，最终将所有时间步的激活值和预测值和缓存输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#搭建标准rnn单元</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_cell_forward</span><span class="params">(xt,a_prev,parameters)</span>:</span></span><br><span class="line">Wax=parameters[<span class="string">"Wax"</span>]</span><br><span class="line">Waa=parameters[<span class="string">"Waa"</span>]</span><br><span class="line">Wya=parameters[<span class="string">"Wya"</span>]</span><br><span class="line"></span><br><span class="line">ba=parameters[<span class="string">"ba"</span>]</span><br><span class="line">by=parameters[<span class="string">"by"</span>]</span><br><span class="line"></span><br><span class="line">a_next=np.tanh(np.dot(Waa,a_prev)+np.dot(Wax,xt)+ba)</span><br><span class="line"></span><br><span class="line">y_next=ru.softmax(np.dot(Wya,a_next)+by)</span><br><span class="line"></span><br><span class="line">cache=(a_next,a_prev,xt,parameters)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> a_next,y_next,cache</span><br><span class="line"></span><br><span class="line"><span class="comment">#将所有标准单元连接起来，进行前向传播</span></span><br><span class="line"><span class="comment">#这里Tx=Ty</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_forward</span><span class="params">(x,a0,parameters)</span>:</span></span><br><span class="line">n_x,m,T_x=x.shape</span><br><span class="line">Wya=parameters[<span class="string">"Wya"</span>]</span><br><span class="line">n_y,n_a=Wya.shape</span><br><span class="line"></span><br><span class="line">a_next=a0</span><br><span class="line">a=np.zeros((n_a,m,T_x))</span><br><span class="line">y_hat=np.zeros((n_y,m,T_x))</span><br><span class="line">caches=[]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(T_x):</span><br><span class="line">a_next,y_next,cache=rnn_cell_forward(x[:,:,t],a_next,parameters)</span><br><span class="line"></span><br><span class="line">a[:,:,t]=a_next</span><br><span class="line"></span><br><span class="line">y_hat[:,:,t]=y_next</span><br><span class="line"></span><br><span class="line">caches.append(cache)</span><br><span class="line">caches=(caches,x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> a,y_hat,caches</span><br></pre></td></tr></table></figure><p>（3）反向传播</p><p>反向传播需要从最后开始（for t in reversed(range(T_x))），求得所有参数的梯度。计算和激活值和输入值相关的梯度即可，通过损失调整相应权重和偏置量的值。</p><p>根据对tanh(a)求导等于(1-tanh^2) * da，易求得Wax,Waa,xt,ba,a&lt; t-1 &gt;的梯度如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618170023.png" alt></p><p>编码时，也是先得出一个基本神经元的反向传播过程，再根据时间步以此类推得到其他的，并将梯度修正并进行爆粗。</p><p>注：对激活值的推导一直要得到最开始的激活值即da0</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#标准rnn的反向传播</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#单个单元的反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_cell_backward</span><span class="params">(da_next,cache)</span>:</span></span><br><span class="line">a_next,a_prev,xt,parameters=cache</span><br><span class="line"></span><br><span class="line">Wax=parameters[<span class="string">"Wax"</span>]</span><br><span class="line">Waa=parameters[<span class="string">"Waa"</span>]</span><br><span class="line">ba=parameters[<span class="string">"ba"</span>]</span><br><span class="line"><span class="comment"># Wya=parameters["Wya"]</span></span><br><span class="line"><span class="comment"># by=parameters["by"]</span></span><br><span class="line"></span><br><span class="line">dtanh=(<span class="number">1</span>-np.square(a_next))*da_next</span><br><span class="line"></span><br><span class="line">dxt=np.dot(Wax.T,dtanh)</span><br><span class="line">dWax=np.dot(dtanh,xt.T)</span><br><span class="line"></span><br><span class="line">da_prev=np.dot(Waa.T,dtanh)</span><br><span class="line">dWaa=np.dot(dtanh,a_prev.T)</span><br><span class="line"></span><br><span class="line">dba=np.sum(dtanh,axis=<span class="number">-1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># dWya=np.dot(dy,dtanh.T)</span></span><br><span class="line"><span class="comment"># dyb=np.sum(dtanh,axis=1,keepdims=True)</span></span><br><span class="line"></span><br><span class="line">gradients=&#123;<span class="string">'dxt'</span>:dxt,<span class="string">'da_prev'</span>:da_prev,<span class="string">'dWaa'</span>:dWaa,<span class="string">'dWax'</span>:dWax,<span class="string">'dba'</span>:dba&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> gradients</span><br><span class="line"></span><br><span class="line"><span class="comment">#一个序列的反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_backward</span><span class="params">(da,caches)</span>:</span></span><br><span class="line">caches,x=caches</span><br><span class="line">a1,a0,x1,parameters=caches[<span class="number">0</span>]</span><br><span class="line">n_x,m=x1.shape</span><br><span class="line">n_a,m,T_x=da.shape</span><br><span class="line"></span><br><span class="line">dx=np.zeros((n_x,m,T_x))</span><br><span class="line">dWaa=np.zeros((n_a,n_a))</span><br><span class="line">dWax=np.zeros((n_a,n_x))</span><br><span class="line">dba=np.zeros((n_a,<span class="number">1</span>))</span><br><span class="line">da0=np.zeros((n_a,m))</span><br><span class="line">da_prevt=np.zeros((n_a,m))</span><br><span class="line"></span><br><span class="line"><span class="comment">#倒序</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> reversed(range(T_x)):</span><br><span class="line">gradient=rnn_cell_backward(da[:,:,t]+da_prevt,caches[t])</span><br><span class="line">dxt,da_prevt,dWaat,dWaxt,dbat=gradient[<span class="string">'dxt'</span>],gradient[<span class="string">'da_prev'</span>],gradient[<span class="string">'dWaa'</span>],gradient[<span class="string">'dWax'</span>],gradient[<span class="string">'dba'</span>]</span><br><span class="line"></span><br><span class="line">dx[:,:,t]=dxt</span><br><span class="line">dWaa+=dWaat</span><br><span class="line">dWax+=dWaxt</span><br><span class="line">dba+=dbat</span><br><span class="line">da0=da_prevt</span><br><span class="line"></span><br><span class="line">gradients=&#123;<span class="string">'dx'</span>:dx,<span class="string">'da0'</span>:da0,<span class="string">'dWaa'</span>:dWaa,<span class="string">'dWax'</span>:dWax,<span class="string">'dba'</span>:dba&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><h2 id="基本RNN网络存在问题"><a href="#基本RNN网络存在问题" class="headerlink" title="基本RNN网络存在问题"></a>基本RNN网络存在问题</h2><p>当网络深度变深，权重将以指数形式扩大或减小，会出现以下几个问题：</p><p>（1）梯度爆炸——解决方法：梯度修剪</p><p>当出现梯度爆炸时，在使用当前梯度更新参数之后，对更新后的参数判断是否在[- 最大值，最大值]之间（这里的最大值和最小值需要用户给出），当不在时，按照最大值和最小值设置，这样保证梯度不会变的很大。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#梯度修剪</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clip</span><span class="params">(gradients,maxValue)</span>:</span></span><br><span class="line">dWax=gradients[<span class="string">"dWax"</span>]</span><br><span class="line">dWaa=gradients[<span class="string">"dWaa"</span>]</span><br><span class="line">dWya=gradients[<span class="string">"dWya"</span>]</span><br><span class="line"></span><br><span class="line">db=gradients[<span class="string">"db"</span>]</span><br><span class="line">dby=gradients[<span class="string">"dby"</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> gradient <span class="keyword">in</span> [dWax,dWaa,dWya,db,dby]:</span><br><span class="line">np.clip(gradient,-maxValue,maxValue,out=gradient)</span><br><span class="line">gradients=&#123;<span class="string">'dWax'</span>:dWax,<span class="string">'dWaa'</span>:dWaa,<span class="string">'dWya'</span>:dWya,<span class="string">'db'</span>:db,<span class="string">'dby'</span>:dby&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><p>（2）梯度消失——解决方法：修改网络的基本单元</p><p>采用GRU单元或者LSTM单元，能更好捕捉深层连接</p><h2 id="LSTM原理"><a href="#LSTM原理" class="headerlink" title="LSTM原理"></a>LSTM原理</h2><p>（1）网络结构</p><p>LSTM单元和基本RNN单元类似，但其中加入了记忆细胞，每个神经元都会有一个记忆细胞，并输出一个新记忆细胞值，这样保证即使很深，后面的值也会有前面神经元的值。</p><p>LSTM中设置更新门，遗忘门，根据更新门决定保留新的记忆细胞的部分，根据遗忘门决定保留前一个记忆细胞的部分，通过记忆细胞贯穿所有时间步，从而获得更远的连接。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618170626.png" alt></p><p>（2）前向传播</p><p>1° 记忆细胞根据当前时间步x&lt; t &gt;和上一个输出激活值得出新的记忆细胞</p><p>2° 将更新门、遗忘门、输出门按照权重得到最新的值</p><p>3° 根据更新门、遗忘门得到当前的记忆细胞</p><p>4° 根据输出门和当前记忆细胞德奥激活值</p><p>5° 最终经过softmax得到最终的预测</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618170550.png" alt></p><p>编码同上面基本RNN单元，得出单一LSTM单元的前向传播，再对所有时间步求得预测值。</p><p>注：这里有个小技巧，将前一个激活值a_prev和当前x进行堆叠，减少参数个数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#加入LTSM解决梯度消失的问题</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#搭建单个lstm单元</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_cell_forward</span><span class="params">(xt,a_prev,c_prev,parameters)</span>:</span></span><br><span class="line">Wf=parameters[<span class="string">'Wf'</span>]</span><br><span class="line">bf=parameters[<span class="string">'bf'</span>]</span><br><span class="line">Wu=parameters[<span class="string">'Wu'</span>]</span><br><span class="line">bu=parameters[<span class="string">'bu'</span>]</span><br><span class="line">Wc=parameters[<span class="string">'Wc'</span>]</span><br><span class="line">bc=parameters[<span class="string">'bc'</span>]</span><br><span class="line">Wo=parameters[<span class="string">'Wo'</span>]</span><br><span class="line">bo=parameters[<span class="string">'bo'</span>]</span><br><span class="line">Wy=parameters[<span class="string">'Wy'</span>]</span><br><span class="line">by=parameters[<span class="string">'by'</span>]</span><br><span class="line"></span><br><span class="line">n_a,m=a_prev.shape</span><br><span class="line">n_xt,m=xt.shape</span><br><span class="line"></span><br><span class="line">ax_prev=np.zeros(((n_a+n_xt),m))</span><br><span class="line"></span><br><span class="line"><span class="comment">#将a_prev和xt堆叠起来</span></span><br><span class="line">ax_prev[:n_a,:]=a_prev</span><br><span class="line">ax_prev[n_a:,:]=xt</span><br><span class="line"></span><br><span class="line"><span class="comment">#遗忘门</span></span><br><span class="line">ft=ru.sigmoid(np.dot(Wf,ax_prev)+bf)</span><br><span class="line"><span class="comment">#更新门</span></span><br><span class="line">ut=ru.sigmoid(np.dot(Wu,ax_prev)+bu)</span><br><span class="line"></span><br><span class="line"><span class="comment">#候选值</span></span><br><span class="line">cct=np.tanh(np.dot(Wc,ax_prev)+bc)</span><br><span class="line"><span class="comment">#记忆细胞</span></span><br><span class="line">c_next=ut*cct+ft*c_prev</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出门</span></span><br><span class="line">ot=ru.sigmoid(np.dot(Wo,ax_prev)+bo)</span><br><span class="line"></span><br><span class="line">a_next=ot*np.tanh(c_next)</span><br><span class="line"></span><br><span class="line">y_next=ru.softmax(np.dot(Wy,a_next)+by)</span><br><span class="line"></span><br><span class="line">cache=(a_next,c_next,a_prev,c_prev,ft,ut,cct,ot,xt,parameters)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> a_next,c_next,y_next,cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#lstm的前向传播</span></span><br><span class="line"><span class="comment">#c0使用0来初始化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_forward</span><span class="params">(x,a0,parameters)</span>:</span></span><br><span class="line">caches=[]</span><br><span class="line"></span><br><span class="line">n_x,m,T_x=x.shape</span><br><span class="line">Wy=parameters[<span class="string">'Wy'</span>]</span><br><span class="line">n_y,n_a=Wy.shape</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a=np.zeros((n_a,m,T_x))</span><br><span class="line">c=np.zeros((n_a,m,T_x))</span><br><span class="line">y_hat=np.zeros((n_y,m,T_x))</span><br><span class="line"></span><br><span class="line">a_next=a0</span><br><span class="line">c_next=np.zeros((n_a,m))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(T_x):</span><br><span class="line">a_next,c_next,y_next,cache=lstm_cell_forward(x[:,:,t],a_next,c_next,parameters)</span><br><span class="line"></span><br><span class="line">a[:,:,t]=a_next</span><br><span class="line">c[:,:,t]=c_next</span><br><span class="line">y_hat[:,:,t]=y_next</span><br><span class="line"></span><br><span class="line">caches.append(cache)</span><br><span class="line"></span><br><span class="line">caches=(caches,x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> a,y_hat,c,caches</span><br></pre></td></tr></table></figure><p>（3）反向传播</p><p>反向从最后前向求得所有的梯度值（for t in reversed(range(T_x))），输出值y中的权重，偏置量不更新。</p><p>1° 求三个门和记忆细胞的梯度</p><p>输出门：根据最终输出的激活值公式以及对sigmoid的求导得出</p><p>记忆细胞候选值、更新门、遗忘门：一部分受求当前记忆细胞的影响，一部分受求激活值的影响</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618170127.png" alt></p><p>2° 各个权重系数的梯度，根据1中求得的梯度能很容易求到</p><p>同理，各个偏置量梯度也可求得</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618170142.png" alt></p><p>3° 上一个激活值，上一个记忆细胞候选值，当前输入值的梯度</p><p>上一个激活值和记忆细胞候选值、更新门、遗忘门、输出门均有联系，所以将各个梯度相加</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618170213.png" alt></p><p>上一个记忆细胞候选值一部分受求当前记忆细胞的影响，一部分受求激活值的影响</p><p>当前输入值的梯度和记忆细胞候选值、更新门、遗忘门、输出门均有联系，所以将各个梯度相加</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618170231.png" alt></p><p>编码同上面基本RNN单元，得出单一LSTM单元的反向传播，再对所有时间步求得梯度。</p><p>注：前向传播中用的小技巧，将前一个激活值a_prev和当前x进行堆叠，减少参数个数。求梯度的时候要将值进行拆分求梯度，即将权重前部分用来求a_prev，后部分用于求xt。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#lstm单个单元的反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_cell_backward</span><span class="params">(da_next,dc_next,cache)</span>:</span></span><br><span class="line">a_next,c_next,a_prev,c_prev,ft,ut,cct,ot,xt,parameters=cache</span><br><span class="line">n_a,m=a_next.shape</span><br><span class="line"></span><br><span class="line">Wf=parameters[<span class="string">'Wf'</span>]</span><br><span class="line">bf=parameters[<span class="string">'bf'</span>]</span><br><span class="line">Wu=parameters[<span class="string">'Wu'</span>]</span><br><span class="line">bu=parameters[<span class="string">'bu'</span>]</span><br><span class="line">Wc=parameters[<span class="string">'Wc'</span>]</span><br><span class="line">bc=parameters[<span class="string">'bc'</span>]</span><br><span class="line">Wo=parameters[<span class="string">'Wo'</span>]</span><br><span class="line">bo=parameters[<span class="string">'bo'</span>]</span><br><span class="line">Wy=parameters[<span class="string">'Wy'</span>]</span><br><span class="line">by=parameters[<span class="string">'by'</span>]</span><br><span class="line"></span><br><span class="line">dot=da_next*np.tanh(c_next)*ot*(<span class="number">1</span>-ot)</span><br><span class="line">dcct=(dc_next*ut+ot*(<span class="number">1</span>-np.square(np.tanh(c_next)))*ut*da_next)*(<span class="number">1</span>-np.square(cct))</span><br><span class="line">dut=(dc_next*cct+ot*(<span class="number">1</span>-np.square(np.tanh(c_next)))*cct*da_next)*ut*(<span class="number">1</span>-ut)</span><br><span class="line">dft=(dc_next*c_prev+ot*(<span class="number">1</span>-np.square(np.tanh(c_next)))*c_prev*da_next)*ft*(<span class="number">1</span>-ft)</span><br><span class="line"></span><br><span class="line">concat=np.concatenate((a_prev,xt),axis=<span class="number">0</span>).T</span><br><span class="line"></span><br><span class="line">dWf=np.dot(dft,concat)</span><br><span class="line">dbf=np.sum(dft,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">dWu=np.dot(dut,concat)</span><br><span class="line">dbu=np.sum(dut,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">dWc=np.dot(dcct,concat)</span><br><span class="line">dbc=np.sum(dcct,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">dWo=np.dot(dot,concat)</span><br><span class="line">dbo=np.sum(dot,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">da_prev=np.dot(Wf[:,:n_a].T,dft)+np.dot(Wu[:,:n_a].T,dut)+np.dot(Wc[:,:n_a].T,dcct)+np.dot(Wo[:,:n_a].T,dot)</span><br><span class="line">dc_prev=dc_next*ft+ot*(<span class="number">1</span>-np.square(np.tanh(c_next)))*ft*da_next</span><br><span class="line">dxt=np.dot(Wf[:,n_a:].T,dft)+np.dot(Wu[:,n_a:].T,dut)+np.dot(Wc[:,n_a:].T,dcct)+np.dot(Wo[:,n_a:].T,dot)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">gradients=&#123;<span class="string">'dxt'</span>:dxt,<span class="string">'da_prev'</span>:da_prev,<span class="string">'dc_prev'</span>:dc_prev,<span class="string">'dWf'</span>:dWf,<span class="string">'dbf'</span>:dbf,<span class="string">'dWu'</span>:dWu,<span class="string">'dbu'</span>:dbu,<span class="string">'dWc'</span>:dWc,<span class="string">'dbc'</span>:dbc,<span class="string">'dWo'</span>:dWo,<span class="string">'dbo'</span>:dbo&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> gradients</span><br><span class="line"></span><br><span class="line"><span class="comment">#lstm一个时间序列的反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_backward</span><span class="params">(da,caches)</span>:</span></span><br><span class="line">caches,x=caches</span><br><span class="line">a_next1,c_next1,a_prev1,c_prev1,ft1,ut1,cct1,ot1,xt1,parameters=caches[<span class="number">0</span>]</span><br><span class="line">n_a,m,T_x=da.shape</span><br><span class="line">n_x,m=xt1.shape</span><br><span class="line">n_c,m=c_next1.shape</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dx=np.zeros((n_x,m,T_x))</span><br><span class="line">da0=np.zeros((n_a,m))</span><br><span class="line"></span><br><span class="line">dWf=np.zeros((n_a,n_a+n_x))</span><br><span class="line">dWu=np.zeros((n_a,n_a+n_x))</span><br><span class="line">dWc=np.zeros((n_a,n_a+n_x))</span><br><span class="line">dWo=np.zeros((n_a,n_a+n_x))</span><br><span class="line">dbf=np.zeros((n_a,<span class="number">1</span>))</span><br><span class="line">dbu=np.zeros((n_a,<span class="number">1</span>))</span><br><span class="line">dbc=np.zeros((n_a,<span class="number">1</span>))</span><br><span class="line">dbo=np.zeros((n_a,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">da_prevt=np.zeros((n_a,m))</span><br><span class="line">dc_prevt=np.zeros((n_c,m))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> reversed(range(T_x)):</span><br><span class="line">gradient=lstm_cell_backward(da[:,:,t]+da_prevt,dc_prevt,caches[t])</span><br><span class="line">dxt,da_prevt,dc_prevt,dWft,dbft,dWut,dbut,dWct,dbct,dWot,dbot=gradient[<span class="string">'dxt'</span>],gradient[<span class="string">'da_prev'</span>],gradient[<span class="string">'dc_prev'</span>],gradient[<span class="string">'dWf'</span>],gradient[<span class="string">'dbf'</span>],gradient[<span class="string">'dWu'</span>],gradient[<span class="string">'dbu'</span>],gradient[<span class="string">'dWc'</span>],gradient[<span class="string">'dbc'</span>],gradient[<span class="string">'dWo'</span>],gradient[<span class="string">'dbo'</span>]</span><br><span class="line">dx[:,:,t]=dxt</span><br><span class="line">dWf+=dWft</span><br><span class="line">dbf+=dbft</span><br><span class="line">dWu+=dWut</span><br><span class="line">dbu+=dbut</span><br><span class="line">dWc+=dWct</span><br><span class="line">dWo+=dWot</span><br><span class="line">dbo+=dbot</span><br><span class="line">da0=da_prevt</span><br><span class="line"><span class="comment"># dc0=dc_prevt</span></span><br><span class="line">gradients=&#123;<span class="string">'dx'</span>:dx,<span class="string">'da0'</span>:da0,<span class="string">'dWf'</span>:dWf,<span class="string">'dbf'</span>:dbf,<span class="string">'dWu'</span>:dWu,<span class="string">'dbu'</span>:dbu,<span class="string">'dWc'</span>:dWc,<span class="string">'dbc'</span>:dbc,<span class="string">'dWo'</span>:dWo,<span class="string">'dbo'</span>:dbo&#125;</span><br><span class="line"><span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><h2 id="GRU原理"><a href="#GRU原理" class="headerlink" title="GRU原理"></a>GRU原理</h2><p>（1）网络结构</p><p>GRU可以看成是简化版的LSTM，去除了遗忘门，并且将激活值就作为当前记忆细胞，简化神经单元。</p><p>GRU中同样使用记忆细胞，并使用更新门，当前的新的记忆细胞使用更新门保留新的候选值，使用（1-更新门）保留之前的记忆细胞，并将得到的新记忆细胞作为激活值输出，并经过softmax得到预测值y。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618170407.png" alt></p><p>（2）前向传播</p><p>GRU采用a&lt; t &gt;=c&lt; t &gt;</p><p>1° 得到记忆细胞候选值，使用相关门衡量前一个记忆细胞和当前候选值的相关性</p><p>2° 计算更新门、更新门</p><p>3° 按照更新门来计算新的记忆细胞</p><p>4° 将记忆细胞新的值作为激活值，采用softmax得到预测值</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200618170430.png" alt></p><h2 id="采样"><a href="#采样" class="headerlink" title="采样"></a>采样</h2><p>作用：合成数据，得到每个时间步的预测并传递给下一个神经元，从而得到训练模型后的预测结果。</p><p>当模型经过搭建、编译、训练之后，通过采样得到模型对新的数据（或者不输入数据）的预测值。</p><p>0° 初始采用零向量（独热向量）作为初始输入，根据模型训练好的参数进入循环</p><p>1° 根据RNN网络前向传播得到预测值y</p><p>2° 按照随机采样的方式取其中一个值作为预测值</p><p>3° 按照当前预测值的位置，生成独热向量作为下一个时间步的输入</p><p>4° 不断循环，就得到了合成数据，即使用训练好的模型预测的结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#采样</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(parameters,char_to_ix,seed)</span>:</span></span><br><span class="line">Wax=parameters[<span class="string">'Wax'</span>]</span><br><span class="line">Waa=parameters[<span class="string">'Waa'</span>]</span><br><span class="line">b=parameters[<span class="string">'b'</span>]</span><br><span class="line">Wya=parameters[<span class="string">'Wya'</span>]</span><br><span class="line">by=parameters[<span class="string">'by'</span>]</span><br><span class="line"></span><br><span class="line">vocab_size=by.shape[<span class="number">0</span>]</span><br><span class="line">n_a=Waa.shape[<span class="number">1</span>]</span><br><span class="line">a_prev=np.zeros((n_a,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#创造独热向量</span></span><br><span class="line">x=np.zeros((vocab_size,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">counter=<span class="number">0</span></span><br><span class="line"></span><br><span class="line">idx=<span class="number">-1</span></span><br><span class="line">indies=[]</span><br><span class="line">check_character=char_to_ix[<span class="string">'\n'</span>]</span><br><span class="line"><span class="keyword">while</span> idx!=check_character <span class="keyword">and</span> counter&lt;<span class="number">50</span>:</span><br><span class="line">a=np.tanh(np.dot(Waa,a_prev)+np.dot(Wax,x)+b)</span><br><span class="line">z=np.dot(Wya,a)+by</span><br><span class="line">y=cu.softmax(z)</span><br><span class="line"></span><br><span class="line"><span class="comment">#按照概率采样</span></span><br><span class="line">np.random.seed(seed+counter)</span><br><span class="line">idx=np.random.choice(list(range(vocab_size)),p=y.ravel())</span><br><span class="line">indies.append(idx)</span><br><span class="line"></span><br><span class="line"><span class="comment">#按照选择的位置生成独热向量</span></span><br><span class="line">x=np.zeros((vocab_size,<span class="number">1</span>))</span><br><span class="line">x[idx]=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#将上一个预测值直接作为下一个的输入</span></span><br><span class="line">a_prev=a</span><br><span class="line">seed+=<span class="number">1</span></span><br><span class="line">counter+=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> counter==<span class="number">50</span>:</span><br><span class="line">indies.append(char_to_ix[<span class="string">'\n'</span>])</span><br><span class="line"><span class="keyword">return</span> indies</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;RNN原理&quot;&gt;&lt;a href=&quot;#RNN原理&quot; class=&quot;headerlink&quot; title=&quot;RNN原理&quot;&gt;&lt;/a&gt;RNN原理&lt;/h2&gt;&lt;p&gt;（0）标记符号含义&lt;/p&gt;&lt;p&gt;输入的样本为（X,Y），其中X的维度为（m，T_x，n），m代表样本个数，T_x代表时间步，n代表字典长度。&lt;/p&gt;&lt;p&gt;RNN中每个样本采用独热向量表示，在训练集中选择不重复数据作为字典，根据字典构造每个独热向量（当前词在字典中的位置标为1，其余标为0）。&lt;/p&gt;&lt;p&gt;（1）网络结构&lt;/p&gt;&lt;p&gt;RNN网络结果如图，在t时刻输入当前时刻的x值，经过隐藏层，通过计算前一层的激活值和输入值x得到预测的y。&lt;/p&gt;
    
    </summary>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="循环神经网络" scheme="https://www.xiapf.com/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络——神经风格转换</title>
    <link href="https://www.xiapf.com/blogs/convNet5/"/>
    <id>https://www.xiapf.com/blogs/convNet5/</id>
    <published>2020-06-10T03:10:55.000Z</published>
    <updated>2020-06-10T03:11:44.179Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>将一个图像作为内容图像，另一幅图作为风格图像，两个图像经过神经网络生辰一个新的图像，该图像既有前一个图像的内容，又有后一个图像的风格，这称为神经风格转换。</p><h2 id="损失函数的确定"><a href="#损失函数的确定" class="headerlink" title="损失函数的确定"></a>损失函数的确定</h2><p>神经网络浅层学习图片的点、线或者颜色等基础特征，深层学习图片中物体或者更具体的特征。</p><p>神经风格转换是让合成的图片中的内容更接近内容图片，风格更接近风格图片，即让合成的图片中的内容和内容图片相比损失更小，让合成的图片中的风格和风格图片相比损失更小，最终得到的总成本最小，通过成本函数调整网络的参数。</p><a id="more"></a><p>因此需要定义网络中的内容损失函数，风格损失函数，总成本函数。</p><h3 id="内容损失函数"><a href="#内容损失函数" class="headerlink" title="内容损失函数"></a>内容损失函数</h3><p>将每层的通道按照长宽进行展开，每个位置和内容图片的相应位置之间使用二者之差范数求内容之间的差异：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200610105338.png" alt></p><p>即将两个矩阵对应元素相减再求平方</p><p>说明：（1）最终内容损失函数前的系数可定义为归一化1/2，或者其他值。（因为最终成本函数会通过alpha调整系数）</p><p>（2）这里需要将激活值中的通道数放在前面，因为网络中导入模型的时候是将通道放在最前面处理的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#内容函数成本</span></span><br><span class="line"><span class="comment">#输入内容图像和合成图像的激活值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_content_cost</span><span class="params">(a_C,a_G)</span>:</span></span><br><span class="line">m,n_h,n_w,n_c=a_G.get_shape().as_list()</span><br><span class="line"></span><br><span class="line"><span class="comment">#保证通道在前</span></span><br><span class="line">a_C=tf.transpose(tf.reshape(a_C,(n_h*n_w,n_c)))</span><br><span class="line">a_G=tf.transpose(tf.reshape(a_G,(n_h*n_w,n_c)))</span><br><span class="line">C_G=tf.reduce_sum(tf.square(tf.subtract(a_C,a_G)))</span><br><span class="line"></span><br><span class="line">content_cost=<span class="number">1</span>/(<span class="number">4</span>*n_h*n_w*n_c)*C_G</span><br><span class="line"><span class="keyword">return</span> content_cost</span><br></pre></td></tr></table></figure><h3 id="风格损失函数"><a href="#风格损失函数" class="headerlink" title="风格损失函数"></a>风格损失函数</h3><p>（1）求单层的风格差异</p><p>得到当前图片按照通道展开的风格矩阵（使用相关系数衡量不同位置出现相应风格的概率），再按照范数定义风格之间的差异：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200610110834.png" alt></p><p>1° 求风格矩阵：即将展开的当前通道的矩阵对应元素相乘</p><p>2° 按照范数，将风格图片和合成图片的风格矩阵求差，取平方得到范数值，以此作为风格的损失，即风格之间的差异</p><p>（2）将各层的风格差异合并</p><p>多层风格合并的原因：多考虑几层的风格能使得结果更准确，选择的层数：style_layers=[(“conv1_1”,0.2),(“conv2_1”,0.2),(“conv3_1”,0.2),(“conv4_1”,0.2),(“conv5_1”,0.2)]</p><p>方法：遍历每层，得到激活值输入单层风格求差异，累加每层的风格差异，并乘上每层的系数（衡量每层的风格所占的比重，这里权重定量选择0.2），最终构成总体风格损失函数</p><p>注：为什么内容函数中没有使用多层合并，因为浅层网站中只学习了图形的基本样式，深层网络才学习到真正的图像内容。（个人的理解）</p><p>说明：（1）最终风格损失函数前的系数可定义为归一化1/(2 * nh * nw * nc)^2，或者其他值。（因为最终成本函数会通过beta调整系数）</p><p>（2）这里需要将激活值中的通道数放在前面，因为网络中导入模型的时候是将通道放在最前面处理的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#风格函数成本</span></span><br><span class="line"><span class="comment">#矩阵内元素相乘</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gram_matrix</span><span class="params">(A)</span>:</span></span><br><span class="line">gram=tf.matmul(A,tf.transpose(A))</span><br><span class="line"><span class="keyword">return</span> gram</span><br><span class="line"></span><br><span class="line"><span class="comment">#某一层的风格函数成本</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_layer_style_cost</span><span class="params">(a_S,a_G)</span>:</span></span><br><span class="line">m,n_h,n_w,n_c=a_G.get_shape().as_list()</span><br><span class="line"></span><br><span class="line">a_S=tf.transpose(tf.reshape(a_S,(n_h*n_w,n_c)))</span><br><span class="line">a_G=tf.transpose(tf.reshape(a_G,(n_h*n_w,n_c)))</span><br><span class="line"></span><br><span class="line">a_Svalue=gram_matrix(a_S)</span><br><span class="line">a_Gvalue=gram_matrix(a_G)</span><br><span class="line"></span><br><span class="line">S_G=tf.reduce_sum(tf.square(tf.subtract(a_Svalue,a_Gvalue)))</span><br><span class="line"></span><br><span class="line">style_layer_cost=<span class="number">1</span>/(<span class="number">4</span>*(n_c**<span class="number">2</span>)*((n_h*n_w))**<span class="number">2</span>)*S_G</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> style_layer_cost</span><br><span class="line"></span><br><span class="line"><span class="comment">#从不同的层得到风格函数成本进行合并，效果会更好</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_style_cost</span><span class="params">(model,style_layers)</span>:</span></span><br><span class="line">style_cost=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> layer,coeff <span class="keyword">in</span> style_layers:</span><br><span class="line">out=model[layer]</span><br><span class="line">a_S=sess.run(out)</span><br><span class="line">a_G=out</span><br><span class="line"></span><br><span class="line">style_layer_cost=compute_layer_style_cost(a_S,a_G)</span><br><span class="line"></span><br><span class="line">style_cost+=coeff*style_layer_cost</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> style_cost</span><br></pre></td></tr></table></figure><h3 id="总体损失函数"><a href="#总体损失函数" class="headerlink" title="总体损失函数"></a>总体损失函数</h3><p>将内容损失和风格损失乘以相应系数得到总成本函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#得到总成本</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">total_cost</span><span class="params">(J_content,J_style,alpha=<span class="number">10</span>,beta=<span class="number">40</span>)</span>:</span></span><br><span class="line">J_cost=alpha*J_content+J_style*beta</span><br><span class="line"><span class="keyword">return</span> J_cost</span><br></pre></td></tr></table></figure><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>（1）建立交互会话</p><p>使用交互式对话，可以先定义会话，再定义操作，较直接会话方式使用更方便</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#0.使用交互式会话及导入模型</span></span><br><span class="line">tf.reset_default_graph()</span><br><span class="line">sess=tf.InteractiveSession()<span class="comment">#tf.session是需要将操作都建立好才能进行会话操作，使用交互式会话可以先定义一个会话，再定义操作</span></span><br></pre></td></tr></table></figure><p>（2）导入模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model=nu.load_vgg_model(<span class="string">"./pretrained-model/imagenet-vgg-verydeep-19.mat"</span>)</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure><p>模型采用神经风格转换提出者论文中的模型，采用VGG-19模型，本地保存了该模型网络，打印如下，可见模型存储在字典中，字典的键中保存每个变量，值保存包含该变量值的张量，当需要通过网络运行图像的时候，使用tf.assign将图像输入网络即可</p><p>模型结构如下：</p><blockquote><p>{‘input’: &lt;tf.Variable ‘Variable:0’ shape=(1, 300, 400, 3) dtype=float32_ref&gt;, ‘conv1_1’: &lt;tf.Tensor ‘Relu:0’ shape=(1, 300, 400, 64) dtype=float32&gt;, ‘conv1_2’: &lt;tf.Tensor ‘Relu_1:0’ shape=(1, 300, 400, 64) dtype=float32&gt;, ‘avgpool1’: &lt;tf.Tensor ‘AvgPool:0’ shape=(1, 150, 200, 64) dtype=float32&gt;, ‘conv2_1’: &lt;tf.Tensor ‘Relu_2:0’ shape=(1, 150, 200, 128) dtype=float32&gt;, ‘conv2_2’: &lt;tf.Tensor ‘Relu_3:0’ shape=(1, 150, 200, 128) dtype=float32&gt;, ‘avgpool2’: &lt;tf.Tensor ‘AvgPool_1:0’ shape=(1, 75, 100, 128) dtype=float32&gt;, ‘conv3_1’: &lt;tf.Tensor ‘Relu_4:0’ shape=(1, 75, 100, 256) dtype=float32&gt;, ‘conv3_2’: &lt;tf.Tensor ‘Relu_5:0’ shape=(1, 75, 100, 256) dtype=float32&gt;, ‘conv3_3’: &lt;tf.Tensor ‘Relu_6:0’ shape=(1, 75, 100, 256) dtype=float32&gt;, ‘conv3_4’: &lt;tf.Tensor ‘Relu_7:0’ shape=(1, 75, 100, 256) dtype=float32&gt;, ‘avgpool3’: &lt;tf.Tensor ‘AvgPool_2:0’ shape=(1, 38, 50, 256) dtype=float32&gt;, ‘conv4_1’: &lt;tf.Tensor ‘Relu_8:0’ shape=(1, 38, 50, 512) dtype=float32&gt;, ‘conv4_2’: &lt;tf.Tensor ‘Relu_9:0’ shape=(1, 38, 50, 512) dtype=float32&gt;, ‘conv4_3’: &lt;tf.Tensor ‘Relu_10:0’ shape=(1, 38, 50, 512) dtype=float32&gt;, ‘conv4_4’: &lt;tf.Tensor ‘Relu_11:0’ shape=(1, 38, 50, 512) dtype=float32&gt;, ‘avgpool4’: &lt;tf.Tensor ‘AvgPool_3:0’ shape=(1, 19, 25, 512) dtype=float32&gt;, ‘conv5_1’: &lt;tf.Tensor ‘Relu_12:0’ shape=(1, 19, 25, 512) dtype=float32&gt;, ‘conv5_2’: &lt;tf.Tensor ‘Relu_13:0’ shape=(1, 19, 25, 512) dtype=float32&gt;, ‘conv5_3’: &lt;tf.Tensor ‘Relu_14:0’ shape=(1, 19, 25, 512) dtype=float32&gt;, ‘conv5_4’: &lt;tf.Tensor ‘Relu_15:0’ shape=(1, 19, 25, 512) dtype=float32&gt;, ‘avgpool5’: &lt;tf.Tensor ‘AvgPool_4:0’ shape=(1, 10, 13, 512) dtype=float32&gt;}</p></blockquote><p>（3）输入内容图片</p><p>从网络结构中可以看出，网络输入是300 * 400大小的图像，所以要定义好图像大小</p><p>使用PIL 中的open函数读取图片</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#图片大小为300*400</span></span><br><span class="line"><span class="comment">#1.导入内容图片</span></span><br><span class="line">image_path=<span class="string">"./images/"</span></span><br><span class="line">content_image=Image.open(image_path+<span class="string">"cat.jpg"</span>)</span><br><span class="line">content_image=nu.reshape_and_normalize_image(np.array(content_image))</span><br></pre></td></tr></table></figure><p>（4）输入风格图片</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#2.导入风格图片</span></span><br><span class="line">style_image=Image.open(image_path+<span class="string">"camp-nou.jpg"</span>)</span><br><span class="line">style_image=nu.reshape_and_normalize_image(np.array(style_image))</span><br></pre></td></tr></table></figure><p>（5）计算成本函数</p><p>内容成本：1° 将内容图片通过assign方法输入到模型中，并使用会话运行网络；</p><p>2° 网络一共5层，选择第四层的内容作为比对的内容，将模型运行该层得到的激活值作为a_C，即内容图片输出（需要比对的内容）</p><p>3° a_G这里使用对应层数的线性值作为占位符，当运行模型的时候，更新值</p><p>4° 最终输入内容损失函数中</p><p>风格成本：1° 将风格图片通过assign方法输入到模型中，并使用会话运行网络；</p><p>2° 网络一共5层，选择第1-5层的风格和系数作为比对的内容，将模型和需要比对的网络层输入到风格损失函数中</p><p>综上，将二者成本输入到总成本中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#4.分别计算三个成本</span></span><br><span class="line">sess.run(model[<span class="string">"input"</span>].assign(content_image))</span><br><span class="line">out=model[<span class="string">'conv4_2'</span>]</span><br><span class="line">a_C=sess.run(out)</span><br><span class="line">a_G=out</span><br><span class="line">J_content=compute_content_cost(a_C,a_G)</span><br><span class="line"></span><br><span class="line">sess.run(model[<span class="string">"input"</span>].assign(style_image))</span><br><span class="line">J_style=compute_style_cost(model,style_layers)</span><br><span class="line">J_cost=total_cost(J_content,J_style,alpha=<span class="number">10</span>,beta=<span class="number">40</span>)</span><br></pre></td></tr></table></figure><p>（6）定义网络的优化算法</p><p>选择adam优化算法和学习率，以及需要最小化的成本函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#5.定义优化算法</span></span><br><span class="line">optimizer=tf.train.AdamOptimizer(learning_rate=<span class="number">2</span>).minimize(J_cost)</span><br></pre></td></tr></table></figure><p>（7）搭建网络</p><p>0° 此时运行会话初始化所有变量</p><p>1° 将合成图片作为网络输入，并运行会话</p><p>2° 每次迭代都按照优化算法，并更新合成的图片，将每次的合成图片按照路径保存</p><p>3° 迭代结束得到最终合成效果图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#6.搭建总体模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_nn</span><span class="params">(sess,input_image,iteration=<span class="number">200</span>)</span>:</span></span><br><span class="line"><span class="comment">#初始化所有变量</span></span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line"><span class="comment">#将输入的图片作为初始图片输入网络</span></span><br><span class="line">generate_image=sess.run(model[<span class="string">"input"</span>].assign(input_image))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(iteration):</span><br><span class="line">sess.run(optimizer)</span><br><span class="line"></span><br><span class="line"><span class="comment">#每次迭代更新合成的图片</span></span><br><span class="line">generate_image=sess.run(model[<span class="string">"input"</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> i%<span class="number">20</span>==<span class="number">0</span>:</span><br><span class="line">Jc,Js,Jt=sess.run([J_content,J_style,J_cost])</span><br><span class="line">print(<span class="string">"iteration:"</span>,i)</span><br><span class="line">print(<span class="string">"内容函数损失："</span>,Jc)</span><br><span class="line">print(<span class="string">"风格函数损失："</span>,Js)</span><br><span class="line">print(<span class="string">"总体损失："</span>,Jt)</span><br><span class="line">nu.save_image(<span class="string">"./out/"</span>+str(i)+<span class="string">".png"</span>,generate_image)</span><br><span class="line"></span><br><span class="line">nu.save_image(<span class="string">"./out/generate_image.jpg"</span>,generate_image)</span><br><span class="line"><span class="keyword">return</span> generate_image</span><br></pre></td></tr></table></figure><p>（8）最终结果</p><p>内容图片：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200610093228.png" alt></p><p>风格图片：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200610093247.png" alt></p><p>输入model_nn(sess,generate_image)进行测试，每次迭代得到的结果输出在out文件夹下，迭代结束后最终得到的合成结果如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200610093114.png" alt></p><p>从合成图像可以看出，很好的保留了内容图像中内容，也很好的融合了风格图片中的风格。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;问题描述&lt;/h2&gt;&lt;p&gt;将一个图像作为内容图像，另一幅图作为风格图像，两个图像经过神经网络生辰一个新的图像，该图像既有前一个图像的内容，又有后一个图像的风格，这称为神经风格转换。&lt;/p&gt;&lt;h2 id=&quot;损失函数的确定&quot;&gt;&lt;a href=&quot;#损失函数的确定&quot; class=&quot;headerlink&quot; title=&quot;损失函数的确定&quot;&gt;&lt;/a&gt;损失函数的确定&lt;/h2&gt;&lt;p&gt;神经网络浅层学习图片的点、线或者颜色等基础特征，深层学习图片中物体或者更具体的特征。&lt;/p&gt;&lt;p&gt;神经风格转换是让合成的图片中的内容更接近内容图片，风格更接近风格图片，即让合成的图片中的内容和内容图片相比损失更小，让合成的图片中的风格和风格图片相比损失更小，最终得到的总成本最小，通过成本函数调整网络的参数。&lt;/p&gt;
    
    </summary>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="卷积神经网络" scheme="https://www.xiapf.com/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络——人脸识别</title>
    <link href="https://www.xiapf.com/blogs/convNet4/"/>
    <id>https://www.xiapf.com/blogs/convNet4/</id>
    <published>2020-06-09T12:48:18.000Z</published>
    <updated>2020-06-09T12:53:22.059Z</updated>
    
    <content type="html"><![CDATA[<p>人脸识别系统包括人脸识别和活体检测（使用监督学习），这里着重介绍人脸识别部分。</p><h2 id="人脸验证和人脸识别区别"><a href="#人脸验证和人脸识别区别" class="headerlink" title="人脸验证和人脸识别区别"></a>人脸验证和人脸识别区别</h2><p>（1）人脸验证</p><p>输入：图片和ID/姓名</p><p>输出：验证这个图片是否是ID/姓名代表的这个人</p><p>这是个1：1问题</p><p>（2）人脸识别</p><p>输入：图片</p><p>输出：查找输入的图片是否是k个人中的一个</p><p>这是个1:K问题</p><p>总结：人脸识别是以人脸验证作为基础模块</p><a id="more"></a><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>人脸识别是为了解决one shot learning问题，即一次学习就能识别出人的身份。</p><p>一次学习即使用一张图片训练出来的神经网络识别率很差，而且当有新的人图片出现还需要重新训练网络，这样很麻烦。</p><p>因此，人脸识别的核心：</p><p>（1）使用能够识别人脸图片的网络</p><p>（2）将输入的图片输入网络得到图片编码与数据库内图片对比，差异性小的就认为两者一致，反之不一致</p><p>所以，重点在于训练的网络要使得相同人的差异性小，不同人的差异性大。</p><p>于是得到损失函数为三元组损失函数，网络结构采用Siamese网络结构能够对图片编码，下面两部分分别介绍</p><h3 id="“similarity”函数-gt-三元组损失函数的确定"><a href="#“similarity”函数-gt-三元组损失函数的确定" class="headerlink" title="“similarity”函数-&gt;三元组损失函数的确定"></a>“similarity”函数-&gt;三元组损失函数的确定</h3><p>“similarity”函数：用于衡量两个图片的相似度，定义为d(img1,img2)=两张图片的不同，即输入两张图片，输出两张图片的相似度。</p><p>人脸识别的三元组损失函数triplets为：</p><p>（1）相同的人：anchor和positive差异小</p><p>（2）不同的人：anchor和negative差异大</p><p>即d(A,P)&lt;=d(A,N)，为了让两者相差多一点，这里加上一个间隔alpha，同时为了防止出现损失函数为0的情况（这样输出无意义），即需要保证：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200609203322.png" alt></p><p>即满足triplet&lt;=0，则最终网络的损失函数在triplets和0之间取较大值，当triplets小于等于0时，即损失等于0，则此时网络训练结束，当大于0时，损失不为0，此时需要让d(A,P)更小，或者d(A,N)更大，以满足条件。</p><p>成本函数是损失函数求和，则网络的损失和成本定义为：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200609203435.png" alt></p><h3 id="Siamese网络结构"><a href="#Siamese网络结构" class="headerlink" title="Siamese网络结构"></a>Siamese网络结构</h3><p>将输入的图像转换为编码形式，即网络结构和convnet相似，但是最后全连接层输出部分不输入softmax，直接将全连接层的输出作为输入图像的编码。</p><p>总结：</p><p>（1）使用Siamese网络得到输入图片的编码</p><p>（2）选择难和输入图片区分的图片作为negative图片，同时选择相同图片，加上输入图片构成三元组</p><p>（3）计算这三元组的距离，从而得出损失，采用反向传播算法更新网络参数</p><p>最终得到能够识别人脸的网络。</p><h3 id="附：可以将人脸识别转换为监督学习（二分类）问题"><a href="#附：可以将人脸识别转换为监督学习（二分类）问题" class="headerlink" title="附：可以将人脸识别转换为监督学习（二分类）问题"></a>附：可以将人脸识别转换为监督学习（二分类）问题</h3><p>将一对图片经过卷积池化等操作，经过全连接层后输入logitic回归，最终输出是否是同一个人，即输出0或者1，这样转换成了监督学习方法。</p><p>这里的输入是一对图片，logistic回归内的预测函数为：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200609204309.png" alt></p><p>对编码后的向量每个元素取绝对值，再加上权重和偏置量，训练w,b，得到最终的预测结果。</p><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>（）网络架构</p><p>网络的架构上遵循Szegedy et al.等人的初始模型，应用该网络结构并且指明优化算法和损失函数，将模型进行编译，最后导入本地的权重，生成得出的预训练模型。</p><p>这里的图片格式是通道在前，因此在最开始需要设置通道的位置：K.set_image_data_format(“channels_first”)，同时输入网络的图片长宽为96 * 96</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.加载模型结构</span></span><br><span class="line">FRmodel=ibv2.faceRecoModel(input_shape=(<span class="number">3</span>,<span class="number">96</span>,<span class="number">96</span>))</span><br><span class="line"><span class="comment">#编译模型，损失采用三元组损失</span></span><br><span class="line">FRmodel.compile(<span class="string">"adam"</span>,loss=triplet_loss,metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"><span class="comment">#导入权重</span></span><br><span class="line">fu.load_weights_from_FaceNet(FRmodel)</span><br></pre></td></tr></table></figure><p>（1）使用三元组损失函数</p><p>分别求出d(A,P),d(A,N)的值，加上间隔并求和得到网络的成本函数，公式参照三元损失函数的定义。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#triplets_loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">triplet_loss</span><span class="params">(yred,alpha=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">anchor,postive,negative=yred[<span class="number">0</span>],yred[<span class="number">1</span>],yred[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">AP_loss=tf.reduce_sum(tf.square(tf.subtract(anchor,postive)),axis=<span class="number">-1</span>)</span><br><span class="line">AN_loss=tf.reduce_sum(tf.square(tf.subtract(anchor,negative)),axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">APN_loss=tf.subtract(AP_loss,AN_loss)</span><br><span class="line">APN_loss=tf.add(APN_loss,alpha)</span><br><span class="line"></span><br><span class="line">loss=tf.reduce_sum(tf.maximum(APN_loss,<span class="number">0</span>))</span><br><span class="line"><span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><p>（2）保存模型</p><p>将具有网格结构与权重的预训练模型进行保存</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FRmodel.save(<span class="string">'model.h5'</span>)</span><br></pre></td></tr></table></figure><p>（3）导入训练的模型</p><p>由于模型中的损失函数是使用的自定义损失函数，因此在导入模型时，需要指明保存的模型中的损失函数的具体实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FRmodel=load_model(<span class="string">"model.h5"</span>,custom_objects=&#123;<span class="string">"triplet_loss"</span>:triplet_loss&#125;)</span><br></pre></td></tr></table></figure><p>（4）建立员工人脸编码数据库</p><p>将现有员工的人脸图像经过模型得到128位的编码，之后将数据库内的编码和输入的图像编码比对，预先将这些编码保存好能节省人脸识别的时间</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据库：生成128维向量编码</span></span><br><span class="line"><span class="comment">#加载自定义的loss函数</span></span><br><span class="line">FRmodel=load_model(<span class="string">"model.h5"</span>,custom_objects=&#123;<span class="string">"triplet_loss"</span>:triplet_loss&#125;)</span><br><span class="line">img_path=<span class="string">'./images/'</span></span><br><span class="line">dataset=&#123;&#125;</span><br><span class="line">dataset[<span class="string">'bajie'</span>]=fu.img_to_encoding(img_path+<span class="string">'bajie.jpg'</span>,FRmodel)</span><br><span class="line"></span><br><span class="line">dataset[<span class="string">'andrew'</span>]=fu.img_to_encoding(img_path+<span class="string">'andrew.jpg'</span>,FRmodel)</span><br><span class="line">dataset[<span class="string">'arnaud'</span>]=fu.img_to_encoding(img_path+<span class="string">'arnaud.jpg'</span>,FRmodel)</span><br><span class="line">dataset[<span class="string">'benoit'</span>]=fu.img_to_encoding(img_path+<span class="string">'benoit.jpg'</span>,FRmodel)</span><br><span class="line">dataset[<span class="string">'bertrand'</span>]=fu.img_to_encoding(img_path+<span class="string">'bertrand.jpg'</span>,FRmodel)</span><br><span class="line">dataset[<span class="string">'dan.jpg'</span>]=fu.img_to_encoding(img_path+<span class="string">'dan.jpg'</span>,FRmodel)</span><br><span class="line">dataset[<span class="string">'younes'</span>]=fu.img_to_encoding(img_path+<span class="string">'younes.jpg'</span>,FRmodel)</span><br><span class="line">dataset[<span class="string">'kian'</span>]=fu.img_to_encoding(img_path+<span class="string">'kian.jpg'</span>,FRmodel)</span><br></pre></td></tr></table></figure><p>（5）人脸验证 1：1问题</p><p>将输入的图像经过网络转换为编码形式</p><p>根据用户身份找到数据库中的编码</p><p>将二者编码对比，如果距离小于阈值（这里设定0.7），则人脸和身份符合，反之不符合</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#根据输入的门禁id和当前识别的人脸比对</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">verfy</span><span class="params">(img,model,identity,dataset)</span>:</span></span><br><span class="line"><span class="comment">#将验证的图片转换为编码形式</span></span><br><span class="line">img_code=fu.img_to_encoding(img,model)</span><br><span class="line"></span><br><span class="line"><span class="comment">#计算验证图片和数据库内图片的距离</span></span><br><span class="line">distance=np.linalg.norm(img_code-dataset[identity])</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> distance&lt;<span class="number">0.7</span>:</span><br><span class="line">print(<span class="string">"验证通过，欢迎你，"</span>+str(identity))</span><br><span class="line">is_open=<span class="literal">True</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">print(<span class="string">"验证失败，"</span>+<span class="string">"你与"</span>+str(identity)+<span class="string">"不符合"</span>)</span><br><span class="line">is_open=<span class="literal">False</span></span><br><span class="line"><span class="keyword">return</span> distance,is_open</span><br></pre></td></tr></table></figure><p>将本人图片编码保存在数据库中，图片信息如图：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200609173436.png" alt></p><p>进行测试</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test_img=img_path+<span class="string">'camera_6.jpg'</span></span><br><span class="line">distance,is_open=verfy(test_img,FRmodel,<span class="string">'bajie'</span>,dataset)</span><br><span class="line">print(distance,is_open)</span><br></pre></td></tr></table></figure><p>使用非本人图片测试</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200609173550.png" alt></p><p>结果：验证失败，你与bajie不符合<br>1.041191 False</p><p>使用本人的其他图片测试</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200609173613.png" alt></p><p>结果：验证通过，欢迎你，bajie<br>0.6963635 True</p><p>最终输出的是差异度及是否开门</p><p>（6）人脸识别 1：K问题</p><p>和人脸验证相比，这里不输入用户的身份，直接根据输入的图像判断：</p><p>将输入的图像经过网络转换为编码形式</p><p>遍历数据库，找到当前编码和数据库内距离最近的编码</p><p>当距离小于阈值（这里设定0.7），则说明在数据库中找到了与之相似的人脸，则找到了该员工，反之则未找到，即不开门</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 人脸识别系统</span></span><br><span class="line"><span class="comment">#当前识别的人脸与数据库内所有人员进行比对，得到最小的距离则为最相似的人</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">config</span><span class="params">(img,model,dataset)</span>:</span></span><br><span class="line">img_code=fu.img_to_encoding(img,model)</span><br><span class="line">min_dist=<span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (name,dcode) <span class="keyword">in</span> dataset.items():</span><br><span class="line">dist=np.linalg.norm(img_code-dcode)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(dist&lt;min_dist):</span><br><span class="line">min_dist=dist</span><br><span class="line">identity=name</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> min_dist&lt;<span class="number">0.7</span>:</span><br><span class="line">print(<span class="string">"验证通过，欢迎你，"</span>+str(identity))</span><br><span class="line">is_open=<span class="literal">True</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">print(<span class="string">"验证失败，你不是本系统成员"</span>)</span><br><span class="line">is_open=<span class="literal">False</span></span><br><span class="line"><span class="keyword">return</span> min_dist,is_open</span><br></pre></td></tr></table></figure><p>进行测试</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test_img=img_path+<span class="string">'camera_7.jpg'</span></span><br><span class="line">distance,is_open=config(test_img,FRmodel,dataset)</span><br><span class="line">print(distance,is_open)</span><br></pre></td></tr></table></figure><p>当使用上述camera_7非系统内成员图像时：</p><p>验证失败，你不是本系统成员<br>0.79701996 False</p><p>当使用上述camera_6系统内成员的其他图像时：</p><p>验证通过，欢迎你，bajie<br>0.6963635 True</p><p>说明：最终输出当前识别出的人的身份，以及门是否打开</p><h2 id="附：将图像转换为想要的尺寸大小"><a href="#附：将图像转换为想要的尺寸大小" class="headerlink" title="附：将图像转换为想要的尺寸大小"></a>附：将图像转换为想要的尺寸大小</h2><p>使用PIL中Image将对应路径的图像打开，并进行尺寸调整，最终使用save对调整好的图像进行保存</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">img_path=<span class="string">"./images/"</span></span><br><span class="line">file_img=img_path+<span class="string">"camera_7.jpg"</span></span><br><span class="line">img=Image.open(file_img).resize((<span class="number">96</span>,<span class="number">96</span>))</span><br><span class="line">img.save(img_path+<span class="string">"camera_7.jpg"</span>)</span><br></pre></td></tr></table></figure><p>上述实验中使用的自己的图片需要转换为网络能识别的大小，即96 * 96</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;人脸识别系统包括人脸识别和活体检测（使用监督学习），这里着重介绍人脸识别部分。&lt;/p&gt;&lt;h2 id=&quot;人脸验证和人脸识别区别&quot;&gt;&lt;a href=&quot;#人脸验证和人脸识别区别&quot; class=&quot;headerlink&quot; title=&quot;人脸验证和人脸识别区别&quot;&gt;&lt;/a&gt;人脸验证和人脸识别区别&lt;/h2&gt;&lt;p&gt;（1）人脸验证&lt;/p&gt;&lt;p&gt;输入：图片和ID/姓名&lt;/p&gt;&lt;p&gt;输出：验证这个图片是否是ID/姓名代表的这个人&lt;/p&gt;&lt;p&gt;这是个1：1问题&lt;/p&gt;&lt;p&gt;（2）人脸识别&lt;/p&gt;&lt;p&gt;输入：图片&lt;/p&gt;&lt;p&gt;输出：查找输入的图片是否是k个人中的一个&lt;/p&gt;&lt;p&gt;这是个1:K问题&lt;/p&gt;&lt;p&gt;总结：人脸识别是以人脸验证作为基础模块&lt;/p&gt;
    
    </summary>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="卷积神经网络" scheme="https://www.xiapf.com/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络——识别并定位车辆</title>
    <link href="https://www.xiapf.com/blogs/convNet3/"/>
    <id>https://www.xiapf.com/blogs/convNet3/</id>
    <published>2020-06-08T03:03:00.000Z</published>
    <updated>2020-06-09T12:49:28.456Z</updated>
    
    <content type="html"><![CDATA[<h2 id="YOLO算法"><a href="#YOLO算法" class="headerlink" title="YOLO算法"></a>YOLO算法</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>（1）目标检测与定位</p><p>目标检测：输入一个图像，经过convnet，判读图片中是否有物体（或者输出该物体是几个类别中的哪个）。方法类似于对图片的分类。</p><p>目标定位：输入一个图像，经过convert，将图片中物体使用定位框将其标注出来。方法：此时的训练集中标签项需要重新设置，设置是否有物体pc，物体的位置参数（中心点bx,by，长宽bw,bh），物体属于的类别。</p><a id="more"></a><p>（2）滑动窗口一次卷积</p><p>上述的目标定位适用于图片中只有一个物体，当有多个物体时，可以将图像划分成一个个网格，将每个格子输入convert，在每个格子上进行目标检测。</p><p>缺点：计算成本大，耗时</p><p>此时可以采用一次卷积的方法：图片按照网格划分，每个格子对应一个输出标签，将图片整体输入convert中，最终得到每个格子的结果，即每个格子输出一个定位框。这样比将每个格子输入网络中的计算成本小。</p><h3 id="物体中点认定——让一个物体只被一个格子检测"><a href="#物体中点认定——让一个物体只被一个格子检测" class="headerlink" title="物体中点认定——让一个物体只被一个格子检测"></a>物体中点认定——让一个物体只被一个格子检测</h3><p>为了保证一个物体只被一个格子检测，定义物体的中点所在的格子识别出该对象，其余有该对象的格子不识别该对象。</p><h3 id="anchor-box——一个格子能检测出多个物体"><a href="#anchor-box——一个格子能检测出多个物体" class="headerlink" title="anchor box——一个格子能检测出多个物体"></a>anchor box——一个格子能检测出多个物体</h3><p>当有多个物体的中点落在同一个格子时，物体识别的类别可能有多个，此时网络无法识别输出，为了能让一个格子检测出多个物体，引入anchor box，按照物体的形状设置anchor box，和设置的anchor box的Iou值高说明该物体匹配该anchor box，类别设置在该anchor box中，因此每个标签y中有多个anchor box，对应不同形状的物体，这样一个格子就能检测出多个物体。</p><h3 id="非最大值抑制——只输出一个定位框"><a href="#非最大值抑制——只输出一个定位框" class="headerlink" title="非最大值抑制——只输出一个定位框"></a>非最大值抑制——只输出一个定位框</h3><p>（1）阈值过滤</p><p>使用pc * 每个类别得到每个类别出现的概率，取最大的概率作为该格子识别出的类别及该格子判定的物体的概率，将该概率和过滤的阈值比较，保留大于等于该阈值的类别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#分类阈值过滤</span></span><br><span class="line"><span class="comment">#box_confidence  （19，19，5，1）</span></span><br><span class="line"><span class="comment">#boxes  （19，19，5，4）</span></span><br><span class="line"><span class="comment">#box_classes_prob    （19，19，5，80）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">yolo_filter_boxes</span><span class="params">(box_confidence,boxes,box_classes_prob,threshold)</span>:</span></span><br><span class="line"><span class="comment">#得到每个概率</span></span><br><span class="line">box_scores=box_confidence*box_classes_prob</span><br><span class="line"></span><br><span class="line"><span class="comment">#找到最大的概率的分类和对应的pc</span></span><br><span class="line"><span class="comment">#用最后的80个分类来比较</span></span><br><span class="line">box_class=K.argmax(box_scores,axis=<span class="number">-1</span>)</span><br><span class="line">box_class_scores=K.max(box_scores,axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#根据每个的pc和阈值比较</span></span><br><span class="line">box_max=(box_class_scores&gt;=threshold)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scores=tf.boolean_mask(box_class_scores,box_max)</span><br><span class="line">boxes=tf.boolean_mask(boxes,box_max)</span><br><span class="line">classes=tf.boolean_mask(box_class,box_max)</span><br><span class="line"><span class="keyword">return</span> scores,boxes,classes</span><br></pre></td></tr></table></figure><p>（2）Iou</p><p>定义：Iou=两个目标定位框的相交的面积/两个目标定位框的总面积</p><p>使用Iou去除不符合要求的定位框，一般Iou设置为0.5，当小于该值的定位框直接删除</p><p>方法：输入两个定位框的左上角和右下角坐标，得到相交的面积再除以总面积。</p><p>原理：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iou</span><span class="params">(box1,box2)</span>:</span></span><br><span class="line"><span class="comment">#box1(x1,y1,x2,y2)</span></span><br><span class="line"><span class="comment">#box2(x3,y3,x4,y4)</span></span><br><span class="line"><span class="comment">#左上角</span></span><br><span class="line">xi1=np.maximum(box1[<span class="number">0</span>],box2[<span class="number">0</span>])</span><br><span class="line">yi1=np.maximum(box1[<span class="number">1</span>],box2[<span class="number">1</span>])</span><br><span class="line"><span class="comment">#右下角</span></span><br><span class="line">xi2=np.minimum(box1[<span class="number">2</span>],box2[<span class="number">2</span>])</span><br><span class="line">yi2=np.minimum(box1[<span class="number">3</span>],box2[<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#计算面积</span></span><br><span class="line">box1_area=(box1[<span class="number">2</span>]-box1[<span class="number">0</span>])*(box1[<span class="number">3</span>]-box1[<span class="number">1</span>])</span><br><span class="line">box2_area=(box2[<span class="number">2</span>]-box2[<span class="number">0</span>])*(box2[<span class="number">3</span>]-box2[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">box_inter_area=(xi2-xi1)*(yi2-yi1)</span><br><span class="line">box_union_area=box1_area+box2_area-box_inter_area</span><br><span class="line"></span><br><span class="line">iou_area=box_inter_area/box_union_area</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> iou_area</span><br></pre></td></tr></table></figure><p>yolo自带non_max_suppression方法，定义一个图像中输出的最大定位框个数和iou阈值，自动得到过滤之后的定位框</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">yolo_non_max_suppression</span><span class="params">(scores,boxes,classes,max_box=<span class="number">10</span>,iou_threshold=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">max_box_tensor=K.variable(max_box,dtype=<span class="string">"int32"</span>)</span><br><span class="line">K.get_session().run(tf.variables_initializer([max_box_tensor]))</span><br><span class="line"></span><br><span class="line">num_dims=tf.image.non_max_suppression(boxes,scores,max_box,iou_threshold)</span><br><span class="line"></span><br><span class="line">scores=K.gather(scores,num_dims)</span><br><span class="line">boxes=K.gather(boxes,num_dims)</span><br><span class="line">classes=K.gather(classes,num_dims)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> scores,boxes,classes</span><br></pre></td></tr></table></figure><p>（3）总结：对所有框进行过滤</p><p>根据官方的yolo模型经过convnet得到的输出转换为中心点形式</p><p>将其经过阈值过滤，得到定位框并将其扩展以适应新图像的大小（模型得到的定位框是680x680）</p><p>经过非最大值抑制得到最终每个格子过滤之后的物体定位框</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#对所有框进行过滤</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">yolo_eval</span><span class="params">(output,image_shape=<span class="params">(<span class="number">720.</span>,<span class="number">1280.</span>)</span>,max_box=<span class="number">10</span>,scores_threshold=<span class="number">0.6</span>,iou_threshold=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">box_confidence,box_xy,boxhw,box_classes_prob=output</span><br><span class="line"></span><br><span class="line"><span class="comment">#将中心点转换为坐标形式</span></span><br><span class="line">boxes=yolo_boxes_to_corners(box_xy,boxhw)</span><br><span class="line"></span><br><span class="line"><span class="comment">#阈值过滤</span></span><br><span class="line">scores,boxes,classes=yolo_filter_boxes(box_confidence,boxes,box_classes_prob,scores_threshold)</span><br><span class="line"></span><br><span class="line"><span class="comment">#缩放anchor box以适应原始图像</span></span><br><span class="line">boxes=yu.scale_boxes(boxes,image_shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#nms过滤</span></span><br><span class="line">scores,boxes,classes=yolo_non_max_suppression(scores,boxes,classes,max_box,iou_threshold)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> scores,boxes,classes</span><br></pre></td></tr></table></figure><h3 id="训练集设置"><a href="#训练集设置" class="headerlink" title="训练集设置"></a>训练集设置</h3><p>将每个图像划分为n * n的网格，常用的网格个数n=19，每一个网格对应一对数据（x,y）</p><p>x：代表每个网格中输入的图片</p><p>y：代表当前格子对应物体的概率，定位框以及类别。假设类别数为80，则y的维度为pc+定位框系数（4个）+类别数（80个）=85个，当有5个anchor box时，每个anchor box中都有85个数据，填充在y中，y的维度=85 * 5=425</p><h3 id="算法如何预测"><a href="#算法如何预测" class="headerlink" title="算法如何预测"></a>算法如何预测</h3><p>（1）输入图像，经过convert，得到预测的标签y：每个格子对应一个425的向量，19 * 19的格子即一张图片对应（19，19，425）维的向量，每个图片的y输出（19，19，425）维的向量，预测对应格子中物体出现的概率以及定位框</p><p>（2）过滤定位框：通过计算每个格子中识别出的物体的概率，进行阈值过滤，小于阈值的定位框去除，每个类别的无图再根据非最大值抑制，删除Iou高的定位框，保证一个物体只有一个定位框。</p><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>（1）导入模型</p><p>使用官网yolo模型，导入anchor box的类型及分类的类别数量，使用keras.models中的load_model得到训练好的模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#加载模型</span></span><br><span class="line">sess=K.get_session()</span><br><span class="line"><span class="comment">#定义anchor box,class,输入图像大小</span></span><br><span class="line">boxes=yu.read_anchors(<span class="string">"./model_data/yolo_anchors.txt"</span>)</span><br><span class="line">classes_name=yu.read_classes(<span class="string">"./model_data/object_classes.txt"</span>)</span><br><span class="line">image_shape=(<span class="number">720.</span>,<span class="number">1280.</span>)</span><br><span class="line">car_model=load_model(<span class="string">"./model_data/yolov2.h5"</span>)</span><br><span class="line">car_model.summary()</span><br></pre></td></tr></table></figure><p>（2）批量预测</p><p>1° 输入图像，并将图像按照模型中680x680尺寸换算，并输入模型中</p><p>2° 运行模型，将yolo模型的数据转换为边框坐标形式，经过阈值与非最大值抑制过滤边框，得到预测的物体即边框</p><p>3° 根据得到的边框进行过滤，并且边框需要适应原始图像大小，最终得到过滤后的边框</p><p>4° 根据模型类别得到颜色，并将边框在原始图像上画出来</p><p>5° 使用os.path.join得出预测后图像的保存地址，进行保存 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将模型输出转换为边框</span></span><br><span class="line">yolo_outputs=yolo_head(car_model.output,boxes,len(classes_name))</span><br><span class="line"></span><br><span class="line"><span class="comment">#调用eval进行边框过滤</span></span><br><span class="line">scores,boxes,classes=yolo_eval(yolo_outputs,image_shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用模型进行车辆识别</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(sess,img_file)</span>:</span></span><br><span class="line"><span class="comment">#图片处理（归一化和延展）</span></span><br><span class="line">file=<span class="string">'./images/'</span></span><br><span class="line">img_path=file+img_file</span><br><span class="line"></span><br><span class="line">img,img_data=yu.preprocess_image(img_path,model_image_size=(<span class="number">608</span>,<span class="number">608</span>))</span><br><span class="line"><span class="comment">#设置占位符</span></span><br><span class="line">out_scores,out_boxes,out_classes=sess.run([scores,boxes,classes],feed_dict=&#123;car_model.input:img_data,K.learning_phase():<span class="number">0</span>&#125;)</span><br><span class="line"></span><br><span class="line">print(img_file+<span class="string">"中找到了anchor box："</span>+str(len(out_boxes)))</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置输出的颜色</span></span><br><span class="line">colors=yu.generate_colors(classes_name)</span><br><span class="line"></span><br><span class="line"><span class="comment">#画边框</span></span><br><span class="line">yu.draw_boxes(img,out_scores,out_boxes,out_classes,classes_name,colors)</span><br><span class="line"></span><br><span class="line"><span class="comment">#保存画好边框的图片</span></span><br><span class="line">img.save(os.path.join(<span class="string">'out'</span>,img_file),quatity=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#展示图片</span></span><br><span class="line">p_image=Image.open(os.path.join(<span class="string">'out'</span>,img_file))</span><br><span class="line">plt.imshow(p_image)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> out_scores,out_boxes,out_classes</span><br></pre></td></tr></table></figure><p>（3）结果</p><p>zfill:字符串右对齐，不够长度的左边补0，调用predict，输入需要的图片进行物体识别定位</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>,<span class="number">18</span>):</span><br><span class="line">img=str(i).zfill(<span class="number">4</span>)+<span class="string">".jpg"</span></span><br><span class="line">out_scores,out_boxes,out_classes=predict(sess,img)</span><br><span class="line">print(<span class="string">"识别结束"</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200605162058.png" alt></p><p>0066.jpg中找到了anchor box：2<br>traffic light 0.62 (532, 68) (564, 113)<br>car 0.77 (372, 281) (454, 333)</p><p>结果输出该图像中找到的物体以及物体的坐标</p><p>说明：从结果中可以看出，识别出了车辆和红灯，并用方框将其定位出来了。</p><p>再测试另一个图片</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200608105320.png" alt></p><p>0017.jpg中找到了anchor box：3<br>truck 0.64 (666, 274) (1063, 392)<br>car 0.79 (1103, 300) (1271, 356)<br>car 0.82 (1, 358) (183, 427)</p><p>说明：此时识别出一个卡车和两个汽车，并用不同颜色框将其定位。</p><h2 id="附：R-CNN带区域的卷积神经网络"><a href="#附：R-CNN带区域的卷积神经网络" class="headerlink" title="附：R-CNN带区域的卷积神经网络"></a>附：R-CNN带区域的卷积神经网络</h2><p>思路来源于基于滑动窗口的目标检测算法</p><p>思想：</p><p>（1）使用某种算法（1.分割算法；2.卷积神经网络）得到图像的候选区域</p><p>（2）在候选区域中运行分类器，得到每个候选其余的类别及定位框</p><p>说明：因为该种目标定位与识别是分两步进行，时间效率比yolo总体要低一些。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;YOLO算法&quot;&gt;&lt;a href=&quot;#YOLO算法&quot; class=&quot;headerlink&quot; title=&quot;YOLO算法&quot;&gt;&lt;/a&gt;YOLO算法&lt;/h2&gt;&lt;h3 id=&quot;原理&quot;&gt;&lt;a href=&quot;#原理&quot; class=&quot;headerlink&quot; title=&quot;原理&quot;&gt;&lt;/a&gt;原理&lt;/h3&gt;&lt;p&gt;（1）目标检测与定位&lt;/p&gt;&lt;p&gt;目标检测：输入一个图像，经过convnet，判读图片中是否有物体（或者输出该物体是几个类别中的哪个）。方法类似于对图片的分类。&lt;/p&gt;&lt;p&gt;目标定位：输入一个图像，经过convert，将图片中物体使用定位框将其标注出来。方法：此时的训练集中标签项需要重新设置，设置是否有物体pc，物体的位置参数（中心点bx,by，长宽bw,bh），物体属于的类别。&lt;/p&gt;
    
    </summary>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="卷积神经网络" scheme="https://www.xiapf.com/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>springBoot部署问题</title>
    <link href="https://www.xiapf.com/blogs/tomcatNoWebXml/"/>
    <id>https://www.xiapf.com/blogs/tomcatNoWebXml/</id>
    <published>2020-06-05T06:01:49.000Z</published>
    <updated>2020-06-08T03:02:31.895Z</updated>
    
    <content type="html"><![CDATA[<p>springboot项目通过外置tomcat部署，不需要web.xml的原因分析：</p><p>首先可以看到在spring-web.jar的META-INF有一个配置文件</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200604155440.png" alt="spi配置文件"></p><p>这里是通过SPI机制，执行了ServletContainerInitializer接口的实现类SpringServletContainerInitializer中的onStartup方法</p><a id="more"></a><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onStartup</span><span class="params">(Set&lt;Class&lt;?&gt;&gt; webAppInitializerClasses, ServletContext servletContext)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> ServletException </span>&#123;</span><br><span class="line"></span><br><span class="line">List&lt;WebApplicationInitializer&gt; initializers = <span class="keyword">new</span> LinkedList&lt;WebApplicationInitializer&gt;();</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (webAppInitializerClasses != <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="keyword">for</span> (Class&lt;?&gt; waiClass : webAppInitializerClasses) &#123;</span><br><span class="line"><span class="comment">// Be defensive: Some servlet containers provide us with invalid classes,</span></span><br><span class="line"><span class="comment">// no matter what @HandlesTypes says...</span></span><br><span class="line"><span class="keyword">if</span> (!waiClass.isInterface() &amp;&amp; !Modifier.isAbstract(waiClass.getModifiers()) &amp;&amp;</span><br><span class="line">WebApplicationInitializer<span class="class">.<span class="keyword">class</span>.<span class="title">isAssignableFrom</span>(<span class="title">waiClass</span>)) </span>&#123;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">initializers.add((WebApplicationInitializer) waiClass.newInstance());</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">catch</span> (Throwable ex) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> ServletException(<span class="string">"Failed to instantiate WebApplicationInitializer class"</span>, ex);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (initializers.isEmpty()) &#123;</span><br><span class="line">servletContext.log(<span class="string">"No Spring WebApplicationInitializer types detected on classpath"</span>);</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">servletContext.log(initializers.size() + <span class="string">" Spring WebApplicationInitializers detected on classpath"</span>);</span><br><span class="line">AnnotationAwareOrderComparator.sort(initializers);</span><br><span class="line"><span class="keyword">for</span> (WebApplicationInitializer initializer : initializers) &#123;</span><br><span class="line">initializer.onStartup(servletContext);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>代码最后的循环里是执行WebApplicationInitializer接口的所有实现类onStartup方法</p><p>我们springboot的入口方法，也是继承了抽象方法SpringBootServletInitializer，而SpringBootServletInitializer实现了WebApplicationInitializer接口</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@SpringBootApplication</span></span><br><span class="line"><span class="meta">@EnableEurekaClient</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Application</span> <span class="keyword">extends</span> <span class="title">SpringBootServletInitializer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Application</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        SpringApplication.run(Application<span class="class">.<span class="keyword">class</span>, <span class="title">args</span>)</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> SpringApplicationBuilder <span class="title">configure</span><span class="params">(SpringApplicationBuilder builder)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> builder.sources(Application<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里要重写configure方法，将这个入口类放到source里</p><p>SpringBootServletInitializer的onStartup会调用createRootApplicationContext方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onStartup</span><span class="params">(ServletContext servletContext)</span> <span class="keyword">throws</span> ServletException </span>&#123;</span><br><span class="line"><span class="comment">// Logger initialization is deferred in case a ordered</span></span><br><span class="line"><span class="comment">// LogServletContextInitializer is being used</span></span><br><span class="line"><span class="keyword">this</span>.logger = LogFactory.getLog(getClass());</span><br><span class="line">WebApplicationContext rootAppContext = createRootApplicationContext(</span><br><span class="line">servletContext);</span><br><span class="line"><span class="keyword">if</span> (rootAppContext != <span class="keyword">null</span>) &#123;</span><br><span class="line">servletContext.addListener(<span class="keyword">new</span> ContextLoaderListener(rootAppContext) &#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">contextInitialized</span><span class="params">(ServletContextEvent event)</span> </span>&#123;</span><br><span class="line"><span class="comment">// no-op because the application context is already initialized</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">this</span>.logger.debug(<span class="string">"No ContextLoaderListener registered, as "</span></span><br><span class="line">+ <span class="string">"createRootApplicationContext() did not "</span></span><br><span class="line">+ <span class="string">"return an application context"</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>createRootApplicationContext方法里会调用run方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> WebApplicationContext <span class="title">createRootApplicationContext</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">ServletContext servletContext)</span> </span>&#123;</span><br><span class="line">SpringApplicationBuilder builder = createSpringApplicationBuilder();</span><br><span class="line">builder.main(getClass());</span><br><span class="line">ApplicationContext parent = getExistingRootWebApplicationContext(servletContext);</span><br><span class="line"><span class="keyword">if</span> (parent != <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="keyword">this</span>.logger.info(<span class="string">"Root context already created (using as parent)."</span>);</span><br><span class="line">servletContext.setAttribute(</span><br><span class="line">WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE, <span class="keyword">null</span>);</span><br><span class="line">builder.initializers(<span class="keyword">new</span> ParentContextApplicationContextInitializer(parent));</span><br><span class="line">&#125;</span><br><span class="line">builder.initializers(</span><br><span class="line"><span class="keyword">new</span> ServletContextApplicationContextInitializer(servletContext));</span><br><span class="line">builder.contextClass(AnnotationConfigEmbeddedWebApplicationContext<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">builder = configure(builder);</span><br><span class="line">builder.listeners(<span class="keyword">new</span> WebEnvironmentPropertySourceInitializer(servletContext));</span><br><span class="line">SpringApplication application = builder.build();</span><br><span class="line"><span class="keyword">if</span> (application.getSources().isEmpty() &amp;&amp; AnnotationUtils</span><br><span class="line">.findAnnotation(getClass(), Configuration<span class="class">.<span class="keyword">class</span>) !</span>= <span class="keyword">null</span>) &#123;</span><br><span class="line">application.getSources().add(getClass());</span><br><span class="line">&#125;</span><br><span class="line">Assert.state(!application.getSources().isEmpty(),</span><br><span class="line"><span class="string">"No SpringApplication sources have been defined. Either override the "</span></span><br><span class="line">+ <span class="string">"configure method or add an @Configuration annotation"</span>);</span><br><span class="line"><span class="comment">// Ensure error pages are registered</span></span><br><span class="line"><span class="keyword">if</span> (<span class="keyword">this</span>.registerErrorPageFilter) &#123;</span><br><span class="line">application.getSources().add(ErrorPageFilterConfiguration<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> run(application);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>run方法里最后就是调用application.run，同原来的入口执行方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> WebApplicationContext <span class="title">run</span><span class="params">(SpringApplication application)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> (WebApplicationContext) application.run();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;springboot项目通过外置tomcat部署，不需要web.xml的原因分析：&lt;/p&gt;&lt;p&gt;首先可以看到在spring-web.jar的META-INF有一个配置文件&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200604155440.png&quot; alt=&quot;spi配置文件&quot;&gt;&lt;/p&gt;&lt;p&gt;这里是通过SPI机制，执行了ServletContainerInitializer接口的实现类SpringServletContainerInitializer中的onStartup方法&lt;/p&gt;
    
    </summary>
    
    
      <category term="java" scheme="https://www.xiapf.com/categories/java/"/>
    
    
      <category term="java" scheme="https://www.xiapf.com/tags/java/"/>
    
      <category term="springBoot" scheme="https://www.xiapf.com/tags/springBoot/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络——构建残差网络</title>
    <link href="https://www.xiapf.com/blogs/convNet2/"/>
    <id>https://www.xiapf.com/blogs/convNet2/</id>
    <published>2020-06-03T04:28:01.000Z</published>
    <updated>2020-06-08T03:03:51.209Z</updated>
    
    <content type="html"><![CDATA[<h2 id="不同数据量下的迁移学习"><a href="#不同数据量下的迁移学习" class="headerlink" title="不同数据量下的迁移学习"></a>不同数据量下的迁移学习</h2><p>可以使用开源的别人的训练好的神经网络，将其网络中学习的知识迁移到自己的网络中，通常开源的网络配置在prototxt中</p><p>（1）少量的标定数据</p><p>将softmax输出类别层之前的神经元进行冻结，即直接使用别人训练好的参数，在softmax层重新用自己的标定数据进行训练。</p><p>（2）充足的标定数据</p><a id="more"></a><p>从softmax层向前选择一些层数进行训练，其余层进行冻结直接使用别人训练好的参数。</p><p>（3）巨多的标定数据</p><p>将别人的网络参数作为初始化，使用自己的标定数据重新训练网络。</p><p>总结，当标定数据越多，需要冻结的网络层数越少，训练的网络层数越多。</p><h2 id="数据扩充的方法"><a href="#数据扩充的方法" class="headerlink" title="数据扩充的方法"></a>数据扩充的方法</h2><p>计算机视觉需要大量数据，可以通过以下方式在原有数据基础上进行扩充</p><p>（1）垂直镜像翻转</p><p>将图像镜像翻转</p><p>（2）随机图像修剪</p><p>在原有图像中剪裁一小部分图像，但不影响数据的特征及标签情况</p><p>还有旋转图像，扭曲图像等措施</p><p>（3）颜色变换</p><p>将图像的三通道颜色按照一定概率分布进行数值加减得到新的图像</p><h2 id="Incepton-Net"><a href="#Incepton-Net" class="headerlink" title="Incepton Net"></a>Incepton Net</h2><h3 id="1x1卷积的作用"><a href="#1x1卷积的作用" class="headerlink" title="1x1卷积的作用"></a>1x1卷积的作用</h3><p>可以减少输入的通道数即nC，降低计算的成本</p><p>注：池化层是减少输入的长宽，即nH,nW</p><h3 id="谷歌的Incepton-Net"><a href="#谷歌的Incepton-Net" class="headerlink" title="谷歌的Incepton Net"></a>谷歌的Incepton Net</h3><p>不需要手工设置网络的结构，直接将过滤器类型或者池化层等加入到网络中，让网络自动学习，将所有的输出堆叠在一起，将所有incepetion模型连接在一起就是Incepton Net，该网络中间层也能输出图像特征和预测结果，防止了过拟合。</p><p>注：如果一个模型中有一种情况输出的通道数过大，可以使用1x1卷积来减小通道数</p><p>以下是一个inception模块：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200603123833.png" alt></p><h2 id="构建残差块"><a href="#构建残差块" class="headerlink" title="构建残差块"></a>构建残差块</h2><h3 id="恒等块"><a href="#恒等块" class="headerlink" title="恒等块"></a>恒等块</h3><p>输入的激活值a[l]和输出的激活值a[l+1]维度相同</p><p>网络结构：直接从输入点加一个shortcut捷径到输出</p><p>（1）使用keras.layers自带的Conv2D,BatchNormalization,Activation,MaxPooling2D构造正常网络结果，指明过滤器个数和维度，以及参数初始化方法（均匀正态分布），并进行命名。</p><p>（2）将捷径输入的x和正常网络得出的x使用Add函数进行合并，并通过激活函数得到最终输出</p><p>注：BatchNormalization的作用是对隐藏层进行归一化，使得每层都能尽量单独学习，限制了前层参数变化对后层参数的数值分布程度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#恒等块 连接前后维度相同</span></span><br><span class="line"><span class="comment">#stage - 整数，根据每层的位置来命名每一层，与block参数一起使用。</span></span><br><span class="line"><span class="comment">#block - 字符串，据每层的位置来命名每一层，与stage参数一起使用。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">identity_block</span><span class="params">(X,filters,f,stage,block)</span>:</span></span><br><span class="line"><span class="comment">#结构：conv-bn-relu,conv-bn-relu,conv-bn    relu</span></span><br><span class="line">conv_name_base=<span class="string">"res"</span>+str(stage)+block+<span class="string">"_branch"</span></span><br><span class="line">bn_name_base=<span class="string">"bn"</span>+str(stage)+block+<span class="string">"_branch"</span></span><br><span class="line"></span><br><span class="line">F1,F2,F3=filters</span><br><span class="line">X_input=X</span><br><span class="line"><span class="comment">#均匀正态分布初始化</span></span><br><span class="line">X=Conv2D(filters=F1,kernel_size=(<span class="number">1</span>,<span class="number">1</span>),strides=(<span class="number">1</span>,<span class="number">1</span>),padding=<span class="string">'valid'</span>,kernel_initializer=glorot_uniform(seed=<span class="number">0</span>),name=conv_name_base+<span class="string">'2a'</span>)(X)</span><br><span class="line">X=BatchNormalization(axis=<span class="number">3</span>,name=bn_name_base+<span class="string">'2a'</span>)(X)</span><br><span class="line">X=Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line">X=Conv2D(filters=F2,kernel_size=(f,f),strides=(<span class="number">1</span>,<span class="number">1</span>),padding=<span class="string">'same'</span>,kernel_initializer=glorot_uniform(seed=<span class="number">0</span>),name=conv_name_base+<span class="string">'2b'</span>)(X)</span><br><span class="line">X=BatchNormalization(axis=<span class="number">3</span>,name=bn_name_base+<span class="string">'2b'</span>)(X)</span><br><span class="line">X=Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line">X=Conv2D(filters=F3,kernel_size=(<span class="number">1</span>,<span class="number">1</span>),strides=(<span class="number">1</span>,<span class="number">1</span>),padding=<span class="string">'valid'</span>,kernel_initializer=glorot_uniform(seed=<span class="number">0</span>),name=conv_name_base+<span class="string">'2c'</span>)(X)</span><br><span class="line">X=BatchNormalization(axis=<span class="number">3</span>,name=bn_name_base+<span class="string">'2c'</span>)(X)</span><br><span class="line"></span><br><span class="line">X=Add()([X_input,X])</span><br><span class="line">X=Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><h3 id="卷积块"><a href="#卷积块" class="headerlink" title="卷积块"></a>卷积块</h3><p>输入的激活值a[l]和输出的激活值a[l+1]维度不同</p><p>网络结构：从输入点出发的一个shortcut捷径需要增加卷积层，如何再到输出</p><p>（1）使用keras.layers自带的Conv2D,BatchNormalization,Activation,MaxPooling2D构造正常网络结果，指明过滤器个数和维度，以及参数初始化方法（均匀正态分布），并进行命名。</p><p>（2）捷径输入x使用Conv2D,BatchNormalization,经过一个卷积层得到捷径的输出x</p><p>（3）将捷径输出的x和正常网络得出的x使用Add函数进行合并，并通过激活函数得到最终输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#卷积块 连接前后维度不同,需要在捷径里加上conv层</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convolutional_block</span><span class="params">(X,filters,f,stage,block,s=<span class="number">2</span>)</span>:</span></span><br><span class="line"><span class="comment">#结构：conv-bn-relu,conv-bn-relu,conv-bn    relu</span></span><br><span class="line">conv_name_base=<span class="string">"res"</span>+str(stage)+block+<span class="string">"_branch"</span></span><br><span class="line">bn_name_base=<span class="string">"bn"</span>+str(stage)+block+<span class="string">"_branch"</span></span><br><span class="line"></span><br><span class="line">F1,F2,F3=filters</span><br><span class="line">X_input=X</span><br><span class="line"><span class="comment">#均匀正态分布初始化</span></span><br><span class="line">X=Conv2D(filters=F1,kernel_size=(<span class="number">1</span>,<span class="number">1</span>),strides=(s,s),padding=<span class="string">'valid'</span>,kernel_initializer=glorot_uniform(seed=<span class="number">0</span>),name=conv_name_base+<span class="string">'2a'</span>)(X)</span><br><span class="line">X=BatchNormalization(axis=<span class="number">3</span>,name=bn_name_base+<span class="string">'2a'</span>)(X)</span><br><span class="line">X=Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line">X=Conv2D(filters=F2,kernel_size=(f,f),strides=(<span class="number">1</span>,<span class="number">1</span>),padding=<span class="string">'same'</span>,kernel_initializer=glorot_uniform(seed=<span class="number">0</span>),name=conv_name_base+<span class="string">'2b'</span>)(X)</span><br><span class="line">X=BatchNormalization(axis=<span class="number">3</span>,name=bn_name_base+<span class="string">'2b'</span>)(X)</span><br><span class="line">X=Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line">X=Conv2D(filters=F3,kernel_size=(<span class="number">1</span>,<span class="number">1</span>),strides=(<span class="number">1</span>,<span class="number">1</span>),padding=<span class="string">'valid'</span>,kernel_initializer=glorot_uniform(seed=<span class="number">0</span>),name=conv_name_base+<span class="string">'2c'</span>)(X)</span><br><span class="line">X=BatchNormalization(axis=<span class="number">3</span>,name=bn_name_base+<span class="string">'2c'</span>)(X)</span><br><span class="line"></span><br><span class="line"><span class="comment">#捷径</span></span><br><span class="line">X_input=Conv2D(filters=F3,kernel_size=(<span class="number">1</span>,<span class="number">1</span>),strides=(s,s),padding=<span class="string">'valid'</span>,kernel_initializer=glorot_uniform(seed=<span class="number">0</span>),name=conv_name_base+<span class="string">'1'</span>)(X_input)</span><br><span class="line">X_input=BatchNormalization(axis=<span class="number">3</span>,name=bn_name_base+<span class="string">'1'</span>)(X_input)</span><br><span class="line"></span><br><span class="line">X=Add()([X_input,X])</span><br><span class="line">X=Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><h2 id="构建残差网络"><a href="#构建残差网络" class="headerlink" title="构建残差网络"></a>构建残差网络</h2><h3 id="将残差块一个个连接起来即为残差网络"><a href="#将残差块一个个连接起来即为残差网络" class="headerlink" title="将残差块一个个连接起来即为残差网络"></a>将残差块一个个连接起来即为残差网络</h3><p>按照如图的网络结构搭建残差网络</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200603120554.png" alt></p><p>（1）先定义一个tensor的占位符，并根据输入图像的大小定义维度</p><p>（2）使用kears.layers自带的ZeroPadding2D对图像进行填充</p><p>（3）网络第一个模块结构为conv-bn-maxpoool-relu</p><p>（4）后面的模块是使用卷积块和恒等块进行组合</p><p>（5）最终的到的结果通过平均池化，并进行拉伸后，将图像向量传入全连接层</p><p>（6）根据输入和输出，得到ResNet50的模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#搭建模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ResNet50</span><span class="params">(input_shape=<span class="params">(<span class="number">64</span>,<span class="number">64</span>,<span class="number">3</span>)</span>,classes=<span class="number">6</span>)</span>:</span></span><br><span class="line"><span class="comment">#定义一个tensor的占位符，并制定维度</span></span><br><span class="line">X_input=Input(input_shape)</span><br><span class="line">X=ZeroPadding2D((<span class="number">3</span>,<span class="number">3</span>))(X_input)</span><br><span class="line"></span><br><span class="line">X=Conv2D(<span class="number">64</span>,kernel_size=(<span class="number">7</span>,<span class="number">7</span>),strides=(<span class="number">2</span>,<span class="number">2</span>),name=<span class="string">'conv1'</span>,kernel_initializer=glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">X=BatchNormalization(axis=<span class="number">3</span>,name=<span class="string">'bn_conv1'</span>)(X)</span><br><span class="line">X=Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line">X=MaxPooling2D((<span class="number">3</span>,<span class="number">3</span>),strides=(<span class="number">2</span>,<span class="number">2</span>),name=<span class="string">'max_pool'</span>)(X)</span><br><span class="line"></span><br><span class="line">filters=[<span class="number">64</span>,<span class="number">64</span>,<span class="number">256</span>]</span><br><span class="line">f=<span class="number">3</span></span><br><span class="line">stage=<span class="number">2</span></span><br><span class="line">X=convolutional_block(X,filters,f,stage,block=<span class="string">'a'</span>)</span><br><span class="line">X=identity_block(X,filters,f,stage,block=<span class="string">'b'</span>)</span><br><span class="line">X=identity_block(X,filters,f,stage,block=<span class="string">'c'</span>)</span><br><span class="line"></span><br><span class="line">filters=[<span class="number">128</span>,<span class="number">128</span>,<span class="number">512</span>]</span><br><span class="line">f=<span class="number">3</span></span><br><span class="line">stage=<span class="number">3</span></span><br><span class="line">X=convolutional_block(X,filters,f,stage,block=<span class="string">'a'</span>)</span><br><span class="line">X=identity_block(X,filters,f,stage,block=<span class="string">'b'</span>)</span><br><span class="line">X=identity_block(X,filters,f,stage,block=<span class="string">'c'</span>)</span><br><span class="line">X=identity_block(X,filters,f,stage,block=<span class="string">'d'</span>)</span><br><span class="line"></span><br><span class="line">filters=[<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>]</span><br><span class="line">f=<span class="number">3</span></span><br><span class="line">stage=<span class="number">4</span></span><br><span class="line">X=convolutional_block(X,filters,f,stage,block=<span class="string">'a'</span>)</span><br><span class="line">X=identity_block(X,filters,f,stage,block=<span class="string">'b'</span>)</span><br><span class="line">X=identity_block(X,filters,f,stage,block=<span class="string">'c'</span>)</span><br><span class="line">X=identity_block(X,filters,f,stage,block=<span class="string">'d'</span>)</span><br><span class="line">X=identity_block(X,filters,f,stage,block=<span class="string">'e'</span>)</span><br><span class="line">X=identity_block(X,filters,f,stage,block=<span class="string">'f'</span>)</span><br><span class="line"></span><br><span class="line">filters=[<span class="number">512</span>,<span class="number">512</span>,<span class="number">2048</span>]</span><br><span class="line">f=<span class="number">3</span></span><br><span class="line">stage=<span class="number">5</span></span><br><span class="line">X=convolutional_block(X,filters,f,stage,block=<span class="string">'a'</span>)</span><br><span class="line">X=identity_block(X,filters,f,stage,block=<span class="string">'b'</span>)</span><br><span class="line">X=identity_block(X,filters,f,stage,block=<span class="string">'c'</span>)</span><br><span class="line"></span><br><span class="line">X=AveragePooling2D((<span class="number">2</span>,<span class="number">2</span>),padding=<span class="string">'same'</span>,name=<span class="string">'avg_pool'</span>)(X)</span><br><span class="line">X=Flatten()(X)</span><br><span class="line">X=Dense(classes,activation=<span class="string">'softmax'</span>,name=<span class="string">'fc'</span>+str(classes),kernel_initializer=glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line"></span><br><span class="line">model=Model(inputs=X_input,outputs=X,name=<span class="string">'ResNet50'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><h3 id="残差网络作用"><a href="#残差网络作用" class="headerlink" title="残差网络作用"></a>残差网络作用</h3><p>当网络很深时，训练集的误差反而会增大，误差先减小后增大。</p><p>（1）使用残差网络能很好的学习恒等式，并不降低效率</p><p>（2）能将输入的复杂的非线性函数传递到更深层，轻松学习输入输出之间的映射</p><p>最终当网络很深，训练集误差随着层数增加而减少。</p><p>a[l+2]=g(z[l+2]+a[l])=g(w[l+2] * a[l+1]+b[l+2]+ws * a[l])，当输入输出维度不同，可以在a[l]前面加上ws进行维度调整</p><p>其中a[l]是捷径输入，z[l+2]是正常网络输出，a[l+2]是最终输出</p><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>（1）在手势图片上使用残差网络进行模型训练</p><p>输入每个图片的维度和输出标签数量，进行模型训练。</p><p>选择模型中采用的算法和损失函数。</p><p>最后使用fit将数据输入模型进行训练。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#编译模型</span></span><br><span class="line">mymodel=ResNet50(input_shape=(<span class="number">64</span>,<span class="number">64</span>,<span class="number">3</span>),classes=<span class="number">6</span>)</span><br><span class="line">mymodel.compile(optimizer=<span class="string">'adam'</span>,loss=<span class="string">'categorical_crossentropy'</span>,metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"><span class="comment">#标签转换为独热矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_one_hot</span><span class="params">(Y,C)</span>:</span></span><br><span class="line">Y=np.eye(C)[Y.reshape(<span class="number">-1</span>).T]</span><br><span class="line"><span class="keyword">return</span> Y</span><br><span class="line"><span class="comment">#导入数据</span></span><br><span class="line">train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes=ru.load_dataset()</span><br><span class="line"><span class="comment">#归一化</span></span><br><span class="line">train_x=train_set_x_orig/<span class="number">255</span></span><br><span class="line">test_x=test_set_x_orig/<span class="number">255</span></span><br><span class="line"></span><br><span class="line">train_y=convert_one_hot(train_set_y_orig,<span class="number">6</span>)</span><br><span class="line">test_y=convert_one_hot(test_set_y_orig,<span class="number">6</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#输入数据，训练模型</span></span><br><span class="line">mymodel.fit(train_x,train_y,epochs=<span class="number">3</span>,batch_size=<span class="number">64</span>)</span><br></pre></td></tr></table></figure><p>（2）保存模型</p><p>使用save保存模型：将模型的结构，参数，导出的模型可以直接进行模型预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mymodel.save(<span class="string">"my_resnet50.h5"</span>)</span><br></pre></td></tr></table></figure><p>（3）导入保存的模型，得到模型的准确率</p><p>使用keras.models下的load_model方法将之前训练好的模型导入，并计算准确度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mymodel=load_model(<span class="string">'my_resnet50.h5'</span>)</span><br><span class="line">p=mymodel.evaluate(test_x,test_y)</span><br><span class="line">print(<span class="string">"误差率："</span>,p[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">"准确率："</span>,p[<span class="number">1</span>])mymodel.save(<span class="string">"my_resnet50.h5"</span>)</span><br></pre></td></tr></table></figure><p>误差率： 0.7191567063331604<br>准确率： 0.7833333611488342</p><p>由于原算法迭代数仅2次，所以准确率不是特别高。</p><p>（4）利用已经训练好的模型测试</p><p>使用keras.preprocessing下的image导入图像，将图像转换为数组，并进行归一化后输入训练好的网络，最终通过np.argmax得到分类的结果</p><p>注：这里图像需要拓展维度，从(64, 64, 3)拓展为(1, 64, 64, 3)，代码为x=np.expand_dims(x,axis=0)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用自己的图像测试</span></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">image_path=<span class="string">'./fingers/2.jpg'</span></span><br><span class="line">x=image.load_img(image_path,target_size=(<span class="number">64</span>,<span class="number">64</span>))</span><br><span class="line"></span><br><span class="line">x=image.img_to_array(x)</span><br><span class="line"><span class="comment">#拓展维度</span></span><br><span class="line">x=np.expand_dims(x,axis=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># x=preprocess_input(x)</span></span><br><span class="line">x=x/<span class="number">255</span></span><br><span class="line"></span><br><span class="line">res=np.argmax(mymodel.predict(x))</span><br><span class="line">print(res)</span><br><span class="line">print(mymodel.predict(x))</span><br><span class="line"></span><br><span class="line">test_image=Image.open(image_path).convert(<span class="string">'RGB'</span>).resize((<span class="number">64</span>,<span class="number">64</span>))</span><br><span class="line">plt.imshow(test_image)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>输入图像</p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200602210826.jpg" style="zoom:25%;"><p>预测结果为：</p><p>2<br>[[0.1477255  0.15454543 0.18347658 0.16707171 0.18196131 0.16521947]]</p><p>输入的手势是2，预测也是2，说明预测正确</p><p>说明：由于训练集中的手势图片不是自己拍摄的，所以预测的效果不一定很准确。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;不同数据量下的迁移学习&quot;&gt;&lt;a href=&quot;#不同数据量下的迁移学习&quot; class=&quot;headerlink&quot; title=&quot;不同数据量下的迁移学习&quot;&gt;&lt;/a&gt;不同数据量下的迁移学习&lt;/h2&gt;&lt;p&gt;可以使用开源的别人的训练好的神经网络，将其网络中学习的知识迁移到自己的网络中，通常开源的网络配置在prototxt中&lt;/p&gt;&lt;p&gt;（1）少量的标定数据&lt;/p&gt;&lt;p&gt;将softmax输出类别层之前的神经元进行冻结，即直接使用别人训练好的参数，在softmax层重新用自己的标定数据进行训练。&lt;/p&gt;&lt;p&gt;（2）充足的标定数据&lt;/p&gt;
    
    </summary>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="卷积神经网络" scheme="https://www.xiapf.com/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络——识别手势图片</title>
    <link href="https://www.xiapf.com/blogs/convNet1/"/>
    <id>https://www.xiapf.com/blogs/convNet1/</id>
    <published>2020-05-28T07:47:32.000Z</published>
    <updated>2020-06-03T04:28:36.213Z</updated>
    
    <content type="html"><![CDATA[<h2 id="算法简介"><a href="#算法简介" class="headerlink" title="算法简介"></a>算法简介</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>使用普通的神经网络时，当输入的图片维度大，假设为1000 * 1000 * 3，第一层的神经元为1000个，则权重参数达到三亿个（1000 * 1000 * 3 * 1000），这时候网络会变得很大，有巨大的参数，因此需要使用卷积神经网络。</p><p>卷积神经网络通过参数共享和稀疏连接的方式使得参数变得很少，可以用很少的训练集训练，避免网络过拟合。</p><a id="more"></a><p>参数共享：图片的不同区域都可以用相同的过滤器计算</p><p>稀疏连接：每个输出的像素仅和部分输入像素相关，其他像素不影响相应输出</p><p>假设为1000 * 1000 * 3，过滤器为4 * 4 * 3，加上偏置项，共10个过滤器，则需要的参数为（4 * 4 * 3 +1） * 10=490个，原小于原使用普通的神经网络</p><h3 id="网络构成"><a href="#网络构成" class="headerlink" title="网络构成"></a>网络构成</h3><p>卷积神经网络通常有三层（也有只有卷积层的网络，具体网络的配置，如层数，超参设置可以参照文献中其他人的方法），这里将卷积层+池化层作为神经网络的一层。</p><p>（1）卷积层</p><p>作用：使用过滤器对输入的图像进行卷积操作（按照步长，在图像相应区域做乘积并求和），功能类似于求Z=W * X +b</p><p>经过卷积层的图像使用非线性函数进行计算得到A=relu(Z)</p><p>padding的作用：由于卷积之后图像会变小，可以通过事先在输入图像外部进行填充0。有两种填充方式：valid padding即p=0，不填充；same padding，即填充完成后，输入输出的维度相同，即n+2p-f+1=n，由此德驰p=(f-1)/2</p><p>附（公式）：</p><p>当输入的维度为n * n * n_c，过滤器为f * f，步长为s，填充为p，则输出的数据维度为：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200528143258.png" alt></p><p>（2）池化层</p><p>作用：缩小模型，加快计算速度</p><p>分类：最大池化，取图像对应区域内的最大值；平均池化，取图像区域内的平均值</p><p>接收经过卷积层的输出A，通过池化层，选择池化方式最终得到P，作为一层网络的输出。</p><p>注：池化层中参数不在反向传播中更新，即池化层没有要学习的参数</p><p>常用参数：f=2，s=2（长宽缩小一半）</p><p>（3）全连接层</p><p>作用：全连接层类似于单神经网络，假设通过两层网络（卷积+池化）得到图像将其拉伸成一维向量，其维度为n，全连接层的维度为m，则中间的权重维度为（m,n），可见卷积神经网络中参数主要集中在全连接层。</p><p>经过全连接层的图像会最终输出一个一维向量作为结果，当最后接softmax函数时，则最终输出一个结果。</p><h3 id="简单网络结构示意图"><a href="#简单网络结构示意图" class="headerlink" title="简单网络结构示意图"></a>简单网络结构示意图</h3><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200527180339.png" alt></p><p>如图所示，是一个简单的二层卷积神经网络，输入的图像为32 * 32 * 3</p><p>（1）卷积层+池化层</p><p>第一层网络：卷积层设置过滤器为5 * 5，数量为6个，步长s=1，p=0，则输出28 * 28 * 6的图像（套用公式即可得到输出的维度），第三个维度6代表6个过滤器</p><p>池化层选择最大池化，f=2，s=2（长宽缩小一半），则输出14 * 14 *6的图像</p><p>第二层网络同理，最终得到5 * 5 * 16的图像，将其拉伸为400 * 1的一维向量，输入全连接层中</p><p>（2）全连接层</p><p>可以设置多个全连接层，这里设置了两层，将拉伸了的图像输入全连接层中，最终得到的数据输入softmax函数中得到最终输出，数字最大的数字代表输入图像代表的类别</p><h3 id="网络中各数据的维度"><a href="#网络中各数据的维度" class="headerlink" title="网络中各数据的维度"></a>网络中各数据的维度</h3><p>填充：p</p><p>步长：s</p><p>每个过滤器：f * f * n_c_pre （这里的n_c_pre是指过滤器维度需要和输入，即上一层的输入保持一致）</p><p>本层过滤器数量：n_c</p><p>权重W：f * f *n_c_pre * n_c （n_c是指本层过滤器数量，权重的维度等于每个过滤器维度 * 过滤器数量）</p><p>偏置量b：1 * 1 * 1 * n_c</p><p>激活值a：n_h * n_w * n_c（本层的输出）</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200527180726.png" alt></p><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>（1）padding 填充数据</p><p>使用numpy中的pad，填充过滤器的长宽，填充大小为pad，当不指定constant_value时，自动填充0</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.将输入的图像，按照指定的padding进行填充</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zero_pad</span><span class="params">(X,pad)</span>:</span></span><br><span class="line">  X=np.pad(X,((<span class="number">0</span>,<span class="number">0</span>),(pad,pad),(pad,pad),(<span class="number">0</span>,<span class="number">0</span>)),<span class="string">'constant'</span>) <span class="comment">#不指定constant_value,默认填充0</span></span><br><span class="line">  <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><p>（2）前向传播</p><p>1.卷积层</p><p>1° 计算需要输出的图像Z的维度，并填充输入的图像</p><p>2° 对每个输入的图像，计算需要卷积的区域：</p><p>h_start=h * stride<br>h_end=h_start+f_h<br>w_start=w * stride<br>w_end=w_start+f_w</p><p>按照卷积的区域计算每个样本的卷积值</p><p>3° 保存卷积层的上一层输入，当前层的权重和超参（包含s和pad），保存为cache</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#卷积层的前向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_forward</span><span class="params">(A_pre,W,b,hparameters)</span>:</span></span><br><span class="line">  pad=hparameters[<span class="string">'pad'</span>]</span><br><span class="line">  stride=hparameters[<span class="string">'stride'</span>]</span><br><span class="line">  m,n_h_pre,n_w_pre,n_c_pre=A_pre.shape</span><br><span class="line">  f_h,f_w,n_c_pre,n_c=W.shape</span><br><span class="line"></span><br><span class="line">  <span class="comment">#将每个输入样本进行填充</span></span><br><span class="line">  A_pre_single=zero_pad(A_pre,pad)</span><br><span class="line"></span><br><span class="line">  <span class="comment">#计算输出的维度</span></span><br><span class="line">  n_h=int(((n_h_pre+<span class="number">2</span>*pad-f_h)/stride)+<span class="number">1</span>)</span><br><span class="line">  n_w=int(((n_w_pre+<span class="number">2</span>*pad-f_w)/stride)+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  Z=np.zeros((m,n_h,n_w,n_c))</span><br><span class="line"></span><br><span class="line">  <span class="comment">#对填充的样本一一用过滤器处理得到结果</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">    a_pre_single=A_pre_single[i]</span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> range(n_h):</span><br><span class="line">      <span class="keyword">for</span> w <span class="keyword">in</span> range(n_w):</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> range(n_c):</span><br><span class="line">          h_start=h*stride</span><br><span class="line">          h_end=h_start+f_h</span><br><span class="line">          w_start=w*stride</span><br><span class="line">          w_end=w_start+f_w</span><br><span class="line">          <span class="comment">#将对应通道的矩阵进行一次卷积</span></span><br><span class="line">          Z[i,h,w,c]=conv_single_step(a_pre_single[h_start:h_end,w_start:w_end,:],W[:,:,:,c],b[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,c])</span><br><span class="line">          <span class="comment">#激活值可以在这里计算</span></span><br><span class="line">          <span class="comment">#....</span></span><br><span class="line"></span><br><span class="line">    cache=(A_pre,W,b,hparameters)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> Z,cache</span><br></pre></td></tr></table></figure><p>2.池化层</p><p>接收卷积层的输入，按照卷积方式，在指定区域进行取最大值或者取平均值，最终得到P输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#池化层前向传播 A_pre是卷积之后的值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pool_forward</span><span class="params">(A_pre,hparameters,pool=<span class="string">'max'</span>,pad=<span class="number">0</span>)</span>:</span></span><br><span class="line">  m,n_h_pre,n_w_pre,n_c_pre=A_pre.shape</span><br><span class="line"></span><br><span class="line">  f=hparameters[<span class="string">'f'</span>]</span><br><span class="line">  stride=hparameters[<span class="string">'stride'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#计算输出的维度</span></span><br><span class="line">  n_h=int(((n_h_pre+<span class="number">2</span>*pad-f)/stride)+<span class="number">1</span>)</span><br><span class="line">  n_w=int(((n_w_pre+<span class="number">2</span>*pad-f)/stride)+<span class="number">1</span>)</span><br><span class="line">  n_c=n_c_pre</span><br><span class="line"></span><br><span class="line">  Z_pool=np.zeros((m,n_h,n_w,n_c))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> range(n_h):</span><br><span class="line">      <span class="keyword">for</span> w <span class="keyword">in</span> range(n_w):</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> range(n_c):</span><br><span class="line">          h_start=h*stride</span><br><span class="line">          h_end=h_start+f</span><br><span class="line">          w_start=w*stride</span><br><span class="line">          w_end=w_start+f</span><br><span class="line">            <span class="comment">#指定第i个样本和其维度</span></span><br><span class="line">          a_single=A_pre[i,h_start:h_end,w_start:w_end,c]</span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span>(pool==<span class="string">"max"</span>):</span><br><span class="line">            Z_pool[i,h,w,c]=np.max(a_single)</span><br><span class="line">          <span class="keyword">elif</span>(pool==<span class="string">"average"</span>):</span><br><span class="line">            Z_pool[i,h,w,c]=np.mean(a_single)</span><br><span class="line">  cache=(A_pre,hparameters)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> Z_pool,cache</span><br></pre></td></tr></table></figure><p>（3）反向传播（了解）</p><p>1.卷积层</p><p>利用公式dA=W * DZ，dW= 1/m *(A_pre * dZ)，db=dZ</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#卷积层反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_backward</span><span class="params">(dZ,cache)</span>:</span></span><br><span class="line">  m,n_h,n_w,n_c=dZ.shape</span><br><span class="line">  A_pre,W,b,hparameters=cache</span><br><span class="line">  m,n_h_pre,n_w_pre,n_c_pre=A_pre.shape</span><br><span class="line">  f,f,n_c_pre,n_c=W.shape</span><br><span class="line"></span><br><span class="line">  pad=hparameters[<span class="string">'pad'</span>]</span><br><span class="line">  stride=hparameters[<span class="string">'stride'</span>]</span><br><span class="line"></span><br><span class="line">  dA_pre=np.zeros((m,n_h_pre,n_w_pre,n_c_pre))</span><br><span class="line">  dW=np.zeros((f,f,n_c_pre,n_c))</span><br><span class="line">  db=np.zeros((<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,n_c))</span><br><span class="line"></span><br><span class="line">  <span class="comment">#填充A</span></span><br><span class="line">  A_pre_pad=zero_pad(A_pre,pad)</span><br><span class="line">  dA_pre_pad=zero_pad(dA_pre,pad)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">    a_pre_single=A_pre_pad[i]</span><br><span class="line">    da_pre_single=dA_pre_pad[i]</span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> range(n_h):</span><br><span class="line">      <span class="keyword">for</span> w <span class="keyword">in</span> range(n_w):</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> range(n_c):</span><br><span class="line">          h_start=h*stride</span><br><span class="line">          h_end=h_start+f</span><br><span class="line">          w_start=w*stride</span><br><span class="line">          w_end=w_start+f</span><br><span class="line"></span><br><span class="line">          a_single=a_pre_single[h_start:h_end,w_start:w_end,:]</span><br><span class="line"></span><br><span class="line">          da_pre_single[h_start:h_end,w_start:w_end,:]+=W[:,:,:,c]*dZ[i,h,w,c]</span><br><span class="line"></span><br><span class="line">          dW[:,:,:,c]+=a_single*dZ[i,h,w,c]</span><br><span class="line">          db[:,:,:,c]+=dZ[i,h,w,c]</span><br><span class="line">    dA_pre[i,:,:,:]=da_pre_single[pad:-pad,pad:-pad,:]</span><br><span class="line">  grads=(dA_pre,dW,db)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure><p>2.池化层</p><p>最大池化使用掩码矩阵标记最大值，得到在最大值作用下的梯度，平均池化是根据图像维度计算出平均值，得到在平均值作用下的梯度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#池化层反向传播</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#最大池化</span></span><br><span class="line"><span class="comment">#创建矩阵保存最大值所在的位置，即创建掩码矩阵</span></span><br><span class="line"><span class="comment">#为了反向传播的时候，用值定位到卷积层中的位置</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_mask_from_window</span><span class="params">(x)</span>:</span></span><br><span class="line">  mask=(x==np.max(x))</span><br><span class="line">  <span class="keyword">return</span> mask</span><br><span class="line"></span><br><span class="line"><span class="comment">#平均池化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distribute_value</span><span class="params">(dz,shape)</span>:</span></span><br><span class="line">  n_h,n_w=shape</span><br><span class="line">  average=dz/(n_h*n_w)</span><br><span class="line">  a=np.ones((shape))*average</span><br><span class="line">  <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line"><span class="comment">#反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pool_backward</span><span class="params">(dA,cache,pool=<span class="string">"max"</span>)</span>:</span></span><br><span class="line">  A_pre,hparameters=cache</span><br><span class="line">  m,n_h,n_w,n_c=dA.shape</span><br><span class="line"></span><br><span class="line">  dA_pre=np.zeros_like(A_pre)</span><br><span class="line"></span><br><span class="line">  f=hparameters[<span class="string">'f'</span>]</span><br><span class="line">  stride=hparameters[<span class="string">'stride'</span>]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> range(n_h):</span><br><span class="line">      <span class="keyword">for</span> w <span class="keyword">in</span> range(n_w):</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> range(n_c):</span><br><span class="line">          h_start=h*stride</span><br><span class="line">          h_end=h_start+f</span><br><span class="line">          w_start=w*stride</span><br><span class="line">          w_end=w_start+f</span><br><span class="line">          <span class="comment">#指定第i个样本和其维度</span></span><br><span class="line">          a_single=A_pre[i,h_start:h_end,w_start:w_end,c]</span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span>(pool==<span class="string">"max"</span>):</span><br><span class="line">            mask=create_mask_from_window(a_single)</span><br><span class="line">            dA_pre[i,h_start:h_end,w_start:w_end,c]+=np.multiply(mask,dA[i,h,w,c])</span><br><span class="line">            </span><br><span class="line">          <span class="keyword">elif</span>(pool==<span class="string">"average"</span>):</span><br><span class="line">            shape=(f,f)</span><br><span class="line">            dA_pre[i,h_start:h_end,w_start:w_end,c]+=distribute_value(dA[i,h,w,c],shape)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> dA_pre</span><br></pre></td></tr></table></figure><h2 id="应用实例——使用tensorflow框架"><a href="#应用实例——使用tensorflow框架" class="headerlink" title="应用实例——使用tensorflow框架"></a>应用实例——使用tensorflow框架</h2><p>（0）导入数据</p><p>导入h5文件下的数据集，并进行归一化</p><p>这里输入数据的格式是（m,n_h,n_w,n_c），所以不需要修正大小</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将输入的标签转换为独热矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_one_hot</span><span class="params">(Y,C)</span>:</span></span><br><span class="line"><span class="comment">#Y.reshape(-1)转成一维的</span></span><br><span class="line">one_hot=np.eye(C)[Y.reshape(<span class="number">-1</span>)].T</span><br><span class="line"><span class="keyword">return</span> one_hot</span><br><span class="line">train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes=cu.load_dataset()</span><br><span class="line">train_x=train_set_x_orig/<span class="number">255</span></span><br><span class="line">test_x=test_set_x_orig/<span class="number">255</span></span><br></pre></td></tr></table></figure><p>（1）建立占位符，初始化参数</p><p>因为不确定输入图片的个数，第一位设置为none</p><p>使用tf下的get_variable初始化权重</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.创建占位符</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_placeholders</span><span class="params">(n_h,n_w,n_c,n_y)</span>:</span></span><br><span class="line">  <span class="comment">#m不确定</span></span><br><span class="line">  X=tf.placeholder(tf.float32,[<span class="literal">None</span>,n_h,n_w,n_c])</span><br><span class="line">  Y=tf.placeholder(tf.float32,[<span class="literal">None</span>,n_y])</span><br><span class="line">  <span class="keyword">return</span> X,Y</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.初始化权重</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">()</span>:</span></span><br><span class="line">  n_h1,n_w1,n_c_pre1,n_c1=[<span class="number">4</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">8</span>] <span class="comment">#f*f*n_c_pre*n_c</span></span><br><span class="line">  n_h2,n_w2,n_c_pre2,n_c2=[<span class="number">2</span>,<span class="number">2</span>,<span class="number">8</span>,<span class="number">16</span>]</span><br><span class="line">  tf.set_random_seed(<span class="number">1</span>)</span><br><span class="line">  W1=tf.get_variable(<span class="string">"W1"</span>,[n_h1,n_w1,n_c_pre1,n_c1],initializer=tf.contrib.layers.xavier_initializer(seed=<span class="number">0</span>))</span><br><span class="line">  W2=tf.get_variable(<span class="string">"W2"</span>,[n_h2,n_w2,n_c_pre2,n_c2],initializer=tf.contrib.layers.xavier_initializer(seed=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">  parameters=&#123;<span class="string">"W1"</span>:W1,<span class="string">"W2"</span>:W2&#125;</span><br><span class="line">  <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>（2）前向传播</p><p>使用tf.nn下的conv2d、relu、max_pool实现卷积和池化，其中步长[1,s,s,1]是指指对于输入 (m, n_H_prev, n_W_prev, n_C_prev)而言，每次滑动的步伐，即长宽滑动s步，同理池化中的ksize的窗口大小也是值长宽滑动的大小。</p><p>最终得到结果使用tf.contrib.layer下的flatten进行拉伸，最后调用fully_connected输出全连接层的值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#3.前向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(X,parameters)</span>:</span></span><br><span class="line">  W1=parameters[<span class="string">"W1"</span>]</span><br><span class="line">  W2=parameters[<span class="string">'W2'</span>]</span><br><span class="line">  <span class="comment">#1.1卷积</span></span><br><span class="line">  <span class="comment">#[1,s,s,1]是指对于输入 (m, n_H_prev, n_W_prev, n_C_prev)而言，每次滑动的步伐</span></span><br><span class="line">  <span class="comment">#步长1，指定填充方式</span></span><br><span class="line">  Z1=tf.nn.conv2d(X,W1,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line">  A1=tf.nn.relu(Z1)</span><br><span class="line">  <span class="comment">#1.2池化</span></span><br><span class="line">  <span class="comment">#窗口大小8*8，步长8</span></span><br><span class="line">  P1=tf.nn.max_pool(A1,ksize=[<span class="number">1</span>,<span class="number">8</span>,<span class="number">8</span>,<span class="number">1</span>],strides=[<span class="number">1</span>,<span class="number">8</span>,<span class="number">8</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line"></span><br><span class="line">  Z2=tf.nn.conv2d(P1,W2,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line">  A2=tf.nn.relu(Z2)</span><br><span class="line">  P2=tf.nn.max_pool(A2,ksize=[<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">1</span>],strides=[<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line"></span><br><span class="line">  P3=tf.contrib.layers.flatten(P2)</span><br><span class="line">  <span class="comment">#6是输出的维度</span></span><br><span class="line">  Z3=tf.contrib.layers.fully_connected(P3,<span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> Z3</span><br></pre></td></tr></table></figure><p>（3）计算损失</p><p>调用tf.nn下计算softmax的交叉熵模型，输入的数据维度是（数据集大小，分布）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#4.计算成本</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(Z3,Y)</span>:</span></span><br><span class="line">  <span class="comment">#参数维度[数据集大小，分布]-&gt;logits,labels</span></span><br><span class="line">  cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Z3,labels=Y))</span><br><span class="line">  <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><p>（4）反向传播</p><p>使用tf。train下的adam函数</p><p>optimizer=tf.train.AdamOptimizer(learning_rate).minimize(cost)</p><p>（5）建立会话，导入数据</p><p>1° 重现建立模型而不覆盖tf变量：ops.reset_default_graph()</p><p>2° 构建占位符，初始化参数，前向传播，计算成本，反向传播</p><p>3° 创建会话，对每个批量计算其损失</p><p>4° 保存模型的参数，将得到的结果使用tf下的argmax得到最大值和真实值比较，并计算其准确度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#5.构建模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X_train,Y_train,X_test,Y_test,num_ecpom=<span class="number">50</span>,learning_rate=<span class="number">0.001</span>,mini_batches_size=<span class="number">64</span>)</span>:</span></span><br><span class="line">  <span class="comment">#重新运行模型不覆盖tf变量</span></span><br><span class="line">  ops.reset_default_graph()</span><br><span class="line">  tf.set_random_seed(<span class="number">1</span>)</span><br><span class="line">  seed=<span class="number">3</span></span><br><span class="line">  costs=[]</span><br><span class="line">  m,n_h,n_w,n_c=X_train.shape</span><br><span class="line">  m,n_y=Y_train.shape</span><br><span class="line"></span><br><span class="line">  X,Y=create_placeholders(n_h,n_w,n_c,n_y)</span><br><span class="line"></span><br><span class="line">  parameters=initialize_parameters()</span><br><span class="line"></span><br><span class="line">  Z3=forward_propagation(X,parameters)</span><br><span class="line"></span><br><span class="line">  cost=compute_cost(Z3,Y)</span><br><span class="line"></span><br><span class="line">  optimizer=tf.train.AdamOptimizer(learning_rate).minimize(cost)</span><br><span class="line"></span><br><span class="line">  <span class="comment">#创建会话，导入训练集数据</span></span><br><span class="line">  init=tf.global_variables_initializer()</span><br><span class="line">  session=tf.Session()</span><br><span class="line">  session.run(init)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(num_ecpom):</span><br><span class="line">    <span class="comment">#按照小批量梯度下降</span></span><br><span class="line">    mini_batches=cu.random_mini_batches(X_train,Y_train,mini_batches_size,seed)</span><br><span class="line">    <span class="comment">#每个数据块的大小-&gt;得到在该数据块上的错误率</span></span><br><span class="line">    num_mini_batch=int(m/mini_batches_size)</span><br><span class="line">    mini_batch_cost=<span class="number">0</span></span><br><span class="line">    seed=seed+<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> mini_batch <span class="keyword">in</span> mini_batches:</span><br><span class="line">      mini_batch_x,mini_batch_y=mini_batch</span><br><span class="line">      _,ecpom_cost=session.run([optimizer,cost],feed_dict=&#123;X:X_train,Y:Y_train&#125;)</span><br><span class="line">      mini_batch_cost=mini_batch_cost+ecpom_cost/num_mini_batch</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> i%<span class="number">1</span>==<span class="number">0</span>:</span><br><span class="line">      costs.append(mini_batch_cost)</span><br><span class="line">      <span class="keyword">if</span> i%<span class="number">5</span>==<span class="number">0</span>:</span><br><span class="line">        print(<span class="string">"第"</span>+str(i)+<span class="string">"次，cost:"</span>+str(mini_batch_cost))</span><br><span class="line">  plt.plot(costs)</span><br><span class="line">  plt.title(<span class="string">"learning_rate="</span>+str(learning_rate))</span><br><span class="line">  plt.xlabel(<span class="string">"iteration"</span>)</span><br><span class="line">  plt.ylabel(<span class="string">"cost"</span>)</span><br><span class="line">  plt.show()</span><br><span class="line"></span><br><span class="line">  <span class="comment">#保存得到的参数</span></span><br><span class="line">  parameters=session.run(parameters)</span><br><span class="line">  print(<span class="string">"参数已经保存至session中"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">#计算准确率</span></span><br><span class="line">  <span class="comment">#取一行中最大的那个</span></span><br><span class="line">  <span class="comment">#predict=tf.argmax(Z3,1)</span></span><br><span class="line">  predict_matrix=tf.equal(tf.argmax(Z3,<span class="number">1</span>),tf.argmax(Y,<span class="number">1</span>))</span><br><span class="line">  accuary=tf.reduce_mean(tf.cast(predict_matrix,<span class="string">"float32"</span>))</span><br><span class="line"></span><br><span class="line">  print(<span class="string">"训练集准确率："</span>,session.run(accuary,&#123;X:X_train,Y:Y_train&#125;))</span><br><span class="line">  print(<span class="string">"测试集准确率："</span>,session.run(accuary,&#123;X:X_test,Y:Y_test&#125;))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>结果：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200527180218.jpg" alt></p><p>训练集准确率： 0.8074074<br>测试集准确率： 0.7416667</p><p>实验中选择迭代50次，学习率为0.001，为取得更好的结果，可以将迭代次数增加或者修改学习率</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;算法简介&quot;&gt;&lt;a href=&quot;#算法简介&quot; class=&quot;headerlink&quot; title=&quot;算法简介&quot;&gt;&lt;/a&gt;算法简介&lt;/h2&gt;&lt;h3 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h3&gt;&lt;p&gt;使用普通的神经网络时，当输入的图片维度大，假设为1000 * 1000 * 3，第一层的神经元为1000个，则权重参数达到三亿个（1000 * 1000 * 3 * 1000），这时候网络会变得很大，有巨大的参数，因此需要使用卷积神经网络。&lt;/p&gt;&lt;p&gt;卷积神经网络通过参数共享和稀疏连接的方式使得参数变得很少，可以用很少的训练集训练，避免网络过拟合。&lt;/p&gt;
    
    </summary>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="卷积神经网络" scheme="https://www.xiapf.com/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>图像数据集：将本地图片批量写入h5文件</title>
    <link href="https://www.xiapf.com/blogs/imgToH5/"/>
    <id>https://www.xiapf.com/blogs/imgToH5/</id>
    <published>2020-05-27T14:45:26.000Z</published>
    <updated>2020-05-27T14:48:17.429Z</updated>
    
    <content type="html"><![CDATA[<h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p>当需要用大量图片训练神经网络时，保存为图像格式的图片所占的内存大，因此将本地图片存入h5文件中减少数据集所占内存。</p><p>使用手机拍摄剪刀、石头、布共三组图片，每组图片四张，共12张图像，选择11张作为训练集，剩余的作为测试集。</p><h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><p>（1）读取本地图片并将数据打乱</p><p>1° 读取文件：使用os的listdir函数读取目录下的文件：这里读取并存储的的是本地图像的路径，同时将图片的标签进行存储</p><a id="more"></a><p>2° 合并图像和标签：使用numpy下的hstack将不同标签的图像数据进行堆叠保存，并合并</p><p>3° 打乱数据集：使用numpy.random下的shuffle打乱数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_files</span><span class="params">(file_dir)</span>:</span></span><br><span class="line">stone=[]</span><br><span class="line">label_stone=[]</span><br><span class="line">cut=[]</span><br><span class="line">label_cut=[]</span><br><span class="line">cloth=[]</span><br><span class="line">label_cloth=[]</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取指定目录下的文件</span></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> os.listdir(file_dir+<span class="string">'/stone'</span>):</span><br><span class="line">stone.append(file_dir+<span class="string">'/stone/'</span>+file)</span><br><span class="line">label_stone.append(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> os.listdir(file_dir+<span class="string">'/cut'</span>):</span><br><span class="line">cut.append(file_dir+<span class="string">'/cut/'</span>+file)</span><br><span class="line">label_cut.append(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> os.listdir(file_dir+<span class="string">'/cloth'</span>):</span><br><span class="line">cloth.append(file_dir+<span class="string">'/cloth/'</span>+file)</span><br><span class="line">label_cloth.append(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将数据集合并</span></span><br><span class="line">merge_x=np.hstack((stone,cut,cloth))</span><br><span class="line">merge_y=np.hstack((label_stone,label_cut,label_cloth))</span><br><span class="line">merge=np.array([merge_x,merge_y]).T</span><br><span class="line"></span><br><span class="line"><span class="comment">#打乱数据集</span></span><br><span class="line">np.random.shuffle(merge)</span><br><span class="line"></span><br><span class="line">image_list=merge[:,<span class="number">0</span>]</span><br><span class="line">label_list=merge[:,<span class="number">1</span>]</span><br><span class="line"><span class="comment">#将标签化为整型数据</span></span><br><span class="line">label_list=[int(i) <span class="keyword">for</span> i <span class="keyword">in</span> label_list]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> image_list,label_list</span><br></pre></td></tr></table></figure><p>（2）划分训练集和测试集</p><p>1° 随机初始化：设置四个数组分别保存训练集和测试集数据</p><p>这里需要将数组的类型使用astype转换为int，因为Matplotlib显示图像，如果是0-1区间，值为float，如果是0-255区间，值为int，需要转换，否则无法显示，空白图像，报错：Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).</p><p>这里的图像值在0-255之间，所以需要转换为int.</p><p>2° 按比例划分数据</p><p>输入的图片可能大小不一样，使用PIL中的Image中的resize转换图像到合适的大小</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#3个训练，1个测试</span></span><br><span class="line">num_px=<span class="number">64</span></span><br><span class="line">train_x=np.random.rand(m<span class="number">-1</span>,num_px,num_px,<span class="number">3</span>).astype(<span class="string">'int'</span>)</span><br><span class="line">train_y=np.random.rand(m<span class="number">-1</span>,<span class="number">1</span>).astype(<span class="string">'int'</span>)</span><br><span class="line">test_x=np.random.rand(<span class="number">1</span>,num_px,num_px,<span class="number">3</span>).astype(<span class="string">'int'</span>)</span><br><span class="line">test_y=np.random.rand(<span class="number">1</span>,<span class="number">1</span>).astype(<span class="string">'int'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m<span class="number">-1</span>):</span><br><span class="line">image=image_list[i]</span><br><span class="line">image_convert=Image.open(image).convert(<span class="string">'RGB'</span>).resize((num_px,num_px))</span><br><span class="line">train_x[i]=np.array(image_convert)</span><br><span class="line">train_y[i]=np.array(label_list[i])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m<span class="number">-1</span>,m):</span><br><span class="line">image=image_list[i]</span><br><span class="line">image_convert=Image.open(image).convert(<span class="string">'RGB'</span>).resize((num_px,num_px))</span><br><span class="line">test_x[i+<span class="number">1</span>-m]=np.array(image_convert)</span><br><span class="line">test_y[i+<span class="number">1</span>-m]=np.array(label_list[i])</span><br></pre></td></tr></table></figure><p>（3）将图像数据写入h5文件并测试</p><p>使用create_dataset将数据按照对应名称存储，最后以只读方式读取h5对应名称中的图像数据，用plt方法显示指定图片。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将图像写入h5文件</span></span><br><span class="line">f=h5py.File(<span class="string">'data.h5'</span>,<span class="string">'w'</span>)</span><br><span class="line">f.create_dataset(<span class="string">'X_train'</span>,data=train_x)</span><br><span class="line">f.create_dataset(<span class="string">'y_train'</span>,data=train_y)</span><br><span class="line">f.create_dataset(<span class="string">'X_test'</span>,data=test_x)</span><br><span class="line">f.create_dataset(<span class="string">'y_test'</span>,data=test_y)</span><br><span class="line">f.close()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span><span class="params">()</span>:</span></span><br><span class="line"><span class="comment">#读取h5文件中的图像</span></span><br><span class="line">dataset=h5py.File(<span class="string">'data.h5'</span>,<span class="string">'r'</span>)</span><br><span class="line">my_train_x=np.array(dataset[<span class="string">'X_train'</span>][:])</span><br><span class="line">my_train_y=np.array(dataset[<span class="string">'y_train'</span>][:])</span><br><span class="line">my_test_x=np.array(dataset[<span class="string">'X_test'</span>][:])</span><br><span class="line">my_test_y=np.array(dataset[<span class="string">'y_test'</span>][:])</span><br><span class="line"><span class="keyword">return</span> my_train_x,my_train_y,my_test_x,my_test_y</span><br><span class="line"></span><br><span class="line">my_train_x,my_train_y,my_test_x,my_test_y=load_dataset()</span><br><span class="line">print(my_train_x.shape)</span><br><span class="line">print(my_train_y.shape)</span><br><span class="line">print(my_test_x.shape)</span><br><span class="line">print(my_test_y.shape)</span><br><span class="line">index=<span class="number">1</span></span><br><span class="line">plt.imshow(my_train_x[index])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>（1）将图像保存为h5文件后</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200527212520.png" alt></p><p>可见，原图像大小58.5MB，保存为h5文件后只有1.5MB，明显所占内存减少了</p><p>（2）指定查看第一个图像显示如下</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200527212728.png" alt></p><p>（3）训练集和测试集维度为：</p><p>(11, 64, 64, 3)<br>(11, 1)<br>(1, 64, 64, 3)<br>(1, 1)</p><p>参考：</p><p><a href="https://blog.csdn.net/chenkz123/article/details/79640658" target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/chenkz123/article/details/79640658</a></p><p><a href="https://blog.csdn.net/weixin_43615222/article/details/84577293" target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/weixin_43615222/article/details/84577293</a></p><p><a href="https://www.jianshu.com/p/778d78463028" target="_blank" rel="external nofollow noopener noreferrer">https://www.jianshu.com/p/778d78463028</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;描述&quot;&gt;&lt;a href=&quot;#描述&quot; class=&quot;headerlink&quot; title=&quot;描述&quot;&gt;&lt;/a&gt;描述&lt;/h2&gt;&lt;p&gt;当需要用大量图片训练神经网络时，保存为图像格式的图片所占的内存大，因此将本地图片存入h5文件中减少数据集所占内存。&lt;/p&gt;&lt;p&gt;使用手机拍摄剪刀、石头、布共三组图片，每组图片四张，共12张图像，选择11张作为训练集，剩余的作为测试集。&lt;/p&gt;&lt;h2 id=&quot;步骤&quot;&gt;&lt;a href=&quot;#步骤&quot; class=&quot;headerlink&quot; title=&quot;步骤&quot;&gt;&lt;/a&gt;步骤&lt;/h2&gt;&lt;p&gt;（1）读取本地图片并将数据打乱&lt;/p&gt;&lt;p&gt;1° 读取文件：使用os的listdir函数读取目录下的文件：这里读取并存储的的是本地图像的路径，同时将图片的标签进行存储&lt;/p&gt;
    
    </summary>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>结构化机器学习项目——第二周学习笔记</title>
    <link href="https://www.xiapf.com/blogs/struLearning2/"/>
    <id>https://www.xiapf.com/blogs/struLearning2/</id>
    <published>2020-05-25T03:22:54.000Z</published>
    <updated>2020-05-25T03:48:10.582Z</updated>
    
    <content type="html"><![CDATA[<h2 id="误差分析"><a href="#误差分析" class="headerlink" title="误差分析"></a>误差分析</h2><h3 id="如何进误差分析"><a href="#如何进误差分析" class="headerlink" title="如何进误差分析"></a>如何进误差分析</h3><p>当需要提高系统的正确率时，需要一一看发生错误的样本并进行记录（主要是看假阳和假阴样本），列成如下的表格，纵向代表每个出错的样本，横向代表出错的类型：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200525102748.png" alt></p><p><strong>最后一行记录每个错误类型的百分比，根据百分比确定不同错误类型对错误率的影响，选择影响大的作为后续系统改进的方向。</strong></p><p>注：可以将百分比理解为是确定每个错误类型的优先级顺序，同时要考虑解决错误类型的难易度，即添加数据的难易度，获取难可以采用人工合成数据</p><a id="more"></a><h3 id="对标记错误的例子如何处理"><a href="#对标记错误的例子如何处理" class="headerlink" title="对标记错误的例子如何处理"></a>对标记错误的例子如何处理</h3><p>在误差分析之后，有些错误样本可能是初始标记的时候发生了错误，这时候分类处理：</p><p>（1）如果是训练集中原始标记错误，当训练集足够大，可以忽略错误，不进行改正</p><p>（2）如果是开发集或者测试集，就需要查看原始标记错误的百分比是否很大，即是否严重影响了系统的正确率，如果是，则需要修正，反之不需要。</p><h2 id="快速搭建自己的系统"><a href="#快速搭建自己的系统" class="headerlink" title="快速搭建自己的系统"></a>快速搭建自己的系统</h2><h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><p>（1）确定<strong>目标</strong>（后续出现问题也可修改）：确定开发集、测试集和衡量算法的单一实数指标</p><p>（2）搭建一个机器学习系统<strong>原型</strong>：在训练集上进行训练看效果，在开发集、测试集上测试看表现</p><p>（3）用<strong>偏差</strong>和<strong>方差</strong>分析，确定下一步做什么，进行<strong>误差分析</strong>，了解大部分出错的样本是什么，提高系统正确率</p><h3 id="数据不够如何处理"><a href="#数据不够如何处理" class="headerlink" title="数据不够如何处理"></a>数据不够如何处理</h3><p>当真正关心的数据比较少，假设只有10000条，非真正关心的数据有500000条，如何分配训练集、开发集、测试集：</p><p>根据“<u>瞄准需要处理的目标即真正关系的数据</u>”的原则划分数据集，保证开发集和测试集数据来自同分布</p><p>（1）选择1：500000条作为训练集，真正关系的数据中5000条作为开发集，5000条作为测试集</p><p>（2）选择2：500000条+真正关心数据中5000条=505000条作为训练集，真正关心的数据中2500条作为开发集，2500条作为测试集</p><h3 id="数据不匹配如何处理"><a href="#数据不匹配如何处理" class="headerlink" title="数据不匹配如何处理"></a>数据不匹配如何处理</h3><p>当按照数据不够的处理原则处理后即数据不匹配的系统的方差偏差分析：</p><p>由于训练集和开发集数据不同分布，为了了解训练集上的模型的推广程度（即方差），从原始数据中取一部分作物训练—开发集，来衡量方差的值。（设定500000条作为训练集，真正关系的数据中5000条作为开发集，5000条作为测试集）</p><table><thead><tr><th>误差类型</th><th>误差</th><th>数据量</th></tr></thead><tbody><tr><td>1</td><td>人类表现（即贝叶斯误差）</td><td>/</td></tr><tr><td>2</td><td>训练集误差</td><td>495000（在该数据上进行训练）</td></tr><tr><td>3</td><td>训练—开发集误差</td><td>5000（该数据上不进行训练）</td></tr><tr><td>4</td><td>开发集误差</td><td>5000</td></tr><tr><td>5</td><td>训练集误差</td><td>5000</td></tr></tbody></table><p>（1）1，2之间是可避免误差，措施：训练更大的网络或者训练时间更长</p><p>（2）2，3之间是方差，措施：正则化或者获取更多的训练集数据</p><p>（3）3，4之间是数据不匹配，措施：人工合成数据</p><p>快速制造更多训练数据，例如语音识别，用大量清晰数据+噪音数据制造出关心的数据：在噪音下说话的声音。</p><p>缺点：当噪音数据过小，容易对当前噪音数据过拟合。</p><p>（4）4，5之间是对开发集过拟合程度，措施：需要更大的开发集或者开发集数据更多些</p><h2 id="迁移学习和多任务学习"><a href="#迁移学习和多任务学习" class="headerlink" title="迁移学习和多任务学习"></a>迁移学习和多任务学习</h2><h3 id="迁移学习——串行"><a href="#迁移学习——串行" class="headerlink" title="迁移学习——串行"></a>迁移学习——串行</h3><p>神经网络从一个任务中学习到知识，并将这些知识应用到其他任务中叫做迁移学习。</p><h4 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h4><p>A-&gt;B（将A任务中知识迁移到B任务中）</p><p>（1）A的数据量比B多很多</p><p>（2）A，B有相同的输入</p><p>（3）A任务中低层次特征的学习能够帮助B任务</p><p>注：当A的数据量比B小，迁移学习效果不太好</p><h4 id="example"><a href="#example" class="headerlink" title="example"></a>example</h4><p>将对猫的识别迁移到对x射线图像的识别中去</p><p>A:对猫的识别</p><p>B:对x射线图像的识别</p><p>两者都是输入图像，满足条件（2），同时A任务中能够学习图像的轮廓等能保住B任务，满足条件（3），任务A的数据量很大。</p><p>方法：预训练+微调</p><p>预训练：使用数据量大的A任务训练的模型</p><p>微调：使用数据量小的B任务去掉最后输出层，重新训练权重的过程</p><p>（1）方法1（x射线图像的识别数据量小）：将对猫识别网络最后一层去掉，初始化最后一层的权重等参数（随机权重），使用原始前n-1层参数进行训练</p><p>（2）方法2（x射线图像的识别数据量大）：将对猫识别网络最后一层去掉，初始化所有层的权重等参数进行训练</p><p>注：去掉最后一层之后，可以加几层隐藏层再接输出</p><h3 id="多任务学习——并行"><a href="#多任务学习——并行" class="headerlink" title="多任务学习——并行"></a>多任务学习——并行</h3><p>同时开始学习，神经网络每次学习几件事，并希望每个任务都能帮助其他任务称为多任务学习</p><h4 id="应用场景-1"><a href="#应用场景-1" class="headerlink" title="应用场景"></a>应用场景</h4><p>（1）训练的一组任务，可以共用低层次特征（例如自动车驾驶，需要训练对物体的识别，识别行人，标志，斑马线等，此时可以共用低层次特征）</p><p>（2）每个单独的任务数据量接近</p><p>（3）建立一个大的神经网络同时训练一组任务比单独训练每个任务效率高</p><p>一般网络最后输出代表是否有该组任务，即每个任务赋予一个标签。（和softmaxt层不同，softmax最后数据代表单个样本的分类情况）</p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200525111013.png" alt></p><p>红色框中代表对四个任务分类的求和</p><h2 id="端到端的深度学习"><a href="#端到端的深度学习" class="headerlink" title="端到端的深度学习"></a>端到端的深度学习</h2><h3 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h3><p>语音识别任务（x,y），x代表输入的音频，y代表听写文本</p><p>一般深度学习：从x-&gt;提取特征-&gt;提取音位-&gt;生成词-&gt;y，中间包含很多手工生成的组件，里面包含人类对知识的分析，从而传递给网络</p><p>端到端的深度学习：从x直接映射到y，完全依靠数据来学习知识，没有人类的干预即手工生成组件</p><h3 id="应用场景-2"><a href="#应用场景-2" class="headerlink" title="应用场景"></a>应用场景</h3><p>从x直接映射到y有大量训练样本</p><p>当从x直接映射到y没有很多样本，这时候可以换分为子任务，在子任务中进行端到端的深度学习</p><p>例如：人脸识别系统（x,y），x代表输入的人像图片，y代表这个人的身份</p><p>由于人脸识别任务中的输入图片有很多，各种角度和各种距离的，这样的数据集较少，因此可以将任务分为两个子任务：</p><p>（1）（x1,y1），x代表输入的人脸图片，y1代表将输入的图片裁剪出人脸并居中</p><p>（2）（x2,y2），x2代表输入的居中人脸图片，y2代表这个人的身份（通过x2人脸和库中人脸进行匹配训练模型）</p><p>子任务（1）、（2）数据量充足，划分之后任务简单也易训练模型。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;误差分析&quot;&gt;&lt;a href=&quot;#误差分析&quot; class=&quot;headerlink&quot; title=&quot;误差分析&quot;&gt;&lt;/a&gt;误差分析&lt;/h2&gt;&lt;h3 id=&quot;如何进误差分析&quot;&gt;&lt;a href=&quot;#如何进误差分析&quot; class=&quot;headerlink&quot; title=&quot;如何进误差分析&quot;&gt;&lt;/a&gt;如何进误差分析&lt;/h3&gt;&lt;p&gt;当需要提高系统的正确率时，需要一一看发生错误的样本并进行记录（主要是看假阳和假阴样本），列成如下的表格，纵向代表每个出错的样本，横向代表出错的类型：&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200525102748.png&quot; alt&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;最后一行记录每个错误类型的百分比，根据百分比确定不同错误类型对错误率的影响，选择影响大的作为后续系统改进的方向。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;注：可以将百分比理解为是确定每个错误类型的优先级顺序，同时要考虑解决错误类型的难易度，即添加数据的难易度，获取难可以采用人工合成数据&lt;/p&gt;
    
    </summary>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>结构化机器学习项目——第一周学习笔记</title>
    <link href="https://www.xiapf.com/blogs/struLearning1/"/>
    <id>https://www.xiapf.com/blogs/struLearning1/</id>
    <published>2020-05-21T08:38:04.000Z</published>
    <updated>2020-05-21T08:39:20.931Z</updated>
    
    <content type="html"><![CDATA[<h2 id="正交化"><a href="#正交化" class="headerlink" title="正交化"></a>正交化</h2><p>正交化在数学中是垂直的含义，每个方法单独影响一个功能。</p><p>一个监督学习系统</p><p>（1）训练集</p><p>当训练集拟合不佳时，采用更大的网络或者优化的算法（如Adam算法），这些措施单一的影响训练集训练的效果，这样能方便对模型修改。</p><p>（2）开发集</p><p>当开发集错误率高，说明存在较大的方差，可能是训练集过拟合了，可以采用正则化或者获取更多的训练数据，这些事正交化措施。如果使用早停的方法，虽然可以抑制过拟合，但是可能会导致训练集训练不完全，这种非正交化的措施不建议使用。</p><a id="more"></a><p>（3）测试集</p><p>当测试集错误率高，说明在开发集上效果良好的模型没有很好的推广到测试集，说明开发集可能过拟合了，这时就需要更多的开发集数据，这也是正交化措施，只单独影响测试集。</p><p>（4）用户体验（即成本函数）</p><p>根据原有的成本函数生成的模型效果好，但是在实际应用中效果不好，说明可能是成本函数的评价标准发生错误，这时就需要重新定义成本函数，或者改变验证集。</p><p>总结：因此，在搭建模型的时候，需要采用正交化的方法，以便以方便的解决问题。</p><p>正交化步骤：</p><p>（1）首先设定一个目标来衡量想做的事</p><p>（2）调试算法表现：分开考虑改善系统在指标上的表现</p><h2 id="单一实数指标"><a href="#单一实数指标" class="headerlink" title="单一实数指标"></a>单一实数指标</h2><p>单一的实数指标能很方便的判断众多模型中最优的那个。</p><p>一般评估模型常用查准率P，即模型标记为真的数据中有多少确实是真；指标查全率T，即在真的数据中模型标记出了多少。然而这两个指标具有平衡性，所以常采用F1分数评估模型，即F和P的调和平均数：F1=2/(1/P+1/T)</p><p>总结：通过设置开发集和单一实数指标能够快速判断模型的好坏，加快迭代速度。</p><p>注：当指标不能很好的衡量模型的好坏时，需要更换新的指标。例如对于假阳的例子，对其设置惩罚权重，提高其错误率。</p><h2 id="满足指标和优化指标"><a href="#满足指标和优化指标" class="headerlink" title="满足指标和优化指标"></a>满足指标和优化指标</h2><p>单一的实数指标不一定能够满足模型的所有条件，当需要符合N个条件时，选择一个作为优化指标，剩余的N-1个作为满足指标。</p><p>优化指标：尽可能优化，即精度越高，数值越大即可</p><p>满足指标：设定一个阈值，在阈值线以内即可，不要求最优</p><p>这样模型就转换为尽可能优化设定的优化指标即可，满足指标只要达到条件就行。</p><h2 id="如何设定训练集，开发集，测试集"><a href="#如何设定训练集，开发集，测试集" class="headerlink" title="如何设定训练集，开发集，测试集"></a>如何设定训练集，开发集，测试集</h2><p>（1）三者分别作用</p><p>训练集作用：根据不同思路训练出不同模型</p><p>开发集作用：用于评估不同模型中最好的模型</p><p>测试集作用：系统开发之后评估性能（评估成本函数），能否满足用户需求</p><p>（2）大小如何设置</p><p>数据量很大的情况，训练集占98%以上，开发集和测试集占很少部分（1%左右）。</p><p>（3）设置注意点</p><p>为了让算法集中于想要的目标数据，开发集和测试集的数据分布要相同。</p><p>为了让在开发集上运行的模型能很好的推广到测试集上。</p><h2 id="人的表现"><a href="#人的表现" class="headerlink" title="人的表现"></a>人的表现</h2><p>（1）估计bayes error</p><p>人的表现误差可以用来贝叶斯最优误差，贝叶斯最优误差时理论上最小的误差</p><p>（2）制定算法的策略</p><p>假定hunman error(约等于bayes error)=a%，training eror=b%，dev error=c%</p><p>则可避免偏差为b%-a%，方差为c%-b%，以此可以可以根据不同的场景确定不同的策略。当可避免偏差大时，采取训练更大的网络或者训练时间更长，当方差大时，可以采取正则化或者选取更多的训练集数据。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;正交化&quot;&gt;&lt;a href=&quot;#正交化&quot; class=&quot;headerlink&quot; title=&quot;正交化&quot;&gt;&lt;/a&gt;正交化&lt;/h2&gt;&lt;p&gt;正交化在数学中是垂直的含义，每个方法单独影响一个功能。&lt;/p&gt;&lt;p&gt;一个监督学习系统&lt;/p&gt;&lt;p&gt;（1）训练集&lt;/p&gt;&lt;p&gt;当训练集拟合不佳时，采用更大的网络或者优化的算法（如Adam算法），这些措施单一的影响训练集训练的效果，这样能方便对模型修改。&lt;/p&gt;&lt;p&gt;（2）开发集&lt;/p&gt;&lt;p&gt;当开发集错误率高，说明存在较大的方差，可能是训练集过拟合了，可以采用正则化或者获取更多的训练数据，这些事正交化措施。如果使用早停的方法，虽然可以抑制过拟合，但是可能会导致训练集训练不完全，这种非正交化的措施不建议使用。&lt;/p&gt;
    
    </summary>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>改善深层神经网络——识别手势图片</title>
    <link href="https://www.xiapf.com/blogs/ipvNNet3/"/>
    <id>https://www.xiapf.com/blogs/ipvNNet3/</id>
    <published>2020-05-19T12:57:17.000Z</published>
    <updated>2020-06-03T04:42:45.917Z</updated>
    
    <content type="html"><![CDATA[<h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p>使用TensorFlow深度学习框架建立识别手势图片的网络</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>注：（1）— （5）步骤并没有实际执行，所有训练模型过程都在会话（session）中进行，即模型训练基于会话的创建，<strong>所有输入的数据在会话中赋值，其他地方均处理的是变量</strong>。</p><p>网络结构为三层网络，各层的激活函数relu，relu，softmax，即模型结构为linear-&gt;relu-&gt;linear-&gt;relu-&gt;linear-&gt;softmax</p><a id="more"></a><h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><p>（0）数据处理</p><p>导入数据，并将数据归一化，对标签项转换为独热矩阵编码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.导入数据并归一化</span></span><br><span class="line">train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes=tu.load_dataset()</span><br><span class="line"><span class="comment">#每幅图片64*64*3，将每个样本堆叠起来</span></span><br><span class="line">train_x_flattern=train_set_x_orig.reshape(train_set_x_orig.shape[<span class="number">0</span>],<span class="number">-1</span>).T</span><br><span class="line">test_x_flattern=test_set_x_orig.reshape(test_set_x_orig.shape[<span class="number">0</span>],<span class="number">-1</span>).T</span><br><span class="line"><span class="comment">#归一化</span></span><br><span class="line">train_x=train_x_flattern/<span class="number">255</span></span><br><span class="line">test_x=test_x_flattern/<span class="number">255</span></span><br><span class="line"><span class="comment">#y全部转换为独热矩阵</span></span><br><span class="line">train_y=tu.convert_to_one_hot(train_set_y_orig,<span class="number">6</span>)</span><br><span class="line">test_y=tu.convert_to_one_hot(test_set_y_orig,<span class="number">6</span>)</span><br></pre></td></tr></table></figure><p>注：使用np.eye(C)[Y.reshape(-1)].T，eye后面跟一维数组，数组中每个元素代表1的偏移量，由于每一列代表一个样本，所以最后需要转置</p><p>（1）创建占位符用于输入训练集数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#2.构造占位符，便于输入训练集数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_placeholder</span><span class="params">(n_x,n_y)</span>:</span></span><br><span class="line">X=tf.placeholder(tf.float32,[n_x,<span class="literal">None</span>])</span><br><span class="line">Y=tf.placeholder(tf.float32,[n_y,<span class="literal">None</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> X,Y</span><br></pre></td></tr></table></figure><p>（2）初始化权重等参数</p><p>利用tf创建变量：tf.get_variable()</p><p>权重初始化：initializer=tf.contrib.layers.xavaier_initializer</p><p>偏置量初始化：initializer=tf.zeros_initializer</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#3.初始化参数</span></span><br><span class="line"><span class="comment">#构建变量</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(layer_dims)</span>:</span></span><br><span class="line">n_0,n_1,n_2,n_3=layer_dims</span><br><span class="line">parameters=&#123;&#125;</span><br><span class="line">tf.set_random_seed(<span class="number">1</span>)</span><br><span class="line">W1=tf.get_variable(<span class="string">"W1"</span>,[n_1,n_0],initializer=tf.contrib.layers.xavier_initializer(seed=<span class="number">1</span>))</span><br><span class="line">b1=tf.get_variable(<span class="string">"b1"</span>,[n_1,<span class="number">1</span>],initializer=tf.zeros_initializer())</span><br><span class="line">W2=tf.get_variable(<span class="string">"W2"</span>,[n_2,n_1],initializer=tf.contrib.layers.xavier_initializer(seed=<span class="number">1</span>))</span><br><span class="line">b2=tf.get_variable(<span class="string">"b2"</span>,[n_2,<span class="number">1</span>],initializer=tf.zeros_initializer())</span><br><span class="line">W3=tf.get_variable(<span class="string">"W3"</span>,[n_3,n_2],initializer=tf.contrib.layers.xavier_initializer(seed=<span class="number">1</span>))</span><br><span class="line">b3=tf.get_variable(<span class="string">"b3"</span>,[n_3,<span class="number">1</span>],initializer=tf.zeros_initializer())</span><br><span class="line"></span><br><span class="line">parameters=&#123;<span class="string">"W1"</span>:W1,<span class="string">"b1"</span>:b1,<span class="string">"W2"</span>:W2,<span class="string">"b2"</span>:b2,<span class="string">"W3"</span>:W3,<span class="string">"b3"</span>:b3&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>（3）前向传播</p><p>根据传入的参数和上一层数据计算Z，A</p><p>注：最后一层不用计算激活值，根据得出的Z3，利用tf.argmax得出概率最大（最大值概率最大），即为当前分类结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#4.前向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propogation</span><span class="params">(parameters,X)</span>:</span></span><br><span class="line">W1=parameters[<span class="string">"W1"</span>]</span><br><span class="line">b1=parameters[<span class="string">"b1"</span>]</span><br><span class="line">W2=parameters[<span class="string">"W2"</span>]</span><br><span class="line">b2=parameters[<span class="string">"b2"</span>]</span><br><span class="line">W3=parameters[<span class="string">"W3"</span>]</span><br><span class="line">b3=parameters[<span class="string">"b3"</span>]</span><br><span class="line"></span><br><span class="line">Z1=tf.add(tf.matmul(W1,X),b1)</span><br><span class="line">A1=tf.nn.relu(Z1)</span><br><span class="line">Z2=tf.add(tf.matmul(W2,A1),b2)</span><br><span class="line">A2=tf.nn.relu(Z2)</span><br><span class="line"><span class="comment">#Z3最大相当于概率最大</span></span><br><span class="line">Z3=tf.add(tf.matmul(W3,A2),b3)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> Z3</span><br></pre></td></tr></table></figure><p>（4）计算损失</p><p>利用框架中softmax的损失函数：tf.softmax_cross_entropy_with_logits(logits=….，labels=…..)，该损失函数数据输入的维度是（数据集大小，分布），所以输入的堆叠数据需要转置</p><p>使用reduce_mean计算均值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#5.计算损失</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(Z3,Y)</span>:</span></span><br><span class="line">logits=tf.transpose(Z3)</span><br><span class="line">labels=tf.transpose(Y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#最后一层用softmax,并求平均</span></span><br><span class="line"><span class="comment">#参数维度[数据集大小，分布]</span></span><br><span class="line">cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=labels))</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><p>（5）反向传播</p><p>tensorflow框架在前向传播时，自动计算导数，因此反向传播用一行代码即可，这里使用adam算法</p><p>tf.train.AdamOptimizer(学习率)).minimize(cost)</p><p>（6）创建会话执行训练模型过程</p><p>1° 创建会话，并初始化所有变量</p><p>2° 将训练数据输入占位符中进行模型训练</p><p>利用小批量梯度下降法：</p><p>a）每次获取一小批样本数据，将其输入占位符中；</p><p>b）在会话中输入反向传播的算法和损失函数，得到当前总损失</p><p>c）根据样本总数得到当前批量的损失，并进行记录</p><p>注：模型的参数需要保存到会话中，不然parameters一直是只有物理空间，没有赋值的状态：</p><p>{‘W1’: &lt;tf.Variable ‘W1:0’ shape=(25, 12288) dtype=float32_ref&gt;, ‘b1’: &lt;tf.Variable ‘b1:0’ shape=(25, 1) dtype=float32_ref&gt;, ‘W2’: &lt;tf.Variable ‘W2:0’ shape=(12, 25) dtype=float32_ref&gt;, ‘b2’: &lt;tf.Variable ‘b2:0’ shape=(12, 1) dtype=float32_ref&gt;, ‘W3’: &lt;tf.Variable ‘W3:0’ shape=(6, 12) dtype=float32_ref&gt;, ‘b3’: &lt;tf.Variable ‘b3:0’ shape=(6, 1) dtype=float32_ref&gt;}</p><p>最后利用argmax得到输出最大概率对应的类别，将其和正确输出对比，即可得到准确率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#6.构建模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(train_x,train_y,test_x,test_y,layer_dims,num_ecpho=<span class="number">100</span>,mini_batch_size=<span class="number">32</span>,learning_rate=<span class="number">0.0001</span>)</span>:</span></span><br><span class="line"><span class="comment">#重新运行模型不覆盖tf变量</span></span><br><span class="line">ops.reset_default_graph()</span><br><span class="line">tf.set_random_seed(<span class="number">1</span>)</span><br><span class="line">costs=[]</span><br><span class="line">seed=<span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#1.占位符：输入训练数据</span></span><br><span class="line">n_x,m=train_x.shape</span><br><span class="line">n_y=train_y.shape[<span class="number">0</span>]</span><br><span class="line">X,Y=create_placeholder(n_x,n_y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.初始化参数</span></span><br><span class="line">parameters=initialize_parameters(layer_dims)</span><br><span class="line"></span><br><span class="line"><span class="comment">#3.前向传播</span></span><br><span class="line">Z3=forward_propogation(parameters,X)</span><br><span class="line"></span><br><span class="line"><span class="comment">#4.计算损失</span></span><br><span class="line">cost=compute_cost(Z3,Y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#5.反向传播</span></span><br><span class="line">optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)</span><br><span class="line"></span><br><span class="line">init=tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment">#5.创建会话，训练模型</span></span><br><span class="line">session=tf.Session()</span><br><span class="line">session.run(init)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_ecpho):</span><br><span class="line">seed=seed+<span class="number">1</span></span><br><span class="line">mini_batchs=tu.random_mini_batches(train_x,train_y,mini_batch_size,seed)</span><br><span class="line"><span class="comment">#每一代的成本</span></span><br><span class="line">ecpho_cost=<span class="number">0</span></span><br><span class="line">num_mini_batch=int(m/mini_batch_size)</span><br><span class="line"><span class="keyword">for</span> mini_batch <span class="keyword">in</span> mini_batchs:</span><br><span class="line">mini_batch_x,mini_batch_y=mini_batch</span><br><span class="line">_,mini_cost=session.run([optimizer,cost],feed_dict=&#123;X:mini_batch_x,Y:mini_batch_y&#125;)</span><br><span class="line">ecpho_cost=ecpho_cost+mini_cost/num_mini_batch</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> i%<span class="number">5</span>==<span class="number">0</span>:</span><br><span class="line">costs.append(ecpho_cost)</span><br><span class="line"><span class="keyword">if</span> i%<span class="number">100</span>==<span class="number">0</span>:</span><br><span class="line">print(<span class="string">"第"</span>+str(i)+<span class="string">"次，cost:"</span>+str(ecpho_cost))</span><br><span class="line"></span><br><span class="line"><span class="comment">#6.画图</span></span><br><span class="line">plt.plot(costs)</span><br><span class="line">plt.xlabel(<span class="string">"iteration"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"cost"</span>)</span><br><span class="line">plt.title(<span class="string">"learning_rate:"</span>+str(learning_rate))</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#7.保存参数 ！！！！！要在session中保存，不然parameters一直是只有物理空间，没有赋值的状态</span></span><br><span class="line">parameters=session.run(parameters)</span><br><span class="line">print(<span class="string">"参数已保存到session"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#8.计算准确率</span></span><br><span class="line">predit_matrix=tf.equal(tf.argmax(Z3),tf.argmax(Y))</span><br><span class="line">accury=tf.reduce_mean(tf.cast(predit_matrix,<span class="string">"float32"</span>))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"训练集正确率:"</span>,session.run(accury,&#123;X:train_x,Y:train_y&#125;))</span><br><span class="line">print(<span class="string">"测试集正确率:"</span>,session.run(accury,&#123;X:test_x,Y:test_y&#125;))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>（7）输入自己数据进行测试</p><p>利用matplotlib中的图片工厂读入图片，并进行大小转换，将上述得出的模型参数和当前图像输入预测函数中（预测函数是进行一次前向传播，并根据最终的输出判断类别）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> image <span class="keyword">as</span> mpimg</span><br><span class="line">my_image=<span class="string">"1.png"</span></span><br><span class="line">file_name=<span class="string">"./datasets/fingers/"</span>+my_image</span><br><span class="line">img=mpimg.imread(file_name)</span><br><span class="line">plt.imshow(img)</span><br><span class="line">plt.show()</span><br><span class="line">my_test=img.reshape(<span class="number">1</span>,<span class="number">64</span>*<span class="number">64</span>*<span class="number">3</span>).T</span><br><span class="line">predict=tu.predict(my_test,parameters)</span><br><span class="line">print(<span class="string">"预测结果为："</span>+str(predict))</span><br></pre></td></tr></table></figure><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>（1）训练结果</p><p>即算法中第6步结果</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200519160728.png" alt></p><p>训练集正确率: 0.9990741<br>测试集正确率: 0.71666664</p><p>可见损失不断减少，训练集和测试集正确率效果可以。</p><p>（2）测试结果</p><p>输入新的图片</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200519205603.png" alt></p><p>预测结果为</p><p>CPU运行时间407.137855s.<br>预测结果为：[1]</p><p>可见预测无误，训练好的网络能识别出手势。</p><h2 id="附"><a href="#附" class="headerlink" title="附"></a>附</h2><h3 id="超参调试"><a href="#超参调试" class="headerlink" title="超参调试"></a>超参调试</h3><p>（1）超参调试采用随机取值方式：因为不知道哪个参数更重要</p><p>（2）采取从粗糙到精细的策略：先在大范围内随机取值，在其中找到最好效果的区间，在这个区间进行精细化，即更密集的取值</p><p>（3）注意标尺的选择：当小的变化对结果影响很大时，需要使用对数标尺</p><p>如参数取值为0.9 ~ 0.99，则转换为1-（0.1 ~ 0.11）,最大值0.1对应10^-1，最小值0.11对应10^-2</p><p>则取值范围指数r=[-2，-1]，参数取值为1-10^r</p><p>附：各超参的重要性排序：</p><p>学习率learning_rate &gt; momentum中的beta(常用取值0.9) = 隐藏层神经元数量 = mini_batch_size &gt; 网络层数 = 学习率衰减率</p><p>最不重要Adam算法中的beta1(取0.9)，beta2(取0.99)，epsilon(取1e-8)</p><h3 id="batch-norm归一化"><a href="#batch-norm归一化" class="headerlink" title="batch norm归一化"></a>batch norm归一化</h3><p>当深层网络层数很多，即网络很深时，可以使用batch norm归一化对隐藏层数据进行归一化。</p><p>作用：使得每层隐藏层的输入更趋于稳定范围内，即限制了在前层网络参数变化时，会影响数值的分布，使得后层的值趋于稳定，即后面单元不过分依赖前面单元，具有一定的独立性，可单独进行学习。</p><p>方法：用参数β,γ控制隐藏层的方差和均值</p><p>（为了使得方差和均值不全为0，1，加入的这两个参数，当方差和均值为0，1，则激活函数只在某一段变化，不能完全学习到所有特征）</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200519202110.png" alt></p><p>更新参数时和更新权重一样，可以使用梯度下降或者Adam等其他算法。</p><p>注：测试时没有均值和方差，则需要在训练时，根据指数加权平均方法得到训练集整体的均值和方差，作为测试的均值和方差使用</p><h3 id="多分类使用softmax"><a href="#多分类使用softmax" class="headerlink" title="多分类使用softmax"></a>多分类使用softmax</h3><p>当输出层的分类不止2个时，使用softmax函数：对输出的数值进行判断，将最大概率对应的数值作为当前分类（输出层得到的最大的数即概率最大）</p><p>当到最后一层时，得到线性值Z[l]，激活函数使用对幂指数取平均的方式，如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200519203045.png" alt></p><p>得到最大的值作为输出值。</p><p>损失函数：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200519203054.png" alt></p><h3 id="其他深度学习框架"><a href="#其他深度学习框架" class="headerlink" title="其他深度学习框架"></a>其他深度学习框架</h3><p>对不同领域的不同应用有不同的深度学习框架</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200519203341.png" alt></p><p>（深度学习框架学习待续…）</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;描述&quot;&gt;&lt;a href=&quot;#描述&quot; class=&quot;headerlink&quot; title=&quot;描述&quot;&gt;&lt;/a&gt;描述&lt;/h2&gt;&lt;p&gt;使用TensorFlow深度学习框架建立识别手势图片的网络&lt;/p&gt;&lt;h2 id=&quot;算法&quot;&gt;&lt;a href=&quot;#算法&quot; class=&quot;headerlink&quot; title=&quot;算法&quot;&gt;&lt;/a&gt;算法&lt;/h2&gt;&lt;p&gt;注：（1）— （5）步骤并没有实际执行，所有训练模型过程都在会话（session）中进行，即模型训练基于会话的创建，&lt;strong&gt;所有输入的数据在会话中赋值，其他地方均处理的是变量&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;网络结构为三层网络，各层的激活函数relu，relu，softmax，即模型结构为linear-&amp;gt;relu-&amp;gt;linear-&amp;gt;relu-&amp;gt;linear-&amp;gt;softmax&lt;/p&gt;
    
    </summary>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>改善深层神经网络——第二周编程作业</title>
    <link href="https://www.xiapf.com/blogs/ipvNNet2/"/>
    <id>https://www.xiapf.com/blogs/ipvNNet2/</id>
    <published>2020-05-15T07:11:21.000Z</published>
    <updated>2020-05-18T11:03:58.463Z</updated>
    
    <content type="html"><![CDATA[<p>可以通过优化算法提高网络训练速度</p><h2 id="Mini-batch梯度下降算法"><a href="#Mini-batch梯度下降算法" class="headerlink" title="Mini_batch梯度下降算法"></a>Mini_batch梯度下降算法</h2><p>梯度下降算法将样本全部遍历一遍后，再计算梯度，当样本很多时，训练时间太长。</p><p>这时候需要用Mini_batch梯度下降算法：</p><p>（1）将原始的样本打乱，随机分布，使用numpy库中permutation生成随机数组，打乱样本集</p><p>（2）将样本按照用户输入的大小mini_batch_size（一般是64 ~ 512，即符合计算机内存大小的2的幂次方，这样训练速度会快），将样本重新分为x{1}，x{2} …x{n}，其中划分的个数num_mini_batch=样本总个数m / mini_batch_size，则第一个mini_batch，即x{1}=train_x[:，0:mini_batch_size * 1]，第二个mini_batch，即x{2}=train_x[:，mini_batch_size * 1：mini_batch_size * 2]</p><a id="more"></a><p>（3）由于划分的个数num_mini_batch可能有小数部分，第二步是处理其整数部分，当处理结束后，需要判断是否有余下部分，有余下部分需要将数据补全在最后x{n}=train_x[:，mini_batch_size * num_mini_batch：]</p><p>（4）注：当用户输入的大小mini_batch_size=样本总数时，算法变为梯度下降法，即一次遍历整个样本集，当用户输入的大小mini_batch_size==1时，算法变为随机梯度下降算法，即一次遍历一个样本，此时失去了将样本向量化的加速作用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用mini_batch的梯度下降（小批量梯度下降）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_mini_batches</span><span class="params">(X,Y,mini_batch_size,seed)</span>:</span></span><br><span class="line">np.random.seed(seed)</span><br><span class="line">mini_batchs=[]</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取样本的数量</span></span><br><span class="line">m=X.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#1.打乱训练集</span></span><br><span class="line"><span class="comment">#返回长度为m的随机数组</span></span><br><span class="line">permutation=list(np.random.permutation(m))</span><br><span class="line">shuffle_X=X[:,permutation]</span><br><span class="line">shuffle_Y=Y[:,permutation].reshape((<span class="number">1</span>,m))</span><br><span class="line"></span><br><span class="line">num_mini_batch=math.floor(m/mini_batch_size)</span><br><span class="line"><span class="comment">#2.按照大小切分数据</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(num_mini_batch):</span><br><span class="line">mini_batch_X=shuffle_X[:,mini_batch_size*k:mini_batch_size*(k+<span class="number">1</span>)]</span><br><span class="line">mini_batch_Y=shuffle_Y[:,mini_batch_size*k:mini_batch_size*(k+<span class="number">1</span>)]</span><br><span class="line">mini_batch=(mini_batch_X,mini_batch_Y)</span><br><span class="line"></span><br><span class="line">mini_batchs.append(mini_batch)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#3.处理余下部分的数据</span></span><br><span class="line"><span class="keyword">if</span>(m%mini_batch_size!=<span class="number">0</span>):</span><br><span class="line">mini_batch_X=shuffle_X[:,mini_batch_size*num_mini_batch:]</span><br><span class="line">mini_batch_Y=shuffle_Y[:,mini_batch_size*num_mini_batch:]</span><br><span class="line">mini_batch=(mini_batch_X,mini_batch_Y)</span><br><span class="line">mini_batchs.append(mini_batch)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> mini_batchs</span><br></pre></td></tr></table></figure><p>a ）当mini_batch_size设置为样本总数时</p><p>损失变化情况</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200515143104.png" alt></p><p>train data:<br>Accuracy: 0.66</p><p>打印出的决策边界</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200515143125.png" alt></p><p>b ） 当mini_batch_size设置为64时</p><p>损失变化情况</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200515142350.png" alt></p><p>train data:<br>Accuracy: 0.7966666666666666</p><p>打印出的决策边界</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200515142739.png" alt></p><p>分析</p><p>可以看出梯度下降算法基本无摆动，但是损失下降到0.64左右变化幅度很小，从分类情况来看误分角度，而mini_batch梯度下降算法摆动较大，分类准确率比梯度下降要高一些。</p><h2 id="Momentun算法（带动量的梯度下降算法）"><a href="#Momentun算法（带动量的梯度下降算法）" class="headerlink" title="Momentun算法（带动量的梯度下降算法）"></a>Momentun算法（带动量的梯度下降算法）</h2><p>Momentun算法使用了指数加权平均项来计算梯度，动量参数beta设置为0.9，每个梯度的变化前都加入了指数项，随着时间增加，梯度前的指数项衰减，即每次梯度的变化都参照了前面梯度的变化方向（越近的梯度权重越高），从而减少向最低点前进时的摆动。</p><p>注：指数加权平均项 v(t)=beta * v(t-1)+(1-beta) * θ</p><p>（1）在初始化参数的时候，按照参数W，b的维度生成动量项</p><p>（2）当选择带动量的梯度下降算法时，按照指数加权平均公式，计算动量项，当单纯使用momentum算法时，不需要偏差修正，根据动量项更新参数W，b</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#momentum算法</span></span><br><span class="line"><span class="comment">#通过指数加权平均方法改变梯度，减少摆动幅度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_momentum</span><span class="params">(parameters)</span>:</span></span><br><span class="line">L=len(parameters)//<span class="number">2</span></span><br><span class="line">v=&#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(L):</span><br><span class="line">v[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]=np.zeros_like(parameters[<span class="string">"W"</span>+str(i+<span class="number">1</span>)])</span><br><span class="line">v[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]=np.zeros_like(parameters[<span class="string">"b"</span>+str(i+<span class="number">1</span>)])</span><br><span class="line"><span class="keyword">return</span> v</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameter_momentum</span><span class="params">(parameters,grads,v,beta,learning_rate)</span>:</span></span><br><span class="line">L=len(parameters)//<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(L):</span><br><span class="line">v[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]=beta*v[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]+(<span class="number">1</span>-beta)*grads[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]</span><br><span class="line">v[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]=beta*v[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]+(<span class="number">1</span>-beta)*grads[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">parameters[<span class="string">"W"</span>+str(i+<span class="number">1</span>)]=parameters[<span class="string">"W"</span>+str(i+<span class="number">1</span>)]-learning_rate*v[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]</span><br><span class="line">parameters[<span class="string">"b"</span>+str(i+<span class="number">1</span>)]=parameters[<span class="string">"b"</span>+str(i+<span class="number">1</span>)]-learning_rate*v[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]</span><br><span class="line"><span class="keyword">return</span> parameters,v</span><br></pre></td></tr></table></figure><p>结果</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200515143700.png" alt></p><p>train data:<br>Accuracy: 0.8766666666666667</p><p>打印出的决策边界</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200515143712.png" alt></p><p>分析</p><p>momentun算法中由于每次梯度的变化都是参考之前的梯度进行变化，即增加了动量，摆动幅度比只使用mini_batch算法较小，并且分类正确率也提高了。</p><h2 id="Adam算法（结合Momentun和RMSprop算法）"><a href="#Adam算法（结合Momentun和RMSprop算法）" class="headerlink" title="Adam算法（结合Momentun和RMSprop算法）"></a>Adam算法（结合Momentun和RMSprop算法）</h2><p>RMSprop算法是保留微分平方的加权平均数的梯度下降算法，也是用于较少梯度变化中的摆动。</p><p>计算公式：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200515144636.png" alt></p><p>在对每个mini_batch迭代中，按照指数加权平均的衰减方式计算梯度，但是这里梯度是需要将微分平方，然后再更新参数，为了让更新参数中的分母为0，这里将分母加上epsilon(默认值为1e-8)</p><p>Adam算法是结合Momentun和RMSprop算法</p><p>（1）momentun部分：在初始化参数的时候，按照参数W，b的维度生成动量项；RMSprop部分：在初始化参数的时候，按照参数W，b生成生成初始项Sdw，Sdb</p><p>（2）momentun部分：按照指数加权平均公式，计算动量项，并进行偏差修正(v /（1-np.power（beta1,t）))，得到修正之后的动量项；RMSprop部分：按照保留微分平方的加权平均数方式，得到Sdw，Sdb，并进行偏差修正(s /（1-np.power（beta2,t）))，其中momentum部分的beat1默认为0.9，RMSprop部分的beta2默认为0.99</p><p>（3）使用指数衰减之后的梯度值更新参数W，b</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200515145910.png" alt></p><p>注：偏差修正是将需要修正的项目v或者s除以（1-np.power（beta,t）），其中t代表第几个mini_catch，这里的beta都是小于1的（由于是进行指数衰减），当t越大，np.power（beta,t）的值越接近于0，分母就接近1，此时基本没有修正，当t越小，修正越多，这是由于算法初始的时候，由于迭代次数少，数值出现偏差可能大，所以在初始时需要修正多一些。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#adam算法</span></span><br><span class="line"><span class="comment">#结合了momentun算法和RMSprop算法，并进行偏差修正</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_adam</span><span class="params">(parameters)</span>:</span></span><br><span class="line">L=len(parameters)//<span class="number">2</span></span><br><span class="line">v=&#123;&#125;</span><br><span class="line">s=&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(L):</span><br><span class="line">v[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]=np.zeros_like(parameters[<span class="string">"W"</span>+str(i+<span class="number">1</span>)])</span><br><span class="line">v[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]=np.zeros_like(parameters[<span class="string">"b"</span>+str(i+<span class="number">1</span>)])</span><br><span class="line">s[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]=np.zeros_like(parameters[<span class="string">"W"</span>+str(i+<span class="number">1</span>)])</span><br><span class="line">s[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]=np.zeros_like(parameters[<span class="string">"b"</span>+str(i+<span class="number">1</span>)])</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> v,s</span><br><span class="line"></span><br><span class="line"><span class="comment">#t表示迭代的mini_batch的次数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameter_adam</span><span class="params">(parameters,grads,v,s,t,learning_rate=<span class="number">0.01</span>,beta1=<span class="number">0.9</span>,beta2=<span class="number">0.999</span>,epsilon=<span class="number">1e-8</span>)</span>:</span></span><br><span class="line">L=len(parameters)//<span class="number">2</span></span><br><span class="line">vcorrect=&#123;&#125;</span><br><span class="line">scorrect=&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(L):</span><br><span class="line">v[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]=beta1*v[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]+(<span class="number">1</span>-beta1)*grads[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]</span><br><span class="line">v[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]=beta1*v[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]+(<span class="number">1</span>-beta1)*grads[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]</span><br><span class="line">vcorrect[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]=v[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]/(<span class="number">1</span>-np.power(beta1,t))</span><br><span class="line">vcorrect[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]=v[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]/(<span class="number">1</span>-np.power(beta1,t))</span><br><span class="line"></span><br><span class="line">s[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]=beta2*s[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]+(<span class="number">1</span>-beta2)*(np.square(grads[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]))</span><br><span class="line">s[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]=beta2*s[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]+(<span class="number">1</span>-beta2)*(np.square(grads[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]))</span><br><span class="line">scorrect[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]=s[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]/(<span class="number">1</span>-np.power(beta2,t))</span><br><span class="line">scorrect[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]=s[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]/(<span class="number">1</span>-np.power(beta2,t))</span><br><span class="line"></span><br><span class="line">parameters[<span class="string">"W"</span>+str(i+<span class="number">1</span>)]=parameters[<span class="string">"W"</span>+str(i+<span class="number">1</span>)]-learning_rate*vcorrect[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]/np.sqrt(scorrect[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]+epsilon)</span><br><span class="line">parameters[<span class="string">"b"</span>+str(i+<span class="number">1</span>)]=parameters[<span class="string">"b"</span>+str(i+<span class="number">1</span>)]-learning_rate*vcorrect[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]/np.sqrt(scorrect[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]+epsilon)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> parameters,v,s</span><br></pre></td></tr></table></figure><p>结果</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200515150033.png" alt></p><p>train data:<br>Accuracy: 0.94</p><p>打印出的决策边界</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200515150045.png" alt></p><p>分析</p><p>可以看出，adam算法基本无摆动，分类准确率也很高</p><h2 id="附：学习率衰减"><a href="#附：学习率衰减" class="headerlink" title="附：学习率衰减"></a>附：学习率衰减</h2><p>在学习初期，学习率大；在收敛时，学习率小，按照以下公式设置学习率</p><p>alpha=(1 / （1+衰减率 * 迭代次数）) * alpha0</p><p>还有其他指数衰减或者离散衰减方式</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200515162005.png" alt></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>（1）当样本集数据量很大时，使用mini_batch算法加快训练速度，每次遍历mini_batch_size大小的数据，当用户输入的大小mini_batch_size=样本总数时，算法变为梯度下降法，即一次遍历整个样本集，当用户输入的大小mini_batch_size==1时，算法变为随机梯度下降算法，即一次遍历一个样本，此时失去了将样本向量化的加速作用。</p><p>（2）由于梯度下降时存在摆动，即噪声很大，此时学习率需要很小，即向最低点前进的步长小，这时也会导致网络训练速度减慢，这时候就需要使 用指数加权平均项来得到当前的梯度：每个梯度的变化前都加入了指数项，随着时间增加，梯度前的指数项衰减，即每次梯度的变化都参照了前面梯度的变化方向（越近的梯度权重越高），从而减少向最低点前进时的摆动。其中momentun算法和Adam算法能有效减少摆动，可以增加学习率，即加大每次参数变化的步长，从而加快了网络训练速度，其中Adam算法由于结合了结合Momentun和RMSprop算法，优化效果最好，并且分类准确率也高。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;可以通过优化算法提高网络训练速度&lt;/p&gt;&lt;h2 id=&quot;Mini-batch梯度下降算法&quot;&gt;&lt;a href=&quot;#Mini-batch梯度下降算法&quot; class=&quot;headerlink&quot; title=&quot;Mini_batch梯度下降算法&quot;&gt;&lt;/a&gt;Mini_batch梯度下降算法&lt;/h2&gt;&lt;p&gt;梯度下降算法将样本全部遍历一遍后，再计算梯度，当样本很多时，训练时间太长。&lt;/p&gt;&lt;p&gt;这时候需要用Mini_batch梯度下降算法：&lt;/p&gt;&lt;p&gt;（1）将原始的样本打乱，随机分布，使用numpy库中permutation生成随机数组，打乱样本集&lt;/p&gt;&lt;p&gt;（2）将样本按照用户输入的大小mini_batch_size（一般是64 ~ 512，即符合计算机内存大小的2的幂次方，这样训练速度会快），将样本重新分为x{1}，x{2} …x{n}，其中划分的个数num_mini_batch=样本总个数m / mini_batch_size，则第一个mini_batch，即x{1}=train_x[:，0:mini_batch_size * 1]，第二个mini_batch，即x{2}=train_x[:，mini_batch_size * 1：mini_batch_size * 2]&lt;/p&gt;
    
    </summary>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>（转载）windows10专业版,我们无法在此设备上激活windows因为无法连接到你的组织的激活服务器解决方法</title>
    <link href="https://www.xiapf.com/blogs/sovConSever/"/>
    <id>https://www.xiapf.com/blogs/sovConSever/</id>
    <published>2020-05-15T02:27:50.000Z</published>
    <updated>2020-05-15T02:37:38.047Z</updated>
    
    <content type="html"><![CDATA[<p>转载于 <a href="https://www.jianshu.com/p/953c1517e436" target="_blank" rel="external nofollow noopener noreferrer">https://www.jianshu.com/p/953c1517e436</a> </p><p>1.管理员运行cmd，然后输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">slmgr /ipk W269N-WFGWX-YVC9B-4J6C9-T83GX</span><br></pre></td></tr></table></figure><p>2.再输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">slmgr.vbs -skms zh.us.to</span><br></pre></td></tr></table></figure><p>3.再设置–更新和安全–激活中，点击更新产品密钥，输入以上密钥 W269N-WFGWX-YVC9B-4J6C9-T83GX，并点击下一步激活该密钥。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转载于 &lt;a href=&quot;https://www.jianshu.com/p/953c1517e436&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;https://www.jianshu.com/
      
    
    </summary>
    
    
      <category term="win10" scheme="https://www.xiapf.com/categories/win10/"/>
    
    
      <category term="win10" scheme="https://www.xiapf.com/tags/win10/"/>
    
  </entry>
  
  <entry>
    <title>改善深层神经网络——第一周编程作业</title>
    <link href="https://www.xiapf.com/blogs/ipvNNet1/"/>
    <id>https://www.xiapf.com/blogs/ipvNNet1/</id>
    <published>2020-05-13T03:29:13.000Z</published>
    <updated>2020-05-13T03:34:44.685Z</updated>
    
    <content type="html"><![CDATA[<h2 id="训练集、验证集、测试集选取"><a href="#训练集、验证集、测试集选取" class="headerlink" title="训练集、验证集、测试集选取"></a>训练集、验证集、测试集选取</h2><p>训练集用于根据算法训练不同的模型</p><p>验证集用于验证不同的算法中哪一种更有效</p><p>测试集用于评估算法的性能</p><p>对于大数据（如一万条数据以上），训练集，验证集，测试集的比列为98%，1%，1%，训练集占绝大部分。</p><h2 id="提高训练速度"><a href="#提高训练速度" class="headerlink" title="提高训练速度"></a>提高训练速度</h2><h3 id="归一化输入特征"><a href="#归一化输入特征" class="headerlink" title="归一化输入特征"></a>归一化输入特征</h3><p>未归一化的输入特征得到的损失函数是比较狭长的，而且在反向传播中使用梯度下降时，步长需要很小，否则无法找到最小值。</p><a id="more"></a><p>所有需要归一化输入特征，将输入特征统一减去数据均值并除以方差，使得得出的损失函数更匀称，无论从什么步长都能得到最小值。</p><p>需要注意的是，测试集，验证集都需要很训练集一样的归一化方式</p><h3 id="初始权重的选取"><a href="#初始权重的选取" class="headerlink" title="初始权重的选取"></a>初始权重的选取</h3><p>不同初始化方法的对比</p><p>（1）权重全都初始化为0</p><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.初始化参数为0</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_zeros</span><span class="params">(layer_dims)</span>:</span></span><br><span class="line">L=len(layer_dims)</span><br><span class="line">parameters=&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,L):</span><br><span class="line">parameters[<span class="string">"W"</span>+str(i)]=np.zeros((layer_dims[i],layer_dims[i<span class="number">-1</span>]))</span><br><span class="line">parameters[<span class="string">"b"</span>+str(i)]=np.zeros((layer_dims[i],<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>结果</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200513104721.png" alt></p><p>分析</p><p>可以看出使用该种初始化权重的方式，损失没有变化，并且预测精度也很低：训练集Accuracy: 0.5，测试集Accuracy: 0.5。主要是因为权重初始化为0，使得所有神经元都做相同的事，存在对称性。</p><p>（2）权重用很大的随机值初始化</p><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#2.初始化参数为很大的随机数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_random</span><span class="params">(layer_dims)</span>:</span></span><br><span class="line">L=len(layer_dims)</span><br><span class="line">parameters=&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,L):</span><br><span class="line">parameters[<span class="string">"W"</span>+str(i)]=np.random.randn(layer_dims[i],layer_dims[i<span class="number">-1</span>])*<span class="number">10</span></span><br><span class="line">parameters[<span class="string">"b"</span>+str(i)]=np.zeros((layer_dims[i],<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>结果</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200513105002.png" alt></p><p>分析</p><p>可以看出使用该种初始化权重的方式，损失一开始由于权重较大，下降快，但到后半段，损失下降很慢，因为w变大，z变大，此时的梯度变化下，训练速度减慢。预测精度也不是很高：训练集Accuracy: 0.8833333333333333，测试集Accuracy：0.85。</p><p>（3）采用抑制梯度爆炸和梯度消失的方式初始化</p><p>当权重大于1时，梯度以指数方式增长，当权重小于1时，梯度递减，为了防止出现这种梯度爆炸和梯度消失的出现，需要将权重初始化为不比1大很多，不比1小很多的值：w=np.random.randn(shape) * sqrt(1/n[l-1])，即需要除以上一层神经元的个数（n[l-1]代表上一层神经元个数），将权重w归一化。（当激活函数是tanh时，使用sqrt(2/n[l-1]效果更好)</p><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#3.初始化参数使用抑梯度异常方式  乘以sqrt(2/上一层神经元数量)  把参数w初始化到了1附近</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_he</span><span class="params">(layer_dims)</span>:</span></span><br><span class="line">L=len(layer_dims)</span><br><span class="line">parameters=&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,L):</span><br><span class="line">parameters[<span class="string">"W"</span>+str(i)]=np.random.randn(layer_dims[i],layer_dims[i<span class="number">-1</span>])*np.sqrt(<span class="number">2</span>/layer_dims[i<span class="number">-1</span>])</span><br><span class="line">parameters[<span class="string">"b"</span>+str(i)]=np.zeros((layer_dims[i],<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>结果</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200513104411.png" alt></p><p>分析</p><p>可以看出使用该种初始化权重的方式，损失逐渐减少，并且预测精度也很高：训练集Accuracy: 0.9933333333333333，测试集Accuracy: 0.93。</p><p>总结</p><p>采用抑制梯度爆炸和梯度消失的方式初始化权重</p><h2 id="高偏差高方差的解决"><a href="#高偏差高方差的解决" class="headerlink" title="高偏差高方差的解决"></a>高偏差高方差的解决</h2><h3 id="高偏差"><a href="#高偏差" class="headerlink" title="高偏差"></a>高偏差</h3><p>高偏差是指训练集的误差大，即模型没有学到训练集的所有特征，不能很好的拟合训练数据。</p><p>解决方法：</p><p>（1）选择更大的网络（增加网络层数或者神经元数量）</p><p>（2）训练时间更长</p><h3 id="高方差"><a href="#高方差" class="headerlink" title="高方差"></a>高方差</h3><p>高方差是指训练的模型在测试集的误差大，即模型存在过拟合，过度拟合训练集数据。</p><p>解决方法：</p><p>（1）正则化</p><p>（2）更多训练数据</p><p>一般第二种方法成本较高，当出现高方差的时候选择正则化方法</p><p>正则化方法简单来说就是减少对输入数据的学习，以防止过拟合。</p><p>不同正则化方法对比：</p><p>（0）当不使用正则化</p><p>lambd是L2正则化中的参数,keep_prop是dropout正则化中的参数，lambd=0,keep_prop=1.0代表不使用正则化</p><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#是否加入正则化的模型  lambd是L2正则化中的参数,keep_prop是dropout正则化中的参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_regularization</span><span class="params">(X,Y,layer_dims,iter=<span class="number">30000</span>,learning_rate=<span class="number">0.3</span>,lambd=<span class="number">0</span>,keep_prop=<span class="number">1.0</span>)</span>:</span></span><br><span class="line"><span class="comment">#1.初始化参数 防止梯度爆炸和梯度消失</span></span><br><span class="line">parameters=ru.initialize_parameters(layer_dims)</span><br><span class="line">costs=[]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(iter):</span><br><span class="line"><span class="comment">#2.前向传播 是否进行keep_prop正则化</span></span><br><span class="line"><span class="keyword">if</span>(keep_prop==<span class="number">1.0</span>):</span><br><span class="line">A,cache=ru.forward_propagation(X,parameters)</span><br><span class="line"><span class="keyword">elif</span>(keep_prop&lt;<span class="number">1.0</span>):</span><br><span class="line">A,cache=forward_propagation_dropout(X,parameters,keep_prop)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">print(<span class="string">"keep_prop参数错误"</span>)</span><br><span class="line">exit</span><br><span class="line"></span><br><span class="line"><span class="comment">#3.计算损失，判断是否使用L2正则化</span></span><br><span class="line"><span class="keyword">if</span>(lambd==<span class="number">0</span>):</span><br><span class="line">cost=ru.compute_cost(A,Y)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">cost=compute_loss_L2(A,Y,parameters,lambd)</span><br><span class="line"></span><br><span class="line"><span class="comment">#4.反向传播 是否进行正则化(dropout或者L2)</span></span><br><span class="line"><span class="keyword">if</span>(keep_prop==<span class="number">1</span>) <span class="keyword">and</span> (lambd==<span class="number">0</span>):</span><br><span class="line">grad=ru.backward_propagation(X,Y,cache)</span><br><span class="line"><span class="keyword">elif</span>(lambd!=<span class="number">0</span>):</span><br><span class="line">grad=backward_propagation_L2(X,Y,cache,lambd)</span><br><span class="line"><span class="keyword">elif</span>(keep_prop&lt;<span class="number">1</span>):</span><br><span class="line">grad=backward_propagation_dropout(X,Y,cache,keep_prop)</span><br><span class="line"></span><br><span class="line"><span class="comment">#5.更新参数</span></span><br><span class="line">parameters=ru.update_parameters(parameters,grad,learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(i%<span class="number">1000</span>==<span class="number">0</span>):</span><br><span class="line">costs.append(cost)</span><br><span class="line">print(<span class="string">"第"</span>+str(i)+<span class="string">"次，cost:"</span>+str(cost))</span><br><span class="line"></span><br><span class="line"><span class="comment">#绘制曲线</span></span><br><span class="line">plt.plot(costs)</span><br><span class="line">plt.xlabel(<span class="string">"iteration"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"cost"</span>)</span><br><span class="line">plt.title(<span class="string">"learning_rate="</span>+str(learning_rate))</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>结果</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200513110235.png" alt></p><p>训练集<br>Accuracy: 0.9478672985781991<br>测试集<br>Accuracy: 0.915</p><p>分析</p><p>从图中可以看出模型存在过拟合，训练集准确率明显高于测试集很多。</p><p>（1）L2范数</p><p>在损失函数中加入L2参数，使得权重缩减，即减少对一些不重要特征的学习。</p><p><strong>L2范数正则化是在计算损失时加入正则化参数，并在反向传播结果调整权重。</strong></p><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用L2范数进行正则化</span></span><br><span class="line"><span class="comment">#计算损失</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_loss_L2</span><span class="params">(A,Y,parameters,lambd)</span>:</span></span><br><span class="line">m=A.shape[<span class="number">1</span>]</span><br><span class="line">W1=parameters[<span class="string">"W1"</span>]</span><br><span class="line">W2=parameters[<span class="string">"W2"</span>]</span><br><span class="line">W3=parameters[<span class="string">"W3"</span>]</span><br><span class="line"></span><br><span class="line">loss=ru.compute_cost(A,Y)</span><br><span class="line">L2_regularization=(lambd/(<span class="number">2</span>*m))*(np.sum(np.square(W1))+np.sum(np.square(W2))+np.sum(np.square(W3)))</span><br><span class="line">all_loss=loss+L2_regularization</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> all_loss</span><br><span class="line"></span><br><span class="line"><span class="comment">#反向传播过程</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation_L2</span><span class="params">(X,Y,cache,lambd)</span>:</span></span><br><span class="line">Z1,A1,W1,b1,Z2,A2,W2,b2,Z3,A3,W3,b3=cache</span><br><span class="line"></span><br><span class="line">m=X.shape[<span class="number">1</span>]</span><br><span class="line">dZ3=A3-Y</span><br><span class="line">dW3=(<span class="number">1</span>/m)*np.dot(dZ3,A2.T)+(lambd/m)*W3</span><br><span class="line">db3=(<span class="number">1</span>/m)*np.sum(dZ3,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">dA2=np.dot(W3.T,dZ3)</span><br><span class="line">dZ2=np.multiply(dA2,np.int64(A2&gt;<span class="number">0</span>))</span><br><span class="line">dW2=(<span class="number">1</span>/m)*np.dot(dZ2,A1.T)+(lambd/m)*W2</span><br><span class="line">db2=(<span class="number">1</span>/m)*np.sum(dZ2,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">dA1=np.dot(W2.T,dZ2)</span><br><span class="line">dZ1=np.multiply(dA1,np.int64(A1&gt;<span class="number">0</span>))</span><br><span class="line">dW1=(<span class="number">1</span>/m)*np.dot(dZ1,X.T)+(lambd/m)*W1</span><br><span class="line">db1=(<span class="number">1</span>/m)*np.sum(dZ1,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">grad=&#123;<span class="string">"dW1"</span>:dW1,<span class="string">"dW2"</span>:dW2,<span class="string">"dW3"</span>:dW3,<span class="string">"db1"</span>:db1,<span class="string">"db2"</span>:db2,<span class="string">"db3"</span>:db3&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> grad</span><br></pre></td></tr></table></figure><p>结果</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200513110551.png" alt></p><p>训练集<br>Accuracy: 0.9383886255924171<br>测试集<br>Accuracy: 0.93</p><p>分析</p><p>从图中可以看出决策边界变得平滑，训练集和测试集误差相近，减少了模型的过拟合情况</p><p>（b）dropout随机失活</p><p><strong>dropout随机失活方法是在每次训练模型时，按照一定概率随机删除一些节点，即每次都训练一个小型的网络，keep_prop是保留节点的概率。</strong></p><p>但由于随机删除一些节点，在随机失活方法中成本函数没有明确的定义</p><p>1°前向传播</p><p>在前向传播过程中根据阈值keep_prop决定每个节点是否保留：D=np.random.rand(A.shape[0],A.shape[1])&lt;keep_prop</p><p>过滤所有为0的节点，并且按照阈值修补缺失节点后的函数值：A=A * D / keep_prop</p><p>2°反向传播</p><p>根据前向传播中每个节点的保留情况，删除缺失的节点，并计算缺失节点后的梯度值：dA=dA * D / keep_prop</p><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用dropout进行正则化，随机删除一些节点</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#正向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation_dropout</span><span class="params">(X,parameters,keep_prop=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">W1=parameters[<span class="string">"W1"</span>]</span><br><span class="line">W2=parameters[<span class="string">"W2"</span>]</span><br><span class="line">W3=parameters[<span class="string">"W3"</span>]</span><br><span class="line">b1=parameters[<span class="string">"b1"</span>]</span><br><span class="line">b2=parameters[<span class="string">"b2"</span>]</span><br><span class="line">b3=parameters[<span class="string">"b3"</span>]</span><br><span class="line"></span><br><span class="line">Z1=np.dot(W1,X)+b1</span><br><span class="line">A1=ru.relu(Z1)</span><br><span class="line">D1=np.random.rand(A1.shape[<span class="number">0</span>],A1.shape[<span class="number">1</span>])</span><br><span class="line"><span class="comment">#小于keep_prop的设置为1，反之为0</span></span><br><span class="line">D1=D1&lt;keep_prop</span><br><span class="line">A1=A1*D1</span><br><span class="line">A1=A1/keep_prop</span><br><span class="line"></span><br><span class="line">Z2=np.dot(W2,A1)+b2</span><br><span class="line">A2=ru.relu(Z2)</span><br><span class="line">D2=np.random.rand(A2.shape[<span class="number">0</span>],A2.shape[<span class="number">1</span>])</span><br><span class="line">D2=D2&lt;keep_prop</span><br><span class="line">A2=A2*D2</span><br><span class="line">A2=A2/keep_prop</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Z3=np.dot(W3,A2)+b3</span><br><span class="line">A3=ru.sigmoid(Z3)</span><br><span class="line"><span class="comment"># D3=np.random.rand(A3.shape[0],A3.shape[1])</span></span><br><span class="line"><span class="comment"># D3=D3&lt;keep_prop</span></span><br><span class="line"><span class="comment"># A3=A3*D3</span></span><br><span class="line"><span class="comment"># A3=A3/keep_prop</span></span><br><span class="line"></span><br><span class="line">cache=[Z1,D1,A1,W1,b1,Z2,D2,A2,W2,b2,Z3,A3,W3,b3]</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> A3,cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation_dropout</span><span class="params">(X,Y,cache,keep_prop)</span>:</span></span><br><span class="line">Z1,D1,A1,W1,b1,Z2,D2,A2,W2,b2,Z3,A3,W3,b3=cache</span><br><span class="line"></span><br><span class="line">m=X.shape[<span class="number">1</span>]</span><br><span class="line">dZ3=A3-Y</span><br><span class="line">dW3=(<span class="number">1</span>/m)*np.dot(dZ3,A2.T)</span><br><span class="line">db3=(<span class="number">1</span>/m)*np.sum(dZ3,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">dA2=np.dot(W3.T,dZ3)</span><br><span class="line">dA2=dA2*D2</span><br><span class="line">dA2=dA2/keep_prop</span><br><span class="line"></span><br><span class="line">dZ2=np.multiply(dA2,np.int64(A2&gt;<span class="number">0</span>))</span><br><span class="line">dW2=(<span class="number">1</span>/m)*np.dot(dZ2,A1.T)</span><br><span class="line">db2=(<span class="number">1</span>/m)*np.sum(dZ2,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">dA1=np.dot(W2.T,dZ2)</span><br><span class="line">dA1=dA1*D1</span><br><span class="line">dA1=dA1/keep_prop</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dZ1=np.multiply(dA1,np.int64(A1&gt;<span class="number">0</span>))</span><br><span class="line">dW1=(<span class="number">1</span>/m)*np.dot(dZ1,X.T)</span><br><span class="line">db1=(<span class="number">1</span>/m)*np.sum(dZ1,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">grad=&#123;<span class="string">"dW1"</span>:dW1,<span class="string">"dW2"</span>:dW2,<span class="string">"dW3"</span>:dW3,<span class="string">"db1"</span>:db1,<span class="string">"db2"</span>:db2,<span class="string">"db3"</span>:db3&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> grad</span><br></pre></td></tr></table></figure><p>结果</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200513111546.png" alt></p><p>训练集<br>Accuracy: 0.943127962085308<br>测试集<br>Accuracy: 0.935</p><p>分析</p><p>从图中可以看出决策边界变得平滑，训练集和测试集误差相近，减少了模型的过拟合情况。</p><h2 id="梯度检验"><a href="#梯度检验" class="headerlink" title="梯度检验"></a>梯度检验</h2><p>通过双边误差检验反向传播是否很好的实现，主要用于调试</p><p>（1）得到算法中的梯度grads</p><p>（2）计算双边误差gradprox=J_plus(θ+eplison)-J_minus(θ+eplison)/2 * eplison</p><p>（3）比较双边误差和梯度的距离d=||grads-gradprox||2 / (||grads||2+||gradprox||2)</p><p>当距离小于1e-7说明反向传播中梯度正确实现了，反之则没有</p><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#梯度校验</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grandient_check_n</span><span class="params">(parameters,grad,X,Y,eplison=<span class="number">1e-7</span>)</span>:</span></span><br><span class="line">parameters_values=gu.dictionary_to_vector(parameters)</span><br><span class="line">grad_values=gu.gradients_to_vector(grad)</span><br><span class="line">num_iter=parameters_values.shape[<span class="number">0</span>]</span><br><span class="line">J_min=np.zeros((num_iter,<span class="number">1</span>))</span><br><span class="line">J_max=np.zeros((num_iter,<span class="number">1</span>))</span><br><span class="line">gradprox=np.zeros((num_iter,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_iter):</span><br><span class="line">temp_para=np.copy(parameters_values)</span><br><span class="line">temp_para[i][<span class="number">0</span>]=temp_para[i][<span class="number">0</span>]-eplison</span><br><span class="line">J_min[i],cache=forward_propagation_n(X,Y,gu.vector_to_dictionary(temp_para))</span><br><span class="line"></span><br><span class="line">temp_para2=np.copy(parameters_values)</span><br><span class="line">temp_para2[i][<span class="number">0</span>]=temp_para2[i][<span class="number">0</span>]+eplison</span><br><span class="line">J_max[i],cache=forward_propagation_n(X,Y,gu.vector_to_dictionary(temp_para))</span><br><span class="line"></span><br><span class="line">gradprox[i]=J_max[i]-J_min[i]/<span class="number">2</span>*eplison</span><br><span class="line"></span><br><span class="line"><span class="comment">#向量之间的距离</span></span><br><span class="line">denum=np.liang.norm(grad_values)+np.liang.norm(gradprox)</span><br><span class="line">nenum=np.liang.norm(grad_values-gradprox)</span><br><span class="line">difference=nenum/denum</span><br><span class="line"><span class="keyword">if</span> difference&lt;<span class="number">1e-7</span>:</span><br><span class="line">print(<span class="string">"梯度检验正确"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">print(<span class="string">"梯度检验有误"</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;训练集、验证集、测试集选取&quot;&gt;&lt;a href=&quot;#训练集、验证集、测试集选取&quot; class=&quot;headerlink&quot; title=&quot;训练集、验证集、测试集选取&quot;&gt;&lt;/a&gt;训练集、验证集、测试集选取&lt;/h2&gt;&lt;p&gt;训练集用于根据算法训练不同的模型&lt;/p&gt;&lt;p&gt;验证集用于验证不同的算法中哪一种更有效&lt;/p&gt;&lt;p&gt;测试集用于评估算法的性能&lt;/p&gt;&lt;p&gt;对于大数据（如一万条数据以上），训练集，验证集，测试集的比列为98%，1%，1%，训练集占绝大部分。&lt;/p&gt;&lt;h2 id=&quot;提高训练速度&quot;&gt;&lt;a href=&quot;#提高训练速度&quot; class=&quot;headerlink&quot; title=&quot;提高训练速度&quot;&gt;&lt;/a&gt;提高训练速度&lt;/h2&gt;&lt;h3 id=&quot;归一化输入特征&quot;&gt;&lt;a href=&quot;#归一化输入特征&quot; class=&quot;headerlink&quot; title=&quot;归一化输入特征&quot;&gt;&lt;/a&gt;归一化输入特征&lt;/h3&gt;&lt;p&gt;未归一化的输入特征得到的损失函数是比较狭长的，而且在反向传播中使用梯度下降时，步长需要很小，否则无法找到最小值。&lt;/p&gt;
    
    </summary>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>神经网络和深度学习——搭建识别图片的多层神经网络</title>
    <link href="https://www.xiapf.com/blogs/NNet4/"/>
    <id>https://www.xiapf.com/blogs/NNet4/</id>
    <published>2020-05-09T08:22:07.000Z</published>
    <updated>2020-05-09T09:05:31.485Z</updated>
    
    <content type="html"><![CDATA[<h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p>利用猫咪图片搭建能够识别猫图片的神经网络，并使用新的图片测试网络效果</p><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>神经网络的搭建主要分为两部分：前向传播和反向传播</p><p>（1）前向传播</p><p>根据输入的参数及上一层的输入得出每层的函数值，通过每层不同的激活函数得到输出，最终输出结果根据交叉熵模型得出前向传播中的损失。</p><p>（2）反向传播</p><p>根据损失值，从最后一层向前得到每层的梯度，根据梯度下降原则更新参数，向着损失减小的方向变化。</p><a id="more"></a><p>前向传播和反向传播不断循环，当损失达到最小或者趋于稳定时，网络模型则训练结束</p><p>具体原理流程图参照下图：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200509151835.png" alt></p><h2 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h2><p>为了使得网络不陷入当数值增大，激活函数梯度减小而导致学习速度减慢，所有的隐藏层选择使用Relu激活函数，由于是识别分类问题，输出层使用sigmoid函数。</p><p>设搭建L层网络，则输入层是第0层，隐藏层为第1层 ~ 第L-2层，输出层为第L-1层</p><p>搭建多层神经网络步骤</p><p>（1）初始化各层参数（W，b）</p><p>需要初始化参数的层数为第1层 ~ 第L-1层，即参数索引为1 ~ L-1</p><p>设有L层网络，则第l层的权重W和偏置量b的维度满足：</p><p>shape(W)=(n[l],n[l-1])</p><p>shpe(b)=(n[l],1)</p><p>使用正态分布数据初始化每层神经元权重，为防止梯度爆炸，将每层的权重数值除以前一层权重数值的二分之一次方，偏置量初始化为0，将初始化参数进行保存使用。</p><p>（2）前向传播</p><p>第1层 ~ 第L-2层，每层计算前向函数的线性值，并传入relu函数中计算每层的激活值，将前一层的输入和当前层的参数保存为linear_cahe,将每层的函数线性值保存为active_cache，最终得到每一层的cache和每层的输出值。第L-1层，同理，调用sigmoid激活函数。</p><p>最终，将缓存cache保存在caches中，在反向传播中的计算梯度时使用。</p><p>（3）计算损失</p><p>每次迭代，根据最后输出层数据和实际数据，代入交叉熵模型公式中得出损失值，并每百次输出。</p><p>（4）反向传播</p><p>从第L-1层向前更新参数梯度：</p><p>第L-1层，计算输出层的导数，即dAL（交叉熵公式对输出值的导数），根据最后一层的导数，及最后一层的cache（保存了前一层的前向函数线性值，本层的参数，本层的前向函数线性值），根据sigmoid激活函数求导得出dZ，根据导数公式dW=(1/m) * multiply（dZ,dA_pre），db=(1/m) * sum（dZ）,得到参数的梯度，将参数梯度保存在grad中，索引为L（为了和参数索引对应）。</p><p>第L-2层 ~ 第1层，同理，根据后一层的dA值（从保存的grads中取，当前grads索引为i+1，后一层的梯度索引为i+2），当前层的cache，得到梯度保存在索引为i+1的梯度中。</p><p>（5）更新参数</p><p>根据得出的梯度，按照梯度下降公式，即损失减小的方向变化。</p><p>（5）模型预测</p><p>将输入的图像向量和类别，以及之前训练的各层参数及已经训练好的模型传入预测函数中，根据参数和输入的图像向量得出最终的输出A，得出每个输入图像向量对应的输出值，并根据sigmoid特性进行分类，大于0.5的则判定为猫，反之不是猫，将预测的类别和真实类别比较得出最终正确率。</p><p>效果</p><p>搭建5层神经网络，隐藏层神经元个数分别为20，7，5，迭代2500次，学习率为0.0075，最终随着迭代次数的增加损失变化为：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200509160309.png" alt></p><p>训练集正确率为：99.52%</p><p>测试集正确率为：78%</p><p>当输入新的测试图像时，由于神经网络中处理的图像都转换为向量处理，这里将读入的图像使用PIL中的Image模块读取，并将其转换为RGB类型的图片，使用array将图像转换为和训练图片一样大小的矩阵。</p><p>当输入图片是猫的图像时：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200509160906.jpg" alt></p><p>最终得出：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200509161327.png" alt></p><p>当输入图片不是猫的图像时：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200509161223.jpg" alt></p><p>最终得出：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200509161153.png" alt></p><p>可见网络正确识别了猫和非猫的图片</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#搭建一个两层网络和深层网络</span></span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> dnn_utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> testCases <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">from</span> lr_utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> misc,ndimage</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#1.导入数据</span></span><br><span class="line">train_x_original,train_y_original,test_x_original,test_y_original,classes=load_dataset()</span><br><span class="line"></span><br><span class="line"><span class="comment">#把向量竖起来排列</span></span><br><span class="line">train_x=train_x_original.reshape(train_x_original.shape[<span class="number">0</span>],<span class="number">-1</span>).T</span><br><span class="line">test_x=test_x_original.reshape(test_x_original.shape[<span class="number">0</span>],<span class="number">-1</span>).T</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#归一化</span></span><br><span class="line">train_x=train_x/<span class="number">255</span></span><br><span class="line">print()</span><br><span class="line">test_x=test_x/<span class="number">255</span></span><br><span class="line">train_y=train_y_original</span><br><span class="line">test_y=test_y_original</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.参数初始化</span></span><br><span class="line"><span class="comment">#2.0两层网络参数初始化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inital_parameter</span><span class="params">(n_x,n_h,n_y)</span>:</span></span><br><span class="line">W1=random.randn(n_h,n_x)*<span class="number">0.01</span></span><br><span class="line">b1=zeros((n_h,<span class="number">1</span>))</span><br><span class="line">W2=random.randn(n_y,n_h)*<span class="number">0.01</span></span><br><span class="line">b2=zeros((n_y,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">parameter=&#123;<span class="string">"W1"</span>:W1,<span class="string">"b1"</span>:b1,<span class="string">"W2"</span>:W2,<span class="string">"b2"</span>:b2&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> parameter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#2.1深层网络参数初始化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inital_parameter_deep</span><span class="params">(layer_dims)</span>:</span></span><br><span class="line">np.random.seed(<span class="number">3</span>)</span><br><span class="line">parameter=&#123;&#125;</span><br><span class="line">L=len(layer_dims)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,L):</span><br><span class="line">parameter[<span class="string">"W"</span>+str(i)]=random.randn(layer_dims[i],layer_dims[i<span class="number">-1</span>])/sqrt(layer_dims[i<span class="number">-1</span>])</span><br><span class="line">parameter[<span class="string">"b"</span>+str(i)]=zeros((layer_dims[i],<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> parameter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#3.前向传播</span></span><br><span class="line"><span class="comment">#3.1线性值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_forward</span><span class="params">(A,W,b)</span>:</span></span><br><span class="line"></span><br><span class="line">Z=dot(W,A)+b</span><br><span class="line">cache=(A,W,b)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> Z,cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#3.2激活值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward</span><span class="params">(A_pre,W,b,active)</span>:</span></span><br><span class="line"><span class="keyword">if</span> active==<span class="string">"sigmoid"</span>:</span><br><span class="line">Z,linear_cache=linear_forward(A_pre,W,b)</span><br><span class="line">A,active_cache=sigmoid(Z)</span><br><span class="line"></span><br><span class="line"><span class="keyword">elif</span> active==<span class="string">"relu"</span>:</span><br><span class="line">Z,linear_cache=linear_forward(A_pre,W,b)</span><br><span class="line">A,active_cache=relu(Z)</span><br><span class="line"></span><br><span class="line">cache=(linear_cache,active_cache)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> A,cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#深层网络的前向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_forward</span><span class="params">(A,parameter)</span>:</span></span><br><span class="line"><span class="comment">#前L-1个用relu激活，最后一个用sigmoid激活</span></span><br><span class="line">L=len(parameter)//<span class="number">2</span></span><br><span class="line">caches=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,L):</span><br><span class="line">A_pre=A</span><br><span class="line">A,cache=linear_activation_forward(A_pre,parameter[<span class="string">"W"</span>+str(i)],parameter[<span class="string">"b"</span>+str(i)],<span class="string">"relu"</span>)</span><br><span class="line">caches.append(cache)</span><br><span class="line"></span><br><span class="line">A_pre=A</span><br><span class="line">A,cache=linear_activation_forward(A_pre,parameter[<span class="string">"W"</span>+str(L)],parameter[<span class="string">"b"</span>+str(L)],<span class="string">"sigmoid"</span>)</span><br><span class="line">caches.append(cache)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> A,caches</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#4.计算损失</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_loss</span><span class="params">(Yhat,Y)</span>:</span></span><br><span class="line">m=shape(Y)[<span class="number">1</span>]</span><br><span class="line">cost=-sum(multiply(log(Yhat),Y)+multiply(log(<span class="number">1</span>-Yhat),(<span class="number">1</span>-Y)))</span><br><span class="line">cost=(<span class="number">1</span>/m)*cost</span><br><span class="line"><span class="comment"># AL=Yhat</span></span><br><span class="line"><span class="comment"># cost = -sum(multiply(np.log(Yhat),Y) + multiply(np.log(1 - Yhat), 1 - Y)) / m</span></span><br><span class="line"><span class="comment"># cost = squeeze(cost)</span></span><br><span class="line"><span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#5.反向传播</span></span><br><span class="line"><span class="comment">#5.2线性值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_backward</span><span class="params">(dZ,linear_cache)</span>:</span></span><br><span class="line">A_pre,W,b=linear_cache</span><br><span class="line">m=shape(A_pre)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">dW=(<span class="number">1</span>/m)*dot(dZ,A_pre.T)</span><br><span class="line">db=(<span class="number">1</span>/m)*sum(dZ,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">dA_pre=dot(W.T,dZ)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> dA_pre,dW,db</span><br><span class="line"></span><br><span class="line"><span class="comment">#5.1激活值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_backward</span><span class="params">(dA,cache,active)</span>:</span></span><br><span class="line">linear_cache,active_cache=cache</span><br><span class="line"><span class="keyword">if</span> active==<span class="string">"sigmoid"</span>:</span><br><span class="line">dZ=sigmoid_backward(dA,active_cache) <span class="comment">#activecache 保存z</span></span><br><span class="line">dA_pre,dW,db=linear_backward(dZ,linear_cache) <span class="comment">#linear_cache保存A,w,b</span></span><br><span class="line"><span class="keyword">elif</span> active==<span class="string">"relu"</span>:</span><br><span class="line">dZ=relu_backward(dA,active_cache)</span><br><span class="line">dA_pre,dW,db=linear_backward(dZ,linear_cache)</span><br><span class="line"><span class="keyword">return</span> dA_pre,dW,db</span><br><span class="line"></span><br><span class="line"><span class="comment">#深层网络的反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward</span><span class="params">(AL,Y,caches)</span>:</span></span><br><span class="line">L=len(caches)</span><br><span class="line"></span><br><span class="line">grads=&#123;&#125;</span><br><span class="line"></span><br><span class="line">dAL=-(divide(Y,AL)-divide(<span class="number">1</span>-Y,<span class="number">1</span>-AL))</span><br><span class="line">dA_pre,dW,db=linear_activation_backward(dAL,caches[L<span class="number">-1</span>],<span class="string">"sigmoid"</span>)</span><br><span class="line">grads[<span class="string">"dA"</span>+str(L)]=dA_pre</span><br><span class="line">grads[<span class="string">"dW"</span>+str(L)]=dW</span><br><span class="line">grads[<span class="string">"db"</span>+str(L)]=db</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> reversed(range(L<span class="number">-1</span>)): <span class="comment">#注意反转</span></span><br><span class="line">dA_pre,dW,db=linear_activation_backward(grads[<span class="string">"dA"</span>+str(i+<span class="number">2</span>)],caches[i],<span class="string">"relu"</span>)</span><br><span class="line">grads[<span class="string">"dA"</span>+str(i+<span class="number">1</span>)]=dA_pre</span><br><span class="line">grads[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]=dW</span><br><span class="line">grads[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]=db</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> grads</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#6.更新参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameter</span><span class="params">(parameter,grads,learning_rate)</span>:</span></span><br><span class="line">L=len(parameter)//<span class="number">2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(L):</span><br><span class="line">parameter[<span class="string">"W"</span>+str(i+<span class="number">1</span>)]=parameter[<span class="string">"W"</span>+str(i+<span class="number">1</span>)]-learning_rate*grads[<span class="string">"dW"</span>+str(i+<span class="number">1</span>)]</span><br><span class="line">parameter[<span class="string">"b"</span>+str(i+<span class="number">1</span>)]=parameter[<span class="string">"b"</span>+str(i+<span class="number">1</span>)]-learning_rate*grads[<span class="string">"db"</span>+str(i+<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> parameter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#7.搭建模型</span></span><br><span class="line"><span class="comment">#两层神经网络</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">two_layer_model</span><span class="params">(X,Y,layer_dims,learning_rate=<span class="number">0.0075</span>,num_iter=<span class="number">3000</span>)</span>:</span></span><br><span class="line"><span class="comment">#初始化参数</span></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">grads=&#123;&#125;</span><br><span class="line">costs=[]</span><br><span class="line">(n_x,n_h,n_y)=layer_dims</span><br><span class="line">parameter=inital_parameter(n_x,n_h,n_y)</span><br><span class="line">W1=parameter[<span class="string">"W1"</span>]</span><br><span class="line">b1=parameter[<span class="string">"b1"</span>]</span><br><span class="line">W2=parameter[<span class="string">"W2"</span>]</span><br><span class="line">b2=parameter[<span class="string">"b2"</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_iter):</span><br><span class="line"><span class="comment">#前向传播</span></span><br><span class="line">A1,cache1=linear_activation_forward(X,W1,b1,<span class="string">"relu"</span>)</span><br><span class="line">A2,cache2=linear_activation_forward(A1,W2,b2,<span class="string">"sigmoid"</span>)</span><br><span class="line"><span class="comment">#计算损失</span></span><br><span class="line">cost=compute_loss(A2,Y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#反向传播</span></span><br><span class="line">dAL=-(divide(Y,A2)-divide(<span class="number">1</span>-Y,<span class="number">1</span>-A2))</span><br><span class="line">dA_pre1,dW2,db2=linear_activation_backward(dAL,cache2,<span class="string">"sigmoid"</span>)</span><br><span class="line">dA_pre0,dW1,db1=linear_activation_backward(dA_pre1,cache1,<span class="string">"relu"</span>)</span><br><span class="line"></span><br><span class="line">grads[<span class="string">"dW1"</span>]=dW1</span><br><span class="line">grads[<span class="string">"dW2"</span>]=dW2</span><br><span class="line">grads[<span class="string">"db1"</span>]=db1</span><br><span class="line">grads[<span class="string">"db2"</span>]=db2</span><br><span class="line"></span><br><span class="line"><span class="comment">#更新参数</span></span><br><span class="line">parameter=update_parameter(parameter,grads,learning_rate)</span><br><span class="line"></span><br><span class="line">W1=parameter[<span class="string">"W1"</span>]</span><br><span class="line">b1=parameter[<span class="string">"b1"</span>]</span><br><span class="line">W2=parameter[<span class="string">"W2"</span>]</span><br><span class="line">b2=parameter[<span class="string">"b2"</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># print("第 "+str(i)+" 次，cost: "+str(cost))</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(i%<span class="number">100</span>==<span class="number">0</span>):</span><br><span class="line">costs.append(cost)</span><br><span class="line">print(<span class="string">"第 "</span>+str(i)+<span class="string">" 次，cost: "</span>+str(cost))</span><br><span class="line"></span><br><span class="line">plt.plot(costs)</span><br><span class="line">plt.xlabel(<span class="string">"iter"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"cost"</span>)</span><br><span class="line">plt.title(<span class="string">"learning_rate:"</span>+str(learning_rate))</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> parameter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#深层神经网络</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_layer_model</span><span class="params">(X,Y,layer_dims,learning_rate,num_iter)</span>:</span></span><br><span class="line"><span class="comment">#初始化参数</span></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">costs=[]</span><br><span class="line">parameter=inital_parameter_deep(layer_dims)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_iter):</span><br><span class="line"><span class="comment">#前向传播</span></span><br><span class="line">A,caches=L_model_forward(X,parameter)</span><br><span class="line"></span><br><span class="line"><span class="comment">#计算损失</span></span><br><span class="line">cost=compute_loss(A,Y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#反向传播</span></span><br><span class="line">grads=L_model_backward(A,Y,caches)</span><br><span class="line"></span><br><span class="line"><span class="comment">#更新参数</span></span><br><span class="line">parameter=update_parameter(parameter,grads,learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(i%<span class="number">100</span>==<span class="number">0</span>):</span><br><span class="line">costs.append(cost)</span><br><span class="line">print(<span class="string">"第 "</span>+str(i)+<span class="string">" 次，cost: "</span>+str(cost))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.plot(costs)</span><br><span class="line">plt.xlabel(<span class="string">"iter"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"cost"</span>)</span><br><span class="line">plt.title(<span class="string">"learning_rate:"</span>+str(learning_rate))</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> parameter</span><br><span class="line"><span class="comment">#7.预测</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(X,Y,parameter)</span>:</span></span><br><span class="line"><span class="comment">#按照参数走一次前向传播，查看预测值和实际值是否相同</span></span><br><span class="line">A,caches=L_model_forward(X,parameter)</span><br><span class="line"></span><br><span class="line">m=shape(A)[<span class="number">1</span>]</span><br><span class="line">n=shape(X)[<span class="number">1</span>]</span><br><span class="line">p=zeros((<span class="number">1</span>,n))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line"><span class="keyword">if</span>(A[<span class="number">0</span>,i]&gt;<span class="number">0.5</span>):</span><br><span class="line">p[<span class="number">0</span>,i]=<span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">p[<span class="number">0</span>,i]=<span class="number">0</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"正确率为："</span>+str(sum(Y==p)/n))</span><br><span class="line"><span class="keyword">return</span> p</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#查看分类错误的图片</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_mislabeled_image</span><span class="params">(X,y,p)</span>:</span></span><br><span class="line"><span class="comment">#y:实际类别  #p:预测类别</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#得到预测错误的下标</span></span><br><span class="line">a=p+y</span><br><span class="line">print(a)</span><br><span class="line">error_index=asarray(where(a==<span class="number">1</span>))</span><br><span class="line">print(error_index)</span><br><span class="line">num_range=len(error_index[<span class="number">0</span>])</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>]=(<span class="number">40.0</span>,<span class="number">40.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#打印错误图片</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_range):</span><br><span class="line">index=error_index[<span class="number">1</span>][i]</span><br><span class="line">plt.subplot(<span class="number">2</span>,num_range,i+<span class="number">1</span>)</span><br><span class="line">plt.imshow(X[:,index].reshape(<span class="number">64</span>,<span class="number">64</span>,<span class="number">3</span>))</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#8.两层网络测试</span></span><br><span class="line"><span class="comment"># n_x=shape(train_x)[0]</span></span><br><span class="line"><span class="comment"># n_h=7</span></span><br><span class="line"><span class="comment"># n_y=shape(train_y)[0]</span></span><br><span class="line"><span class="comment"># layer_dims = [12288,7,1] </span></span><br><span class="line"><span class="comment"># parameter=two_layer_model(train_x,train_y,layer_dims,learning_rate=0.0075,num_iter=2500)</span></span><br><span class="line"><span class="comment"># p_train=predict(train_x,train_y,parameter)</span></span><br><span class="line"><span class="comment"># p_test=predict(test_x,test_y,parameter)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#9.多层网络测试</span></span><br><span class="line">layer_dims = [<span class="number">12288</span>,<span class="number">20</span>,<span class="number">7</span>,<span class="number">5</span>,<span class="number">1</span>] </span><br><span class="line">parameter=L_layer_model(train_x,train_y,layer_dims,learning_rate=<span class="number">0.0075</span>,num_iter=<span class="number">2500</span>)</span><br><span class="line">p_train=predict(train_x,train_y,parameter)</span><br><span class="line">p_test=predict(test_x,test_y,parameter)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#10.使用自己图像测试</span></span><br><span class="line"><span class="comment">#导入图像处理模块</span></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="comment">#输入自己图像的数据特征及标签</span></span><br><span class="line">my_image=<span class="string">"my_image.jpg"</span></span><br><span class="line">my_image_y=[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#将自己的图像转换为向量，以便网络处理</span></span><br><span class="line">num_px=<span class="number">64</span></span><br><span class="line">image=Image.open(my_image).convert(<span class="string">'RGB'</span>).resize((num_px,num_px))</span><br><span class="line">my_input=array(image).reshape((num_px*num_px*<span class="number">3</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#输入网络进行测试</span></span><br><span class="line">my_predict=predict(my_input,my_image_y,parameter)</span><br><span class="line">plt.imshow(image)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y = "</span> + str(squeeze(my_predict)) + <span class="string">", your L-layer model predicts a \""</span> + classes[int(squeeze(my_predict)),].decode(<span class="string">"utf-8"</span>) +  <span class="string">"\" picture."</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;描述&quot;&gt;&lt;a href=&quot;#描述&quot; class=&quot;headerlink&quot; title=&quot;描述&quot;&gt;&lt;/a&gt;描述&lt;/h2&gt;&lt;p&gt;利用猫咪图片搭建能够识别猫图片的神经网络，并使用新的图片测试网络效果&lt;/p&gt;&lt;h2 id=&quot;原理&quot;&gt;&lt;a href=&quot;#原理&quot; class=&quot;headerlink&quot; title=&quot;原理&quot;&gt;&lt;/a&gt;原理&lt;/h2&gt;&lt;p&gt;神经网络的搭建主要分为两部分：前向传播和反向传播&lt;/p&gt;&lt;p&gt;（1）前向传播&lt;/p&gt;&lt;p&gt;根据输入的参数及上一层的输入得出每层的函数值，通过每层不同的激活函数得到输出，最终输出结果根据交叉熵模型得出前向传播中的损失。&lt;/p&gt;&lt;p&gt;（2）反向传播&lt;/p&gt;&lt;p&gt;根据损失值，从最后一层向前得到每层的梯度，根据梯度下降原则更新参数，向着损失减小的方向变化。&lt;/p&gt;
    
    </summary>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="深层网络" scheme="https://www.xiapf.com/tags/%E6%B7%B1%E5%B1%82%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>react+springboot前后台分离项目</title>
    <link href="https://www.xiapf.com/blogs/react-springboot/"/>
    <id>https://www.xiapf.com/blogs/react-springboot/</id>
    <published>2020-05-09T06:41:07.000Z</published>
    <updated>2020-05-09T06:59:08.836Z</updated>
    
    <content type="html"><![CDATA[<h2 id="实验描述"><a href="#实验描述" class="headerlink" title="实验描述"></a>实验描述</h2><p>训练部分：使用面向对象的方法实现两种机器学习算法BP、SVM，并将两种算法进行封装，通过接口调用两种算法，利用已知1-5月气象数据训练2个气温预测模型，并预测后续3日气温值。</p><p>测试部分：利用训练好的模型，当输入最大温度，最小温度，湿度和光照时，可以得到预测得气温。</p><h2 id="通过dubbo-zookeeper实现通过接口远程调用算法模型"><a href="#通过dubbo-zookeeper实现通过接口远程调用算法模型" class="headerlink" title="通过dubbo+zookeeper实现通过接口远程调用算法模型"></a>通过dubbo+zookeeper实现通过接口远程调用算法模型</h2><a id="more"></a><p>实现功能：生产者实现两种机器学习算法BP,SVM，消费者通过远程调用生产者的接口，能够用自己的数据训练BP和SVM的模型，并根据训练的模型，使用数据进行测试。</p><h3 id="安装配置zookeeper"><a href="#安装配置zookeeper" class="headerlink" title="安装配置zookeeper"></a>安装配置zookeeper</h3><p>在官网<a href="http://zookeeper.apache.org/" target="_blank" rel="external nofollow noopener noreferrer">http://zookeeper.apache.org/</a>选择一个版本下载，我下载的是zookeeper-3.5.7，解压并修改配置文件，配置文件中dataDir，我使用的默认路径/bmp/zookeeper。切换至bin目录下，使用命令./zkServer.sh start，启动服务</p><p>出现如下字样说明服务启动成功：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /Users/Local/Resource/zookeeper/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br></pre></td></tr></table></figure><h3 id="新建生产者pc2（providerCenter）项目"><a href="#新建生产者pc2（providerCenter）项目" class="headerlink" title="新建生产者pc2（providerCenter）项目"></a>新建生产者pc2（providerCenter）项目</h3><p>（1）提供接口</p><p>在provider项目中新建client模块，其中定义IUseModels接口，该接口就是要注册到zookeeper中心。供远程消费者调用。</p><p>IUseModels接口：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">IUseModels</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  Map&lt;String, <span class="keyword">double</span>[][]&gt; bpTrain(<span class="keyword">double</span>[][] inData,<span class="keyword">double</span>[][] target, <span class="keyword">int</span> inputSize, <span class="keyword">int</span> hideSize, <span class="keyword">int</span> outputSize, <span class="keyword">int</span> iter);</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">double</span> <span class="title">bpTest</span><span class="params">(<span class="keyword">double</span>[] inData,Map&lt;String, <span class="keyword">double</span>[][]&gt; map)</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">double</span>[] svmTrain(<span class="keyword">double</span>[][] inData,<span class="keyword">double</span>[] y,<span class="keyword">int</span> iter,<span class="keyword">double</span> parameter);</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">double</span> <span class="title">svmTest</span><span class="params">(<span class="keyword">double</span>[] inData,<span class="keyword">double</span>[] w)</span></span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接口定义了bp算法的训练模型部分、测试数据功能，svm算法的训练模型部分、测试数据功能。</p><p>接口的具体实现在主模块的UseModelsImpl中：</p><p>bpTrain根据用户输入的训练数据，输入层、隐藏层、输出层神经元个数，训练次数进行模型训练，并返回训练的模型。</p><p>bpTest根据用户输入的测试数据，之前训练的模型得出输出的结果返回。</p><p>svmTrain根据用户输入的训练数据，损失函数的参数，训练次数进行模型训练，并返回训练的模型</p><p>svmTest根据用户输入的测试数据，之前训练的模型得出输出的结果返回。</p><p>（2）引用服务</p><p>生产者配置文件中添加服务引用，需要配置dubbo依赖和zookeeper依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.dubbo<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>dubbo<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.6<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.zookeeper<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>zookeeper<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.5.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>（3）生产者在zookeeper上注册</p><p>在项目内新建配置文件provider.xml，指定当前服务的名字，指定注册中心的地址，这里默认注册中心在本地，指定通信规则是dubbo，通信端口为7070，声明需要暴露的服务接口，同时指向容器中的接口实现对象。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">dubbo:application</span> <span class="attr">name</span> = <span class="string">"dubbo_provider"</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dubbo:protocol</span> <span class="attr">name</span> = <span class="string">"dubbo"</span> <span class="attr">port</span>=<span class="string">"7070"</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dubbo:registry</span> <span class="attr">address</span> = <span class="string">"zookeeper://127.0.0.1:2181"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dubbo:service</span> <span class="attr">interface</span> = <span class="string">"com.cl.client.IUseModels"</span> <span class="attr">ref</span> = <span class="string">"UseModelsImpl"</span>/&gt;</span></span><br></pre></td></tr></table></figure><p>使用zkui-master查看注册中心上已注册的生产者节点如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507202451.png" alt></p><p>（4）暴露接口</p><p>使用maven中的install命令将接口单独进行编译，并把接口编译的jar包安装至本地maven仓库。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507202521.png" alt></p><p>查看本地maven仓库，发现接口成功上传至仓库，以供消费者使用。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507202529.png" alt></p><h3 id="新建消费者oc2（consumerCenter）项目"><a href="#新建消费者oc2（consumerCenter）项目" class="headerlink" title="新建消费者oc2（consumerCenter）项目"></a>新建消费者oc2（consumerCenter）项目</h3><p>（1）消费者在zookeeper上调用</p><p>在项目内新建配置文件consumer.xml，指定当前服务的名字，指定注册中心的地址，指定需要调用的服务的接口名称，以便调用。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dubbo:application</span> <span class="attr">name</span>=<span class="string">"demo_consumer"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dubbo:registry</span> <span class="attr">address</span>=<span class="string">"zookeeper://127.0.0.1:2181"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dubbo:reference</span> <span class="attr">id</span>=<span class="string">"UseModelsImpl"</span> <span class="attr">interface</span>=<span class="string">"com.cl.client.IUseModels"</span> <span class="attr">timeout</span>=<span class="string">"3000"</span> <span class="attr">check</span>=<span class="string">"false"</span>/&gt;</span></span><br></pre></td></tr></table></figure><p>（2）导入生产者接口jar包</p><p>在pom.xml添加对应包的依赖，maven自动下载本地仓库中的jar包</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.cl<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.0.1-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>（3）实现远程调用</p><p>启动生产者端后，生产者端通过dubbo服务将自己的url地址注册到zookeeper上，消费者端通过调用dubbo服务，根据配置的注册中心以及接口名称到zookeeper上查找对应的生产者端中的接口实现。</p><p>消费者端将IUseModels接口实例化，编写方法testbp用于训练bp模型并进行气温预测：导入本地数据，并调用IUseModels中的bpTrain方法，消费者端找到生产者端中的端口实现，传入训练的样本，输入层、隐藏层、输出层的结构进行模型训练，训练结束后，得到完成的模型，并根据测试数据返回预测得后三天气温。</p><p>同理，消费者端的testsvm方法调用IUseModels中的svmTrain方法得到预测得气温。</p><p>bp和svm模型训练后进行保存，消费者端的calcTemp方法通过以训练的模型和前台传入的测试数据预测六月某天的气温。</p><p>使用zkui-master查看注册中心上已注册的消费者节点如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507204011.png" alt></p><p>可以看到在bubbo节点下的consumers消费者节点中包含了当前提供服务的生产者提供的接口名称和接口方法。</p><h2 id="使用springboot-react-antd实现项目前后台分离并发布训练的模型及测试的结果"><a href="#使用springboot-react-antd实现项目前后台分离并发布训练的模型及测试的结果" class="headerlink" title="使用springboot+react+antd实现项目前后台分离并发布训练的模型及测试的结果"></a>使用springboot+react+antd实现项目前后台分离并发布训练的模型及测试的结果</h2><h3 id="使用react-antd进行前台搭建"><a href="#使用react-antd进行前台搭建" class="headerlink" title="使用react+antd进行前台搭建"></a>使用react+antd进行前台搭建</h3><p>（1）创建前台页面工程</p><p>使用react脚手架create-react-app创建工程test。</p><p>步骤参照官网<a href="https://ant.design/docs/react/use-with-create-react-app-cn" target="_blank" rel="external nofollow noopener noreferrer">https://ant.design/docs/react/use-with-create-react-app-cn</a></p><p>安装脚手架：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install -g create-react-app</span><br></pre></td></tr></table></figure><p>使用脚手架创建项目：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create react-app test</span><br></pre></td></tr></table></figure><p>引入antd：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn add antd</span><br></pre></td></tr></table></figure><p>修改package.json,让项目可以自定义配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&quot;scripts&quot;: &#123;</span><br><span class="line">  &quot;start&quot;: &quot;react-app-rewired start&quot;,</span><br><span class="line">  &quot;build&quot;: &quot;react-app-rewired build&quot;,</span><br><span class="line">  &quot;test&quot;: &quot;react-app-rewired test&quot;,</span><br><span class="line">  &quot;eject&quot;: &quot;react-scripts eject&quot;</span><br><span class="line"> &#125;,</span><br></pre></td></tr></table></figure><p>在项目根目录创建一个 config-overrides.js 用于修改默认配置</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> &#123; override, fixBabelImports &#125; = <span class="built_in">require</span>(<span class="string">'customize-cra'</span>);</span><br><span class="line"><span class="built_in">module</span>.exports = override(</span><br><span class="line"> fixBabelImports(<span class="string">'import'</span>, &#123;</span><br><span class="line">  libraryName: <span class="string">'antd'</span>,</span><br><span class="line">  libraryDirectory: <span class="string">'es'</span>,</span><br><span class="line">  style: <span class="string">'css'</span>,</span><br><span class="line"> &#125;),</span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>利用yarn安装项目需要的依赖：yarn install</p><p>进入项目，并启动：cd antd-demo  yarn start</p><p>项目默认端口是3000，当没有程序占用时，直接打开浏览器，显示如下，说明运行成功。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507204025.png" alt></p><p>当端口被占用时，会出现如下界面，选择y,即用其他端口打开即可</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507204034.png" alt></p><p>如果需要修改默认端口号，可以找到项目下/node_modules/react-scripts/scripts/start.js，找到DEFAULT_PORT，修改端口即可。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507204201.png" alt></p><p>（2）react脚手架创建项目结构</p><p>创建的项目默认目录如下</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507204210.png" alt></p><p>其中node_mudules文件夹内是安装的所有依赖模块，package.json里包含项目基本设置，如名称、版本号等，最主要的是src文件夹，里面包含的js文件是页面展示的内容,css文件是样式文件，项目默认index.js是项目的入口。index.js内容如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507204219.png" alt></p><p>使用 React 构建的应用通常只有单一的根 DOM 节点，通过‘root’也可判断index是代码入口，其中ReactDOM.render后面是显示在页面上的内容，这里是直接显示App页面，App页面中简单设置了文字和图像，即项目初始运行时显示的内容。</p><p>（3）创建页面</p><p>在src文件夹下新建js文件，通过class组件建立显示的页面内容，class组件格式如下：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">App</span> <span class="keyword">extends</span> <span class="title">React</span>.<span class="title">Component</span></span>&#123;</span><br><span class="line">  内容<span class="number">1</span></span><br><span class="line">  render() &#123;</span><br><span class="line">    <span class="keyword">return</span>(</span><br><span class="line">    内容<span class="number">2</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  &#125;</span><br><span class="line"><span class="keyword">export</span> <span class="keyword">default</span> App;</span><br></pre></td></tr></table></figure><p>内容1部分设置需要用到的对象，构造函数，触发的事件，与后台数据交互等内容。</p><p>内容2部分是设置页面显示的部分，这里直接使用antd组件。</p><p>1°内容2部分:使用antd组件创建页面内容</p><p>进入antd官网<a href="https://ant.design/components/button-cn/" target="_blank" rel="external nofollow noopener noreferrer">https://ant.design/components/button-cn/</a> ，选择组件，以button组件为例：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507204229.png" alt></p><p>第一步，选择需要的组件button</p><p>第二步，在“代码演示”部分找到自己需要的样式</p><p>第三步，显示当前组件的代码</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507204238.png" alt></p><p>如图所示，展示了四种按钮代码，分别对应上面四种不同样式的按钮，选择第一种按钮，赋值代码，并把需要的库文件导入，选择方框内的代码复制进App.js文件中</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507204248.png" alt></p><p>页面显示如下</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507204257.png" alt></p><p>antd组件进行按需选择，项目中构建了如下文件：</p><p>index.js:项目入口，显示Demo.js页面。</p><p>Demo.js:主页面，设置为侧边布局，当页面横向空间有限时，侧边导航可以收起，当点击导航上对应菜单跳转对应页面（首页页面show.js、训练页面Train.js,测试页面Res.js,关于页面About.js）。</p><p>show.js: 首页页面，项目默认显示首页页面，由Card组件和文字组成。</p><p>Train.js: 训练页面，实现获取后台数据功能，当点击按钮时，后台训练对应算法模型，并将结果传回前台进行显示。</p><p>Res.js: 测试页面，以表单形式实现通过前台提交数据，后台接收数据并计算，最终将结果返回前台的功能。</p><p>About.js:关于页面，显示项目及作者信息。</p><p>最终搭建的项目页面如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507204304.png" alt></p><p>（4）前台页面转发</p><p>项目主页面Demo.js通过点击侧边导航栏，能够跳转对应页面，主要是使用react-router-dom,从react-router-dom中导入路由Router,Demo页面使用路由组件渲染，使用link组件实现至路由跳转。</p><p>第一步，将需要跳转的导航菜单上设置Link组件，设置需要跳转的页面<link to="/">首页</p><p>第二步，由页面路由设置连接需要跳转的页面<route exact path="/" component="{show}">，component后面就是跳转的页面内容,这里exact path设置系统默认进入的页面。</route></p><p>转发前台页面核心代码：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;Router&gt;</span><br><span class="line">   &lt;Menu theme=<span class="string">"dark"</span> defaultSelectedKeys=&#123;[<span class="string">'0'</span>]&#125; mode=<span class="string">"inline"</span>&gt;</span><br><span class="line">        &lt;Menu.Item key=<span class="string">"0"</span>&gt;</span><br><span class="line">            &lt;Link to=<span class="string">"/"</span>&gt;首页&lt;<span class="regexp">/Link&gt;</span></span><br><span class="line"><span class="regexp">         &lt;/</span>Menu.Item&gt;</span><br><span class="line">         &lt;Menu.Item key=<span class="string">"1"</span>&gt;</span><br><span class="line">             &lt;Link to=<span class="string">"/Train"</span>&gt;训练&lt;<span class="regexp">/Link&gt;</span></span><br><span class="line"><span class="regexp">          &lt;/</span>Menu.Item&gt;</span><br><span class="line">   &lt;<span class="regexp">/Menu&gt;</span></span><br><span class="line"><span class="regexp">   &lt;Route exact path="/</span><span class="string">" component=&#123;show&#125; /&gt;</span></span><br><span class="line"><span class="string">   &lt;Route  path="</span>/Train<span class="string">" component=&#123;Train&#125; /&gt;</span></span><br><span class="line"><span class="string">&lt;/Router&gt;</span></span><br></pre></td></tr></table></figure><h3 id="前台和后台数据交互"><a href="#前台和后台数据交互" class="headerlink" title="前台和后台数据交互"></a>前台和后台数据交互</h3><p>后台开发使用springboot框架,开发软件为IntelliJ IDEA。</p><p>（1）跨域问题</p><p>跨域是指当客户端向服务器发起一个网络请求，url会有包含三个主要信息：协议（protocol），域名（host），端口号（port）。当三部分都和服务器相同的情况下，属于同源。但是只要有一个不同,就属于构成了跨域调用。会受到同源策略的限制。同源策略限制从一个源加载的文档或脚本如何与来自另一个源的资源进行交互。springboot项目默认端口是8080，react默认端口是3000,前后端端口不一样，所以有跨域问题。</p><p>这里采用代理方式进行处理：（这里springboot项目的端口为9091），在react项目中的package.json中设置转发”proxy”:”<a href="http://localhost:9091&quot;" target="_blank" rel="external nofollow noopener noreferrer">http://localhost:9091&quot;</a> ,将对前端的所有请求全部转发到9091端口即后台项目的入口中，这样就解决了跨域问题。</p><p>（2）数据交互</p><p>这里的项目是采用前后端分离的思想，前端从后端剥离，形成一个前端工程，前端只利用Json来和后端进行交互，后端不返回页面，只返回Json数据。前后端之间完全通过public API约定。前后端之间使用fetch进行数据交互。</p><p>1°前台接收后台数据</p><p>后台：springboot设置control层，返回json数据。这里的testbp调用生产者端的bp算法，得到模型后使用数据测试，最后返回“后三天的温度”的json数据。</p><p>后台核心代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="meta">@RequestMapping</span>(<span class="string">"/submit"</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SubmitController</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> IUseModels useModels;</span><br><span class="line">    <span class="keyword">public</span> Map&lt;String,<span class="keyword">double</span>[][]&gt; mapRes;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@RequestMapping</span>(<span class="string">"/testbp"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">testbp</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">//导入数据</span></span><br><span class="line">        ..</span><br><span class="line">        <span class="comment">//训练网络</span></span><br><span class="line">        mapRes= useModels.bpTrain(inData,target,inputSize,hideSize,outputSize,iter);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//测试数据</span></span><br><span class="line">        <span class="keyword">double</span>[] x = <span class="keyword">new</span> <span class="keyword">double</span>[]&#123;<span class="number">14.7</span>,<span class="number">0</span>,<span class="number">11.7</span>,<span class="number">22.4</span>&#125;;</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">double</span> res=useModels.bpTest(x,mapRes);</span><br><span class="line">    <span class="comment">//设置输出精度</span></span><br><span class="line">        DecimalFormat df = <span class="keyword">new</span> DecimalFormat( <span class="string">"0.00 "</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"后三天的温度为："</span>+df.format(res)+<span class="string">" , "</span>+df.format(res2)+<span class="string">" , "</span>+df.format(res3);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>前台：前台就需要将请求转发到<a href="http://localhost:9091/submit/testbp" target="_blank" rel="external nofollow noopener noreferrer">http://localhost:9091/submit/testbp</a> 上，因为上面设置了跨域转发，所以使用fetch(‘/submit/testbp’)即可以将请求转发至后台，当获取到json数据后将其转换成text（then(response =&gt; response.text())），并读取其中的message（then(message =&gt; {this.setState({message: message});});）,即后台传回的后三天的温度数据。</p><p>项目中设置的是，当点击按钮时，向后台获取数据，因此将fetch写在事件中，当点击按钮时，触发该事件，这里使用button组件自带的onclick方法绑定事件。</p><p>需要注意的是在 JavaScript 中，class 的方法默认不会绑定 this。如果忘记绑定 this.handleClick 并把它传入了 onClick，当调用这个函数的时候 this 的值为 undefined，所以需要在构造函数中绑定事件。</p><p>注：如果需要进入页面就从后台获取数据，则需要在生命周期函数componentDidMount()中添加fetch</p><p>前台获取后台数据过程：</p><p>第一步，编写事件，通过fetch获取json数据，并通过setState设置文本值</p><p>第二步，将时间传入button的onclick属性中，当点击按钮时进行触发</p><p>第三步，在构造函数中绑定触发的事件</p><p>第四步，将当前state中的文本值显示在页面上</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507204315.png" alt></p><p>注：state是组件内部进行传递数据，具有从上到下的特点，在构造函数中进行初始化，当获得后台数据时，通过setState设置新的state的值，最终在页面上进行显示。（state不能直接赋值，只能通过setState设置新的值）</p><p>前台运行结果：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507204328.png" alt></p><p>前台通过点击训练模型按钮，后台分别通过接口调用生产者端的算法，进行模型训练，得到模型后，根据输入的测试数据得到后三天的气温情况，并将数据传送给前台页面进行显示。</p><p>后台将传输的数据打印在控制台</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507204347.png" alt></p><p>2°后台数据接收前台数据，并计算返回结果给前台</p><p>后台：springboot设置controller层，通过requestbody接收前台传入的json数据，依次获取数据，并调用训练的模型进行测试，并将结果封装成json数据传回后台。</p><p>后台核心代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="meta">@RequestMapping</span>(<span class="string">"/submit"</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SubmitController</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> IUseModels useModels;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@RequestMapping</span>(<span class="string">"/calc"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">calcTemp</span><span class="params">(@RequestBody String str)</span> </span>&#123;</span><br><span class="line">        Map&lt;String, String&gt; map = JSON.parseObject(str, Map<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        <span class="comment">//解析json数据并传入x中</span></span><br><span class="line">  …</span><br><span class="line">        Map result = <span class="keyword">new</span> HashMap();</span><br><span class="line">        <span class="keyword">double</span> res=useModels.svmTest(x,weight);</span><br><span class="line">        <span class="keyword">double</span> res2=useModels.bpTest(x,mapRes);</span><br><span class="line">        DecimalFormat df = <span class="keyword">new</span> DecimalFormat( <span class="string">"0.00 "</span>);</span><br><span class="line">        result.put(<span class="string">"result"</span>, df.format(res));</span><br><span class="line">        result.put(<span class="string">"result2"</span>,df.format(res2));</span><br><span class="line">        <span class="keyword">return</span> JSON.toJSONString(result);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>前台：</p><p>a）提交前台数据：前台页面主要设置了一个表单，每个输入框都设置name属性，利用input框的onchange属性绑定handleInputChange事件，当输入框内数据发生变化时即触发handleInputChange事件，通过setState将当前变化值作为输入框对应name新value值。</p><p>当点击提交按钮时，通过onClick属性绑定calc事件，每次点击提交，则触发该事件，事件中定义需要提交的对象，将需要提交的数据封装到该对象中，并通过fetch转发到对应后台页面，为方便后台处理，将提交的内容转换为字符串提交。</p><p>b）接收后台数据：通过fetch提交结束后，接收返回的json数据，并通过setState获取数值并赋值给相应组件。</p><p>这里所有事件都需要在构造函数中进行绑定，否则触发事件时会找不到。因为react内部通过state设置组件的显示的内容，所以需要在构造函数中设置state的初值。由于算法中处理的是double数据，当前台提交double数据至后台时，系统会自动转换为BigDecimal，不方便后续处理，所以这里将各个输入框的初始设置为非数值类型，通过字符串类型传回后台再转换为double类型处理。</p><p>前台运行结果：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507204339.png" alt></p><p>后台将接受的前台数据打印在控制台中</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507204355.png" alt></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>项目中采用dubbo+zookeeper方式实现面向接口编程，在生产者端通过面向对象的编程实现了两种机器学习算法：BP和SVM，设置接口可以调用者两种算法，并通过dubbo协议将生产者端的接口实现注册至zookeeper端，以供消费者使用；在消费者端通过dubbo协议，根据生产者的接口方法在zookeeper中找到接口实现方法，并进行调用，实现模型训练。</p><p>采用springboot+react+antd实现前后端分离，前端采用react+antd编写项目，使用react脚手架创建前端页面工程，利用antd组件方便快速的搭建页面显示内容，后端采用springboot框架，编写训练模型和测试数据的功能，两者之间的跨域问题通过代理转发解决，同时前端和后端之间通过fetch传输json数据，前后端之间完全通过public API约定。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;实验描述&quot;&gt;&lt;a href=&quot;#实验描述&quot; class=&quot;headerlink&quot; title=&quot;实验描述&quot;&gt;&lt;/a&gt;实验描述&lt;/h2&gt;&lt;p&gt;训练部分：使用面向对象的方法实现两种机器学习算法BP、SVM，并将两种算法进行封装，通过接口调用两种算法，利用已知1-5月气象数据训练2个气温预测模型，并预测后续3日气温值。&lt;/p&gt;&lt;p&gt;测试部分：利用训练好的模型，当输入最大温度，最小温度，湿度和光照时，可以得到预测得气温。&lt;/p&gt;&lt;h2 id=&quot;通过dubbo-zookeeper实现通过接口远程调用算法模型&quot;&gt;&lt;a href=&quot;#通过dubbo-zookeeper实现通过接口远程调用算法模型&quot; class=&quot;headerlink&quot; title=&quot;通过dubbo+zookeeper实现通过接口远程调用算法模型&quot;&gt;&lt;/a&gt;通过dubbo+zookeeper实现通过接口远程调用算法模型&lt;/h2&gt;
    
    </summary>
    
    
      <category term="java" scheme="https://www.xiapf.com/categories/java/"/>
    
    
      <category term="java" scheme="https://www.xiapf.com/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>用zookeeper+dubbo实现rpc远程调用</title>
    <link href="https://www.xiapf.com/blogs/dubboZkNote/"/>
    <id>https://www.xiapf.com/blogs/dubboZkNote/</id>
    <published>2020-05-07T13:46:12.000Z</published>
    <updated>2020-05-09T06:40:36.725Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是zookeeper"><a href="#什么是zookeeper" class="headerlink" title="什么是zookeeper"></a>什么是zookeeper</h2><p>zookeeper是一个分布式服务框架，是Apache Hadoop 的一个子项目，它主要是用来解决分布式应用中经常遇到的一些数据管理问题，如：统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等。简单来说zookeeper是文件系统加上监听通知机制.</p><p>zookeeper维护着文件系统的数据结构，里面包含了很多目录节点，例如NameService、GroupMember等，这些子目录想可以增加删除以及存储数据。客户端注册监听关心的目录节点，当目录节点发生变化，zookeeper会通知客户端。</p><a id="more"></a><p>zookeeper的功能比较多，这里主要用到的是分布式应用配置管理功能。当程序是分布式部署在多台机器上，如果要改变程序的配置文件，需要逐台机器去修改，这里就需要用到zookeeper，将配置放到zookeeper上去，保存其目录节点中，然后客户端对这个目录节点进行监听，一旦配置信息发生变化，客户端就会收到 zookeeper 的通知，然后从 zookeeper 获取新的配置信息。</p><h2 id="什么是dubbo"><a href="#什么是dubbo" class="headerlink" title="什么是dubbo"></a>什么是dubbo</h2><p>dubbo是阿里巴巴于2011年10月正式开源的一个由Java语言编写的分布式服务框架。它是一个提供高性能和透明化的远程服务调用方案和基于服务框架展开的完整SOA服务治理方案。</p><p>dubbo的架构如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507205533.png" alt></p><p>这个过程类似生产者-消费者模型。在此基础上加上了注册中心和监控中心，用于管理提供方提供的url和管理整个过程。</p><p>整个过程就是先启动容器，加载，运行服务提供者。生产者在启动时，在注册中心发布注册自己提供的服务。消费者在启动时，在注册中心订阅自己所需的服务。</p><h2 id="zookeeper-dubbo实现rpc框架"><a href="#zookeeper-dubbo实现rpc框架" class="headerlink" title="zookeeper+dubbo实现rpc框架"></a>zookeeper+dubbo实现rpc框架</h2><p>zookeeper是类似文件系统的数据结果，支持修改删除同时还可以进行变更推送操作，以服务名和类型作为节点路径，符合dubbo订阅和通知的需求,因此此适合做dubbo服务的注册中心.</p><p>zookeeper上的服务节点设计是树形结构，dubbo会在zookeeper上创建一个根节点，下一层的子节点代表dubbo的一个服务，服务的左节点是生产者的根节点，右节点是消费者的根节点，。</p><p>当zookeeper启动时，生产者在服务节点下创建一个子节点，将自己的url地址注册到该节点。此时消费者会读取并订阅生产者根节点下的所有子节点，解析出所有生产者的url地址来作为该服务地址列表，然后发起正常调用，同时在消费者根节点下创建一个子节点写入自己的url地址，代表了使用生产者服务的消费者。监控中心指向此时的服务获取所有生产者和消费者的url地址，并监听其子节点的变化进行计数。</p><p>rpc是远程过程调用，满足分布式系统架构中不同的系统之间的远程通信和相互调用。以zookeeper服务为基础,使用dubbo监视和调度各个服务，生产者在zookeeper上注册服务，消费者去zookeeper上调用服务，以此实现rpc框架。</p><h2 id="实验过程"><a href="#实验过程" class="headerlink" title="实验过程"></a>实验过程</h2><p>项目实现功能：生产者实现方法能够在页面上显示自己喜欢看的书：武侠小说，消费者通过远程调用生产者的接口，在页面上查看生产者喜欢看的书籍。</p><h3 id="安装配置zookeeper"><a href="#安装配置zookeeper" class="headerlink" title="安装配置zookeeper"></a>安装配置zookeeper</h3><p>在<a href="http://zookeeper.apache.org/" target="_blank" rel="external nofollow noopener noreferrer">官网</a>选择一个版本下载，我下载的是zookeeper-3.5.7，解压并修改配置文件，配置文件中dataDir我我使用的默认路径/bmp/zookeeper。切换至bin目录下，使用命令./zkServer.sh start，启动服务</p><p>出现如下字样说明服务启动成功：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /Users/Local/Resource/zookeeper/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br></pre></td></tr></table></figure><h3 id="新建生产者provider项目"><a href="#新建生产者provider项目" class="headerlink" title="新建生产者provider项目"></a>新建生产者provider项目</h3><p>（1）提供接口</p><p>在provider项目中新建client模块，其中定义IBook接口，该接口就是要注册到zookeeper中心。供远程消费者调用。</p><p>IBook接口：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">IBook</span> </span>&#123;</span><br><span class="line">    <span class="function">String <span class="title">getBookName</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br><span class="line">接口方法的具体实现，方法内返回生产者喜欢看的书籍名称：</span><br><span class="line"><span class="meta">@Service</span>(<span class="string">"book"</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BookImp</span> <span class="keyword">implements</span> <span class="title">IBook</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getBookName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"武侠小说"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>服务器端口设置为：server.port=8082</p><p>（2）引用服务</p><p>生产者配置文件中添加服务引用，需要配置dubbo依赖和zookeeper依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--dubbo依赖--&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>dubbo-spring-boot-starter<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.2.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba.spring.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>dubbo-spring-boot-starter<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">       <span class="comment">&lt;!--zkclient依赖--&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.101tec<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>zkclient<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.10<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>（3）生产者在zookeeper上注册</p><p>在项目内新建配置文件provider.xml，指定当前服务的名字，指定注册中心的地址，这里默认注册中心在本地，指定通信规则是dubbo，通信端口为20880，声明需要暴露的服务接口，同时指向容器中的接口实现对象。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dubbo:application</span> <span class="attr">name</span>=<span class="string">"user-provider"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dubbo:registry</span> <span class="attr">address</span>=<span class="string">"zookeeper://localhost:2181"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dubbo:protocol</span> <span class="attr">name</span>=<span class="string">"dubbo"</span> <span class="attr">port</span>=<span class="string">"20880"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dubbo:service</span> <span class="attr">ref</span>=<span class="string">"book"</span> <span class="attr">interface</span>=<span class="string">"com.cl.client.inter.IBook"</span>/&gt;</span></span><br></pre></td></tr></table></figure><p>使用zkui-master查看注册中心上已注册的生产者节点如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507205643.png" alt></p><p>可以看到在bubbo节点下的providers生产者节点中包含了当前提供服务的节点的url地址、接口名称和接口内方法。</p><p>在本地对provider项目测试，输入<a href="http://localhost:8082/provider/sayBook：" target="_blank" rel="external nofollow noopener noreferrer">http://localhost:8082/provider/sayBook：</a></p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507205656.png" alt></p><p>测试结果显示正确在页面上显示了书籍的名称，说明provider项目运行结果无误。</p><p>测试结束后，单独将接口进行编译，导出对应jar包，名称为client.jar。</p><h3 id="新建消费者consumer项目"><a href="#新建消费者consumer项目" class="headerlink" title="新建消费者consumer项目"></a>新建消费者consumer项目</h3><p>（1）消费者在zookeeper上调用</p><p>在项目内新建配置文件consumer.xml，指定当前服务的名字，指定注册中心的地址，指定需要调用的服务的接口名称，以便调用。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dubbo:application</span> <span class="attr">name</span>=<span class="string">"user-consumer"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dubbo:registry</span> <span class="attr">address</span>=<span class="string">"zookeeper://localhost:2181"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dubbo:reference</span> <span class="attr">id</span>=<span class="string">"book"</span> <span class="attr">interface</span>=<span class="string">"com.cl.client.inter.IBook"</span>/&gt;</span></span><br></pre></td></tr></table></figure><p>（2）实现远程调用</p><p>将client.jar包导入consumer项目中，同时将IBook接口实例化，编写测试代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="meta">@RequestMapping</span>(<span class="string">"/consumer"</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ConsumerController</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> IBook book;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@RequestMapping</span>(<span class="string">"/getBook"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getBook</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"生产者喜欢的书是："</span>+book.getBookName();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>先启动provider端，然后在启动consumer端，打开网页，显示如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200507205705.png" alt></p><p>启动后，provider端通过dubbo服务将自己的url地址注册到zookeeper上，consumer端通过调用dubbo服务，根据配置的注册中心以及接口名称到zookeeper上查找对应的provieder端中的接口实现，从而实现了远程调用。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;什么是zookeeper&quot;&gt;&lt;a href=&quot;#什么是zookeeper&quot; class=&quot;headerlink&quot; title=&quot;什么是zookeeper&quot;&gt;&lt;/a&gt;什么是zookeeper&lt;/h2&gt;&lt;p&gt;zookeeper是一个分布式服务框架，是Apache Hadoop 的一个子项目，它主要是用来解决分布式应用中经常遇到的一些数据管理问题，如：统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等。简单来说zookeeper是文件系统加上监听通知机制.&lt;/p&gt;&lt;p&gt;zookeeper维护着文件系统的数据结构，里面包含了很多目录节点，例如NameService、GroupMember等，这些子目录想可以增加删除以及存储数据。客户端注册监听关心的目录节点，当目录节点发生变化，zookeeper会通知客户端。&lt;/p&gt;
    
    </summary>
    
    
      <category term="java" scheme="https://www.xiapf.com/categories/java/"/>
    
    
      <category term="java" scheme="https://www.xiapf.com/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>字节码增强技术-javassist</title>
    <link href="https://www.xiapf.com/blogs/javassist/"/>
    <id>https://www.xiapf.com/blogs/javassist/</id>
    <published>2020-04-28T12:38:02.000Z</published>
    <updated>2020-05-07T13:43:10.156Z</updated>
    
    <content type="html"><![CDATA[<p>javassist动态修改字节码</p><p>直接上代码，修改Hello.say方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//ClassPool classPool = ClassPool.getDefault();</span></span><br><span class="line">ClassPool classPool = <span class="keyword">new</span> ClassPool(<span class="keyword">true</span>);</span><br><span class="line">CtClass ctClass = classPool.get(<span class="string">"com.xpf.study.javassit.Hello"</span>);</span><br><span class="line"><span class="comment">//CtClass ctClass = classPool.get(aClass.getName());</span></span><br><span class="line">CtMethod ctMethod = ctClass.getDeclaredMethod(<span class="string">"say"</span>);</span><br><span class="line">ctMethod.setBody(<span class="string">"System.out.println(\"nihao:\" + $1);"</span>);</span><br><span class="line"><span class="comment">//ctClass.writeFile("./aaaa/");</span></span><br><span class="line">Object instance = ctClass.toClass().newInstance();</span><br><span class="line">Method say = instance.getClass().getDeclaredMethod(<span class="string">"say"</span>, String<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">say.setAccessible(<span class="keyword">true</span>);</span><br><span class="line">say.invoke(instance, <span class="string">"java2"</span>);</span><br></pre></td></tr></table></figure><a id="more"></a><p>注意：ctClass.toClass()，ctClass.writeFile等操作后，会冻结字节码，不在允许修改字节码，所以要解冻，可以使用ctClass.defrost();</p><p>如果在使用Hello之前，jvm中已经加载了Hello</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Hello hello = new Hello();</span><br><span class="line">//修改字节码。。。。。</span><br></pre></td></tr></table></figure><p>会报错，因为同一个类加载器不能同时加载两个相同的class</p><p>所以，做如下修改，增加一个加载器，从另一个加载器中加载</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//ClassPool classPool = ClassPool.getDefault();</span></span><br><span class="line">ClassPool classPool = <span class="keyword">new</span> ClassPool(<span class="keyword">true</span>);</span><br><span class="line">Loader loader = <span class="keyword">new</span> Loader(classPool);</span><br><span class="line">Class&lt;?&gt; clazz = loader.loadClass(<span class="string">"com.xpf.study.javassit.Hello"</span>);</span><br><span class="line">Object instance3 = clazz.newInstance();</span><br><span class="line">Method say3 = instance3.getClass().getDeclaredMethod(<span class="string">"say"</span>, String<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">say3.setAccessible(<span class="keyword">true</span>);</span><br><span class="line">say3.invoke(instance3, <span class="string">"java"</span>);</span><br></pre></td></tr></table></figure><p>但是，注意，此时loader获取的class不再是CtClass类型，而是普通的Class类型，所以不再可以使用javassis的api</p><p>同时原来的Hello代码不变。此种方式取出来的instance虽然表面是Hello类型，但是却不 instance of Hello</p><p>另一个变通方法，修改class名称</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CtClass ctClass2 = classPool.getAndRename(<span class="string">"com.xpf.study.javassit.Hello"</span>, <span class="string">"com.xpf.study.javassit.HelloCopy"</span>);</span><br></pre></td></tr></table></figure><p>此时，可以返回类型还是CtClass，可以自由使用相应api</p><p>注意，使用此方式获取CtClass后，如果增加方法，jdk和其他jvm中已经加载的类库不能直接使用，需要引用，或者使用全路径的方式，如List，需要使用java.util.List，因为此加载器中，并没有加载其他类库。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;javassist动态修改字节码&lt;/p&gt;&lt;p&gt;直接上代码，修改Hello.say方法&lt;/p&gt;&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;//ClassPool classPool = ClassPool.getDefault();&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;ClassPool classPool = &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; ClassPool(&lt;span class=&quot;keyword&quot;&gt;true&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;CtClass ctClass = classPool.get(&lt;span class=&quot;string&quot;&gt;&quot;com.xpf.study.javassit.Hello&quot;&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;//CtClass ctClass = classPool.get(aClass.getName());&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;CtMethod ctMethod = ctClass.getDeclaredMethod(&lt;span class=&quot;string&quot;&gt;&quot;say&quot;&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;ctMethod.setBody(&lt;span class=&quot;string&quot;&gt;&quot;System.out.println(\&quot;nihao:\&quot; + $1);&quot;&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;//ctClass.writeFile(&quot;./aaaa/&quot;);&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Object instance = ctClass.toClass().newInstance();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Method say = instance.getClass().getDeclaredMethod(&lt;span class=&quot;string&quot;&gt;&quot;say&quot;&lt;/span&gt;, String&lt;span class=&quot;class&quot;&gt;.&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt;)&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;say.setAccessible(&lt;span class=&quot;keyword&quot;&gt;true&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;say.invoke(instance, &lt;span class=&quot;string&quot;&gt;&quot;java2&quot;&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="java" scheme="https://www.xiapf.com/categories/java/"/>
    
    
      <category term="java" scheme="https://www.xiapf.com/tags/java/"/>
    
      <category term="字节码" scheme="https://www.xiapf.com/tags/%E5%AD%97%E8%8A%82%E7%A0%81/"/>
    
  </entry>
  
  <entry>
    <title>监督学习笔记（五）——利用AdaBoost元算法提高分类性能</title>
    <link href="https://www.xiapf.com/blogs/adaboost/"/>
    <id>https://www.xiapf.com/blogs/adaboost/</id>
    <published>2020-04-26T14:08:08.000Z</published>
    <updated>2020-04-26T14:09:36.470Z</updated>
    
    <content type="html"><![CDATA[<p>元算法或者集成算法是将不同的分类器组合起来，组合的方式可以为：将不同算法组合起来，或者将数据集不同部分给不同的算法，同一种算法在不同设置下进行集成。简言之，元算法是将分类器进行重新集成，提高分类效果，就像生活中决定一件事需要听多方面的意见。</p><h2 id="构建分类器方法"><a href="#构建分类器方法" class="headerlink" title="构建分类器方法"></a>构建分类器方法</h2><h3 id="bagging"><a href="#bagging" class="headerlink" title="bagging"></a>bagging</h3><p>在源数据集中选择S次，得到S个数据集（可能会有重复的），选择某种算法和这S个数据集训练得到S个分类器，最终的结果由各分类器投票结果产生，如随机森林算法。</p><a id="more"></a><h3 id="boosting"><a href="#boosting" class="headerlink" title="boosting"></a>boosting</h3><p>boosting是通过分类器加权求和得到结果，选择n个分类器，初始状态权重相同，每个根据分类器分错的结果调整权重，权重代表上一个分类器迭代的成功度。</p><h2 id="AdaBoost算法"><a href="#AdaBoost算法" class="headerlink" title="AdaBoost算法"></a>AdaBoost算法</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>通过多个弱分类器构造一个强分类器，在训练集上训练一个弱分类器，并得出其错误率，在第二次训练的时候，对第一次分错的样本提高其权重，最终将所有分类器的结果加权得到最终的结果。</p><p>每个分类器的权值调整alpha<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200426164427.png" alt>,其中ε是当前分类器的错误率</p><p>当样本被正确分类，样本的权重被更新为<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200426164638.png" alt></p><p>当样本被错误分类，样本的权重被更新为<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200426164745.png" alt></p><h3 id="构建弱分类器"><a href="#构建弱分类器" class="headerlink" title="构建弱分类器"></a>构建弱分类器</h3><p>单层决策树：这里的基分类器选择了单层决策树，将分类器结果设置为1，当不满足阈值条件时，赋值为-1</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#单层决策树生成函数</span></span><br><span class="line"><span class="comment">#通过阈值比较来划分数据</span></span><br><span class="line"><span class="comment">#输入</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stumpClassify</span><span class="params">(dataMat,dime,threVal,threInq)</span>:</span></span><br><span class="line">m=shape(dataMat)[<span class="number">0</span>] <span class="comment">#m行数据</span></span><br><span class="line">returnMat=ones((m,<span class="number">1</span>))</span><br><span class="line"><span class="comment">#print(dime)</span></span><br><span class="line"><span class="comment">#不满足等式要求的设置为-1</span></span><br><span class="line"><span class="keyword">if</span> threInq==<span class="string">"lt"</span>: </span><br><span class="line">returnMat[dataMat[:,dime]&lt;=threVal]=<span class="number">-1.0</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">returnMat[dataMat[:,dime]&gt;threVal]=<span class="number">-1.0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> returnMat</span><br></pre></td></tr></table></figure><p>找到最佳单层决策树：</p><p>（1）在所有输入的数据特征上进行遍历，根据每个样本的最大值和最小值，以及步长得出阈值，从而得到当前的单层决策树即弱分类器</p><p>（2）每个弱分类器设置一个错误率矩阵，当当前预测的正确，则设置为0，将分类器的权重乘以错误率，得到新的权重矩阵（样本的）</p><p>（3）每次比较当前分类器的样本的错误率，找到最小的错误率即最佳单层决策树分类器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.遍历每个特征，根据最大值和最小值，得出遍历的步长</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">minVal=min(dataMat[:,i])</span><br><span class="line">maxVal=max(dataMat[:,i])</span><br><span class="line">stepSize=(maxVal-minVal)/numSize</span><br><span class="line"><span class="comment">#枚举步数，从而确定阈值</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(numSize+<span class="number">1</span>):</span><br><span class="line"><span class="keyword">for</span> inq <span class="keyword">in</span> [<span class="string">"lt"</span>,<span class="string">"gt"</span>]:</span><br><span class="line">threVal=minVal+stepSize*j</span><br><span class="line"><span class="comment">#2.调用阈值分类结果</span></span><br><span class="line">predictVal=stumpClassify(dataMat,i,threVal,inq)</span><br><span class="line">errorMat=mat(ones((m,<span class="number">1</span>)))</span><br><span class="line">errorMat[predictVal==classMat]=<span class="number">0</span></span><br><span class="line">       </span><br><span class="line"><span class="comment">#3.得出错误分类的向量 adaboost和分类器交互的地方</span></span><br><span class="line">error=D.T*errorMat</span><br><span class="line"></span><br><span class="line"><span class="comment">#因为是“单层决策树”，即只分裂一次就可直接得到分类结果了。所以直接用分类错误的比率来衡量决策树的好坏就可</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#print("split:dim: %d\t,threshVal: %f\t,threshVal ineq: %s\t,error: %f" % (i,threVal,inq,error))</span></span><br><span class="line"><span class="keyword">if</span>(error&lt;minError):</span><br><span class="line">minError=error</span><br><span class="line">bestClass=predictVal.copy()</span><br><span class="line"><span class="comment">#4.得出最佳决策树的信息</span></span><br><span class="line">bestStump[<span class="string">'dim'</span>]=i</span><br><span class="line">bestStump[<span class="string">'thresh'</span>]=threVal</span><br><span class="line">bestStump[<span class="string">'ineq'</span>]=inq</span><br></pre></td></tr></table></figure><h3 id="调整权重得到最终结果"><a href="#调整权重得到最终结果" class="headerlink" title="调整权重得到最终结果"></a>调整权重得到最终结果</h3><p>（1）假设迭代numiter次（numiter个弱分类器），每次迭代都得到一个最佳分类器的错误率，根据错误率，计算出分类器的权重并进行保存</p><p>（2）根据分类器权重调整每个样本的权重</p><p>（3）根据分类器权重得出结果，每次将错误率累积（即用分类器权重得出的结果和真实值比较，当满足条件时则输出结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#初始化数据权重</span></span><br><span class="line">D=mat(ones((m,<span class="number">1</span>))/m)</span><br><span class="line">weakAdaList=[]</span><br><span class="line">adaClassEst=mat(zeros((m,<span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(numIter):</span><br><span class="line"><span class="comment">#1.得出当前弱分类器</span></span><br><span class="line">bestStump,error,bestClass=buildStump(dataMat,classLabels,D)</span><br><span class="line"><span class="comment">#print(D.T)</span></span><br><span class="line"><span class="comment">#2.弱分类器权重</span></span><br><span class="line">alpha=float(<span class="number">0.5</span>*log((<span class="number">1</span>-error)/max(error,<span class="number">1e-16</span>))) <span class="comment">#1e-6是为了防止溢出alpha=1.2*ln(1-error/error)</span></span><br><span class="line">bestStump[<span class="string">'alpha'</span>]=alpha</span><br><span class="line"><span class="comment">#更新弱分类器集合</span></span><br><span class="line">weakAdaList.append(bestStump)</span><br><span class="line"><span class="comment">#print("bestClass:",bestClass.T)</span></span><br><span class="line"><span class="comment">#3.更新数据权重,将现在的和原来的对比</span></span><br><span class="line">expon=multiply(<span class="number">-1</span>*alpha*mat(classLabels).T,bestClass)</span><br><span class="line">D=multiply(D,exp(expon))</span><br><span class="line">D=D/sum(D)</span><br><span class="line"></span><br><span class="line"><span class="comment">#3.更新结果</span></span><br><span class="line"><span class="comment">#print(alpha)</span></span><br><span class="line">adaClassEst+=alpha*bestClass</span><br><span class="line"><span class="comment">#print("adaClassEst:",adaClassEst.T)</span></span><br><span class="line"><span class="comment">#通过二值分类器，判断是否相同，不同的则增加错误率</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#sign(x)函数表示x&gt;0,返回0;x=0,返回0;x&lt;0;返回-1</span></span><br><span class="line"><span class="comment">#sign(aggClassEst) != mat(classLabels) 返回bool矩阵,对应元素相等则为True,不等则为False</span></span><br><span class="line"><span class="comment">#multiply(),False相当于0，True相当于1，表示累加分类错误个数</span></span><br><span class="line"> </span><br><span class="line">adaError=multiply(sign(adaClassEst)!=mat(classLabels).T,ones((m,<span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">errorRate=sum(adaError)/m</span><br><span class="line">print(<span class="string">"total error rate:"</span>,errorRate)</span><br><span class="line"><span class="keyword">if</span>(errorRate==<span class="number">0</span>):</span><br><span class="line"><span class="keyword">break</span>;</span><br></pre></td></tr></table></figure><h2 id="应用案例"><a href="#应用案例" class="headerlink" title="应用案例"></a>应用案例</h2><p>测试基于adaboost的分类</p><p>利用上述构造得到的分类器，应用单层决策树得到没饿分类器的类别值，并进行累加得到最后的结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#利用得出的多个弱分类器对数据进行分类</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adaClassify</span><span class="params">(dataToClass,classfier)</span>:</span></span><br><span class="line">dataToClass=mat(dataToClass)</span><br><span class="line">m,n=shape(dataToClass)</span><br><span class="line"><span class="comment">#累加分类的结果</span></span><br><span class="line">adaClassEst=mat(zeros((m,<span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment">#遍历所有的弱分类器  得到一个类别的估计值</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(classfier)):</span><br><span class="line"><span class="comment">#用单层决策树得出类别估计值</span></span><br><span class="line">classEst=stumpClassify(dataToClass,classfier[i][<span class="string">'dim'</span>],classfier[i][<span class="string">'thresh'</span>],classfier[i][<span class="string">'ineq'</span>])</span><br><span class="line">adaClassEst+=classfier[i][<span class="string">'alpha'</span>]*classEst</span><br><span class="line"><span class="comment">#print(adaClassEst)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> sign(adaClassEst)</span><br></pre></td></tr></table></figure><p>导入病马疝气病数据集</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200426210006.png" alt></p><p>选择10个分类器，将x的值输入，得到预测值，将预测值和实际值相等的设置为0，求和得出错误预测得样本数量为16个。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">weakAdaList=adaboost.adaBoostTrainDS(dataSet,classLabels,<span class="number">10</span>)</span><br><span class="line">test,ctest=adaboost.loadDtaSet(<span class="string">"horseColicTest2.txt"</span>)</span><br><span class="line">predict10=adaboost.adaClassify(test,weakAdaList)</span><br><span class="line"></span><br><span class="line">ctest=mat(ctest).T</span><br><span class="line">m,n=ctest.shape</span><br><span class="line">error=mat(ones((m,n)))</span><br><span class="line">error[predict10==ctest]=<span class="number">0</span></span><br><span class="line"></span><br><span class="line">print(sum(error))</span><br></pre></td></tr></table></figure><p>10个分类器的错误率为</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200426210138.png" alt></p><h2 id="如何衡量分类的性能"><a href="#如何衡量分类的性能" class="headerlink" title="如何衡量分类的性能"></a>如何衡量分类的性能</h2><p>之前的算法都是用错误率来衡量算法，但是不全面，一般衡量分类的性能使用混淆矩阵</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200426211129.png" alt></p><h3 id="正确率和召回率"><a href="#正确率和召回率" class="headerlink" title="正确率和召回率"></a>正确率和召回率</h3><p>正确率：TP/(TP+FP) 代表预测结果为真的占总样本的比例</p><p>召回率：TP/(TP+FN）代表预测结果为真占总样本中所有为真的样本的比例。召回率大的分类算法中，真的样本判错的少</p><p>一般好的分类是正确率高，召回率也高，但是两者存在制约关系</p><h3 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h3><p>ROC曲线横轴代表假阳率：FP/(FP+TN)，纵轴代表真阳率：TP/(TP+FN)，左下角代表把所有样本判断为反例，右上角代表把所有样本判断为真例，较好的分类器集中在左上角，即真阳率高，假阳率低</p><p>（1）划定起始点，确定x轴步长（分类为假的个数），y轴步长（分类为真的个数）</p><p>（2）将分类样本按照预测强度排序，从低到高，从排名最低的阳例开始，（所有排名更低的阳例都被判为反例，排名更高的样例判为正例，-&gt;排名高的样例真阳率高假阳率低，所以从（1，1）开始，先把低强度的样例判断）如果样例为真，对真阳率修改，如果样例为假，对假阳率修改</p><p>（）当遇到真值向下走，即沿着纵轴走，遇到假值向左走，即沿着横轴走，将变动的坐标画下来</p><p>（4）划定总体范围，进行图像显示</p><p>AUC：中间过程对曲线下的面积进行累计，这个给出了分类器的平均性能</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#ROC曲线的绘制</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotROC</span><span class="params">(predStrength,classLabels)</span>:</span></span><br><span class="line"><span class="comment">#光标起始点</span></span><br><span class="line">cur=(<span class="number">1.0</span>,<span class="number">1.0</span>)</span><br><span class="line"><span class="comment">#AUC面积值</span></span><br><span class="line">ySum=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#x轴y轴步长</span></span><br><span class="line">numPos=sum(array(classLabels)==<span class="number">1.0</span>)</span><br><span class="line">ystep=<span class="number">1</span>/numPos</span><br><span class="line">xstep=<span class="number">1</span>/(len(classLabels)-numPos)</span><br><span class="line">sortPred=predStrength.argsort()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#画图</span></span><br><span class="line"><span class="comment">#fig=plt.figure()</span></span><br><span class="line">ax=plt.subplot(<span class="number">111</span>)</span><br><span class="line">print(sortPred)</span><br><span class="line">print(sortPred.tolist())</span><br><span class="line">print(sortPred.tolist()[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#对真值则向下走，对假值向左边走  光标从1，1开始</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> sortPred.tolist()[<span class="number">0</span>]:</span><br><span class="line"><span class="keyword">if</span>(classLabels[i]==<span class="number">1</span>):</span><br><span class="line">detX=<span class="number">0</span></span><br><span class="line">detY=ystep</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">detX=xstep</span><br><span class="line">detY=<span class="number">0</span></span><br><span class="line">ySum+=cur[<span class="number">1</span>]</span><br><span class="line"><span class="comment">#x从哪里变到哪里，y从哪里变到哪里</span></span><br><span class="line">ax.plot((cur[<span class="number">0</span>],cur[<span class="number">0</span>]-detX),(cur[<span class="number">1</span>],cur[<span class="number">1</span>]-detY),<span class="string">"b"</span>)</span><br><span class="line">cur=(cur[<span class="number">0</span>]-detX,cur[<span class="number">1</span>]-detY)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"the average of roc:"</span>+str(ySum*xstep))</span><br><span class="line">ax.plot((<span class="number">0</span>,<span class="number">1</span>),(<span class="number">0</span>,<span class="number">1</span>),<span class="string">"b--"</span>)</span><br><span class="line">plt.title(<span class="string">"roc"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"false postive"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"true postive"</span>)</span><br><span class="line">ax.axis([<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>]) <span class="comment">#x,y轴范围</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>结果如下图</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200426213015.png" alt></p><p>计算平均性能时，每次对高度累加，矩形的宽度是xstep,最终两者相乘得到面积</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200426220255.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;元算法或者集成算法是将不同的分类器组合起来，组合的方式可以为：将不同算法组合起来，或者将数据集不同部分给不同的算法，同一种算法在不同设置下进行集成。简言之，元算法是将分类器进行重新集成，提高分类效果，就像生活中决定一件事需要听多方面的意见。&lt;/p&gt;&lt;h2 id=&quot;构建分类器方法&quot;&gt;&lt;a href=&quot;#构建分类器方法&quot; class=&quot;headerlink&quot; title=&quot;构建分类器方法&quot;&gt;&lt;/a&gt;构建分类器方法&lt;/h2&gt;&lt;h3 id=&quot;bagging&quot;&gt;&lt;a href=&quot;#bagging&quot; class=&quot;headerlink&quot; title=&quot;bagging&quot;&gt;&lt;/a&gt;bagging&lt;/h3&gt;&lt;p&gt;在源数据集中选择S次，得到S个数据集（可能会有重复的），选择某种算法和这S个数据集训练得到S个分类器，最终的结果由各分类器投票结果产生，如随机森林算法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="boost" scheme="https://www.xiapf.com/tags/boost/"/>
    
  </entry>
  
  <entry>
    <title>神经网络和深度学习——第三周编程作业</title>
    <link href="https://www.xiapf.com/blogs/NNet3/"/>
    <id>https://www.xiapf.com/blogs/NNet3/</id>
    <published>2020-04-26T14:07:22.000Z</published>
    <updated>2020-04-27T07:38:02.493Z</updated>
    
    <content type="html"><![CDATA[<p>主要学习构建一个浅层神经网络</p><p>logistic回归可以理解为是一个单层的网络，这里构造一个隐藏层一个输出层的网络，即两层网络（网络层数计算不加入输入层）</p><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>浅层神经网络有输入层，一个隐藏层，一个输出层</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200427142410.png" alt></p><p>约定：ai代表第i层，ai[j]代表第i层第j个神经元</p><h2 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h2><p>网络搭建的时候需要初始化权值和偏置量</p><a id="more"></a><p>（1）权值不能初始化全为0</p><p>全都初始化为0，在第一个隐藏层每个神经元执行相同的运算，即使后面进行梯度下降操作，每个神经元都会与其他神经元的输出一样</p><p>（2）初始化的数据需要是很小的数</p><p>当激活函数选择sigmoid或者tanh的时候，当初始权重取值很大，那么乘上x得到的z也会很大，此时激活函数的值趋近与1或者0，这时候梯度变化很小，会导致算法学习起来很慢</p><h2 id="前向传播和反向传播"><a href="#前向传播和反向传播" class="headerlink" title="前向传播和反向传播"></a>前向传播和反向传播</h2><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200427142845.png" alt></p><p>其中输入x,w[1],b[1]，隐藏层计算z[1],a[1]，输出层计算z[2],a[2]（这里需要根据w[2],b[2]计算）</p><p>黑色箭头代表前向传播，红色箭头代表反向传播</p><p>将X样本全部都竖向排列，即每一列代表一个样本，X是（nx,m)维向量，其中nx代表输入特征的个数，m代表样本的总数量。</p><p>前向传播的公式为：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200427144106.png" alt></p><p>反向传播的为：</p><h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200427144504.png" alt></p><p>其中dz[l]=dA[l]*g^’(dz[l])，又因为dA[l-1]=W[l]T * dz[l]，即dz[l]=W[l+1]T  dz[l+1] * g^’(dz[l])，所以得出了隐藏层中dz[1]</p><h2 id="激活函数的选择"><a href="#激活函数的选择" class="headerlink" title="激活函数的选择"></a>激活函数的选择</h2><p>网络每层的激活函数可以不同，根据每层特点来选择</p><h3 id="不同的激活函数"><a href="#不同的激活函数" class="headerlink" title="不同的激活函数"></a>不同的激活函数</h3><p>（1）sigmoid函数</p><p>当需要输出的是二分类问题时，使用sigmoid函数，最终输出两个值0或者1，当z的值大于0.5输出1，反之输出0。</p><p>一般用在最后的输出层，激活函数的导数为a（1-a）</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200427142719.png" alt></p><p>（2）tanh函数</p><p>该函数用在隐藏层比sigmoid函数好，因为其输出的数据值平均为0。</p><p>一般用在隐藏层，激活函数的导数为1-a**2</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200427145057.png" alt></p><p>（3）ReLu函数</p><p>斜率变化很快，算法学习速度快。</p><p>激活函数的导数为，当z&lt;0，导数为0，反之为1</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200427145121.png" alt></p><h3 id="为什么需要非线性激活函数"><a href="#为什么需要非线性激活函数" class="headerlink" title="为什么需要非线性激活函数"></a>为什么需要非线性激活函数</h3><p>如果用线性激活函数，相当于是把输入重新线性组合了，隐藏层根本没用</p><h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><p>搭建一个浅层网络，对导入的数据进行分类</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>数据集使用planar_utils中的load_planar_dataset，将数据打印出来可知，输入的样本x是2 * 400的向量，每一列代表其坐标，标签列y是0和1</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200427145525.png" alt></p><p>小技巧：将不同标签的点按照不同颜色打印出来在scatter中用cmap=plt.cm.spectral</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(X[<span class="number">0</span>,:],X[<span class="number">1</span>,:],c=squeeze(Y),cmap=plt.cm.Spectral)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>squeeze是将数据变为1维，scatter不能显示二维的数据</p><h3 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h3><p>用sklearn自带的logistic回归测试分类效果</p><p>首先得到分类器，对X,Y进行训练，打印出决策边界，根据得到的预测值和真实值相乘可以得到错误率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#2.测试使用logistic回归的准确度，并画出决策边界</span></span><br><span class="line">clt=linear_model.LogisticRegressionCV()</span><br><span class="line">clt.fit(X.T,Y.T)</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x:clt.predict(x),X,Y)</span><br><span class="line">m=shape(Y)[<span class="number">1</span>]</span><br><span class="line">rate=(dot(Y,clt.predict(X.T))+dot((<span class="number">1</span>-Y),(<span class="number">1</span>-clt.predict(X.T))))/m</span><br><span class="line">print(<span class="string">"准确率为："</span>+str(rate))</span><br></pre></td></tr></table></figure><p>因为是二分类问题，预测值中为1的和真实值为1的相乘，得到判断为1的数据，同理得到判断为0的数据，除以总数得到预测得准确率为47%</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200427145626.png" alt></p><p>附：决策边界</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_decision_boundary</span><span class="params">(model, X, y)</span>:</span></span><br><span class="line">    <span class="comment"># Set min and max values and give it some padding</span></span><br><span class="line">    x_min, x_max = X[<span class="number">0</span>, :].min() - <span class="number">1</span>, X[<span class="number">0</span>, :].max() + <span class="number">1</span></span><br><span class="line">    y_min, y_max = X[<span class="number">1</span>, :].min() - <span class="number">1</span>, X[<span class="number">1</span>, :].max() + <span class="number">1</span></span><br><span class="line">    h = <span class="number">0.01</span></span><br><span class="line">    <span class="comment"># Generate a grid of points with distance h between them</span></span><br><span class="line">    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))</span><br><span class="line">    <span class="comment"># Predict the function value for the whole grid</span></span><br><span class="line">    Z = model(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line">    Z = Z.reshape(xx.shape)</span><br><span class="line">    <span class="comment"># Plot the contour and training examples</span></span><br><span class="line">    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)</span><br><span class="line">    plt.ylabel(<span class="string">'x2'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'x1'</span>)</span><br><span class="line">    plt.scatter(X[<span class="number">0</span>, :], X[<span class="number">1</span>, :], c=np.squeeze(y), cmap=plt.cm.Spectral)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>最终利用logistic得出的分类为</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200427145550.png" alt></p><p>由结果可以看出，logistic分类效果不好，因为给出的数据是线性不可分的，所以分类不理想。</p><p>下面使用神经网络进行分类</p><h3 id="搭建网络结构"><a href="#搭建网络结构" class="headerlink" title="搭建网络结构"></a>搭建网络结构</h3><p>设定输入层，隐藏层，输出层的神经元个数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.初始化网络结构</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initial_layer_size</span><span class="params">(X,Y,n_h)</span>:</span></span><br><span class="line"><span class="comment">#输入样本的特质数量</span></span><br><span class="line">m=shape(X)[<span class="number">0</span>]</span><br><span class="line">n=shape(Y)[<span class="number">0</span>]</span><br><span class="line">n_x=m</span><br><span class="line">n_h=n_h</span><br><span class="line">n_y=n</span><br><span class="line"><span class="keyword">return</span> n_x,n_h,n_y</span><br></pre></td></tr></table></figure><h3 id="初始化参数"><a href="#初始化参数" class="headerlink" title="初始化参数"></a>初始化参数</h3><p>初始化各层的w和b的初始值，这里的w用正态分布随机数乘以一个很小的数来初始化，以此保证训练的速度（很小的数，梯度变化快），这里将每层的w,b作为parameters保存下来，便于前向传播、反向传播、更新参数中使用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#2.初始化参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initial_parameters</span><span class="params">(n_x,n_h,n_y)</span>:</span></span><br><span class="line"><span class="comment">#初始化w1,b1,w2,b2</span></span><br><span class="line"><span class="comment">#初始化成一个很小的数</span></span><br><span class="line">random.seed(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">W1=random.randn(n_h,n_x)*<span class="number">0.01</span></span><br><span class="line">b1=zeros((n_h,<span class="number">1</span>))</span><br><span class="line">W2=random.randn(n_y,n_h)*<span class="number">0.01</span></span><br><span class="line">b2=zeros((n_y,<span class="number">1</span>))</span><br><span class="line">parameters=&#123;<span class="string">"W1"</span>:W1,<span class="string">"W2"</span>:W2,<span class="string">"b1"</span>:b1,<span class="string">"b2"</span>:b2&#125;</span><br><span class="line"><span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><p>根据公式，计算每层Z，A的值，并进行保存在cache中，便于反向传播中计算导数使用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#3.前向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_progation</span><span class="params">(X,parameters)</span>:</span></span><br><span class="line">W1=parameters[<span class="string">"W1"</span>]</span><br><span class="line">W2=parameters[<span class="string">"W2"</span>]</span><br><span class="line">b1=parameters[<span class="string">"b1"</span>]</span><br><span class="line">b2=parameters[<span class="string">"b2"</span>]</span><br><span class="line">Z1=dot(W1,X)+b1</span><br><span class="line">A1=tanh(Z1)</span><br><span class="line">Z2=dot(W2,A1)+b2</span><br><span class="line">A2=sigmoid(Z2)</span><br><span class="line">cache=&#123;<span class="string">"Z1"</span>:Z1,<span class="string">"Z2"</span>:Z2,<span class="string">"A1"</span>:A1,<span class="string">"A2"</span>:A2&#125;</span><br><span class="line"><span class="keyword">return</span> A2,cache</span><br></pre></td></tr></table></figure><h3 id="计算损失"><a href="#计算损失" class="headerlink" title="计算损失"></a>计算损失</h3><p>损失函数和logistic中一样，使用交叉熵函数，这里需要用到最后一层中的输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#4.计算损失函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(Y,yhat)</span>:</span></span><br><span class="line"><span class="comment">#yhat=cache["A2"]</span></span><br><span class="line">m=shape(Y)[<span class="number">1</span>]</span><br><span class="line">multiplyc=multiply(Y,log(yhat))+multiply((<span class="number">1</span>-Y),log(<span class="number">1</span>-yhat))</span><br><span class="line">cost=-sum(multiplyc)/m</span><br><span class="line"><span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><h3 id="反向传播-1"><a href="#反向传播-1" class="headerlink" title="反向传播"></a>反向传播</h3><p>根据公式，得到每层的dz,dw,db，并将梯度变化用grad保存下来，方便更新参数中使用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#5.计算反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">back_progation</span><span class="params">(X,Y,parameters,cache)</span>:</span></span><br><span class="line">m=shape(X)[<span class="number">1</span>]</span><br><span class="line"><span class="comment">#计算梯度</span></span><br><span class="line">A2=cache[<span class="string">"A2"</span>]</span><br><span class="line">Z2=cache[<span class="string">"Z2"</span>]</span><br><span class="line">A1=cache[<span class="string">"A1"</span>]</span><br><span class="line">Z1=cache[<span class="string">"Z1"</span>]</span><br><span class="line">W1=parameters[<span class="string">"W1"</span>]</span><br><span class="line">W2=parameters[<span class="string">"W2"</span>]</span><br><span class="line"></span><br><span class="line">dZ2=A2-Y</span><br><span class="line">dW2=(<span class="number">1</span>/m)*dot(dZ2,A1.T)</span><br><span class="line">db2=(<span class="number">1</span>/m)*sum(dZ2,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">dZ1=multiply(dot(W2.T,dZ2),(<span class="number">1</span>-A1**<span class="number">2</span>))</span><br><span class="line">dW1=(<span class="number">1</span>/m)*dot(dZ1,X.T)</span><br><span class="line">db1=(<span class="number">1</span>/m)*sum(dZ1,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">grads=&#123;<span class="string">"dW1"</span>:dW1,<span class="string">"dW2"</span>:dW2,<span class="string">"db1"</span>:db1,<span class="string">"db2"</span>:db2&#125;</span><br></pre></td></tr></table></figure><p>附：dot是点乘（数字相乘），最终得到是一个数字，multiply是各个位置相乘（矩阵相乘），最终得到一个矩阵</p><h3 id="更新参数"><a href="#更新参数" class="headerlink" title="更新参数"></a>更新参数</h3><p>根据梯度计算每次更新的w,b，并在parameters中保存，每次根据新的w和b重新前向传播计算损失</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#6.更新参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters,grads,alpha)</span>:</span></span><br><span class="line">W1=parameters[<span class="string">"W1"</span>]</span><br><span class="line">W2=parameters[<span class="string">"W2"</span>]</span><br><span class="line">b1=parameters[<span class="string">"b1"</span>]</span><br><span class="line">b2=parameters[<span class="string">"b2"</span>]</span><br><span class="line"></span><br><span class="line">dW1=grads[<span class="string">"dW1"</span>]</span><br><span class="line">dW2=grads[<span class="string">"dW2"</span>]</span><br><span class="line">db1=grads[<span class="string">"db1"</span>]</span><br><span class="line">db2=grads[<span class="string">"db2"</span>]</span><br><span class="line"></span><br><span class="line">W1=W1-alpha*dW1</span><br><span class="line">W2=W2-alpha*dW2</span><br><span class="line">b1=b1-alpha*db1</span><br><span class="line">b2=b2-alpha*db2</span><br><span class="line">parameters=&#123;<span class="string">"W1"</span>:W1,<span class="string">"W2"</span>:W2,<span class="string">"b1"</span>:b1,<span class="string">"b2"</span>:b2&#125;</span><br><span class="line"><span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h3 id="生成模型"><a href="#生成模型" class="headerlink" title="生成模型"></a>生成模型</h3><p>（1）初始化网络结构</p><p>（2）初始化参数</p><p>（3）每次循环中</p><p>1°先正向传播，根据初始化的参数（parameters），得到cache（保存了每层的A,Z）</p><p>°再根据输出层结果得出损失函数</p><p>3°接着，进行反向传播，根据cache得出每层的梯度保存在grad中</p><p>°最后进行参数更新，按照梯度，利用w=w-alpha * dw（b同理）得出更新的参数</p><p>按照以上步骤进行循环，每1000次输出损失，最终得到训练出的模型，即parameters</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#7.模型整合 训练模型，按照次数输出损失函数的值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X,Y,n_h,numIter,alpha)</span>:</span></span><br><span class="line">random.seed(<span class="number">3</span>)</span><br><span class="line">n_x,n_h,n_y=initial_layer_size(X,Y,n_h)</span><br><span class="line">parameters=initial_parameters(n_x,n_h,n_y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(numIter):</span><br><span class="line">A2,cache=forward_progation(X,parameters)</span><br><span class="line">cost=compute_cost(Y,A2)</span><br><span class="line">grads=back_progation(X,Y,parameters,cache)</span><br><span class="line">parameters=update_parameters(parameters,grads,alpha)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(i%<span class="number">1000</span>==<span class="number">0</span>):</span><br><span class="line">print(<span class="string">"第"</span>+str(i)+<span class="string">"次,损失是："</span>+str(cost))</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h4 id="模型预测"><a href="#模型预测" class="headerlink" title="模型预测"></a>模型预测</h4><p>根据得到的模型，进行一次前向传播得到输出层的值，即为预测值，进行四舍五入得到模型预测值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#8.模型预测</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_predict</span><span class="params">(X,parameters)</span>:</span></span><br><span class="line">A2,cache=forward_progation(X,parameters)</span><br><span class="line"><span class="comment"># prediction=cache["A2"]</span></span><br><span class="line"><span class="comment"># print(prediction)</span></span><br><span class="line">prediction=numpy.round(A2)</span><br><span class="line"><span class="keyword">return</span> prediction</span><br></pre></td></tr></table></figure><p>最终，先训练出模型，再对模型预测，得到最终结果，画出决策边界（决策边界中输入的参数是预测函数模型，即将预测得值输入，和真实值比较，从而画出边界）</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200427153130.png" alt></p><p>可以看出效果比logistic好很多，基本都分类正确。</p><p>最终分类正确率为90.5%</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200427153234.png" alt></p><p>注：这里的隐藏层神经元个数选择了4个，可以选择不同的神经元查看效果。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;主要学习构建一个浅层神经网络&lt;/p&gt;&lt;p&gt;logistic回归可以理解为是一个单层的网络，这里构造一个隐藏层一个输出层的网络，即两层网络（网络层数计算不加入输入层）&lt;/p&gt;&lt;h2 id=&quot;网络结构&quot;&gt;&lt;a href=&quot;#网络结构&quot; class=&quot;headerlink&quot; title=&quot;网络结构&quot;&gt;&lt;/a&gt;网络结构&lt;/h2&gt;&lt;p&gt;浅层神经网络有输入层，一个隐藏层，一个输出层&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200427142410.png&quot; alt&gt;&lt;/p&gt;&lt;p&gt;约定：ai代表第i层，ai[j]代表第i层第j个神经元&lt;/p&gt;&lt;h2 id=&quot;参数初始化&quot;&gt;&lt;a href=&quot;#参数初始化&quot; class=&quot;headerlink&quot; title=&quot;参数初始化&quot;&gt;&lt;/a&gt;参数初始化&lt;/h2&gt;&lt;p&gt;网络搭建的时候需要初始化权值和偏置量&lt;/p&gt;
    
    </summary>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="浅层网络" scheme="https://www.xiapf.com/tags/%E6%B5%85%E5%B1%82%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>神经网络和深度学习——第二周编程作业</title>
    <link href="https://www.xiapf.com/blogs/NNet2/"/>
    <id>https://www.xiapf.com/blogs/NNet2/</id>
    <published>2020-04-17T16:03:55.000Z</published>
    <updated>2020-04-17T16:05:50.033Z</updated>
    
    <content type="html"><![CDATA[<h2 id="神经网络中的设定"><a href="#神经网络中的设定" class="headerlink" title="神经网络中的设定"></a>神经网络中的设定</h2><p>设定(x,y)是单独的一个样本，m个训练样本为{ ((x(1),y(1)), ((x(12),y(2)), ….((x(m),y(m))}</p><p>设定输入的样本为</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200417221326.png" alt></p><p>即将X中所有输入的样本都纵向排列（不使用横向排列的方式），每一列代表一条数据，每一行代表一个输入特征，如原输入样本x1中的特征为a,b,c，a,b,c分别对应了数值，输入样本x2中也有这三个特征，但是数值可能不一样，所以X是nx维向量，有m个列（代表m条数据）</p><a id="more"></a><p>Y则将所有样本输出依次排列成m维行向量</p><h2 id="logistic原理"><a href="#logistic原理" class="headerlink" title="logistic原理"></a>logistic原理</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>logistic是用一条直线去拟合数据点，即需要得到y=wx+b这条直线，当已知输入样本为{ ((x(1),y(1)), ((x(12),y(2)), ….((x(m),y(m))}，通过不断调整w和b的值，让预测输出yhat近似等于实际输出y。</p><p>其中w对应每个输入特征，即需要构建的w是个nx维的列向量，则需要得到的直线为y=w.T*X+b</p><h3 id="模型公式"><a href="#模型公式" class="headerlink" title="模型公式"></a>模型公式</h3><p>y=w.T*X+b</p><p>由于logistic是个二分类器，需要分类函数，这里选择sigmoid，即a=sigmoid(z)，其中z=y</p><p>同时，衡量一个算法运行情况使用误差函数，常用的误差函数为均方差函数，即L=1/2(yhat-y)2，但是该函数不是凸函数，logistic中使用梯度下降法，找到最优的w和b，适合用凸函数，只有一个最优解，均方差函数有多个局部最优解，不适合该求解方法，这里选择的损失函数为L=-(ylogyhat+(1-y)log(1-yhat))，这是训练一个样本时算法的表现，要衡量全部样本的表现需要用到成本函数J=1/m*∑L=-1/m∑-(ylogyhat+(1-y)log(1-yhat))，即通过J的值判断找到的w,b是否合适。J是衡量算法好坏的依据</p><p>综上，logistic模型公式为</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200417225111.png" alt></p><p>注：选择该成本函数的原因</p><p>由上面两个公式，约定yhat=p(y=1|x),即给定训练样本x,y取1的概率</p><p>当y=0时，p(y|x)=1-yhat；当y=1时，p(y|x)=yhat，则可以将该表达式进行合并为p(y|x)=yhat^(y)+(1-yhat)^(1-y)，根据极大似然函数取对数得到第三个式子，为了进行缩放加上了1/m，最终得出了成本函数</p><h3 id="正向传播和反向传播"><a href="#正向传播和反向传播" class="headerlink" title="正向传播和反向传播"></a>正向传播和反向传播</h3><p>logistic算法采用梯度下降算法：每次都朝着下降的快的反向移动，因此需要计算梯度，沿着梯度放下修正w和b的值</p><p>正向传播中，根据输入的x1,x2…,w,b的值，计算z，由z再计算出a，根据a的值得出损失函数L</p><p>反向传播中</p><p>根据得出的L的值，反向得出各个梯度值，最终得出dw,db，即需要沿着这个梯度变化。首先从损失函数L得出a的梯度da=-y/a+(1-y)/(1-a)，再由a得到dz，dz=dL/dz=dL/da * da/dz（链式法则）=a-y，同理得出dw=x * dz，db=dz。</p><p>根据梯度下降公式w:=w-alpha * dw   b:=b-alpha * db每次来修改w和b的值</p><p>反向传播过程图</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200417231506.png" alt></p><h3 id="提升算法性能"><a href="#提升算法性能" class="headerlink" title="提升算法性能"></a>提升算法性能</h3><p>（1）不使用显示的for循环改用向量化</p><p>根据上述公式，为了求出z，需要对每个x(i)，求出z(i)，再依次得出yhat(i),L(i)，当有m个样本时，就需要循环m次，数据集很大的情况下效率很低，因此将该for循环实现的功能向量化，只需要把x用向量表示，即将每个样本纵向排列得到一个m * nx的矩阵，这样直接通过dot(w.T,X)，即可以计算出z的值，省去了遍历数据集的循环</p><p>（2）利用numpy中的库函数的广播功能</p><p>当m * n的矩阵和1 * n的矩阵相加时，后面的矩阵自动扩展成m * n进行计算，如上述式子中的常数b，在计算过程中会自动扩展</p><h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><p>利用logistic回归搭建一个可以识别猫的神经网络</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><h3 id="处理数据"><a href="#处理数据" class="headerlink" title="处理数据"></a>处理数据</h3><p>这里猫的图片数据保存在h5文件中，以64*64的三通道数据保存</p><p>（1）首先，从h5文件中读入训练数据和测试数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.处理数据</span></span><br><span class="line">train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes=load_dataset()</span><br></pre></td></tr></table></figure><p>以训练数据位列，查看数据纬度，发现纬度为（290，64，64，3）代表有290个图像，每个图像都是64 * 64的，里面包含了红绿蓝三个通道数字，因此需要将单个图片中红绿蓝矩阵合并为一个列向量，即</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200417232700.png" alt></p><p>（2）因此，第二步需要进行数据降维</p><p>需要将每个图片数据变为64 * 64 * 3=12288维的列向量，或者也可以不指定列数，直接给个-1，让程序自己计算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_set_x=train_set_x_orig.reshape(train_set_x_orig.shape[<span class="number">0</span>],<span class="number">-1</span>).T <span class="comment">#将每个图片竖着排放</span></span><br><span class="line">test_set_x=test_set_x_orig.reshape(test_set_x_orig.shape[<span class="number">0</span>],<span class="number">-1</span>).T</span><br></pre></td></tr></table></figure><p>（3）数据标准化</p><p>图像中的像素点事0~255之间，只需要把输入的数据除以255即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_set_x=train_set_x/<span class="number">255</span></span><br><span class="line">test_set_x=test_set_x/<span class="number">255</span></span><br></pre></td></tr></table></figure><h3 id="搭建神经网络"><a href="#搭建神经网络" class="headerlink" title="搭建神经网络"></a>搭建神经网络</h3><p>（0）确定分类函数</p><p>因为logistic是二值分类器，这里选择sigmoid函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">sig=<span class="number">1</span>/(<span class="number">1</span>+exp(-z))</span><br><span class="line"><span class="keyword">return</span> sig</span><br></pre></td></tr></table></figure><p>（1）初始化w,b</p><p>根据输入数据的特征纬度，建立w矩阵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inital_wPara_bPara</span><span class="params">(dim)</span>:</span></span><br><span class="line">w=zeros((dim,<span class="number">1</span>))<span class="comment">#初始化为列向量</span></span><br><span class="line">b=<span class="number">0</span></span><br><span class="line"><span class="keyword">return</span> w,b</span><br></pre></td></tr></table></figure><p>（2）搭建模型</p><p>对每次循环</p><p>1°正向传播</p><p>根据输入的值，计算出当前的z，a，J，从而得出dw,db的梯度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">progation</span><span class="params">(w,b,x,y)</span>:</span></span><br><span class="line"><span class="comment">#logictic公式</span></span><br><span class="line">z=dot(w.T,x)+b</span><br><span class="line">a=sigmoid(z)</span><br><span class="line">m=x.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">loss=(<span class="number">-1</span>/m)*sum((y*log(a)+(<span class="number">1</span>-y)*log(<span class="number">1</span>-a)))</span><br><span class="line">dz=a-y</span><br><span class="line">dw=<span class="number">1</span>/m*dot(x,dz.T)</span><br><span class="line">db=<span class="number">1</span>/m*sum(dz)</span><br><span class="line"></span><br><span class="line">grad=&#123;<span class="string">"dw"</span>:dw,<span class="string">"db"</span>:db&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> grad,loss</span><br></pre></td></tr></table></figure><p>2°反向传播</p><p>根据dw,db的梯度，计算新的w，b的值</p><p>3°记录损失和最终找出的最合适的w,b的值</p><p>每次根据迭代次数记录损失</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span><span class="params">(w,b,x,y,alpha,numIter,print_cost=False)</span>:</span></span><br><span class="line"></span><br><span class="line">costs=[]</span><br><span class="line"><span class="comment">#遍历numiter次</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(numIter):</span><br><span class="line"><span class="comment">#得出每次的梯度，进行修正</span></span><br><span class="line">grad,cost=progation(w,b,x,y)</span><br><span class="line">dw=grad[<span class="string">'dw'</span>]</span><br><span class="line">db=grad[<span class="string">'db'</span>]</span><br><span class="line">w=w-alpha*dw</span><br><span class="line">b=b-alpha*db</span><br><span class="line"><span class="keyword">if</span>(i%<span class="number">100</span>==<span class="number">0</span>):</span><br><span class="line">costs.append(cost)</span><br><span class="line"><span class="keyword">if</span>(print_cost) <span class="keyword">and</span> (i%<span class="number">100</span>==<span class="number">0</span>):</span><br><span class="line">print(<span class="string">"i:"</span>+str(i)+<span class="string">"  误差是："</span>+str(cost))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">params=&#123;<span class="string">"w"</span>:w,<span class="string">"b"</span>:b&#125;</span><br><span class="line">grads=&#123;<span class="string">"dw"</span>:dw,<span class="string">"db"</span>:db&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> params,grads,costs</span><br></pre></td></tr></table></figure><p>4°对模型结果进行预测</p><p>根据训练得出的模型w,b,计算z=w*x+b，求出z的值，则得出a=sigmoid(z)，根据a的值记录最终的输出是1还是0（a&gt;0.5是1，反之是0）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(w,b,x)</span>:</span></span><br><span class="line">a=sigmoid(dot(w.T,x)+b)</span><br><span class="line">m=a.shape[<span class="number">1</span>]</span><br><span class="line">yPredict=zeros((<span class="number">1</span>,m))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line"><span class="keyword">if</span>(a[:,i]&gt;<span class="number">0.5</span>):</span><br><span class="line">yPredict[<span class="number">0</span>,i]=<span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">yPredict[<span class="number">0</span>,i]=<span class="number">0</span></span><br><span class="line"><span class="keyword">return</span> yPredict</span><br></pre></td></tr></table></figure><p>（3）最终形成模型，对训练集和测试集上的数据查看准确率</p><p>对得出的预测值减去实际的输出并取绝对值，当预测相同，则对应位置为0，不同则为1，所有数相加起来除以总数，即对所有元素取平均，则是误差率，用1减去这个数，则是正确率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(trainX,trainY,testX,testY,alpha=<span class="number">0.005</span>,numIter=<span class="number">2000</span>,print_cost=True)</span>:</span></span><br><span class="line"><span class="comment">#初始化</span></span><br><span class="line"><span class="comment">#行向量是多少数据特征，列向量是多少条数据，w的个数对应数据特征的个数</span></span><br><span class="line">m=trainX.shape[<span class="number">0</span>]</span><br><span class="line">w,b=inital_wPara_bPara(m)</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练模型</span></span><br><span class="line">params,grads,costs=optimize(w,b,trainX,trainY,alpha,numIter,print_cost)</span><br><span class="line"></span><br><span class="line">w=params[<span class="string">"w"</span>]</span><br><span class="line">b=params[<span class="string">"b"</span>]</span><br><span class="line">trainPredict=predict(w,b,trainX)</span><br><span class="line">testPredict=predict(w,b,testX)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"训练集准确率："</span>,<span class="number">100</span>-mean(abs(trainPredict-trainY))*<span class="number">100</span>)</span><br><span class="line">print(<span class="string">"测试集准确率："</span>,<span class="number">100</span>-mean(abs(testPredict-testY))*<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">d=&#123;<span class="string">"costs"</span>:costs,<span class="string">"trainPredict:"</span>:trainPredict,<span class="string">"testPredict:"</span>:testPredict,<span class="string">"w"</span>:w,<span class="string">"b"</span>:b,<span class="string">"alpha"</span>:alpha,<span class="string">"numIter"</span>:numIter&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> d</span><br></pre></td></tr></table></figure><h3 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h3><p>（1）固定学习率</p><p>读入costs的值，进行显示，得出每一百次costs误差的变化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">d=model(train_set_x,train_set_y_orig,test_set_x,test_set_y_orig)</span><br><span class="line">costs=d[<span class="string">"costs"</span>]</span><br><span class="line">plt.plot(costs)</span><br><span class="line">plt.xlabel(<span class="string">"iter"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"costs"</span>)</span><br><span class="line">plt.title(<span class="string">"alpha="</span>+str(d[<span class="string">"alpha"</span>]))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>在迭代次数2000，学习率为0.005下误差的变化</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200417234704.png" alt></p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200417234727.png" alt></p><p>可以看出训练集准确率很高，但是测试集准确率有所降低，说明训练的模型存在过拟合的现象</p><p>（2）不同学习率</p><p>输入不同的学习率，查看误差的变化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">alpha=[<span class="number">0.01</span>,<span class="number">0.001</span>,<span class="number">0.0001</span>]</span><br><span class="line">m=shape(alpha)[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">print(<span class="string">"alpha="</span>+str(alpha[i]))</span><br><span class="line">d=model(train_set_x,train_set_y_orig,test_set_x,test_set_y_orig,alpha[i])</span><br><span class="line">costs=d[<span class="string">"costs"</span>]</span><br><span class="line">plt.plot(costs,label=str(d[<span class="string">"alpha"</span>]))</span><br><span class="line">print(<span class="string">"------------------------------------------------"</span>)</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">"iter"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"costs"</span>)</span><br><span class="line">legend = plt.legend(loc=<span class="string">'upper center'</span>, shadow=<span class="literal">True</span>)</span><br><span class="line">frame = legend.get_frame()</span><br><span class="line">frame.set_facecolor(<span class="string">'0.90'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>结果为</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200417235032.png" alt></p><p>从结果可以看出，当学习率很小的时候，误差基本不怎么变化，即一直没到嘴有点，当学习率很大的时候，前面由于学习率大，所有误差变化快，出现陡降，当走到最优解之后，误差趋向平缓</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;神经网络中的设定&quot;&gt;&lt;a href=&quot;#神经网络中的设定&quot; class=&quot;headerlink&quot; title=&quot;神经网络中的设定&quot;&gt;&lt;/a&gt;神经网络中的设定&lt;/h2&gt;&lt;p&gt;设定(x,y)是单独的一个样本，m个训练样本为{ ((x(1),y(1)), ((x(12),y(2)), ….((x(m),y(m))}&lt;/p&gt;&lt;p&gt;设定输入的样本为&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200417221326.png&quot; alt&gt;&lt;/p&gt;&lt;p&gt;即将X中所有输入的样本都纵向排列（不使用横向排列的方式），每一列代表一条数据，每一行代表一个输入特征，如原输入样本x1中的特征为a,b,c，a,b,c分别对应了数值，输入样本x2中也有这三个特征，但是数值可能不一样，所以X是nx维向量，有m个列（代表m条数据）&lt;/p&gt;
    
    </summary>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="回归" scheme="https://www.xiapf.com/tags/%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>Leecode学习笔记（六）——系统设计题</title>
    <link href="https://www.xiapf.com/blogs/sysDesignNote/"/>
    <id>https://www.xiapf.com/blogs/sysDesignNote/</id>
    <published>2020-04-13T07:35:46.000Z</published>
    <updated>2020-04-13T07:38:00.472Z</updated>
    
    <content type="html"><![CDATA[<h2 id="缓存机制"><a href="#缓存机制" class="headerlink" title="缓存机制"></a>缓存机制</h2><p>（1）LRU</p><p>least recently use:最近最少使用</p><p>删除缓存依据：按照时间计算，每次删除最久未使用的数据。</p><p>（2）LFU</p><p>least frequently use：最近最不常使用</p><p>删除缓存依据：按照时间和使用频率计算，以使用频率为主，每次删除数据时，查看使用频率最小的数据进行删除，当使用频率相同时，根据最久未使用即时间来删除。</p><a id="more"></a><p>总体思路：</p><p>缓存主要是为了获取数据和插入数据，因此需要对缓存数据进行节点设计，根据其特性设置key.value用于根据key查找值，同时需要设置使用时间time,使用频率cnt.</p><p>为了方便查找可以使用hash表存储key以及node,可以方便的在hash表内通过node-&gt;key查找到node，但是哈希表无序，因此还需要存储节点的使用时间/频率，这时候有两种思路：</p><p>第一种是利用set容器，c++内置的set是使用红黑树排序，即会形成平衡二叉树，使用运算符重载，将set内部根据时间和频率来进行排序，使用频率最少或者时间最少的会排在前面，以set记录时间/频率，但是搜索时间复杂度是O（logn）。</p><p>第二种思路是建立双链表list存储节点，哈希表中存储key映射到list节点的迭代器，当数据使用是，将其从原来位置删除，插入最前面，当需要删除数据的时候，从末位删除，链表操作的时间复杂度是O（1）。</p><p>获取数据（根据密钥获得值）：</p><p>根据哈希表中的key查找节点，并进行缓存数据更新。</p><p>（2）插入数据（将key,value插入）：</p><p>判断key是否存储过了，当存储过之后更新数据值，反之需要插入数据，此时需要判断是否已经达到容量顶端，达到了，需要删除最不常使用的数据。，并进行缓存数据更新。</p><p>注：每次操作过后都需要进行缓存数据更新，即在哈希表中更新数据节点以及set/双向链表list进行更新。</p><h2 id="设计简单推特"><a href="#设计简单推特" class="headerlink" title="设计简单推特"></a>设计简单推特</h2><p>题目：355. 设计推特设计一个简化版的推特(Twitter)，可以让用户实现发送推文，关注/取消关注其他用户，能够看见关注人（包括自己）的最近十条推文。你的设计需要支持以下的几个功能：postTweet(userId, tweetId): 创建一条新的推文,getNewsFeed(userId): 检索最近的十条推文。每个推文都必须是由此用户关注的人或者是用户自己发出的。推文必须按照时间顺序由最近的开始排序。,follow(followerId, followeeId): 关注一个用户,unfollow(followerId, followeeId): 取消关注一个用户</p><p>思路：</p><p>（1）初始化两个hash表</p><p>题目要求中功能3，4是需要能够关注和取消关注一个用户，那么就需要存储当前用户到自己关注的人之间的映射，需要用到hash表。</p><p>题目要求中的功能1，2是需要对推文进行操作，需要存储当前用户在某个时刻写的推文（这里的推文用数字代替），为了方便通过id查找推文，后面的数据用pair形式存储时间和推文。这里需要一个全局变量来计算发推的时间。初始化时间为0，离现在越近，时间越大。</p><p>存储当前用户到自己关注的人之间的映射 map1&lt;int,set<int>&gt;</int></p><p>存储当前用户在某个时刻写的推文的映射 map2&lt;int,vector&lt;pair&lt;int,int&gt;&gt; ，为了方便通过id查找推文，后面的数据用pair形式存储时间和推文</p><p>（2）关注和取消关注</p><p>直接操作map1，加入当前用户关注的id和删除取消关注的id</p><p>（3）创建推文</p><p>直接操作map2，加入当前用户，在当前时间，写下的推文</p><p>（4）检索推文</p><p>需要注意的是搜索推文的时候需要把自己也加上。检索推文的时候需要进行比较，取前10条，这里可以建立优先队列，根据时间进行自动排序。</p><p>建立优先级队列即大顶堆priori_queue&lt;pair&lt;int,int&gt;&gt;，这里会根据pair中第一个元素进行降序排列，当第一个元素相等会根据第二个元素排序，但由于时间肯定是不一样的，这里默认是对时间进行自动排序，离现在越近，时间越大。</p><p>首先将自己加入自己关注的用户列表中，通过当前用户的id查到所有关注的人（这里需要包括自己），再通过查到关注的用户的id，找到他们在某个时间发的推文，将其插入队列中。这时候输出堆顶前10个就是最近的推文。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;缓存机制&quot;&gt;&lt;a href=&quot;#缓存机制&quot; class=&quot;headerlink&quot; title=&quot;缓存机制&quot;&gt;&lt;/a&gt;缓存机制&lt;/h2&gt;&lt;p&gt;（1）LRU&lt;/p&gt;&lt;p&gt;least recently use:最近最少使用&lt;/p&gt;&lt;p&gt;删除缓存依据：按照时间计算，每次删除最久未使用的数据。&lt;/p&gt;&lt;p&gt;（2）LFU&lt;/p&gt;&lt;p&gt;least frequently use：最近最不常使用&lt;/p&gt;&lt;p&gt;删除缓存依据：按照时间和使用频率计算，以使用频率为主，每次删除数据时，查看使用频率最小的数据进行删除，当使用频率相同时，根据最久未使用即时间来删除。&lt;/p&gt;
    
    </summary>
    
    
      <category term="算法" scheme="https://www.xiapf.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="https://www.xiapf.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="系统设计" scheme="https://www.xiapf.com/tags/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>mac下使用nginx实现负载均衡</title>
    <link href="https://www.xiapf.com/blogs/nginx/"/>
    <id>https://www.xiapf.com/blogs/nginx/</id>
    <published>2020-04-10T06:38:52.000Z</published>
    <updated>2020-04-10T06:42:10.471Z</updated>
    
    <content type="html"><![CDATA[<h2 id="安装流程"><a href="#安装流程" class="headerlink" title="安装流程"></a>安装流程</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install nginx</span><br></pre></td></tr></table></figure><h3 id="查看是否安装成功"><a href="#查看是否安装成功" class="headerlink" title="查看是否安装成功"></a>查看是否安装成功</h3><p>nginx默认的端口号为8080，默认的启动页面为index.html，初始页面的默认路径为/usr/local/var/www/index.html，在命令行内输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nginx</span><br></pre></td></tr></table></figure><p>这时已启动nginx服务，打开浏览器输入localhose:8080，显示如下页面说明安装成功。</p><a id="more"></a><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200410142258.png" alt></p><p>配置文件默认的路径为/usr/local/ect/nginx/nginx.conf，对nginx的操作主要是修改配置文件。</p><h2 id="实现负载均衡"><a href="#实现负载均衡" class="headerlink" title="实现负载均衡"></a>实现负载均衡</h2><p>nginx可以抽象的理解为时一个代理服务器，它通过接收网页端的请求，并将其按照一定策略转发到服务器中，实现负载均衡。这里策略主要是按照权重设置每台服务器，权重越大说明转发给其的请求的可能性越大，同时还可以设置备用服务器，平时不给备用服务器发送请求，一旦其他服务器宕机，将会使用备用服务器，来保证服务器中服务的连续性。</p><h3 id="设置服务器"><a href="#设置服务器" class="headerlink" title="设置服务器"></a>设置服务器</h3><p>在同一个项目中新建三个启动项，作为三个服务器，并分别设置端口号为9091，9092，9093，将端口号分别保存为application.properties，application.properties2，application.properties3，并在启动环境变量中将每个服务器对应的端口号进行加载。</p><p>选择edit configurations</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200410141413.png" alt></p><p>分别设置每个服务器的端口</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200410141518.png" alt></p><p>这样三个服务端就设置好了，当调用哪个服务器时会在控制台显示其端口。</p><h3 id="设置nginx"><a href="#设置nginx" class="headerlink" title="设置nginx"></a>设置nginx</h3><p>主要是通过设置配置文件，负载均衡是通过关键字upstream设置，打开nginx.conf配置文件，在http项目下增加upstream，把之前设置的服务器的端口号加入，并设置每个服务器的权值和备用服务器情况，这里设置第一台服务器时主服务器，其他两台服务器时备用服务器。</p><p>（1）upstream后面跟着的是当前分发请求的入口名称，需要在location中增加该名称</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200410142229.png" alt></p><p>（2）找到server项目下的location，nginx接收请求之后会从location中找返回的页面，因为nginx默认监听的端口是8080，启动入口是index.html，为了让nginx从设置的webpoots中找服务器，需要把原来的端口设置为80,原来的入口页面注释掉，并增加设置的接收请求的服务器的名称</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200410142558.png" alt></p><p>这里proxy_pass后面的名称需要和upstream后设置的名称要一致，不然nginx找不到接收请求的入口服务器</p><p>注：upstream后面跟的服务器入口名称不能有下划线，不然会报错。</p><p>（3）修改完nginx配置文件后，输入命令，让配置起作用</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nginx -s reload</span><br></pre></td></tr></table></figure><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>（1）我这里服务器实现的功能是在网页端输入对应手机名称，返回手机的信息，被调用的服务器会在控制台显示自己的端口号。打开浏览器输入localhost/xiaomi，网页显示：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200410143131.png" alt></p><p>（2）这是只有第一台服务器会在控制台显示自己的端口号，点击刷新，其他两个服务器不会显示端口号，即没有将请求转发给备用服务器。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200410143329.png" alt></p><p>（3）当将第一台服务器停止运行后，网页端仍不会报错，这时候调用了另外两台服务器，并按权值接收分发过来的请求。</p><p>暂停第一个服务器</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200410143527.png" alt></p><p>其他两个服务器起作用</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200410143539.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;安装流程&quot;&gt;&lt;a href=&quot;#安装流程&quot; class=&quot;headerlink&quot; title=&quot;安装流程&quot;&gt;&lt;/a&gt;安装流程&lt;/h2&gt;&lt;h3 id=&quot;安装&quot;&gt;&lt;a href=&quot;#安装&quot; class=&quot;headerlink&quot; title=&quot;安装&quot;&gt;&lt;/a&gt;安装&lt;/h3&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;brew install nginx&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;h3 id=&quot;查看是否安装成功&quot;&gt;&lt;a href=&quot;#查看是否安装成功&quot; class=&quot;headerlink&quot; title=&quot;查看是否安装成功&quot;&gt;&lt;/a&gt;查看是否安装成功&lt;/h3&gt;&lt;p&gt;nginx默认的端口号为8080，默认的启动页面为index.html，初始页面的默认路径为/usr/local/var/www/index.html，在命令行内输入&lt;/p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;nginx&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;这时已启动nginx服务，打开浏览器输入localhose:8080，显示如下页面说明安装成功。&lt;/p&gt;
    
    </summary>
    
    
      <category term="负载均衡" scheme="https://www.xiapf.com/categories/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"/>
    
    
      <category term="负载均衡" scheme="https://www.xiapf.com/tags/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"/>
    
      <category term="nginx" scheme="https://www.xiapf.com/tags/nginx/"/>
    
  </entry>
  
  <entry>
    <title>非监督学习笔记（二）——使用Aprior算法进行关联分析</title>
    <link href="https://www.xiapf.com/blogs/apriori/"/>
    <id>https://www.xiapf.com/blogs/apriori/</id>
    <published>2020-04-09T10:50:13.000Z</published>
    <updated>2020-04-09T10:58:40.508Z</updated>
    
    <content type="html"><![CDATA[<h2 id="算法目的"><a href="#算法目的" class="headerlink" title="算法目的"></a>算法目的</h2><p>从数据中找到各属性之间的隐含关系，Aprior算法就是在数据集中构造出频繁项集，从其中找出关联规则。</p><p>这里的频繁项集是指经常出现在一起的数据属性，通过出现的频率来衡量频繁项集。</p><p>关联规则是指两个集合之间存在的联系，如p-&gt;q就是一种关联规则，关联规则通过置信度衡量，两个集合之间的置信度=p和q并集的支持度/p的支持度。</p><a id="more"></a><p>因此Aprior算法主要目的就是从数据中找到频繁集合，从频繁集合中找出关联规则。用户输入最小支持度得出频繁集合，根据设定的最小置信度得出想要的关联规则</p><h2 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h2><h3 id="构造频繁集"><a href="#构造频繁集" class="headerlink" title="构造频繁集"></a>构造频繁集</h3><p>总体思路：</p><p>从集合元素个数为1开始构造频繁集，非频繁集的单个元素不加入后续构造频繁集中，因为非频繁集的超集也是非频繁集。</p><p>当集合个数大于等于2个时，每次取前k-2个集合进行合并，得出候选集，再根据最小置信度求出频繁集合</p><p>1.构造单个元素的集合</p><p>输入数值为整个数据集合</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.对每一个列表数据</span></span><br><span class="line"><span class="keyword">for</span> tran <span class="keyword">in</span> dataSet:</span><br><span class="line"><span class="comment">#2.对每个单个数据项</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> tran:</span><br><span class="line"><span class="keyword">if</span> [item] <span class="keyword">not</span> <span class="keyword">in</span> C1:</span><br><span class="line">C1.append([item])</span><br></pre></td></tr></table></figure><p>2.计算各个数据集合的出现概率，并把大于最小支持度的集合保存，作为频繁集</p><p>2.1计算各个数据集合的出现概率</p><p>输入候选集合，对每个候选集合判断是否是当前集合的子集，如果是就需要增加其出现的次数，如果是首次出现则设置为1</p><p>注：A.issubset(B):判断a是否是b的子集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.计算所有数据项在原始数据集中出现的次数</span></span><br><span class="line">sSet=&#123;&#125;</span><br><span class="line"><span class="keyword">for</span> tran <span class="keyword">in</span> D:</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> C1:</span><br><span class="line"><span class="comment">#判断item是否是tran的子集</span></span><br><span class="line"><span class="keyword">if</span> item.issubset(tran):</span><br><span class="line"><span class="keyword">if</span> item <span class="keyword">not</span> <span class="keyword">in</span> sSet:<span class="comment">#初始字典中没有这个键</span></span><br><span class="line">sSet[item]=<span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">sSet[item]+=<span class="number">1</span></span><br></pre></td></tr></table></figure><p>2.2根据最小支持度保存频繁集合</p><p>将每个候选集合出现的次数除以集合总数，得出出现的概率，再和最小支持度比较</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#2.计算每个数据项的支持度，并把小于最小支持度的数据项删除</span></span><br><span class="line">numLen=len(D)</span><br><span class="line">retList=[]</span><br><span class="line">supportData=&#123;&#125;</span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> sSet:</span><br><span class="line">support=sSet[key]/numLen</span><br><span class="line"><span class="keyword">if</span>(support&gt;=minSupport):</span><br><span class="line"><span class="comment">#在索引为0的位置之前插入</span></span><br><span class="line">retList.insert(<span class="number">0</span>,key)</span><br><span class="line">supportData[key]=support</span><br></pre></td></tr></table></figure><p>3.apriori算法得出频繁集合</p><p>注：当一个集合不是频繁集，那么他的超集也不是频繁集，以这个原则来减少需要构造的集合数。所以每次从一个元素的集合开始寻找频繁集，非频繁集的元素集合不保存，依次作为初始构造起点，非频繁集的元素自然不会加入到后续筛选多个元素的集合中去。</p><p>3.1 从长度为1的集合开始找到频繁集合</p><p>（1）构造数据 从原始数据集中得出不重复的总的数据集合</p><p>（2）构造初始频繁集合</p><p>得出单个数据集中的频繁集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.构造数据</span></span><br><span class="line">D=list(map(set,dataSet))</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.构造初始数据项</span></span><br><span class="line">C1=createC1(dataSet)</span><br><span class="line">L1,supportData=scanD(D,C1,minSupport)</span><br><span class="line">L=[L1]</span><br></pre></td></tr></table></figure><p>（3）构造元素数多于2个的频繁集合</p><p>令k=2,假设初始频繁集为L=[L1],当L[K-2]即前一个元素数量有频繁集合，那么就可以循环查看当前长度的元素数量是否也有频繁集合（假设前一个元素数量是1，频繁集合中元素数为1），从L[K-2]中构造当前的候选集合，并进行支持度判断</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#3.构造长度为2开始的频繁数据集</span></span><br><span class="line">k=<span class="number">2</span></span><br><span class="line"><span class="keyword">while</span>(len(L[k<span class="number">-2</span>])&gt;<span class="number">0</span>):</span><br><span class="line"><span class="comment">#3.1得到构造的频繁数据集</span></span><br><span class="line">Ck=aprioriGen(L[k<span class="number">-2</span>],k)</span><br><span class="line"><span class="comment">#3.2判断是否是频繁项</span></span><br><span class="line">Lk,Suk=scanD(D,Ck,minSupport)</span><br><span class="line"></span><br><span class="line">L.append(Lk)</span><br><span class="line"><span class="comment">#字典插入/更新数值</span></span><br><span class="line">supportData.update(Suk)</span><br><span class="line">k=k+<span class="number">1</span>;</span><br></pre></td></tr></table></figure><p>3.2 构造长度大于2的频繁集合</p><p>注：这里有个小技巧：取前k-2个集合，当长度相同时合并则为该数量下集合构成的候选集，如k=2时，此时输入的L[k-2]={1},{2},{3},取前k-2个，则是{1},{2},合并为{1,2}，每次i从0开始取值，下一层循环j从i+1开始取值，则得到k=2时的候选集为{1,2},{1,3},{2,3}</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(L)):</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>,len(L)):</span><br><span class="line"><span class="comment">#取前k-2个作为集合，如果相同则合并，即为上一层得出下一层的集合</span></span><br><span class="line">L1=list(L[i])[:k<span class="number">-2</span>]<span class="comment">#取出两个集合前k-1个元素list[:]从开始到末尾位置</span></span><br><span class="line">L2=list(L[j])[:k<span class="number">-2</span>]</span><br><span class="line">L1.sort()</span><br><span class="line">L2.sort()</span><br><span class="line"><span class="keyword">if</span>(L1==L2):</span><br><span class="line">retList.append(L[i]|L[j])</span><br></pre></td></tr></table></figure><h3 id="挖掘关联规则"><a href="#挖掘关联规则" class="headerlink" title="挖掘关联规则"></a>挖掘关联规则</h3><p>根据p的置信度=supprot (p|q) /support (p)求出频繁集的置信度</p><p>总体思路：</p><p>输入的频繁集合，支持度</p><p>（1）单元素的频繁集没有关联规则，所以从L[1]开始遍历</p><p>（2）对L[i]中的每个单项，构造出不重复的单个元素集合组成的列表H</p><p>当只有两个元素时，直接得到关联规则</p><p>当有两个以上的元素时，不断构造2个元素及以上的不重复子集来得出关联规则</p><p>1.构造2个元素的频繁集合的置信度</p><p>输入频繁项集和只有单个元素的列表H</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> freq <span class="keyword">in</span> H:</span><br><span class="line">conf=supportData[freqSet]/supportData[freqSet-freq]</span><br><span class="line"><span class="keyword">if</span>(conf&gt;minConf):</span><br><span class="line">print(freqSet-freq,<span class="string">"-&gt;"</span>,freq,<span class="string">":"</span>,conf)</span><br><span class="line">br1.append((freqSet-freq,freq,conf))</span><br><span class="line">cbr1.append(freq)</span><br></pre></td></tr></table></figure><p>2.构造2个元素以上的频繁集合的置信度</p><p>输入单元素的频繁集合，在此基础上不断根据2个元素以上的频繁集合合并，得出不重复的集合元素，并得出关联规则</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">m=len(H[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">if</span>(len(freqSet)&gt;(m+<span class="number">1</span>)):</span><br><span class="line">hmq=aprioriGen(H,m+<span class="number">1</span>) <span class="comment">#得出2个元素以上的频繁集合</span></span><br><span class="line">hmq=calConf(freqSet,hmq,supportData,br1,minConf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(len(hmq)&gt;<span class="number">1</span>):</span><br><span class="line">ruleFromConf(freqSet,hmq,supportData,br1,minConf)</span><br></pre></td></tr></table></figure><p>3.将单个元素得出的关联规则和多个元素的频繁集得出的关联规则合并</p><p>根据输入的频繁集合，支持度中，L[1]是包含2个元素的频繁集，L[2]是包含3个元素的频繁集</p><p>当时2个元素的频繁集合时，直接根据公式得出置信度，即关联规则，当时多个元素的频繁集合时，需要将子集合并得出关联规则</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,len(L)):</span><br><span class="line"><span class="keyword">for</span> freq <span class="keyword">in</span> L[i]:</span><br><span class="line">H1=[frozenset([item]) <span class="keyword">for</span> item <span class="keyword">in</span> freq]</span><br><span class="line">     <span class="comment">#多个元素的频繁集合</span></span><br><span class="line"><span class="keyword">if</span>(i&gt;<span class="number">1</span>):</span><br><span class="line">ruleFromConf(freq,H1,supportData,bigRulesList,minConf)</span><br><span class="line">     <span class="comment">#单个元素的频繁集合</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">calConf(freq,H1,supportData,bigRulesList,minConf)</span><br></pre></td></tr></table></figure><h2 id="应用实例"><a href="#应用实例" class="headerlink" title="应用实例"></a>应用实例</h2><p>问题：发现毒蘑菇的相似特征</p><p>数据集：原始数据集在UCI上，使用关联规则需要把样本数据转换为特征集合，有人做了解析，直接使用该数据集</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200409183650.png" alt="毒蘑菇数据"></p><p>第一列1代表无毒，2代表有毒。</p><p>利用apriori算法得出频繁集合，查看有毒特征一般和哪些特征一起出现，这样就可以通过其他特征分辨出毒蘑菇。</p><p>根据得出的频繁项集，找到长度为2的频繁项集中有毒蘑菇特征的集合为</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200409184446.png" alt="毒蘑菇频繁集"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">data=apriori.loadMushroomData(<span class="string">"mushroom.dat"</span>)</span><br><span class="line">L,supportData=apriori.apriori(data,<span class="number">0.3</span>)</span><br><span class="line">print(L)</span><br><span class="line">print(supportData)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> L[<span class="number">1</span>]:</span><br><span class="line"><span class="keyword">if</span> &#123;<span class="number">2</span>&#125;.issubset(item):</span><br><span class="line">print(item)</span><br></pre></td></tr></table></figure><p>也可以选择其他长度的频繁项集查看</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;算法目的&quot;&gt;&lt;a href=&quot;#算法目的&quot; class=&quot;headerlink&quot; title=&quot;算法目的&quot;&gt;&lt;/a&gt;算法目的&lt;/h2&gt;&lt;p&gt;从数据中找到各属性之间的隐含关系，Aprior算法就是在数据集中构造出频繁项集，从其中找出关联规则。&lt;/p&gt;&lt;p&gt;这里的频繁项集是指经常出现在一起的数据属性，通过出现的频率来衡量频繁项集。&lt;/p&gt;&lt;p&gt;关联规则是指两个集合之间存在的联系，如p-&amp;gt;q就是一种关联规则，关联规则通过置信度衡量，两个集合之间的置信度=p和q并集的支持度/p的支持度。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="关联分析" scheme="https://www.xiapf.com/tags/%E5%85%B3%E8%81%94%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>监督学习笔记（四）——利用回归预测数值型数值</title>
    <link href="https://www.xiapf.com/blogs/regression/"/>
    <id>https://www.xiapf.com/blogs/regression/</id>
    <published>2020-04-02T10:44:05.000Z</published>
    <updated>2020-04-02T10:45:48.222Z</updated>
    
    <content type="html"><![CDATA[<h2 id="算法作用"><a href="#算法作用" class="headerlink" title="算法作用"></a>算法作用</h2><h3 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h3><p>之前的监督学习方法如：贝叶斯分类、logistic回归、svm支持向量机都是将数据点分成对应类，这里是利用回归对连续型数值预测，即给出一个数据能预测其输出。</p><p>这里主要讨论线性回归，通过找到线性回归中的系数构造回归方程来预测数值。算法核心：求回归系数。</p><h3 id="“回归”的含义"><a href="#“回归”的含义" class="headerlink" title="“回归”的含义"></a>“回归”的含义</h3><p>回归是用来预测目标值的输出，例如给定一个x，它满足方程y=a*x+b，那么我们就能通过公式得出x的输出。用回归来预测也类似于这个过程，上面提到的方程是回归中的回顾方程，主要是需要确定回归系数a、b，这里记为w，将所有特征乘以回归系数即可以得到预测值。</p><a id="more"></a><h2 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h2><p>回归的目的是为确定回归系数w，即找到误差最小的w，误差是指实际值y减去预测值yhat，为了使得误差中的正值和负值抵消掉，一般采用平方误差来找到最优w，平方误差定义为：∑（y-w * x)^2，对该式子求导化简可得w=(x^T * x)^-1 * x^T * y，因此可以通过输入的x和y的值求得w，这样就能建立出回归方程即学习模型。</p><p>w的等式中需要对x^T * x求逆矩阵，因此在计算之前需要判断该矩阵是否可逆，可以通过行列式的值判断。</p><h3 id="标准线性回归"><a href="#标准线性回归" class="headerlink" title="标准线性回归"></a>标准线性回归</h3><p>（1）应用场景及原理</p><p>应用场景：几乎所有数据都可以用线性回归来拟合数据</p><p>原理：使用w=(x^T * x)^-1 * x^T * y求解回归系数</p><p>（2）代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">standRegres</span><span class="params">(x,y)</span>:</span></span><br><span class="line">  xMat=mat(x)</span><br><span class="line">  yMat=mat(y).transpose()</span><br><span class="line">  m,n=shape(xMat)</span><br><span class="line">  xTx=xMat.T*xMat</span><br><span class="line">  </span><br><span class="line">  <span class="comment">#判断是否可逆</span></span><br><span class="line">  <span class="keyword">if</span>(linalg.det(xTx)==<span class="number">0</span>):</span><br><span class="line">    print(<span class="string">"行列式不可逆"</span>)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    inv=linalg.inv(xTx)</span><br><span class="line">    ws=inv*xMat.T*yMat</span><br><span class="line">    <span class="keyword">return</span> ws</span><br></pre></td></tr></table></figure><p>导入如图所示的数据</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200401155534.png" alt="数据"></p><p>数据集中前两列为输入的特征，最后一列是预测值，其中第0列是手动设置为1，为了得出回归方程中的常数项，使用xMat*ws即可得到预测值，在图像中对第一列数据画出拟合的直线：plot(xMat[:,1],yhat)</p><p>结果如图：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200401161414.png" alt="线性回归"></p><p>判断模型的好坏可以通过相关系数，使用corrcoef(yMat,yHat)，可得</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200401161238.png" alt="相关系数"></p><p>可以看出模型大致贴合数据。</p><h3 id="局部加权线性回归"><a href="#局部加权线性回归" class="headerlink" title="局部加权线性回归"></a>局部加权线性回归</h3><p>（1）应用场景及原理</p><p>应用场景：由标准线性回归效果可以看出模型存在欠拟合，数据点和直线只能大致匹配，因此需要使用局部加权线性回归，对每个数据点引入权重，通过核函数对周围的点赋权重w(i,i)=exp(|x(i)-x)|/-2k^2，这里的k是由用户各处需要给周围的点赋予多大的权重。</p><p>原理：回归系数w=(x^T * w * x)^-1 * w *x^T * y</p><p>（2）代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#局部加权线性回归</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lwlr</span><span class="params">(testPoint,x,y,k)</span>:</span></span><br><span class="line">  xMat=mat(x)</span><br><span class="line">  yMat=mat(y).transpose()</span><br><span class="line">  m,n=shape(xMat)</span><br><span class="line">  <span class="comment">#创建权重对角矩阵</span></span><br><span class="line">  w=mat(eye(m))</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> m:</span><br><span class="line">    diff=testPoint-xMat[i,:]</span><br><span class="line">    <span class="comment">#通过高斯核赋予权重，越靠近点(i，i),权重越大</span></span><br><span class="line">    w[i,i]=exp(diff*diff.T)/(<span class="number">-2</span>*k**<span class="number">2</span>)</span><br><span class="line">  xTx=xMat.T*w*xMat</span><br><span class="line">  </span><br><span class="line">  <span class="comment">#判断是否可逆</span></span><br><span class="line">  <span class="keyword">if</span>(linalg.det(xTx)==<span class="number">0</span>):</span><br><span class="line">    print(<span class="string">"行列式不可逆"</span>)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    inv=linalg.inv(xTx)</span><br><span class="line">    ws=inv*w*xMat.T*yMat</span><br><span class="line">    <span class="keyword">return</span> testPoint*ws <span class="comment">#返回对该点的预测值，遍历数据集xmat，则可以得到所有数的预测输出</span></span><br></pre></td></tr></table></figure><p>画图时需要将数据重新排序</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sid=argsort(xmat[:,<span class="number">1</span>],axis=<span class="number">0</span>)<span class="comment">#第一列数据按序排列获得位置</span></span><br><span class="line">xcopy=xmat[sid][:,<span class="number">0</span>,:]<span class="comment">#取第一列和第二列</span></span><br><span class="line">plot(xcopy[:,<span class="number">1</span>],yhat[sid])</span><br></pre></td></tr></table></figure><p>运行效果如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200401170052.png" alt="局部回归"></p><p>对uci上鲍鱼年龄进行测试，当使用不同的参数运行可得，当参数越大，在测试集上误差越大，但是当参数越大，在训练集上误差越小，因此需要根据测试集误差选择合适模型。</p><p>这里参数选择0.1、1、10：<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200401170423.png" alt="不同参数"></p><h3 id="缩减系数"><a href="#缩减系数" class="headerlink" title="缩减系数"></a>缩减系数</h3><p>以下两种算法属于缩减系数，即通过将一些不重要的特征缩减至0，来降低模型复杂度，减小误差。因为当模型越复杂，误差就会越大。缩减系数也需要根据具体数据集选择误差小的模型。</p><h3 id="岭回归"><a href="#岭回归" class="headerlink" title="岭回归"></a>岭回归</h3><p>（1）应用场景及原理</p><p>应用场景：当数据特征多余数据点个数时，求解w的公式中xtx会是奇异矩阵；或者当出现数据点特征过多，需要缩减特征的时候也可以使用岭回归。岭回归因为引入了对角矩阵I，其中对角矩阵为n*n矩阵，其中n=数据集中特征的数量，对角线上均是1，形似岭，所以称为岭回归。当引入单位矩阵I之后，可以保证矩阵可逆。</p><p>原理：在单位矩阵前需要乘以一个系数λ，这里的系数需要用户给出。</p><p>回归系数w=(x^T * x+λI)^-1  *x^T * y</p><p>（2）代码</p><p>整体过程类似于标准线性回归，虽然加入了对角矩阵也需要判断x^T * x+λI是否可逆，因为给出的系数可能为0，因为根据输入的λ不同，ws的结果会不同，因此，测试时可以输入30组λ，根据λ取值选择误差最小的回归系数ws。</p><p>（3）注意点——数据标准化与还原</p><p>因为岭回归可以用来缩减数据特征，当标准化之后所有数据特征同等重要，每个数据特征在最终回归方程中的大小会不同，小的数据说明该对应特征相比于其他更不重要，如果需要丢弃特征，可以从小的数据开始。</p><p>所以，岭回归中需要将所有数据进行标准化，最终求得ws之后，需要进行数据还原才能得出最终的回归矩阵。</p><p>用标准化数据进行模型建立：x=(x-xmean)/xvar，y=y-ymean</p><p>当得出ws时，回归方程应为y=((x-xmean) / xvar) * ws+ymean，其中(x-xmean) /xvar * ws是使用标准化后的x得出预测y值（标准化后的值），加上ymean就是实际值。</p><p>（4）代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">standRidge</span><span class="params">(xMat,yMat,lam)</span>:</span></span><br><span class="line">  <span class="comment">#xMat=mat(x)</span></span><br><span class="line">  <span class="comment">#yMat=mat(y).transpose()</span></span><br><span class="line">  m,n=shape(xMat)</span><br><span class="line">  xTx=xMat.T*xMat+lam*eye(n)</span><br><span class="line">  </span><br><span class="line">  <span class="comment">#判断是否可逆</span></span><br><span class="line">  <span class="keyword">if</span>(linalg.det(xTx)==<span class="number">0</span>):</span><br><span class="line">    print(<span class="string">"行列式不可逆"</span>)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    inv=linalg.inv(xTx)</span><br><span class="line">    ws=inv*xMat.T*yMat</span><br><span class="line">    <span class="keyword">return</span> ws</span><br><span class="line">  </span><br><span class="line"><span class="comment">#得到30个系数中ws的变化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ridgeTest</span><span class="params">(x,y)</span>:</span></span><br><span class="line">  numIter=<span class="number">30</span></span><br><span class="line">  xMat=mat(x)</span><br><span class="line">  yMat=mat(y).transpose()</span><br><span class="line">  m,n=shape(xMat)</span><br><span class="line">  xMean=mean(xMat,axis=<span class="number">0</span>)</span><br><span class="line">  xVar=var(xMat,axis=<span class="number">0</span>)</span><br><span class="line">  yMean=mean(yMat,axis=<span class="number">0</span>)</span><br><span class="line">  xTest=(xMat-xMean)/xVar</span><br><span class="line">  yTest=yMat-yMean</span><br><span class="line">  <span class="comment">#将30次系数保存下来</span></span><br><span class="line">  returnMat=zeros((numIter,n))</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(numIter):</span><br><span class="line">    ws=standRidge(xTest,yTest,exp(i<span class="number">-10</span>))</span><br><span class="line">    returnMat[i,:]=ws</span><br><span class="line">   <span class="keyword">return</span> returnMat</span><br></pre></td></tr></table></figure><p>最终得出logλ和回归系数之间的关系</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200402165626.png" alt="logλ和回归系数关系图"></p><p>实际应用中可以通过对wsMat矩阵求预测值，根据预测值-真实值的平方，再求和得出平法误差，找到使得平法误差最小的回归系数矩阵对应的λ则为最佳系数。</p><h3 id="前向逐步回归"><a href="#前向逐步回归" class="headerlink" title="前向逐步回归"></a>前向逐步回归</h3><p>（1）应用场景及原理</p><p>应用场景：该算法属于贪心算法，每次都选择正方向和负方向变化很小的步长，来计算回归系数，每次选择误差最小的回归系数，前向逐步回归也用于缩减特征，当不重要的特征在迭代次数很小的时候不会对数据产生影响即对于特征向量在回归方程中系数等于0，因为需要缩减特征，观察特征的重要性，数据也需要进行标准化。</p><p>（2）代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">standWise</span><span class="params">(x,y,eps=<span class="number">0.001</span>,numIter=<span class="number">300</span>)</span>:</span></span><br><span class="line">  xMat=mat(x)</span><br><span class="line">  yMat=mat(y).transpose()</span><br><span class="line">  m,n=shape(xMat)</span><br><span class="line">  xMean=mean(xMat,axis=<span class="number">0</span>)</span><br><span class="line">  xVar=var(xMat,axis=<span class="number">0</span>)</span><br><span class="line">  yMean=mean(yMat,axis=<span class="number">0</span>)</span><br><span class="line">  xMat=(xMat-xMean)/xVar</span><br><span class="line">  yMat=yMat-yMean</span><br><span class="line">  <span class="comment">#回归系数初始化</span></span><br><span class="line">  ws=zeros((n,<span class="number">1</span>))</span><br><span class="line">  wsMax=ws.copy()</span><br><span class="line">  returnMat=zeros((numIter))</span><br><span class="line">  <span class="comment">#在迭代次数中进行移动ws方向</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(numIter):</span><br><span class="line">    lossErr=inf</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(m):</span><br><span class="line">      <span class="keyword">for</span> sign <span class="keyword">in</span> [<span class="number">-1</span>,<span class="number">1</span>]:</span><br><span class="line">        wsTest=ws.copy()</span><br><span class="line">        wsTest[j]+=eps*sign</span><br><span class="line">        yHat=xMat*ws</span><br><span class="line">        err=rssRrr(yMat.A,yHat.A)</span><br><span class="line">        <span class="keyword">if</span>(err&lt;lossErr):</span><br><span class="line">          lossErr=err</span><br><span class="line">          wsMax=ws</span><br><span class="line">      ws=wsMax</span><br><span class="line">      returnMat[i,:]=ws.T</span><br><span class="line">    <span class="keyword">return</span> returnMat</span><br></pre></td></tr></table></figure><p>得出在鲍鱼数据集上，步长0.001，迭代300次后回归系数和迭代次数的效果图：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200402173125.png" alt="回归系数和迭代次数关系图"></p><h3 id="衡量误差"><a href="#衡量误差" class="headerlink" title="衡量误差"></a>衡量误差</h3><p>判断一个模型的好坏，需要通过预测值和真实值之间的差值进行计算，为了抵消正负值，这里采用平方和形式，假设真实值为ymat，预测值为yhat，则误差error=sum(ymat-yhat)**2，这里需要注意是对应位置相加，所以真实值和预测值都需要转换为数组形式进行计算。</p><p>ymat和yhat需要格式一致，假设ymat为矩阵，则二者需要转换为ymat.A，yhat.A；如果ymat是列表，则则二者需要转换为array(ymat)，yhat.T.A</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rssRrr</span><span class="params">(ymat,yhat)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> sum((ymat-yhat)**<span class="number">2</span>)<span class="comment">#ymat和yhat都需要转换为数组形式进行计算</span></span><br></pre></td></tr></table></figure><h2 id="应用实例"><a href="#应用实例" class="headerlink" title="应用实例"></a>应用实例</h2><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>预测乐高玩具的价格：根据给出的乐高销售网页中不同年份的乐高套装的价格建立回归模型，从而预测玩具价格。</p><h3 id="实现结果"><a href="#实现结果" class="headerlink" title="实现结果"></a>实现结果</h3><p>（1）网页数据获取</p><p>导入bs4中的BeautifulSoup，该模块可以用来获取网页中的数据，主要是读取html文件，find_all可以得出各个属性模块中的值，因为乐高销售网页中的玩具都是放在table中，可以读取每个table，获取玩具的标题，是否全新，是否售出（表格第三列），售出的价格（表格第4列）等。将玩具的年份、套装数量、是否全新、价格存到列表中，作为模型数据集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">获取数据</span><br><span class="line"><span class="comment">#从页面读取数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scrapePage</span><span class="params">(retX,retY,fileName,yr,numPce,OriPrice)</span>:</span></span><br><span class="line"><span class="comment">#1.读取网页</span></span><br><span class="line">fr=open(fileName)</span><br><span class="line"><span class="comment">#2.用beautifulSoup处理,把读取的网页转换为数据</span></span><br><span class="line">soup=BeautifulSoup(fr)</span><br><span class="line">i=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#soup数据全部以列表形式返回</span></span><br><span class="line"><span class="comment">#3.读取table的长度，当还有table即还有商品的时候循环</span></span><br><span class="line">tableCur=soup.find_all(<span class="string">'table'</span>,r=i)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span>(len(tableCur)&gt;<span class="number">0</span>):</span><br><span class="line"><span class="comment">#读取商品每次，根据有无售出来获取价格</span></span><br><span class="line">title=tableCur[<span class="number">0</span>].find_all(<span class="string">'a'</span>)[<span class="number">1</span>].text.lower()<span class="comment">#a标签中第一个有名称,获取文本并转换为小写</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#3.1判断是否全新</span></span><br><span class="line"><span class="keyword">if</span>(title.find(<span class="string">'new'</span>)!=<span class="number">-1</span>) <span class="keyword">or</span> (title.find(<span class="string">'nstd'</span>)!=<span class="number">-1</span>):</span><br><span class="line">newFlag=<span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">newFlag=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#3.2判断是否卖出</span></span><br><span class="line"><span class="comment">#读取td</span></span><br><span class="line">sellFlag=tableCur[<span class="number">0</span>].find_all(<span class="string">'td'</span>)[<span class="number">3</span>].find_all(<span class="string">"span"</span>)</span><br><span class="line"><span class="keyword">if</span>(len(sellFlag)==<span class="number">0</span>):<span class="comment">#此时没有卖出</span></span><br><span class="line">print(<span class="string">"the item"</span>+str(i)+<span class="string">" do not sell out"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">soldPrice=tableCur[<span class="number">0</span>].find_all(<span class="string">'td'</span>)[<span class="number">4</span>]</span><br><span class="line">price=soldPrice.text</span><br><span class="line">price=price.replace(<span class="string">'$'</span>,<span class="string">''</span>)</span><br><span class="line">price=price.replace(<span class="string">','</span>,<span class="string">''</span>)</span><br><span class="line"><span class="keyword">if</span>(len(soldPrice)&gt;<span class="number">1</span>):</span><br><span class="line">price=price.replace(<span class="string">'Free shipping'</span>,<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line">sellPrice=float(price)</span><br><span class="line"></span><br><span class="line"><span class="comment">#3.3去掉不完整的价格</span></span><br><span class="line"><span class="keyword">if</span>(sellPrice&gt;OriPrice*<span class="number">0.5</span>):</span><br><span class="line">retX.append([yr,numPce,newFlag,OriPrice])</span><br><span class="line"><span class="comment">#print(str(price)+"\t"+str(newFlag)+"\t"+title)</span></span><br><span class="line">retY.append(sellPrice)</span><br><span class="line"></span><br><span class="line">i=i+<span class="number">1</span></span><br><span class="line">tableCur=soup.find_all(<span class="string">'table'</span>,r=i)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> retX,retY</span><br></pre></td></tr></table></figure><p>（2）算法选择</p><p>可以采用十折交叉验证：可以建立随机数据randomList=list(range(m))，使用random.shuffle打乱序列中的元素，90%作为训练集，10%作为测试集。</p><p>a）标准线性回归</p><p>为了得出回归方程中的常数项，需要在原有数据上插入常数1，新建一个全为1的矩阵，除了第一列之外其余列用原有数据填充。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m,n=shape(lgX)</span><br><span class="line">retX=mat(ones((m,n+<span class="number">1</span>)))</span><br><span class="line">retX[:,<span class="number">1</span>:<span class="number">5</span>]=mat(lgX)</span><br></pre></td></tr></table></figure><p>根据standRegres得出回归系数，回归系数每一列对应方程中的系数值，以乐高数据中4列数据为例，y=ws[0]+ws[1] * 年份+ws[1] * 套装数量+ws[2] * 是否全新+ws[3] * 价格，得出方程为：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200402180915.png" alt="标准线性回归方程"></p><p>可以看出标准线性回归得出的回归系数有些不合理的地方，当部件数量多，价格会更低，因此选择岭回归查看效果。</p><p>b）岭回归</p><p>因为岭回归中含有参数，根据测试集得出误差最小的参数，对误差集求平均，得出最小的误差所在的行即为最佳回归系数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#最小的误差下的回归系数</span></span><br><span class="line">meanError=mean(testErr,axis=<span class="number">0</span>)</span><br><span class="line">minErr=min(meanError)</span><br><span class="line">minWs=wsMat[nonzero(meanError=minErr)]</span><br></pre></td></tr></table></figure><p>得出回归系数后需要得出回归方程，因为是用标准化数据进行模型建立：x=(x-xmean)/xvar，y=y-ymean。</p><p>当得出ws时，回归方程应为y=((x-xmean) / xvar) * ws+ymean，其中(x-xmean) /xvar * ws是使用标准化后的x得出预测y值（标准化后的值），加上ymean就是实际值。</p><p>所有回归方程可以写成：y=x * ws/xvar -(xmean/xvar) * ws+ymean，后面-(xmean/xvar) * ws+ymean是方程得常数项。记ureg=ws/xvar，以乐高数据中4列数据为例，y=常数项+ureg[0,0] * 年份+ureg[0,1] * 套装数量+ureg[0,2] * 是否全新+ureg[0,3] * 价格，得出方程为：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200402182822.png" alt="岭回归"></p><p>因为测试集随机选取，每次结果略有不同。可以看出岭回归的方程比标准回归方程拟合效果要好，更符合实际。</p><p>这是ws缩减系数中第一次得出的回归系数：</p><blockquote><p>[[-1.07635546e+02]<br> [-1.54119215e+04]<br> [-1.42998055e+01]<br> [ 4.32941501e+04]]</p></blockquote><p>可以看出第4个系数最大，说明其最重要，当只能选择一个特征时，可以选择第4个特征，如果要选择两个特征，可以选择第2个和第4个特征。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;算法作用&quot;&gt;&lt;a href=&quot;#算法作用&quot; class=&quot;headerlink&quot; title=&quot;算法作用&quot;&gt;&lt;/a&gt;算法作用&lt;/h2&gt;&lt;h3 id=&quot;应用场景&quot;&gt;&lt;a href=&quot;#应用场景&quot; class=&quot;headerlink&quot; title=&quot;应用场景&quot;&gt;&lt;/a&gt;应用场景&lt;/h3&gt;&lt;p&gt;之前的监督学习方法如：贝叶斯分类、logistic回归、svm支持向量机都是将数据点分成对应类，这里是利用回归对连续型数值预测，即给出一个数据能预测其输出。&lt;/p&gt;&lt;p&gt;这里主要讨论线性回归，通过找到线性回归中的系数构造回归方程来预测数值。算法核心：求回归系数。&lt;/p&gt;&lt;h3 id=&quot;“回归”的含义&quot;&gt;&lt;a href=&quot;#“回归”的含义&quot; class=&quot;headerlink&quot; title=&quot;“回归”的含义&quot;&gt;&lt;/a&gt;“回归”的含义&lt;/h3&gt;&lt;p&gt;回归是用来预测目标值的输出，例如给定一个x，它满足方程y=a*x+b，那么我们就能通过公式得出x的输出。用回归来预测也类似于这个过程，上面提到的方程是回归中的回顾方程，主要是需要确定回归系数a、b，这里记为w，将所有特征乘以回归系数即可以得到预测值。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="回归" scheme="https://www.xiapf.com/tags/%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>监督学习笔记（三）——logistic回归</title>
    <link href="https://www.xiapf.com/blogs/logistic/"/>
    <id>https://www.xiapf.com/blogs/logistic/</id>
    <published>2020-03-24T09:43:06.000Z</published>
    <updated>2020-04-02T10:45:46.246Z</updated>
    
    <content type="html"><![CDATA[<h2 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h2><h3 id="原理分析"><a href="#原理分析" class="headerlink" title="原理分析"></a>原理分析</h3><p>（1）数据拟合</p><p>给出一些数据，用一条直线来拟合这些数据称为logictic回归</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200324153824.png" alt="直线拟合"></p><p>用logistic回归来进行数据分类是通过给出的所有数据点拟合出分类边界的直线，确定这条直线的系数就称为回归系数，当用回归系数乘以当前特征则得出该条直线：</p><p>z=w0 * x0+w1 * x1+…+wn * xn，其中w0 - wn是回归系数，x0 - xn是各个特征，得出的z是当前特征下的预测值。</p><a id="more"></a><p>为了表示直线上下的浮动，需要将x0设置为1，这时的w0就相当于直线方程中的偏置b。</p><p>（2）分类函数</p><p>当logistic回归用作二值分类器的时候，可以用sigmoid函数进行类别判定，sigmoid（x)=1/1+e-x，图像如下</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200324154722.png" alt="sigmoid"></p><p>当z&gt;0.5时，函数值趋近与1，当z&lt;0.5时，函数趋近于0。可以利用这个特性，对将每个特征乘以回归系数得到的输出类别进行判定。</p><p>因此logistic回归就是要求最佳的回归系数，这是求解最优问题，可以使用梯度上升的方法。</p><p>（3）寻求最优</p><p>梯度上升算法是通过每次都向梯度变化最佳的的地方走，最终找到最优值，迭代公式为：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200324155903.png" alt="迭代公式"></p><p>其中w是回归系数，alpha是每次沿着梯度方向变化的步长，f(w)是梯度变化函数，即损失函数。这里明确一点，求最佳的回归系数的过程中沿着梯度变化最佳的方向变化，这里的最佳可以理解为是误差最小，也就是w0 * x0+w1 * x1+…+wn * xn得出的预测值f和实际值之间的误差最小，那么就是最佳的方向，即对所有特征来说 f(w)=∑（y-f)xi，这是对公式的直观理解，<a href="https://blog.csdn.net/CharlieLincy/article/details/70767791" target="_blank" rel="external nofollow noopener noreferrer">这里也有对人做了对整个过程的详细推导</a>。到这里为止，logistic回归的核心思想很明确了，就是在一个循环中，对所有的向量乘以当前的回归系数，根据得出的误差调整回归系数，不断循环，直至回归系数稳定。</p><h3 id="和SVM的区别"><a href="#和SVM的区别" class="headerlink" title="和SVM的区别"></a>和SVM的区别</h3><p>相同点：二者都是分类器</p><p>不同点：</p><p>logistic回归用到了所有数据点，svm只用了靠近分类平面的点即支持向量。</p><p>logistic回归利用极大似然估计的思想求解参数，svm通过最大化几何间隔求得最优平面</p><h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><h3 id="梯度上升"><a href="#梯度上升" class="headerlink" title="梯度上升"></a>梯度上升</h3><p>批处理的方式，将所有数据点一次输入处理</p><p>（1）初始化回归系数，以及每次向梯度方向变化的步长</p><p>根据每个特征的个数，将回归系数初始化为1，假设有3个特征，则回归系数被初始化为3*1的列向量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m,n=shape(dataMat)</span><br><span class="line">weights=ones((n,<span class="number">1</span>))</span><br><span class="line">alpha=<span class="number">0.01</span></span><br></pre></td></tr></table></figure><p>（2）不断迭代，直至回归系数稳定</p><p>一般来说，回归系数稳定很难判定，因此通过控制迭代次数来修正回归系数</p><p>这里的dataMat格式为<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200324163550.png" alt="dataMat">每个数据点都有三个特征</p><p>weight的格式为<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200324164025.png" alt="weights">dataMat和weight两者相乘，拿第一行来说得到x01w0+x11w1+x21*w0，每行都得出这个值，将每行的结果通过sigmoid函数预测即可得到预测得值，用实际值减去该值，则为误差值。因为这个结果时100 *1 的矩阵，所以实际标签矩阵也应该是100 * 1的矩阵，因此，输入的labelMat需要进行转置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#对输入的数据进行处理</span></span><br><span class="line">labelMat=labelMat.transpose()</span><br><span class="line">numIter=<span class="number">500</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(numIter):</span><br><span class="line">  h=sigmoid(dataMat*weight)</span><br><span class="line">  error=y-h</span><br><span class="line">  weights=weights+alpha*dataMat.transpose()*error <span class="comment">#这里error是100 *1 的矩阵，矩阵乘法要求行列相同，因此x应该转换为3*100的矩阵进行计算</span></span><br></pre></td></tr></table></figure><h3 id="随机梯度上升"><a href="#随机梯度上升" class="headerlink" title="随机梯度上升"></a>随机梯度上升</h3><p>上面的批处理方式的梯度上升方法每次都要遍历整个数据集，当数据量大的时候就比较费时，因此引入一种在线学习方式的梯度上升，每次随机选择一个样本对回归系数进行修正</p><p>（1）初始化回归系数</p><p>因为每次是对一个数据点处理，因此，只需要初始化一个长度为n的数组，n是数据集中数据特征的个数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">m,n=shape(dataMat)</span><br><span class="line">weights=ones(n)</span><br></pre></td></tr></table></figure><p>（2）每次的步长都更新</p><p>当刚开始的时候离最优值很远，可以步长大点，快速向最优值逼近，当到后面的时候，需要减小步长，慢慢逼近，不然可能会错过最优值。</p><p>j控制迭代次数，i控制每个样本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alpha=<span class="number">4</span>/(<span class="number">1</span>+i+j)+<span class="number">0.01</span></span><br></pre></td></tr></table></figure><p>（3）随机选择样本修正回归系数</p><p>设置一个dataIndex里面存储0 - m-1的数字，每次通过random.uniform生成一个在0 - len(dataIndex)之间的随机数，代表此次选择这行的样本进行回归系数的修正，每次将该次选择的随机数在dataIndex中删除，防止下次重复选择</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#为了实现对应位置相乘dataMat需要转换为数组</span></span><br><span class="line">dataArr=array(dataMat)</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(numIter):</span><br><span class="line">  dataIndex=list(range(m))</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">    alpha=<span class="number">4</span>/(<span class="number">1</span>+i+j)+<span class="number">0.01</span></span><br><span class="line">    </span><br><span class="line">    randomIndex=int(random.uniform(<span class="number">0</span>,len(dataIndex)))</span><br><span class="line">    chooseIndex=dataIndex[randomIndex]</span><br><span class="line">    h=sigmoid(dataArr[chooseIndex]*weights) <span class="comment">#datamat[i]是1*3数组，weights是1*3数组，对应位置相乘</span></span><br><span class="line">    error=labelMat[chooseIndex]-h</span><br><span class="line">    weights=weights+alpha*error*dataMat[]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">del</span>(dataIndex[randomIndex])</span><br></pre></td></tr></table></figure><p>该方法不需要对矩阵进行转置，运算简洁，同时每次迭代就修改回归系数m次，而批处理的方法每次只能修改一次，这种方法只需要迭代很少的次数，回归系数就趋于稳定了。</p><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>因为数据集本身只有两列，即两个数据特征，因此x0设置为1，w0作为偏置，最终得出的拟和直线为y=w0+w1x1+w2x2，因为使用的是sigmoid函数，分割的直线的y值应该是0，因为sigmoid(0)=0.5，直线转换为w0+w1x1+w2x2=0 =&gt; x2=-w0-w1x1/w2，用plt.plot(x,x2)，这里的x可以在x的取值范围内，取一定步长进行显示（利用arange），可以将图像画出来，如下图</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200324170500.png" alt="效果图"></p><h2 id="应用实例"><a href="#应用实例" class="headerlink" title="应用实例"></a>应用实例</h2><p>问题：用logistic回归预测得疝气病的病马死亡率</p><h3 id="数据集描述"><a href="#数据集描述" class="headerlink" title="数据集描述"></a>数据集描述</h3><p>数据集是从UCI上下载的，里面包含22列属性，其中前21列是马的一些生理特征，最后一列是是否存活，属于类别标签。该案例是从前21列的特征中的出回归系数，得到拟合的回归直线，对马是否存活进行分类。</p><p>该数据集中部分数据（约30%）丢失，对特征丢失的情况进行处理：由于logistic回归中z=w0x0+w1x1+…，其中当特征等于0的时候不影响等式的结果，同时因为使用的判定类别的函数为sigmoid函数，当取值为0，即simoid(0)=0.5，不影响分类，所有丢失的特质均设置为0；对类别标签丢失的情况进行处理：当类别标签丢失的时候无法判断马的存活情况，因此将该条数据丢弃。</p><h3 id="应用描述"><a href="#应用描述" class="headerlink" title="应用描述"></a>应用描述</h3><p>（1）类别判定</p><p>对输入的向量乘以回归系数求和，如果大于0.5，则分类为1，反之分类为0</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">prob=sigmoid(sum(inX*weights))</span><br><span class="line"><span class="keyword">if</span> prob&gt;<span class="number">0.5</span>:</span><br><span class="line">  <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure><p>（2）建立logistic回归模型并进行测试</p><p>训练集建立模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">trainingFile=open(<span class="string">"horseColicTraining.txt"</span>)</span><br><span class="line">trainingMat=[]</span><br><span class="line">trainingLabels=[]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> curLine <span class="keyword">in</span> trainingFile.readlines():</span><br><span class="line">pstLine=curLine.strip().split(<span class="string">"\t"</span>)</span><br><span class="line">dataList=[]</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">21</span>):</span><br><span class="line">dataList.append(float(pstLine[j]))</span><br><span class="line">trainingMat.append(dataList)</span><br><span class="line">trainingLabels.append(float(pstLine[<span class="number">21</span>]))</span><br><span class="line"></span><br><span class="line">weights=stoGradAscent1(trainingMat,trainingLabels,<span class="number">500</span>)</span><br></pre></td></tr></table></figure><p>测试次测试错误率：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">testFile=open(<span class="string">"horseColicTest.txt"</span>)</span><br><span class="line"></span><br><span class="line">testMat=[]</span><br><span class="line">testLabels=[]</span><br><span class="line"><span class="keyword">for</span> curLine <span class="keyword">in</span> testFile.readlines():</span><br><span class="line">pstLine=curLine.strip().split(<span class="string">"\t"</span>)</span><br><span class="line">dataList=[]</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">21</span>):</span><br><span class="line">dataList.append(float(pstLine[j]))</span><br><span class="line">testMat.append(dataList)</span><br><span class="line">testLabels.append(float(pstLine[<span class="number">21</span>]))</span><br><span class="line"></span><br><span class="line">error=<span class="number">0</span></span><br><span class="line">n=shape(testMat)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">predictData=classfyVec(testMat[i],weights)</span><br><span class="line"><span class="keyword">if</span>(predictData!=testLabels[i]):</span><br><span class="line">error+=<span class="number">1</span></span><br><span class="line">errorRate=float(error)/n</span><br><span class="line">print(<span class="string">"error rate is:"</span>+str(errorRate))</span><br></pre></td></tr></table></figure><p>（3）取平均数</p><p>一次数据可能存在偶然性，因此运行10次取平均值，作为该模型的分类正确率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">numTest=<span class="number">10</span></span><br><span class="line">errorCount=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(numTest):</span><br><span class="line">errorCount+=colicTest()</span><br><span class="line"></span><br><span class="line">allError=float(errorCount/<span class="number">10</span>)</span><br><span class="line">print(<span class="string">"the average of ten times error is:"</span>)</span><br><span class="line">print(allError)</span><br></pre></td></tr></table></figure><p>最终运行10次的结果为：</p><blockquote><p>error rate is:0.6268656716417911<br>error rate is:0.3582089552238806<br>error rate is:0.26865671641791045<br>error rate is:0.2537313432835821<br>error rate is:0.44776119402985076<br>error rate is:0.29850746268656714<br>error rate is:0.47761194029850745<br>error rate is:0.26865671641791045<br>error rate is:0.26865671641791045<br>error rate is:0.5522388059701493<br>the average of ten times error is:<br>25.6</p></blockquote><p>平均错误率25%左右，因为数据集有30%的数据丢失，因此错误率相对较高。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;算法原理&quot;&gt;&lt;a href=&quot;#算法原理&quot; class=&quot;headerlink&quot; title=&quot;算法原理&quot;&gt;&lt;/a&gt;算法原理&lt;/h2&gt;&lt;h3 id=&quot;原理分析&quot;&gt;&lt;a href=&quot;#原理分析&quot; class=&quot;headerlink&quot; title=&quot;原理分析&quot;&gt;&lt;/a&gt;原理分析&lt;/h3&gt;&lt;p&gt;（1）数据拟合&lt;/p&gt;&lt;p&gt;给出一些数据，用一条直线来拟合这些数据称为logictic回归&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200324153824.png&quot; alt=&quot;直线拟合&quot;&gt;&lt;/p&gt;&lt;p&gt;用logistic回归来进行数据分类是通过给出的所有数据点拟合出分类边界的直线，确定这条直线的系数就称为回归系数，当用回归系数乘以当前特征则得出该条直线：&lt;/p&gt;&lt;p&gt;z=w0 * x0+w1 * x1+…+wn * xn，其中w0 - wn是回归系数，x0 - xn是各个特征，得出的z是当前特征下的预测值。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="回归" scheme="https://www.xiapf.com/tags/%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>Leecode算法学习笔记（五）面试高频题——topk问题</title>
    <link href="https://www.xiapf.com/blogs/topK/"/>
    <id>https://www.xiapf.com/blogs/topK/</id>
    <published>2020-03-23T10:56:18.000Z</published>
    <updated>2020-03-23T10:57:39.102Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>给出一个数组，求数组中前k个最大/最小的数或者求数组中第k个最大/最小的数。</p><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><h3 id="暴力法"><a href="#暴力法" class="headerlink" title="暴力法"></a>暴力法</h3><p>（1）思路：调用sort函数将数组排序，选择前k个或者第k个输出</p><p>（2）复杂度分析：</p><p>时间复杂度：主要是在比较排序上了，该方法对整个数组排序，c++中的sort使用的是快速排序，时间复杂度为O(nlogn)</p><p>空间复杂度：没有用到额外的空间，复杂度为O（1）</p><a id="more"></a><p>（3）代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sort(num.<span class="built_in">begin</span>(),num.<span class="built_in">end</span>())</span><br></pre></td></tr></table></figure><h3 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h3><p>（1）思路：</p><p>维护一个k个元素的堆，将数组中元素和队首元素比较，形成最终稳定的堆：</p><p>求前k个最大的数即需要维护一个小顶堆，队首是最小的元素，当读入的数据大于等于队首元素，则加入堆中，保证其余元素都大于队首的元素，每次仅需比较队首元素即可，当遍历到最后，堆中剩下前k个最大的数；</p><p>当求前k个最小的数即需要维护一个大顶堆，队首是最大的元素，当读入的数据小于等于队首元素，则加入堆中，保证其他元素都小于队首的元素，最终，堆中剩下前k个最小的数。</p><p>（2）复杂度分析：</p><p>时间复杂度：每次维护一个k个元素的堆，相当于建立一个二叉树，深度是logk，最差的情况是n个数都需要排序比较，则复杂度是O（nlogk）</p><p>空间复杂度：用到了k大小的堆，复杂度为O（k）</p><p>（3）代码：</p><p>以求前k个最大的元素为例</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//1.建小顶堆 后两个元素缺省，自动建立大顶堆</span></span><br><span class="line">prioriy_queue&lt;<span class="keyword">int</span>,<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;,greater&lt;<span class="keyword">int</span>&gt;&gt; p;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;k;i++)</span><br><span class="line">  p.push(num[i])</span><br><span class="line">  </span><br><span class="line"> <span class="comment">//2.队首元素比较</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=k;i&lt;num.<span class="built_in">size</span>();i++)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="comment">//队首元素小，则把新元素加入</span></span><br><span class="line">  <span class="keyword">if</span>(p.top()&lt;num[i])</span><br><span class="line">  &#123;</span><br><span class="line">    p.pop();</span><br><span class="line">    p.push(num[i]);</span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h3><p>（1）思路：快速排序是以一个基准点进行排序，当在基准点的左边都是小于该数的，右边都是大于该数的。从这个思路出发可以想到，求前k个最小的数可以转换为在第k个位置进行排序，左边是小于的数，右边是大于的数，求前k个最小的数就是前k个数。因为快速排序时每次排序可以得到当前基准点的位置，以此判断是否在第k个位置。这种方法不用遍历整个数组，排序的次数减少了。快速排序处理两边的数，这边只需要对一边的数进行排序。</p><p>当求前k个最大的数时可以进行转换：首先第k个最大的数，如果6个数，从小到大排列，则索引是6-k，即求第6-k个最小的数，前k个最大的数，就是从该位置到数组末尾。</p><p>eg.假设有6个数字2，5，7，1，3，9，排序之后为1，2，3，5，7，9，求第3大的数字即5，从左往右数，元素5的索引为3，3=6-3（k的值），所以第k个最大的数=第6-k个最小的数</p><p>（2）复杂度分析：</p><p>时间复杂度：相比较sort方法来说，减少了排序的次数，获取每次排序的位置，当该位置小于k，则说明下一次排序需要在当前位置和right位置之间寻找第k个位置，反之需要在left和当前位置之间寻找。每次减少一半需要排序的数字，则为n+n/2+…=O（n）</p><p>空间复杂度：递归调用栈，复杂度为O（n）</p><p>（3）代码：</p><p>以求前k个最大的元素为例</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//主函数</span></span><br><span class="line"><span class="keyword">int</span> pos=num.<span class="built_in">size</span>()-k <span class="comment">//进行位置转换</span></span><br><span class="line">quickSort(num,<span class="number">0</span>,num.<span class="built_in">size</span>(),pos)</span><br><span class="line"></span><br><span class="line"><span class="comment">//快速排序函数</span></span><br><span class="line"><span class="keyword">void</span> quickSort(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;num,<span class="keyword">int</span> lef,<span class="keyword">int</span> right,<span class="keyword">int</span> k)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">int</span> p=partition(num,left,right);</span><br><span class="line">  <span class="keyword">if</span>(p==k)</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">if</span>(p&gt;k)</span><br><span class="line">      quickSort(num,left,p<span class="number">-1</span>,k)</span><br><span class="line">     <span class="keyword">else</span></span><br><span class="line">       quickSort(num,p+<span class="number">1</span>,right,k)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//排序位置函数</span></span><br><span class="line"><span class="keyword">int</span> partition(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;num,<span class="keyword">int</span> left,<span class="keyword">int</span> right)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">int</span> i=left;</span><br><span class="line">  <span class="keyword">int</span> j=right;</span><br><span class="line">  <span class="keyword">int</span> base=num[i];</span><br><span class="line">  <span class="comment">//找到基准值的位置</span></span><br><span class="line">  <span class="keyword">while</span>(i&lt;j)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="comment">//找到比基准值小的</span></span><br><span class="line">    <span class="keyword">while</span>(i&lt;j&amp;&amp;num[j]&gt;=base)</span><br><span class="line">      j--;</span><br><span class="line">    <span class="comment">//找到比基准值大的</span></span><br><span class="line">    <span class="keyword">while</span>(i&lt;j&amp;&amp;num[i]&lt;=base)</span><br><span class="line">      i++;</span><br><span class="line">    <span class="comment">//交换</span></span><br><span class="line">    <span class="keyword">if</span>(i&lt;j)</span><br><span class="line">      swap(num[i],num[j]);</span><br><span class="line">  &#125;</span><br><span class="line">  num[left]=num[i];</span><br><span class="line">  num[i]=base;</span><br><span class="line">  <span class="keyword">return</span> i;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;问题描述&lt;/h2&gt;&lt;p&gt;给出一个数组，求数组中前k个最大/最小的数或者求数组中第k个最大/最小的数。&lt;/p&gt;&lt;h2 id=&quot;解题思路&quot;&gt;&lt;a href=&quot;#解题思路&quot; class=&quot;headerlink&quot; title=&quot;解题思路&quot;&gt;&lt;/a&gt;解题思路&lt;/h2&gt;&lt;h3 id=&quot;暴力法&quot;&gt;&lt;a href=&quot;#暴力法&quot; class=&quot;headerlink&quot; title=&quot;暴力法&quot;&gt;&lt;/a&gt;暴力法&lt;/h3&gt;&lt;p&gt;（1）思路：调用sort函数将数组排序，选择前k个或者第k个输出&lt;/p&gt;&lt;p&gt;（2）复杂度分析：&lt;/p&gt;&lt;p&gt;时间复杂度：主要是在比较排序上了，该方法对整个数组排序，c++中的sort使用的是快速排序，时间复杂度为O(nlogn)&lt;/p&gt;&lt;p&gt;空间复杂度：没有用到额外的空间，复杂度为O（1）&lt;/p&gt;
    
    </summary>
    
    
      <category term="算法" scheme="https://www.xiapf.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="https://www.xiapf.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="topK" scheme="https://www.xiapf.com/tags/topK/"/>
    
  </entry>
  
  <entry>
    <title>非监督学习笔记（一）——K均值聚类</title>
    <link href="https://www.xiapf.com/blogs/kMeans/"/>
    <id>https://www.xiapf.com/blogs/kMeans/</id>
    <published>2020-03-19T07:46:28.000Z</published>
    <updated>2020-03-19T07:51:58.568Z</updated>
    
    <content type="html"><![CDATA[<p>非监督学习和监督学习不同，监督学习知道需要寻找的内容，即目标变量，会通过数据训练出能得出目标变量学习模型，而监督学习不知道目标变量，非监督学习中有一种聚类方法，把相似的点放在一个簇（类）中，类似于全自动分类，度量数据的相似有很多函数，包括欧式距离，球面距离等。</p><h2 id="含义"><a href="#含义" class="headerlink" title="含义"></a>含义</h2><p>k均值聚类属于一种聚类算法，通过给定的数值k，将数据点分为k类，分类依据为相似的数据点放到一个簇里，这里簇的中心称为质心，一般可以用距离公式来度量数据点之间的相似度。假设以欧式距离作为相似度评价依据，则点到各个质心中的最小距离的那个簇是该数据点所在的位置，最终可以将数据分为k类。</p><a id="more"></a><h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><p>注：因为度量数据之间相似度的函数通常都需要将数据转换为矩阵进行处理，所以程序中需要注意数据的存储形式，如果是列表形式需要转换为矩阵进行处理。</p><p>当有赋值操作的时候，mat类型不能直接赋值，需要转换为list类型。例如，matop=matop.tolist()[0]</p><p>（1）读入数据</p><p>根据输入的文件名读入数据，根据相似度计算方法，需要把数据转换为float数值类型，同时转换为矩阵</p><p>（2）确定相似度计算方法</p><p>以欧式距离为例：计算两个矩阵对应位置相减平方求和，最后再开根号作为两个矩阵之间的距离</p><p>sqrt(sum(power)(veca-vecb))   列表或者矩阵之间求次方用power函数</p><p>（3）确定初始质心选择方法</p><p>设定输入的数据形式为（x,y）</p><p>一般初始质心使用随机选择的方法，为了将随机矩阵的值控制在数据范围内，根据每列的最大值和最小值，生成k*n（n此时为2）个随机数据，以此作为初始的k个质心。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#获取数据点的列数</span></span><br><span class="line">n=shape(data)[<span class="number">1</span>]</span><br><span class="line"><span class="comment">#随机数组</span></span><br><span class="line">centroids=mat(zeros((k,n)))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">  minI=min(data[:,i])</span><br><span class="line">  maxI=max(data[:,i])</span><br><span class="line">  rangeI=float(maxI-minI)</span><br><span class="line">  centroids[:,i]=mat(minI+rangeI*rand(k,<span class="number">1</span>))<span class="comment">#返回k*1个0~1之间的随机数</span></span><br></pre></td></tr></table></figure><p>（4）根据相似度将数据分类</p><p>a）初始化质心：以随机选择的方式</p><p>b）当簇仍在发生变化</p><p>对每个数据点，计算当前点到各个质心的距离，选择最近的距离，将该点放入</p><p>将簇内所有数据点取均值，重新计算当前的质心</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">n=shape(data)[<span class="number">0</span>]</span><br><span class="line">clusterAssment=mat(zeros((n,<span class="number">2</span>)))<span class="comment">#存储对应行的点属于哪个簇，以及误差</span></span><br><span class="line"><span class="keyword">while</span>(簇在变化):</span><br><span class="line">  flag=false<span class="comment">#标记簇是否在变化</span></span><br><span class="line">  <span class="comment">#1.对每个数据点</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">    minDist=inf</span><br><span class="line">    minIndex=<span class="number">-1</span></span><br><span class="line">    <span class="comment">#计算该点到各个质心的距离</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(k):</span><br><span class="line">      pstCurret=distmes(centroids[k,:],data[i,:])<span class="comment">#distmes为相似度计算方法</span></span><br><span class="line">      <span class="keyword">if</span>(pstCurret&lt;minDist):</span><br><span class="line">        最小值交换</span><br><span class="line">    <span class="keyword">if</span>(clusterAssment[i,<span class="number">0</span>]!=minIndex):</span><br><span class="line">      flag=true<span class="comment">#说明簇还在变化</span></span><br><span class="line">     <span class="comment">#2.将点加入到簇中</span></span><br><span class="line">    clusterAssment[i,:]=minIndex,minDist**<span class="number">2</span></span><br><span class="line">  <span class="comment">#3.对每个簇（即质心），对簇内数据点取均值</span></span><br><span class="line">  <span class="keyword">for</span> cent <span class="keyword">in</span> range(k):</span><br><span class="line">    pstCurret=data[nonzero(clusterAssment[:,<span class="number">0</span>].A[<span class="number">0</span>]==cent)]</span><br><span class="line">    centroids[cent,:]=mean(pstCurret,axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h2 id="算法改进"><a href="#算法改进" class="headerlink" title="算法改进"></a>算法改进</h2><p>k均值聚类算法依赖与用户给定的数值k，此时无法确定根据k得出的簇是最优的。评价算法的优劣是根据误差平方和（称SSE），即数据真实值为y，预测值为y1，则误差平方和为（y-y1）2，k均值聚类算法的改进是根据SSE最小时，将数据分为k个类。</p><p>初始时将所有数据点作为一个簇，每次将簇一分为二，选择哪个簇进行划分要看能否最大化减少sse，当划分之后的sse比原来的小，则保存当前的质心，最终得到的簇误差平方和最小，改进的算法也称二分k均值聚类。</p><p>（1）将所有数据作为一个簇，取均值得到初始的质心</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#所有数据取均值</span></span><br><span class="line">centroids0=mean(data,axis=<span class="number">0</span>).tolist[<span class="number">0</span>]</span><br><span class="line"><span class="comment">#转换为矩阵形式</span></span><br><span class="line">centroids=mat(centroids0)</span><br><span class="line"><span class="comment">#求初始质心到各点的距离</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">  clusterAssment[i,<span class="number">1</span>]=distmes(centroids,data[i,:])**<span class="number">2</span></span><br></pre></td></tr></table></figure><p>（2）当簇的长度小于k</p><p>a）对所有簇</p><p>将簇一分为二，得到此时的误差，找到一分为二之后误差最小的簇</p><p>b）误差最小的簇，将其保存</p><p>注：当有赋值操作的时候，mat类型不能直接赋值，需要转换为list类型。例如，matop=matop.tolist()[0]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将每个质心放入一个大列表中进行操作</span></span><br><span class="line">centroList=[centroids0]</span><br><span class="line"><span class="keyword">while</span>(len(centroList)&lt;k):</span><br><span class="line">  lossSSE=inf</span><br><span class="line">  <span class="comment">#1.对所有簇，找误差最小的</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(len(centroList)):</span><br><span class="line">    <span class="comment">#当前簇的所有数据点 clusterAssment第0列</span></span><br><span class="line">    pstCurrent=data[nonzero(clusterAssment[:,<span class="number">0</span>].A==i)[<span class="number">0</span>],:]</span><br><span class="line">    <span class="comment">#二分簇</span></span><br><span class="line">    centroids,splitCluster=kmeans(pstCurrent,<span class="number">2</span>)</span><br><span class="line">    <span class="comment">#求误差</span></span><br><span class="line">    sseSplit=sum(splitCluster[:,<span class="number">1</span>])</span><br><span class="line">    sseNotSplit=sum(clusterAssment[nonzero(clusterAssment[:,<span class="number">0</span>].A!=i)[<span class="number">0</span>],<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">if</span>(sseSplit+sseNotSplit&lt;lossSSE):</span><br><span class="line">      bestCent=i</span><br><span class="line">      bestCluster=splitCluster.copy()</span><br><span class="line">      bestCentroid=centroids</span><br><span class="line">      lossSSE=sseSplit+sseNotSplit</span><br><span class="line">   <span class="comment">#2.更新簇的结果，一个取代成为第i个簇，另一个成为第m+1个簇，原来一共有m个簇</span></span><br><span class="line">  bestCluster[nonzero(bestCluster[:,<span class="number">0</span>].A==<span class="number">0</span>)[<span class="number">0</span>],<span class="number">0</span>]=bestCent</span><br><span class="line">  bestCluster[nonzero(bestCluster[:,<span class="number">0</span>].A==<span class="number">1</span>)[<span class="number">0</span>],<span class="number">0</span>]=len(centroList)</span><br><span class="line">  </span><br><span class="line">  <span class="comment">#3.将最佳分类的质心加入簇列表 二分类所以质心是两个</span></span><br><span class="line">  centroList[bestCent]=bestCentroid[<span class="number">0</span>,:].tolist()[<span class="number">0</span>]</span><br><span class="line">  centroList.append(bestCentroid[<span class="number">1</span>,:].tolist()[<span class="number">0</span>])</span><br><span class="line">  </span><br><span class="line">  <span class="comment">#4.将簇分类的结果加入列表中</span></span><br><span class="line">  clusterAssment[nonzero(clusterAssment[:,<span class="number">0</span>].A==bestCent)[<span class="number">0</span>],:]=bestCluster</span><br></pre></td></tr></table></figure><h2 id="应用实例"><a href="#应用实例" class="headerlink" title="应用实例"></a>应用实例</h2><p>给出地图上一些点的经度和纬度，将其分为合适的类，使得通过中心点能很快到达其余点。同时观察不同的簇数目k的分类效果。</p><p>（1）数据来源</p><p>从谷歌api上获取一些club的经度和纬度保存为place.txt，其中每一行代表一个club的位置，最后两行保存了对应地点的纬度和经度</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200319144017.png" alt="数据点"></p><p>（2）运行kmeans.clubsCluster(“place.txt”,5)</p><p>结果如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200319150954.png" alt="效果图"></p><p>可以看出数据点被分成了5个簇，每个簇的中心用“+”号标记出来了，此时从簇的中心到其余点是最近的。</p><p>（3）观察不同k下的运行结果</p><p>当分的簇越多，簇的误差就越小，但是数量越多不符合实际，因此需要找一个折中点。</p><p>（4）程序：</p><p>a）相似度计算方法使用球面距离公式：</p><p>设所求点A纬度角β1，经度角α1， 点B 纬度角β2， 经度角α2  R*arccos[cosβ1cosβ2cos（α1-α2）+sinβ1sinβ2] </p><p>b）聚类：</p><p>1.读入文件中最后两行数据，并转换为数值类型的mat类型矩阵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fr=open(filename)</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">  <span class="comment">#以\t分割每行数据，并且去掉每行最后的回车符</span></span><br><span class="line">  lineList=line.rstrip(<span class="string">"\n"</span>).split(<span class="string">"\t"</span>)</span><br><span class="line">  dataList.append((float(lineList[<span class="number">4</span>],float(lineList[<span class="number">5</span>])))</span><br><span class="line">dataList=mat(dataList)</span><br></pre></td></tr></table></figure><p>2.使用二分k均值分类，得出最佳分类簇centroids</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">centroids,clusterAssment=biKmeans(dataList,k,distMes=球面距离公式)</span><br></pre></td></tr></table></figure><p>3.画图</p><p>将得到的分类结果展示出来，使用matplotlib库中的pyplot进行画图</p><p>画图准备</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">fig=plt.figure()</span><br><span class="line"><span class="comment">#设置显示图形的位置和比例</span></span><br><span class="line">ret=[<span class="number">0.1</span>,<span class="number">0.1</span>,<span class="number">0.8</span>,<span class="number">0.8</span>]</span><br><span class="line">.....</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>画背景</p><p>读入背景图片，并显示</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">img=plt.imread(背景图片)</span><br><span class="line"><span class="comment">#按照横纵坐标数据比列设置坐标轴,并会去掉预设坐标数字</span></span><br><span class="line">axpos=dict[xticks[],yticks[]]</span><br><span class="line"><span class="comment">#为显示的图像设置底层区域</span></span><br><span class="line">axis0=fig.add_axis(ret,label=<span class="string">""</span>,**axpos)</span><br><span class="line">axis0.imshow(img)</span><br></pre></td></tr></table></figure><p>加上**axpos效果，此时按照数据点数值大小设置坐标轴</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200319150954.png" alt="加上**axpos"></p><p>去掉**axpos效果，发现坐标轴数字出现重叠</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200319151338.png" alt="去掉**axpos"></p><p>画数据点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#设置图案标记</span></span><br><span class="line">markerList=[<span class="string">'s'</span>,....]</span><br><span class="line"><span class="comment">#为显示的图像增加子区域,frameon设置是否覆盖下面的区域</span></span><br><span class="line">axis1=fig.add_axis(ret,label=<span class="string">""</span>,frameon=false)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">  <span class="comment">#获取第i个簇的数据点</span></span><br><span class="line">  pstCurrent=dataList[nonzero(clusterAssment[:,<span class="number">0</span>].A==i)[<span class="number">0</span>],:]</span><br><span class="line">  pstMarker=markerList[i/len]</span><br><span class="line">  <span class="comment">#二维转换为一维显示</span></span><br><span class="line"> axis1.scatter(pstCurrent[:,<span class="number">0</span>].flattern().A[<span class="number">0</span>],pstCurrent[:,<span class="number">1</span>].flattern().A[<span class="number">0</span>],marker=pstMarker,s=<span class="number">90</span>)</span><br></pre></td></tr></table></figure><p>注：</p><p>1.subplot，add_axis的区别</p><p>subplot是设置一整个区域中的子图，不存在重叠：subplot(221)表示将显示图像的区域分为2*2个，当前图案显示在第一个位置。</p><p>add_axis是设置同一个图像中的区域，存在重叠。</p><p>2.scatter，plot的区别</p><p>scatter是绘制数据点，属于离散型图案</p><p>plot是将数据点连接起来，属于连续型图案</p><p>画每个簇的中心</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(k):  </span><br><span class="line">  axis1.scatter(centroids[i,<span class="number">0</span>],centroids[i,<span class="number">1</span>],marker=<span class="string">"+"</span>,s=<span class="number">300</span>)<span class="comment">#每个质心颜色会不一样</span></span><br><span class="line">  或者 axis1.scatter(centroids[:,<span class="number">0</span>].flattern().A[<span class="number">0</span>],centroids[:,<span class="number">1</span>].flattern().A[<span class="number">0</span>],marker=<span class="string">"+"</span>,s=<span class="number">300</span>)<span class="comment">#每个质心颜色一样</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;非监督学习和监督学习不同，监督学习知道需要寻找的内容，即目标变量，会通过数据训练出能得出目标变量学习模型，而监督学习不知道目标变量，非监督学习中有一种聚类方法，把相似的点放在一个簇（类）中，类似于全自动分类，度量数据的相似有很多函数，包括欧式距离，球面距离等。&lt;/p&gt;&lt;h2 id=&quot;含义&quot;&gt;&lt;a href=&quot;#含义&quot; class=&quot;headerlink&quot; title=&quot;含义&quot;&gt;&lt;/a&gt;含义&lt;/h2&gt;&lt;p&gt;k均值聚类属于一种聚类算法，通过给定的数值k，将数据点分为k类，分类依据为相似的数据点放到一个簇里，这里簇的中心称为质心，一般可以用距离公式来度量数据点之间的相似度。假设以欧式距离作为相似度评价依据，则点到各个质心中的最小距离的那个簇是该数据点所在的位置，最终可以将数据分为k类。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="k均值聚类" scheme="https://www.xiapf.com/tags/k%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>监督学习笔记（二）——朴素贝叶斯分类器</title>
    <link href="https://www.xiapf.com/blogs/bayes/"/>
    <id>https://www.xiapf.com/blogs/bayes/</id>
    <published>2020-03-12T11:25:59.000Z</published>
    <updated>2020-03-12T11:27:00.635Z</updated>
    
    <content type="html"><![CDATA[<h2 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h2><p>（1）问题原理：朴素贝叶斯分类的依据是概率，假设需要分为两类c1和c2，则某个数据点属于哪一类，需要计算p(c1|x)和p{c2|x)，即计算数据点x来自c1和来自c2中的概率哪个大，如果p(c1|x)&gt;p{c2|x)，则x被分到c1类别中，反之被分到c2类别中。</p><p>（2）计算原理：上述概率属于条件概率，根据公式p(c|x)=p(x|c) * p(c)/ p(x)可以得到，当求（x,y）来自哪个类别时，即用（x,y）替换x，即求p(c|x,y),代入条件概率公式中得，p(c|x,y)=p(x,y|c) * p(c)/p(x,y)，因此问题转换为求p(x,y|c) * p(c)/p(x,y)中三个概率的值。</p><a id="more"></a><p>求p(c)：对于所有数据点来说，p(c)是出现某类别的数据/总数据</p><p>求p(x,y|c)：因为朴素贝叶斯的假设是所有特征是独立的，那么p(x,y|c)=p(x1,y1|c) * p(x2,y2|c) *… * p(xn,yn|c)，p(xi,yi|c)是求在某个类别下，每个特征的数据占当前类别总数据的概率，每个特征的概率求解出后连乘即可得到该概率</p><p>求p(x,y)：因为对于所有类别来说p(x,y)都是相同的，所以只要比较上面两个概率乘积的大小即可</p><h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><p>以二分类为例子：</p><p>（1）载入数据集</p><p>1.读取文件路径，并将读取的文件按照单词进行划分，划分使用正则表达式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">regex=re.compile(<span class="string">"\\W+"</span>)</span><br><span class="line">listToken=regex.split(file)</span><br></pre></td></tr></table></figure><p>2.将数据进行存储，同时存储数据标签</p><p>注：因为此时存储的数据集是文本，在测试和训练的时候需要转换为单词向量的形式</p><p>输入词汇表和需要变为单词向量的文本，在词汇表中出现的单词的位置，在单词向量中对应设置为1，其余为0，这样就能构造出单词向量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> inputWord:</span><br><span class="line">  <span class="keyword">if</span> word <span class="keyword">in</span> vocabList:</span><br><span class="line">    returnVec[vocaList.index(word)]+=<span class="number">1</span><span class="comment">#在词汇表中单词的位置的，对应的单词向量的位置设置为1</span></span><br></pre></td></tr></table></figure><p>（ * ）可增加的部分：删除高频词</p><p>因为有些词例如a,about,the等这些常用词在很多地方都会出现，这些高频出现的词，可能会对分类的概率产生影响，可以对所有数据进行遍历，找到出现频率最高的单词在词汇表中删除（可以取前20或30等，根据数据量决定）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcTopWord</span><span class="params">(fullText,vocabList)</span>:</span></span><br><span class="line">  //存储出现概率及单词，用到键值对</span><br><span class="line">  wordList=&#123;&#125;</span><br><span class="line">  <span class="keyword">for</span> word <span class="keyword">in</span> fullText:</span><br><span class="line">    wordlist[word]=fullText.(word)<span class="comment">#在整篇文档计算单词出现的次数，count</span></span><br><span class="line">  //从大到小排序,用值排序</span><br><span class="line">  rankList=sorted(wordList.items(),key=operator.itemgetter(<span class="number">1</span>),reverse=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>（2）得出词汇表</p><p>找到数据集中不重复的单词作为词汇表，为后续统计每个单词出现的频率（即每个特征的概率）做准备</p><p>用set得到不重复的单词</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> dataWord <span class="keyword">in</span> dataSet:</span><br><span class="line">vocaList=vocaList|set(dataWord)</span><br></pre></td></tr></table></figure><p>（3）随机选择一部分作为测试集和训练集</p><p>将训练集设置为总数据集数量，测试集用随机数设置（random.uniform）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">trainSet=list(range(lenData))</span><br><span class="line"><span class="comment">#假设随机选择5个测试集</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">  <span class="comment">#得出位置 int类型</span></span><br><span class="line">  testIndex=(int)(random.uniform(<span class="number">0</span>,len(trainSet)))</span><br><span class="line">  <span class="comment">#测试集的数据位置</span></span><br><span class="line">  testSet.appen(trainSet[testIndex])</span><br><span class="line">  <span class="comment">#训练集的数据位置</span></span><br><span class="line">  <span class="keyword">del</span>(trainSet[testIndex])</span><br></pre></td></tr></table></figure><p>（4）根据训练集训练贝叶斯分类器</p><p>1.计算p(c)、p(x,y|c)</p><p>计算p(c)：因为是两个类别，只需要计算p(c)，用1-p(c)可以得出另一个类别出现的概率。p(c)通过对类别标记中1出现的次数求和即为1类别出现的次数，再除以文档总数，即为p(c)</p><p>计算p(x,y|c)：对所有数据进行遍历，分布对0类别和1类别下，所有单词出现的概率进行计算</p><p>注：为防止出现很多很小的数：计算概率时用log，即p(x,y|c)用logp(x,y|c)表示</p><p>为防止各个特征的概率相乘出现0：初始设置p(x,y|c)=1，p(c)=2</p><p>2.训练的结果就是得出训练集下数据属于不同类别的概率</p><p>以两个类别为例，即最终得出在0类别和1类别下所有特征的出现的概率p(x,y|c0)，p(x,y|c1)和0类别、1类别数据出现的概率p(c)，得出这两个概率后，用测试集数据乘以不同类别下的p(ci|x,y)（即用数据 *p(x,y|ci) * p(c)），比较p(x,y|c0)和p(x,y|c1)得出分属的类别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#已求出p0V,p1V,pAuixs</span></span><br><span class="line"><span class="comment">#属于两个类别的概率，进行比较即可</span></span><br><span class="line">p1=sum(inputMatrix*p1V)+log(pAuixs)</span><br><span class="line">p0=sum(inputMatrix*p0V)+log(<span class="number">1</span>-pAuixs)</span><br></pre></td></tr></table></figure><p>（5）在测试集上，使用贝叶斯分类器，分类结果和实际标记对比，得出总体错误率</p><p>1.计算条件概率，即比较两个类别的概率，确定分类的结果</p><p>2.与实际结果比对，分类错误则进行标记</p><h2 id="应用案例"><a href="#应用案例" class="headerlink" title="应用案例"></a>应用案例</h2><p>广告内容倾向分类</p><p>该案例使用feedparse获取网页的rss源，对网页所有目录的列表读取其中的概要，输入两个网页，根据两个网页中单词出现的概率来进行分类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#获得不重复的单词列表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createVocabList</span><span class="params">(dataSet)</span>:</span></span><br><span class="line"><span class="comment">#set返回的是不重复的数据</span></span><br><span class="line">vocaList=set([])</span><br><span class="line"><span class="keyword">for</span> dataWord <span class="keyword">in</span> dataSet:</span><br><span class="line">vocaList=vocaList|set(dataWord)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> list(vocaList)<span class="comment">#返回列表</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2.0词袋模型，每个词可以出现多次</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bagOfWords2Vec</span><span class="params">(vocaList,inputWord)</span>:</span></span><br><span class="line">returnVec=[<span class="number">0</span>]*len(vocaList)</span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> inputWord:</span><br><span class="line"><span class="keyword">if</span>(word <span class="keyword">in</span> vocaList):</span><br><span class="line">returnVec[vocaList.index(word)]+=<span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">print(<span class="string">"the word:"</span>+str(word)+<span class="string">" is not in the vocaList"</span>)</span><br><span class="line"><span class="keyword">return</span> returnVec</span><br><span class="line"></span><br><span class="line"><span class="comment">#利用朴素贝叶斯进行文档分类</span></span><br><span class="line"><span class="comment">#主要是进行概率比较，条件概率定义为p(c|w)=p(w|c)*p(c)/p(w),对所有类别来说，p(w)都是相同的，因此可以不计算</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainNB0</span><span class="params">(trainMatrix,trainCatalog)</span>:</span></span><br><span class="line"><span class="comment">#0.计算不同类别下的文档概率即p(c)</span></span><br><span class="line"><span class="comment">#此处是二值分类，另一个概率可以用1-p(c)来计算</span></span><br><span class="line"><span class="comment">#0.0文档的个数</span></span><br><span class="line">numDocu=len(trainMatrix)</span><br><span class="line"><span class="comment">#0.1侮辱性文档的概率</span></span><br><span class="line"></span><br><span class="line">pAuixs=sum(trainCatalog)/numDocu</span><br><span class="line"><span class="comment">#0.2初始化累加值</span></span><br><span class="line"><span class="comment">#每行词汇的长度</span></span><br><span class="line">numWords=len(trainMatrix[<span class="number">0</span>])</span><br><span class="line"><span class="comment">#因为是独立的概率，如果其中一个为0，整个为0，避免这种情况，初始化为1</span></span><br><span class="line"><span class="comment">#p1num=zeros((numWords))</span></span><br><span class="line"><span class="comment">#p0num=zeros(numWords)</span></span><br><span class="line"><span class="comment">#p1Denom=0</span></span><br><span class="line"><span class="comment">#p0Denom=0</span></span><br><span class="line">p1num=ones((numWords))</span><br><span class="line">p0num=ones(numWords)</span><br><span class="line">p1Denom=<span class="number">2</span></span><br><span class="line">p0Denom=<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#1.按照不同类别计算p(w|c)即计算不同类别下词向量出现的次数</span></span><br><span class="line"><span class="comment">#根据当前类别下每个词出现的次数除以在该类别下所有单词出现的次数</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(numDocu):</span><br><span class="line"><span class="comment">#1.0 侮辱类别</span></span><br><span class="line"><span class="keyword">if</span>(trainCatalog[i]==<span class="number">1</span>):</span><br><span class="line">p1num+=trainMatrix[i]</span><br><span class="line">p1Denom+=sum(trainMatrix[i])</span><br><span class="line"><span class="comment">#1.1 非侮辱类别</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">p0num+=trainMatrix[i]</span><br><span class="line">p0Denom+=sum(trainMatrix[i])</span><br><span class="line"></span><br><span class="line"><span class="comment">#为了避免出现过小的数,出现下溢出，使用log</span></span><br><span class="line">p1Vec=log(p1num/p1Denom)</span><br><span class="line">p0Vec=log(p0num/p0Denom)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> p0Vec,p1Vec,pAuixs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#朴素贝叶斯分类器</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classfyNB</span><span class="params">(wordVec,p0Vec,p1Vec,pAuixs)</span>:</span></span><br><span class="line"><span class="comment">#比较单词向量乘以p(w|c)*p(c),因为p(w|c)使用了log处理，所以，这边的pauixs也需要加上log,log相加代表相乘</span></span><br><span class="line"><span class="comment">#因为是独立的，所以计算每个单词出现的概率p(wi|c)，因为做了log处理，这里sum等同于去掉log后概率相乘</span></span><br><span class="line">p1=sum(wordVec*p1Vec)+log(pAuixs)</span><br><span class="line">p0=sum(wordVec*p0Vec)+log(<span class="number">1</span>-pAuixs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(p1&gt;p0):</span><br><span class="line"><span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#测试分类器</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testingNB</span><span class="params">()</span>:</span></span><br><span class="line"><span class="comment">#0.载入实验样本</span></span><br><span class="line">listPosts,listLabels=loadDataSet();</span><br><span class="line"><span class="comment">#1.生成词汇表</span></span><br><span class="line">vocabList=createVocabList(listPosts)</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.文本转换为单词向量，方法中是对每行文本进行转换</span></span><br><span class="line">m=len(listPosts)</span><br><span class="line">wordVec=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">wordVec.append(setOfWords2Vec(vocabList,listPosts[i]))</span><br><span class="line"></span><br><span class="line"><span class="comment">#3.构造贝叶斯分类器</span></span><br><span class="line">p0Vec,p1Vec,pAuixs=trainNB0(wordVec,listLabels)</span><br><span class="line"></span><br><span class="line"><span class="comment">#4.测试数据进行测试</span></span><br><span class="line"><span class="comment">#4.0构造测试数据</span></span><br><span class="line">testList=[<span class="string">'love'</span>, <span class="string">'my'</span>, <span class="string">'dalmation'</span>]</span><br><span class="line"><span class="comment">#4.1转换为单词向量</span></span><br><span class="line">testVec=setOfWords2Vec(vocabList,testList)</span><br><span class="line"><span class="comment">#4.2计算概率</span></span><br><span class="line">testP=classfyNB(testVec,p0Vec,p1Vec,pAuixs)</span><br><span class="line">print(str(testList)+<span class="string">" is:"</span>+str(testP))</span><br><span class="line"></span><br><span class="line"><span class="comment">#4.测试数据进行测试</span></span><br><span class="line"><span class="comment">#4.0构造测试数据</span></span><br><span class="line">testList=[<span class="string">'stupid'</span>, <span class="string">'garbage'</span>]</span><br><span class="line"><span class="comment">#4.1转换为单词向量</span></span><br><span class="line">testVec=setOfWords2Vec(vocabList,testList)</span><br><span class="line"><span class="comment">#4.2计算概率</span></span><br><span class="line">testP=classfyNB(testVec,p0Vec,p1Vec,pAuixs)</span><br><span class="line">print(str(testList)+<span class="string">" is:"</span>+str(testP))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#利用贝叶斯分类器进行垃圾邮件过滤</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#读取文件内的文本，对长度小于两个的单词进行过滤</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textParse</span><span class="params">(file)</span>:</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="comment">#0.正则表达式进行分割</span></span><br><span class="line">regex=re.compile(<span class="string">"\\W+"</span>)</span><br><span class="line"><span class="comment">#1.对读入的问津进行分割</span></span><br><span class="line"><span class="comment">#file=file.encode('utf-8')</span></span><br><span class="line"></span><br><span class="line">listTokens=regex.split(file)</span><br><span class="line"><span class="keyword">return</span> [token.lower() <span class="keyword">for</span> token <span class="keyword">in</span> listTokens <span class="keyword">if</span> len(token)&gt;<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#用朴素贝叶斯分类器对个人广告区域进行倾向分类</span></span><br><span class="line"><span class="comment">#从rss中获取文本</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#将文本中出现频率前30的去掉</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcMostFreq</span><span class="params">(vocabList,fullList)</span>:</span></span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"><span class="comment">#代表键值对</span></span><br><span class="line">wordFreq=&#123;&#125;</span><br><span class="line"><span class="comment">#对整篇文档计算单词表中单词出现的频率</span></span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> vocabList:</span><br><span class="line">wordFreq[token]=fullList.count(token)</span><br><span class="line"><span class="comment">#按照字典中值进行排序（即出现的次数），以第几个域排序，按照降序排列</span></span><br><span class="line"><span class="comment">#此时变为了列表，可以返回前30个</span></span><br><span class="line">freqList=sorted(wordFreq.items(),key=operator.itemgetter(<span class="number">1</span>),reverse=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">return</span> freqList[:<span class="number">30</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#输入的参数为：从feedparse中获取的网页的rss源，feed0是0网页，里面的内容标记为0，feed1是1网页，里面的内容标记为1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loacWords</span><span class="params">(feed0,feed1)</span>:</span></span><br><span class="line"><span class="keyword">import</span> feedparser</span><br><span class="line"><span class="comment">#0.获取文本</span></span><br><span class="line">docList=[]</span><br><span class="line">classList=[]</span><br><span class="line">wordList=[]</span><br><span class="line">fullList=[]</span><br><span class="line"><span class="comment">#feed0['entries']是获取所有条目的列表</span></span><br><span class="line">minlen=min(len(feed0[<span class="string">'entries'</span>]),len(feed1[<span class="string">'entries'</span>]))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(minlen):</span><br><span class="line"><span class="comment">#对第i个条目的概要提取内容，并进行单词划分</span></span><br><span class="line">wordList=textParse(feed0[<span class="string">'entries'</span>][i][<span class="string">'summary'</span>])</span><br><span class="line">docList.append(wordList)</span><br><span class="line">classList.append(<span class="string">"0"</span>)</span><br><span class="line"><span class="comment">#把所有单词加入到一个列表方便计数</span></span><br><span class="line">fullList.extend(wordList)</span><br><span class="line"></span><br><span class="line">wordList=textParse(feed1[<span class="string">'entries'</span>][i][<span class="string">'summary'</span>])</span><br><span class="line">docList.append(wordList)</span><br><span class="line">classList.append(<span class="string">"1"</span>)</span><br><span class="line"><span class="comment">#把所有单词加入到一个列表方便计数</span></span><br><span class="line">fullList.extend(wordList)</span><br><span class="line"></span><br><span class="line"><span class="comment">#1.构建词汇表</span></span><br><span class="line">vocabList=createVocabList(docList);</span><br><span class="line"></span><br><span class="line"><span class="comment">##0000.增加的部分，将文本中高频出现的前30个单词从词汇表中去除</span></span><br><span class="line">mostFreq=calcMostFreq(vocabList,fullList)</span><br><span class="line"></span><br><span class="line"><span class="comment">#freq是键值对，[0]第0个代表单词，[1]第一个代表出现的次数</span></span><br><span class="line"><span class="keyword">for</span> freq <span class="keyword">in</span> mostFreq:</span><br><span class="line"><span class="keyword">if</span> freq[<span class="number">0</span>] <span class="keyword">in</span> vocabList:</span><br><span class="line">vocabList.remove(freq[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.文本转换为单词向量</span></span><br><span class="line"><span class="comment">#inputWord=[]</span></span><br><span class="line"><span class="comment">#for i in range(2*minlen):</span></span><br><span class="line"><span class="comment">#inputWord[i]=bagOfWords2Vec(vocabList,docList[i])</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#3.随机选取20个作为测试集</span></span><br><span class="line">trainingSet=list(range(<span class="number">2</span>*minlen))</span><br><span class="line">testingSet=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line"><span class="comment">#生成随机浮点数 random.uniform</span></span><br><span class="line">testIndex=(int)(random.uniform(<span class="number">0</span>,len(trainingSet)))</span><br><span class="line">testingSet.append(trainingSet[testIndex])</span><br><span class="line"><span class="keyword">del</span>(trainingSet[testIndex])</span><br><span class="line"></span><br><span class="line"><span class="comment">#将字符串转换为数字</span></span><br><span class="line">classList=[int(x) <span class="keyword">for</span> x <span class="keyword">in</span> classList]</span><br><span class="line"><span class="comment">#4.剩余的作为训练集，计算分类器的概率，用于分类</span></span><br><span class="line">traingData=[]</span><br><span class="line">traingLabels=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> trainingSet:</span><br><span class="line"><span class="comment">#2.文本转换为单词向量</span></span><br><span class="line">traingData.append(bagOfWords2Vec(vocabList,docList[i]))</span><br><span class="line">traingLabels.append(classList[i])</span><br><span class="line">p0Vec,p1Vec,pAuixs=trainNB0(traingData,traingLabels)</span><br><span class="line"><span class="comment">#5.对测试集进行测试</span></span><br><span class="line">error=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> testingSet:</span><br><span class="line">testingData=bagOfWords2Vec(vocabList,docList[i])</span><br><span class="line">testResult=classfyNB(testingData,p0Vec,p1Vec,pAuixs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(testResult!=classList[i]):</span><br><span class="line">error+=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"the error rate is:"</span>+str((float)(error)/<span class="number">20</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> vocabList,p0Vec,p1Vec</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;算法原理&quot;&gt;&lt;a href=&quot;#算法原理&quot; class=&quot;headerlink&quot; title=&quot;算法原理&quot;&gt;&lt;/a&gt;算法原理&lt;/h2&gt;&lt;p&gt;（1）问题原理：朴素贝叶斯分类的依据是概率，假设需要分为两类c1和c2，则某个数据点属于哪一类，需要计算p(c1|x)和p{c2|x)，即计算数据点x来自c1和来自c2中的概率哪个大，如果p(c1|x)&amp;gt;p{c2|x)，则x被分到c1类别中，反之被分到c2类别中。&lt;/p&gt;&lt;p&gt;（2）计算原理：上述概率属于条件概率，根据公式p(c|x)=p(x|c) * p(c)/ p(x)可以得到，当求（x,y）来自哪个类别时，即用（x,y）替换x，即求p(c|x,y),代入条件概率公式中得，p(c|x,y)=p(x,y|c) * p(c)/p(x,y)，因此问题转换为求p(x,y|c) * p(c)/p(x,y)中三个概率的值。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="贝叶斯分类" scheme="https://www.xiapf.com/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>监督学习笔记（一）——支持向量机</title>
    <link href="https://www.xiapf.com/blogs/svm/"/>
    <id>https://www.xiapf.com/blogs/svm/</id>
    <published>2020-03-06T08:20:10.000Z</published>
    <updated>2020-03-06T08:30:33.620Z</updated>
    
    <content type="html"><![CDATA[<h2 id="含义"><a href="#含义" class="headerlink" title="含义"></a>含义</h2><p>（1）支持向量：支持向量机用于解决分类问题，当给出一组线性可分的数据时，此时可以得出一条直线将数据分隔开，要求这条直线即求出了分类的依据，当根据距离分隔线最近的点，取其距离的最大值就能得到最优分割的直线。其中距离分隔线最近的点称为支持向量。</p><p>（2）机：机是指该方法是一个分类器，会产生二值决策机。</p><p>（3）优点：支持向量机方法只使用支持向量，并没有用全部的数据点，所以内存方面优于knn。</p><a id="more"></a><p>（4）缺点：支持向量机一般用于线性可分的数据，当数据线性不可分时无法使用。对于复杂数据需要借助核函数，将复杂数据映射到高维空间进行处理</p><h2 id="求解的理论依据"><a href="#求解的理论依据" class="headerlink" title="求解的理论依据"></a>求解的理论依据</h2><p>（1）确定分隔平面和输出函数</p><p>分隔平面：wT*x+b，其中w和b描述了所给数据的分隔平面</p><p>因为是二值分类器，输出值是-1和+1，所以使用signmoid函数，当点到直线的距离f(wT*x+b)&gt;0时属于 +1类，反之属于-1类。</p><p>（2）求距离：</p><p>数据点到分隔平面的距离记为<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200306150346.png" alt="点到平面的几何距离"></p><p>要求得最佳分隔平面就要找到距离平面最佳的点的距离将其最大化，则为最佳分类，所以即求<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200306150600.png" alt="求解目标"></p><p>但是该目标函数求解困难，因此使用拉格朗日乘子法进行转换带约束调节的目标函数：<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200306150813.png" alt="转换函数"></p><p>其中约束条件为：<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200306151016.png" alt="约束条件"></p><p>因为实际数据不可能存在100%线性可分所以此时加上一个松弛变量，可以允许一些点在分隔平面的另一边，此时前一个约束条件变为0&lt;=alpha&lt;=C，这里的常数C确保点距离平面的最大间隔和所有数据点的函数间隔小于1.0（因为是支持向量）</p><p>（3）问题转换</p><p>根据（2）中转换后的目标函数和约束条件可以看出，SVM（支持向量机，以下均简称为SVM）的主要求解目标转换为求alpha的值，通过alpha可以表示分隔平面。</p><h2 id="使用的算法"><a href="#使用的算法" class="headerlink" title="使用的算法"></a>使用的算法</h2><p>由求解的理论依据中可知，SVM需要求解的目标为带有约束条件的函数，一般采用二次规划求解方法，但是计算复杂，这里使用platt SMO算法求解。</p><p>（1）算法思路：每次选择一对alpha进行优化，当找到合适的两个alpha后，需要增大一个减小另一个，因为需要满足约束条件：<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200306151016.png" alt="约束条件">，合适是指两个alpha值要在间隔边界之外，同时没有被区间化处理或者不在边界上</p><p>（2）基本算法流程：以简单smo算法为例</p><blockquote><p>#0.初始化alpha向量为0</p><p>#1.当迭代次数小于最大迭代次数（外循环）<br>    #2.对数据集中每个向量（内循环）<br>        #3.如果该向量可以被优化<br>            #4.随机找另一个向量<br>            #5.优化这两个向量<br>            #6.如果这两个向量不能被优化，跳出内循环</p><p>#7.如果所有向量都不能被优化，增加迭代次数，进行下一次的循环</p></blockquote><p>（3）完整platt smo 算法</p><p>platt smo算法在简化smo算法基础上提升时间，对数据量大的数据集执行时间大大减少。</p><p>相比于传统的二次规划求解方法，每次选取两个alpha进行优化，时间效率大幅提高。</p><p>算法流程：</p><blockquote><p>#外循环 使用两种方式交替得到第一个alpha的值</p><p>#1.0 对所有数据点进行遍历扫描</p><p>#2.0 对非边界值即0&lt;alpha并且alpha&gt;c的alpha进行扫描</p><p>#内循环 采用最大化步长的方式得到第二个alpha的值</p></blockquote><h2 id="核函数的引入"><a href="#核函数的引入" class="headerlink" title="核函数的引入"></a>核函数的引入</h2><p>当面对非线性数据的时候，需要使用核函数，将复杂数据映射到高维空间，此时分类器易于理解，即可使用SVM方法。</p><p>主要使用径向基函数的高斯版本：<img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200306155004.png" alt="径向基函数"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kernelTrans</span><span class="params">(X,A,kTup=<span class="params">()</span>)</span>:</span></span><br><span class="line">  m=shape(X)[<span class="number">0</span>]</span><br><span class="line">  K=zeros((m,<span class="number">1</span>))</span><br><span class="line">  <span class="comment">#对核函数的第一个参数进行不同情况的讨论</span></span><br><span class="line">  <span class="comment">#1.线性核</span></span><br><span class="line">  <span class="keyword">if</span>(kTup[<span class="number">0</span>]=<span class="string">"lin"</span>):</span><br><span class="line">    K=X*A.T</span><br><span class="line">  <span class="comment">#2.径向基核</span></span><br><span class="line">  <span class="keyword">elif</span>(kTup[<span class="number">0</span>]=<span class="string">"rbf"</span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">      delta=X[i,:]-A</span><br><span class="line">      K[i]=delta*delta.T</span><br><span class="line">     K=exp(K/(<span class="number">-1</span>*kTup[<span class="number">1</span>]**<span class="number">2</span>))      </span><br><span class="line">  <span class="comment">#3.抛出异常</span></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">raise</span> NameError()</span><br><span class="line">  <span class="keyword">return</span></span><br></pre></td></tr></table></figure><h2 id="应用实例——手写数字识别"><a href="#应用实例——手写数字识别" class="headerlink" title="应用实例——手写数字识别"></a>应用实例——手写数字识别</h2><p>识别手写字——使用svm比knn效率高，占内存小，因为svm只需用到支持向量来进行分类</p><p>（1）将图像转换为向量</p><p>（2）读取文件夹列表中各个图像文件</p><p>（3）处理分类问题</p><p>为方便处理，只保留了1和9两个数字，当是数字1时，分类为+1，当是数字9时分类为-1</p><blockquote><p>#1.读入图像转换后的数据向量和标签属性</p><p>#2.调用platt smo算法得出alpha和b的值</p><p>#3.根据支持向量大于0的特性得出其中alpha&gt;0的支持向量，并得出支持向量的数据点和标签数据</p><p>#4.在训练集上得出分类结果</p><p>#5.将分类的结果应用于测试集</p></blockquote><p>完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#plat smo可以处理数据量大的，上面simplesmo只能处理数据量较小的数据集</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#platsmo的支持函数</span></span><br><span class="line"><span class="comment">#建立一个对象</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">optStruct</span>:</span></span><br><span class="line"><span class="comment">#各种变量初始化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,dataMat,lableMat,C,toler,kTup)</span>:</span><span class="comment">#增加核函数初始化参数</span></span><br><span class="line">self.dataMat=dataMat</span><br><span class="line">self.lableMat=lableMat</span><br><span class="line">self.C=C</span><br><span class="line">self.toler=toler</span><br><span class="line">self.m=shape(dataMat)[<span class="number">0</span>]</span><br><span class="line">self.alpha=mat(zeros((self.m,<span class="number">1</span>)))</span><br><span class="line">self.b=<span class="number">0</span></span><br><span class="line">self.echache=mat(zeros((self.m,<span class="number">2</span>)))<span class="comment">#第一列表示cache是否有效，第二列是实际的E（误差）值</span></span><br><span class="line"><span class="comment">#构建k</span></span><br><span class="line">self.k=mat(zeros((self.m,self.m)))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(self.m):</span><br><span class="line"><span class="comment">#调用函数，填充k</span></span><br><span class="line">self.k[:,i]=kernelTras(self.dataMat,self.dataMat[i,:],kTup)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#保存误差缓存</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcEK</span><span class="params">(oS,ke)</span>:</span></span><br><span class="line">fk=float(multiply(oS.alpha,oS.lableMat).T*oS.k[:,ke]+oS.b)</span><br><span class="line">EK=fk-float(oS.lableMat[ke])</span><br><span class="line"><span class="keyword">return</span> EK</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#计算误差，并且存入缓存</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateEK</span><span class="params">(oS,k)</span>:</span></span><br><span class="line">EK=calcEK(oS,k)</span><br><span class="line">oS.echache[k]=[<span class="number">1</span>,EK]</span><br><span class="line"></span><br><span class="line"><span class="comment">#内循环中的启发方式</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#选择第二个alpha值，保证每次优化都是最大的步长，如果是第一次进入循环就先随机选择一个alpha[j],反之进入循环，根据最大步长选择alpha[j]的值</span></span><br><span class="line"><span class="comment">#alphaj的值和前一个alphai的下标和误差Ei有关</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">selectJ</span><span class="params">(i,oS,Ei)</span>:</span></span><br><span class="line">maxk=<span class="number">-1</span></span><br><span class="line">maxDeltaE=<span class="number">0</span></span><br><span class="line">Ej=<span class="number">0</span></span><br><span class="line">oS.echache[i]=[<span class="number">1</span>,Ei]</span><br><span class="line"><span class="comment">#输入列表为目标的列表值，nozero返回不为空的值即非零alpha值</span></span><br><span class="line">validDeltaList=nonzero(oS.echache[:,<span class="number">0</span>].A)[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">if</span> (len(validDeltaList))&gt;<span class="number">1</span>:</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> validDeltaList:</span><br><span class="line"><span class="keyword">if</span>(k==i):</span><br><span class="line"><span class="keyword">continue</span>;</span><br><span class="line">Ek=calcEK(oS,k)</span><br><span class="line">delta=abs(Ek-Ei)</span><br><span class="line"><span class="keyword">if</span>(delta&gt;maxDeltaE):</span><br><span class="line">maxk=k</span><br><span class="line">maxDeltaE=delta</span><br><span class="line">Ej=Ek</span><br><span class="line"><span class="keyword">return</span> maxk,Ej</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">j=selectRandom(i,oS.m)</span><br><span class="line">Ej=calcEK(oS,j)</span><br><span class="line"><span class="keyword">return</span> j,Ej</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">innerI</span><span class="params">(oS,i)</span>:</span></span><br><span class="line">exi=calcEK(oS,i)</span><br><span class="line"><span class="keyword">if</span> ((oS.lableMat[i]*exi&lt;-oS.toler) <span class="keyword">and</span> (oS.alpha[i]&lt;oS.C)) <span class="keyword">or</span> ((oS.lableMat[i]*exi&gt;oS.toler) <span class="keyword">and</span> (oS.alpha[i]&gt;<span class="number">0</span>)):</span><br><span class="line"><span class="comment">#1.3启发式找另一个向量</span></span><br><span class="line">j,exj=selectJ(i,oS,exi)</span><br><span class="line"></span><br><span class="line"><span class="comment">#1.4优化这两个向量</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#记录旧的，看优化之后有无变换</span></span><br><span class="line">alphaJold=oS.alpha[j].copy()</span><br><span class="line">alphaIold=oS.alpha[i].copy()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(oS.lableMat[i]!=oS.lableMat[j]):</span><br><span class="line">L=max(<span class="number">0</span>,oS.alpha[j]-oS.alpha[i])</span><br><span class="line">H=min(oS.C,oS.C+oS.alpha[j]-oS.alpha[i])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">L=max(<span class="number">0</span>,oS.alpha[j]+oS.alpha[i]-oS.C)</span><br><span class="line">H=min(oS.C,oS.C+oS.alpha[j]+oS.alpha[i]-oS.C)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(L==H):</span><br><span class="line">print(<span class="string">"L==H"</span>)</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#最优修改量</span></span><br><span class="line"><span class="comment">#eta=2.0*oS.dataMat[i,:]*oS.dataMat[j,:].T-oS.dataMat[i,:]*oS.dataMat[i,:].T-oS.dataMat[j,:]*oS.dataMat[j,:].T</span></span><br><span class="line"><span class="comment">#使用核函数之后，修正eta的值</span></span><br><span class="line">eta=<span class="number">2.0</span>*oS.k[i,j]-oS.k[i,i]-oS.k[j,j]</span><br><span class="line"><span class="keyword">if</span>(eta&gt;=<span class="number">0</span>):</span><br><span class="line">print(<span class="string">"eta&gt;=0"</span>)</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">oS.alpha[j]-=oS.lableMat[j]*(exi-exj)/eta</span><br><span class="line">oS.alpha[j]=adjustBig(oS.alpha[j],H,L)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将j的更新存到缓存</span></span><br><span class="line">updateEK(oS,j)</span><br><span class="line"></span><br><span class="line"><span class="comment">#优化j之后无变换</span></span><br><span class="line"><span class="keyword">if</span> abs(alphaJold-oS.alpha[j])&lt;<span class="number">0.00001</span>:</span><br><span class="line">print(<span class="string">"alphaJold-alpha[j])&lt;0.00001"</span>)</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#优化j有变化，接着优化i</span></span><br><span class="line">oS.alpha[i]+=oS.lableMat[j]*oS.lableMat[i]*(alphaJold-oS.alpha[j])</span><br><span class="line"></span><br><span class="line"><span class="comment">#将i的更新存到缓存</span></span><br><span class="line">updateEK(oS,i)</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置偏置量</span></span><br><span class="line"><span class="comment">#b1=oS.b-exi-oS.lableMat[i]*(oS.alpha[i]-alphaIold)*oS.dataMat[i,:]*oS.dataMat[i,:].T-oS.lableMat[j]*(oS.alpha[j]-alphaJold)*oS.dataMat[i,:]*oS.dataMat[j,:].T</span></span><br><span class="line"><span class="comment">#b2=oS.b-exj-oS.lableMat[i]*(oS.alpha[i]-alphaIold)*oS.dataMat[i,:]*oS.dataMat[j,:].T-oS.lableMat[j]*(oS.alpha[j]-alphaJold)*oS.dataMat[j,:]*oS.dataMat[j,:].T</span></span><br><span class="line"><span class="comment">#使用核函数之后，修正b1,b2的值</span></span><br><span class="line">b1=oS.b-exi-oS.lableMat[i]*(oS.alpha[i]-alphaIold)*oS.k[i,i]-oS.lableMat[j]*(oS.alpha[j]-alphaJold)*oS.k[i,j]</span><br><span class="line">b2=oS.b-exj-oS.lableMat[i]*(oS.alpha[i]-alphaIold)*oS.k[i,j]-oS.lableMat[j]*(oS.alpha[j]-alphaJold)*oS.k[j,j]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> oS.alpha[i]&gt;<span class="number">0</span> <span class="keyword">and</span> oS.alpha[i]&lt;oS.C:</span><br><span class="line">oS.b=b1</span><br><span class="line"><span class="keyword">elif</span> oS.alpha[j]&gt;<span class="number">0</span> <span class="keyword">and</span> oS.alpha[j]&lt;oS.C:</span><br><span class="line">oS.b=b2</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">oS.b=(b1+b2)/<span class="number">2.0</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#完整的platt smo 外循环</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">smoP</span><span class="params">(dataMatIn,labelArr,C,toler,maxIter,kTup=<span class="params">(<span class="string">"lin"</span>,<span class="number">0</span>)</span>)</span>:</span></span><br><span class="line"><span class="comment">#0.相关变量初始化</span></span><br><span class="line">oS=optStruct(mat(dataMatIn),mat(labelArr).transpose(),C,toler,kTup)</span><br><span class="line"></span><br><span class="line">entireSet=<span class="literal">True</span></span><br><span class="line">alphaChange=<span class="number">0</span></span><br><span class="line">iter=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#1.主循环</span></span><br><span class="line"><span class="keyword">while</span> (iter&lt;maxIter) <span class="keyword">and</span> ((alphaChange&gt;<span class="number">0</span>) <span class="keyword">or</span> (entireSet)):</span><br><span class="line">alphaChange=<span class="number">0</span></span><br><span class="line"><span class="comment">#1.选择第一个alpha的两种循环</span></span><br><span class="line"><span class="comment">#1.0对所有数据集进行遍历</span></span><br><span class="line"><span class="keyword">if</span>(entireSet):</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(oS.m):</span><br><span class="line">alphaChange+=innerI(oS,i)</span><br><span class="line">print(<span class="string">"fullset: iter:"</span>+str(iter)+<span class="string">" i:"</span>+str(i)+<span class="string">" alphaChange:"</span>+str(alphaChange))</span><br><span class="line">iter+=<span class="number">1</span></span><br><span class="line"><span class="comment">#1.1对非边界值进行遍历</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">nonBoundIs=nonzero((oS.alpha.A&gt;<span class="number">0</span>)*(oS.alpha.A&lt;C))[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> nonBoundIs:</span><br><span class="line">alphaChange+=innerI(oS,i)</span><br><span class="line">print(<span class="string">"nonbound: iter:"</span>+str(iter)+<span class="string">" i:"</span>+str(i)+<span class="string">" alphaChange:"</span>+str(alphaChange))</span><br><span class="line"></span><br><span class="line">iter+=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(entireSet):</span><br><span class="line">entireSet=<span class="literal">False</span></span><br><span class="line"><span class="keyword">elif</span>(alphaChange==<span class="number">0</span>):</span><br><span class="line">entireSet=<span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> oS.alpha,oS.b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#利用求得的alpha值求得分类的超平面，其中可以求出w的值</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcW</span><span class="params">(alpha,dataMatIn,labelArr)</span>:</span></span><br><span class="line">dataMat=mat(dataMatIn)</span><br><span class="line">lableMat=mat(labelArr).transpose()</span><br><span class="line">m,n=shape(dataMat)</span><br><span class="line">w=zeros((n,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line"><span class="comment">#其中起作用的只有支持向量，即不为0的alpha</span></span><br><span class="line">w+=multiply((alpha[i]*lableMat[i]),dataMat[i,:].T)</span><br><span class="line"><span class="keyword">return</span> w</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#在复杂数据上使用核函数</span></span><br><span class="line"><span class="comment">#将数据从一个特征空间映射到另一个空间</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kernelTras</span><span class="params">(X,A,kTup)</span>:</span></span><br><span class="line">m,n=shape(X)</span><br><span class="line">k=zeros((m,<span class="number">1</span>))</span><br><span class="line"><span class="comment">#根据ktup的第一个参数来求核函数的值</span></span><br><span class="line"><span class="keyword">if</span>(kTup[<span class="number">0</span>]==<span class="string">"lin"</span>):</span><br><span class="line">k=X*A.T</span><br><span class="line"><span class="keyword">elif</span>(kTup[<span class="number">0</span>]==<span class="string">"rbf"</span>):</span><br><span class="line"><span class="comment">#对每个元素计算高斯值</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(m):</span><br><span class="line">deltaRow=X[j,:]-A</span><br><span class="line">k[j]=deltaRow*deltaRow.T</span><br><span class="line">k=exp(k/(<span class="number">-1</span>*kTup[<span class="number">1</span>]**<span class="number">2</span>))</span><br><span class="line"><span class="comment">#若元祖无法识别，则抛出异常</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">raise</span> NameError(<span class="string">"the keneral name wrong"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> k</span><br><span class="line"></span><br><span class="line"><span class="comment">#识别手写字——使用svm比knn效率高，占内存小，因为svm只需用到支持向量来进行分类</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#1.将图像转换为向量——将32*32的图像转换为1*1024的向量</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">img2vector</span><span class="params">(filename)</span>:</span></span><br><span class="line"><span class="comment">#0.构造转换后返回的向量</span></span><br><span class="line">returnVect=zeros((<span class="number">1</span>,<span class="number">1024</span>))</span><br><span class="line"><span class="comment">#1.处理图像</span></span><br><span class="line">fr=open(filename)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">32</span>):<span class="comment">#因为图像是32行的</span></span><br><span class="line">lineArr=fr.readline() <span class="comment">#每次读入一行</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">32</span>):<span class="comment">#因为图像是32列的</span></span><br><span class="line">returnVect[<span class="number">0</span>,<span class="number">32</span>*i+j]=int(lineArr[j])</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> returnVect</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.从文件夹中读入所有图片信息并存储为向量和标签——,为方便，只保留了1和9两个数字，出现1的标签记为1，出现9的标签记为-1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadImage</span><span class="params">(dirname)</span>:</span></span><br><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> listdir</span><br><span class="line"><span class="comment">#0.用于存储标签</span></span><br><span class="line">hwLabels=[]</span><br><span class="line"></span><br><span class="line"><span class="comment">#1.用于存储数据向量</span></span><br><span class="line"><span class="comment">#得到文件夹下图像名称的列表 例：1_20.txt</span></span><br><span class="line">trainDirList=listdir(dirname)</span><br><span class="line">m=len(trainDirList)</span><br><span class="line"><span class="comment">#用于存储数据向量</span></span><br><span class="line">trainMat=zeros((m,<span class="number">1024</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.处理图像文件中的内容</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line"><span class="comment">#图像文件完整名称</span></span><br><span class="line">fullName=trainDirList[i]</span><br><span class="line"><span class="comment">#图像文件去除后缀后的名称</span></span><br><span class="line">fileName=fullName.split(<span class="string">"."</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="comment">#图像文件去除下划线后的名称</span></span><br><span class="line">className=int(fileName.split(<span class="string">"_"</span>)[<span class="number">0</span>])</span><br><span class="line"><span class="comment">#为方便，只保留了1和9两个数字，出现1的标签记为1，出现9的标签记为-1</span></span><br><span class="line"><span class="keyword">if</span>(className==<span class="number">1</span>):</span><br><span class="line">hwLabels.append(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">hwLabels.append(<span class="number">-1</span>)</span><br><span class="line"><span class="comment">#将图像文件中的内容转换为向量进行存储</span></span><br><span class="line">trainMat[i,:]=img2vector(<span class="string">"%s/%s"</span>%(dirname,fullName))</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> trainMat,hwLabels</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#3.使用核函数进行分类测试的径向基函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testDigits</span><span class="params">(kTup=<span class="params">(<span class="string">"rbf"</span>,<span class="number">10</span>)</span>)</span>:</span></span><br><span class="line">dataMatIn,labelArr=loadImage(<span class="string">"trainingDigits"</span>)</span><br><span class="line">dataMat=mat(dataMatIn)</span><br><span class="line">lableMat=mat(labelArr).transpose()</span><br><span class="line">m,n=shape(dataMat)</span><br><span class="line"><span class="comment">#根据platt smo算法得到alpha,b的值</span></span><br><span class="line">alpha,b=smoP(dataMatIn,labelArr,<span class="number">200</span>,<span class="number">0.0001</span>,<span class="number">10000</span>,kTup)</span><br><span class="line">print(alpha[alpha&gt;<span class="number">0</span>])</span><br><span class="line">print(b)</span><br><span class="line"><span class="comment">#得到支持向量</span></span><br><span class="line">sVInd=nonzero(alpha.A&gt;<span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line">sVs=dataMat[sVInd]</span><br><span class="line">sVlabel=lableMat[sVInd]</span><br><span class="line">print(<span class="string">"tere are "</span>+str(shape(sVs)[<span class="number">0</span>])+<span class="string">" support vectors"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#在训练集上使用核函数</span></span><br><span class="line">error=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">kernel=kernelTras(sVs,dataMat[i,:],kTup)</span><br><span class="line">precdit=kernel.T*multiply(sVlabel,alpha[sVInd])+b</span><br><span class="line"><span class="keyword">if</span>(sign(precdit)!=sign(lableMat[i])):</span><br><span class="line">error+=<span class="number">1</span></span><br><span class="line">print(<span class="string">"train error rate: "</span>+str(float(error)/m)+<span class="string">"——————"</span>+str(error))</span><br><span class="line"></span><br><span class="line"><span class="comment">#在测试集上使用核函数</span></span><br><span class="line">dataMatIn,labelArr=loadImage(<span class="string">"testDigits"</span>)</span><br><span class="line">dataMat=mat(dataMatIn)</span><br><span class="line">lableMat=mat(labelArr).transpose()</span><br><span class="line">m=shape(dataMat)[<span class="number">0</span>]</span><br><span class="line">error=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">kernel=kernelTras(sVs,dataMat[i,:],kTup)</span><br><span class="line">precdit=kernel.T*multiply(sVlabel,alpha[sVInd])+b</span><br><span class="line"><span class="keyword">if</span>(sign(precdit)!=sign(lableMat[i])):</span><br><span class="line">error+=<span class="number">1</span></span><br><span class="line">print(<span class="string">"test error rate: "</span>+str(float(error)/m)+<span class="string">"——————"</span>+str(error))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;含义&quot;&gt;&lt;a href=&quot;#含义&quot; class=&quot;headerlink&quot; title=&quot;含义&quot;&gt;&lt;/a&gt;含义&lt;/h2&gt;&lt;p&gt;（1）支持向量：支持向量机用于解决分类问题，当给出一组线性可分的数据时，此时可以得出一条直线将数据分隔开，要求这条直线即求出了分类的依据，当根据距离分隔线最近的点，取其距离的最大值就能得到最优分割的直线。其中距离分隔线最近的点称为支持向量。&lt;/p&gt;&lt;p&gt;（2）机：机是指该方法是一个分类器，会产生二值决策机。&lt;/p&gt;&lt;p&gt;（3）优点：支持向量机方法只使用支持向量，并没有用全部的数据点，所以内存方面优于knn。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="支持向量机" scheme="https://www.xiapf.com/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    
  </entry>
  
  <entry>
    <title>Leecode算法学习笔记（四）——动态规划</title>
    <link href="https://www.xiapf.com/blogs/movStateNote/"/>
    <id>https://www.xiapf.com/blogs/movStateNote/</id>
    <published>2020-03-05T03:22:57.000Z</published>
    <updated>2020-04-13T07:33:32.893Z</updated>
    
    <content type="html"><![CDATA[<h2 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h2><h3 id="一般思路"><a href="#一般思路" class="headerlink" title="一般思路"></a>一般思路</h3><p>（1）明确状态含义</p><p>首先要明确dp[i]的含义，例如在求分割回文字符串的次数中，dp[i]表示i个回文字符串的分割次数</p><p>（2）确定初始状态</p><p>可以考虑最多…的时候dp的状态</p><p>例如求分割回文字符串的次数，初始状态即为i个字符串最多分割i次</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//求分割回文字符串的次数</span></span><br><span class="line"><span class="comment">//初始状态</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;len;i++)</span><br><span class="line">  dp[i]=i;</span><br><span class="line"><span class="comment">//i个字符串，最多分割i次</span></span><br></pre></td></tr></table></figure><a id="more"></a><p>（3）确定状态转移方程</p><p>假设已知dp[i]的状态，列出dp[i+1]和dp[i]之间的关系式，即为状态转移方程</p><p>明确目标：要求dp{i]的值</p><p>例如求分割回文字符串的次数，其中状态转移方程如下：</p><p>当i从1变化到len之间，求dp[i]。当0~i本身是回文时，不需要分割，即dp[i]=0。该题dp[i+1]和dp[i]之间没有明确联系，可以从分割边界考虑。假设分割边界为j，已知dp[j]，求dp[i]，就需要分两种情况考虑</p><p>a）j+1~i之间是回文，那么dp[j]再分割一次就可以得到j之前的字符串和后面的回文，即dp[i]=dp[j]+1，但dp[i]有可能更小，所以dp[i]在dp[i]和dp[j]+1之间取小的</p><p>b）j+1~i之间不是回文，继续移动j，找到符合回文的子串</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;len;i++)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">if</span>(<span class="number">0</span>~i是回文)</span><br><span class="line">    dp[i]=<span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;i;j++)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">if</span>(j+<span class="number">1</span>~i是回文)</span><br><span class="line">      dp[i]=<span class="built_in">min</span>(dp[i],dp[j]+<span class="number">1</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>（4）最终输出</p><p>一般输出为dp[len-1]，例如在求分割回文字符串的次数中，最终是求len个字符串的分割次数，即求dp[len-1]</p><h3 id="股票问题"><a href="#股票问题" class="headerlink" title="股票问题"></a>股票问题</h3><p>（1）明确状态含义</p><p>股票有三种状态：买入，卖出，休息（不买入也不卖出）    最终状态为持有股票设置为0，不持有股票1</p><p>转换关系如下：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200304222109.png" alt="stock"></p><p>设置dp[i] [k] [s]表示第天股票的持有情况，k表示至今最多进行k笔交易，整个数组表示可以得到的最大利润</p><p>（2）确定状态转移方程</p><p>转换关系中就两种状态，因此分别考虑两种情况下的转移方程：</p><p>a）今天持有股票，即求dp[i] [k] [1]</p><p>持有股票有两种情况，一种是前一天本来就有股票，并且今天休息，即dp[i-1] [k] [1]；另一种是前一天没有股票，今天购入了，此时前一天进行了k-1笔交易，买入股票，说明利润减少，需要减掉买入的价格,即dp[i-1] [k-1] [0]-price[i]</p><p>综上，求利润最大，即求两种情况的最大者，dp[i] [k] [1]=max(dp[i-1] [k] [1]，dp[i-1] [k-1] [0]-price[i])</p><p>b）今天不持有股票，即求dp[i] [k] [0]</p><p>不持有股票也有两种情况，一种是前一天本来就没有股票，并且今天休息，即dp[i-1] [k] [0]；另一种是前一天有股票，今天卖出了，卖出股票，说明利润增加，需要加上卖出的价格,即dp[i-1] [k] [1]+price[i]</p><p>综上，求利润最大，即求两种情况的最大者，dp[i] [k] [0]=max(dp[i-1] [k] [0]]，dp[i-1] [k] [1]+price[i])</p><p>综合a和b，可以得出状态转移方程</p><p>（3）确定初始状态</p><p>因为状态转移方程中只有两个最终状态，所以对两个最终状态的初始情况进行讨论：</p><p>a）刚开始没有持有股票时，没有利润，即dp[0] [0]=0</p><p>b）刚开四持有股票，之前没有利润，亏损买入的价格，即dp[0] [1]=-price[0]</p><p>（4）最终输出</p><p>最后一天利润是最大的，此时不持有股票的利润肯定大于持有的，即dp[len-1] [k] [0]&gt;dp[len-1] [k] [1]，所以选择dp[len-1] [k] [1]</p><p>（5）实际应用</p><p>a）给定一个数组，它的第 i 个元素是一支给定股票第 i 天的价格。如果你最多只允许完成一笔交易（即买入和卖出一支股票），设计一个算法来计算你所能获取的最大利润。注意你不能在买入股票前卖出股票。</p><p>此时k=1，代入方程得</p><p>dp[i] [1] [1]=max(dp[i-1] [1] [1]，dp[i-1] [0] [0]-price[i]) 其中dp[i-1] [0] [0]=0</p><p>dp[i] [1] [0]=max(dp[i-1] [1] [0]，dp[i-1] [1] [1]+price[i])</p><p>可以看出，方程中k=1对转换无影响，所以省去k</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//初始状态</span></span><br><span class="line">dp[<span class="number">0</span>][<span class="number">0</span>]=<span class="number">0</span>;</span><br><span class="line">dp[<span class="number">0</span>][<span class="number">1</span>]=-price[<span class="number">0</span>];</span><br><span class="line"><span class="comment">//转移方程</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;len;i++)</span><br><span class="line">&#123;</span><br><span class="line">  dp[i][<span class="number">1</span>]=<span class="built_in">max</span>(dp[i<span class="number">-1</span>][<span class="number">1</span>]，-price[i]);</span><br><span class="line">  dp[i][<span class="number">0</span>]=<span class="built_in">max</span>(dp[i<span class="number">-1</span>][<span class="number">0</span>]，dp[i<span class="number">-1</span>][<span class="number">1</span>]+price[i]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>b）给定一个数组，它的第 i 个元素是一支给定股票第 i 天的价格。设计一个算法来计算你所能获取的最大利润。你可以尽可能地完成更多的交易（多次买卖一支股票）。注意：你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。</p><p>此时k=无穷，那么k的值和k-1的值近似一样，因此k同样可以省略，此时的方程为</p><p>dp[i] [1]=max(dp[i-1] [1]，dp[i-1] [0]-price[i]) 其中dp[i-1] [0]不可以省略，因为其值已经不等于0</p><p>dp[i] [0]=max(dp[i-1] [0]，dp[i-1]  [1]+price[i])</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//初始状态</span></span><br><span class="line">dp[<span class="number">0</span>][<span class="number">0</span>]=<span class="number">0</span>;</span><br><span class="line">dp[<span class="number">0</span>][<span class="number">1</span>]=-price[<span class="number">0</span>];</span><br><span class="line"><span class="comment">//转移方程</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;len;i++)</span><br><span class="line">&#123;</span><br><span class="line">  dp[i][<span class="number">1</span>]=<span class="built_in">max</span>(dp[i<span class="number">-1</span>][<span class="number">1</span>]，dp[i<span class="number">-1</span>][<span class="number">0</span>]-price[i]);</span><br><span class="line">  dp[i][<span class="number">0</span>]=<span class="built_in">max</span>(dp[i<span class="number">-1</span>][<span class="number">0</span>]，dp[i<span class="number">-1</span>][<span class="number">1</span>]+price[i]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>c）给定一个数组，它的第 i 个元素是一支给定的股票在第 i 天的价格。设计一个算法来计算你所能获取的最大利润。你最多可以完成 两笔 交易。注意: 你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。</p><p>此时k从1变换到2，不为0是因为要有利润至少会有一笔交易，此时代码中将多一重循环</p><p>dp[i] [k] [1]=max(dp[i-1] [k] [1]，dp[i-1] [k-1] [0]-price[i]) 其中dp[i-1] [0] [0]=0</p><p>dp[i] [k] [0]=max(dp[i-1] [k] [0]，dp[i-1] [k] [1]+price[i])</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//转移方程</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;len;i++)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> k=<span class="number">1</span>;k&lt;=<span class="number">2</span>;k++)</span><br><span class="line">  &#123;</span><br><span class="line">     <span class="comment">//定义初始状态</span></span><br><span class="line">    <span class="keyword">if</span>(i==<span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="comment">//刚开始时，不持有股票，利润为0</span></span><br><span class="line">      dp[i][k][<span class="number">0</span>]=<span class="number">0</span>;</span><br><span class="line">      <span class="comment">//刚开始时，持有股票，利润为亏损买入对应的股票</span></span><br><span class="line">      dp[i][k][<span class="number">1</span>]=-price[i];</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    dp[i][k][<span class="number">1</span>]=<span class="built_in">max</span>(dp[i<span class="number">-1</span>][k][<span class="number">1</span>]，dp[i<span class="number">-1</span>][k<span class="number">-1</span>][<span class="number">0</span>]-price[i]);</span><br><span class="line">    dp[i][k][<span class="number">0</span>]=<span class="built_in">max</span>(dp[i<span class="number">-1</span>][k][<span class="number">0</span>]，dp[i<span class="number">-1</span>][k][<span class="number">1</span>]+price[i]);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h3><p>动态规划题：</p><p>一般都是当前状态和之前的状态有关，或者和其他的位置有关，这时候需要考虑动态规划。常见于字符串问题。</p><p>1：221. 最大正方形</p><p>（0）确定状态方程的含义</p><p>dp[i] [j]代表当前位置最大正方形的边长，明确一点当前位置的边长取决于左边，上边和斜对角，可以理解当前位置为右下角位置，这三个位置的最小值再加上1就是当前位置的边长。加上这三个位置的值一样，那么直接加1，也无需比较，但是假设有一个比较小，就说明有一边缺一个角，所以这时候需要取最小值。</p><p>初始边长设为-1，每次与当前位置边长比较，找到最大的边长</p><p>（1）初始状态：</p><p>第一行和第一列无法形成三角形，即自身的值就是当前正方形的边长</p><p>因为这里存储的是字符，所以需要通过“-’0‘”取得实际数值</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; dp(m,<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;(n));</span><br><span class="line">dp[<span class="number">0</span>][<span class="number">0</span>]=matrix[<span class="number">0</span>][<span class="number">0</span>]-<span class="string">'0'</span>;</span><br><span class="line">res=dp[<span class="number">0</span>][<span class="number">0</span>];</span><br><span class="line"><span class="comment">//第0行</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;i++)</span><br><span class="line">&#123;</span><br><span class="line">    dp[<span class="number">0</span>][i]=matrix[<span class="number">0</span>][i]-<span class="string">'0'</span>;</span><br><span class="line">    res=<span class="built_in">max</span>(res, dp[<span class="number">0</span>][i]);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//第0列</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;m;i++)</span><br><span class="line">&#123;</span><br><span class="line">    dp[i][<span class="number">0</span>]=matrix[i][<span class="number">0</span>]-<span class="string">'0'</span>;</span><br><span class="line">    res=<span class="built_in">max</span>(res, dp[i][<span class="number">0</span>]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>（2）状态转移方程</p><p>只有当前位置是1才更新边长，否则如果是’0‘，说明当前位置不可能有正方形形成</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//2.状态转移方程</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;m;i++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">1</span>;j&lt;n;j++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//注意输入的字符，不是数字</span></span><br><span class="line">        <span class="comment">//1.dp[i][j]表示右下角元素的最大正方形，当当前位置是1时，可以更新正方形大小,找上左斜对角的最小值再加一（因为当前位置是1）</span></span><br><span class="line">        <span class="keyword">if</span>(matrix[i][j]==<span class="string">'1'</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            dp[i][j]=<span class="built_in">min</span>(dp[i<span class="number">-1</span>][j],<span class="built_in">min</span>(dp[i<span class="number">-1</span>][j<span class="number">-1</span>], dp[i][j<span class="number">-1</span>]))+<span class="number">1</span>;</span><br><span class="line">            res=<span class="built_in">max</span>(res, dp[i][j]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//2.当前位置为0，那正方形大小为0</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            dp[i][j]=<span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;动态规划&quot;&gt;&lt;a href=&quot;#动态规划&quot; class=&quot;headerlink&quot; title=&quot;动态规划&quot;&gt;&lt;/a&gt;动态规划&lt;/h2&gt;&lt;h3 id=&quot;一般思路&quot;&gt;&lt;a href=&quot;#一般思路&quot; class=&quot;headerlink&quot; title=&quot;一般思路&quot;&gt;&lt;/a&gt;一般思路&lt;/h3&gt;&lt;p&gt;（1）明确状态含义&lt;/p&gt;&lt;p&gt;首先要明确dp[i]的含义，例如在求分割回文字符串的次数中，dp[i]表示i个回文字符串的分割次数&lt;/p&gt;&lt;p&gt;（2）确定初始状态&lt;/p&gt;&lt;p&gt;可以考虑最多…的时候dp的状态&lt;/p&gt;&lt;p&gt;例如求分割回文字符串的次数，初始状态即为i个字符串最多分割i次&lt;/p&gt;&lt;figure class=&quot;highlight c++&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;//求分割回文字符串的次数&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;//初始状态&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt;(&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; i=&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;;i&amp;lt;len;i++)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  dp[i]=i;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;//i个字符串，最多分割i次&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="算法" scheme="https://www.xiapf.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="https://www.xiapf.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="动态规划" scheme="https://www.xiapf.com/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    
  </entry>
  
  <entry>
    <title>mac下搭建和使用mve</title>
    <link href="https://www.xiapf.com/blogs/useMVE/"/>
    <id>https://www.xiapf.com/blogs/useMVE/</id>
    <published>2020-03-04T03:47:55.000Z</published>
    <updated>2020-03-06T08:31:26.870Z</updated>
    
    <content type="html"><![CDATA[<p>mve可以将多角度的图片序列重构成三维点云</p><h2 id="搭建mve"><a href="#搭建mve" class="headerlink" title="搭建mve"></a>搭建mve</h2><p>搭建过程参照官方文档<a href="https://github.com/simonfuhrmann/mve" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/simonfuhrmann/mve</a></p><p>以下均在终端进行操作</p><p>1.从github上下载代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/simonfuhrmann/mve.git</span><br></pre></td></tr></table></figure><p>下载之后会在根目录建立mve文件夹</p><a id="more"></a><p>2.进入mve文件夹</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd mve</span><br></pre></td></tr></table></figure><p>进行编辑</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make -j8</span><br></pre></td></tr></table></figure><p>3.添加依赖库</p><p>编译过程中报错，是因为缺少依赖库，需要手动安装。主要是对图片处理的函数库。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">brew install libjpeg</span><br><span class="line">brew install libpng</span><br><span class="line">brew install libtiff</span><br></pre></td></tr></table></figure><p>此时就能成功编译，打开mve下的apps文件夹，当所有的文件夹下生成了”.o”后缀的文件，说明环境已经搭建好。</p><h2 id="简单使用"><a href="#简单使用" class="headerlink" title="简单使用"></a>简单使用</h2><p>1.在官网下载数据集</p><p><a href="http://download.hrz.tu-darmstadt.de/media/FB20/GCC/mve_datasets/" target="_blank" rel="external nofollow noopener noreferrer">http://download.hrz.tu-darmstadt.de/media/FB20/GCC/mve_datasets/</a></p><p>我选择的是der_hass-20140923数据集，里面包含雕像不同角度的79张图像，下载后进行解压</p><p>2.设置环境变量</p><p>生成点云过程中是使用apps下中的子文件夹实现，所以需要设置环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#进入根目录</span><br><span class="line">cd ~</span><br><span class="line">#给任意一个文件添加环境变量</span><br><span class="line">vim ~/.bash_profile</span><br></pre></td></tr></table></figure><p>输入vim命令后，进入输入环境变量页面，输入“O”，进入写模式，将apps下所有文件夹路径加入环境变量中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export PATH=/Users/mve/apps/bundle2pset:$PATH</span><br><span class="line">#以下省略</span><br></pre></td></tr></table></figure><p>输入结束后，按esc，在文末输入”:wq”，退出并保存环境变量</p><p>生效环境变量，将环境变量应用到下载好的数据集下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source 数据集路径名称</span><br></pre></td></tr></table></figure><p>查看环境变量是否生效，在数据集文件夹下打开终端，输入：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo $PATH</span><br></pre></td></tr></table></figure><p>当环境变量中包含了apps下文件夹的路径，说明环境变量设置成功</p><p>3.重建三维生成点云</p><p>der_hass-20140923是下载的数据集，der_hass是生成数据所在的文件夹</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">makescene -i der_hass-20140923 der_hass</span><br></pre></td></tr></table></figure><p>按以下命令一个个运行，der_hass是生成的散文图所在的文件夹</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sfmrecon der_hass</span><br><span class="line">dmrecon -s2 der_hass</span><br><span class="line">scene2pset -F2 der_hass der_hass/pset-L2.ply</span><br><span class="line">fssrecon der_hass/pset-L2.ply der_hass/surface-L2.ply</span><br><span class="line">meshclean -t10 der_hass/surface-L2.ply der_hass/surface-L2-clean.ply</span><br></pre></td></tr></table></figure><p>运行过程比较慢，需要耐心等待，大约需3小时左右，最终生成的点云文件为surface-L2-clean.ply</p><p>4.效果展示</p><p>使用meshlab打开点云文件，效果如下</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200120193729.png" alt="效果图"></p><p>可以看出生成的点云完整，不存在不连续的部分，实现了从图像-&gt;稀疏点云-&gt;稠密点云-&gt;点云表面重建的全过程。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;mve可以将多角度的图片序列重构成三维点云&lt;/p&gt;&lt;h2 id=&quot;搭建mve&quot;&gt;&lt;a href=&quot;#搭建mve&quot; class=&quot;headerlink&quot; title=&quot;搭建mve&quot;&gt;&lt;/a&gt;搭建mve&lt;/h2&gt;&lt;p&gt;搭建过程参照官方文档&lt;a href=&quot;https://github.com/simonfuhrmann/mve&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;https://github.com/simonfuhrmann/mve&lt;/a&gt;&lt;/p&gt;&lt;p&gt;以下均在终端进行操作&lt;/p&gt;&lt;p&gt;1.从github上下载代码&lt;/p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;git clone https://github.com/simonfuhrmann/mve.git&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;下载之后会在根目录建立mve文件夹&lt;/p&gt;
    
    </summary>
    
    
      <category term="点云" scheme="https://www.xiapf.com/categories/%E7%82%B9%E4%BA%91/"/>
    
    
      <category term="mve" scheme="https://www.xiapf.com/tags/mve/"/>
    
      <category term="点云" scheme="https://www.xiapf.com/tags/%E7%82%B9%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>Leecode算法学习笔记（三）</title>
    <link href="https://www.xiapf.com/blogs/listbNote/"/>
    <id>https://www.xiapf.com/blogs/listbNote/</id>
    <published>2020-03-03T07:18:41.000Z</published>
    <updated>2020-03-03T07:20:20.057Z</updated>
    
    <content type="html"><![CDATA[<h2 id="关于链表的算法"><a href="#关于链表的算法" class="headerlink" title="关于链表的算法"></a>关于链表的算法</h2><p>链表节点的定义</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">ListNode</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">  <span class="keyword">int</span> val;</span><br><span class="line">  ListNode* next;</span><br><span class="line">  ListNode(<span class="keyword">int</span> x):val(x),next(<span class="literal">NULL</span>)&#123;&#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>链表常用算法思想包括：快慢指针（对空间复杂度有要求，可以考虑使用快慢指针）</p><a id="more"></a><h3 id="快慢指针"><a href="#快慢指针" class="headerlink" title="快慢指针"></a>快慢指针</h3><p>新建两个指针，快指针走两步，慢指针走一步</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//快慢指针法只使用两个指针，空间复杂度很小，只有O(1)</span></span><br><span class="line">ListNode* fast=head;<span class="comment">//快指针</span></span><br><span class="line">ListNode* slow=head;<span class="comment">//慢指针</span></span><br><span class="line"><span class="keyword">while</span>(fast!=<span class="literal">NULL</span>&amp;&amp;fast-&gt;next!=<span class="literal">NULL</span>)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="comment">//慢指针走一步</span></span><br><span class="line">  slow=slow-&gt;next;</span><br><span class="line">  <span class="comment">//快指针走两步</span></span><br><span class="line">  fast=fast-&gt;next-&gt;next;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>应用场景：</p><p>环形链表（判断链表中是否存在环）：</p><p>（1）判断有无环：可以利用快慢指针，如果存在环，那么在某一个链表节点快指针指向的节点就等于慢指针指向的节点（是节点整体相等，包括val和next，不能只用val判断），反之，则不存在环，整体问题抽象成追赶问题，因为存在环，所以一定能追赶上。</p><p>（2）求环的入口节点：当需要求环形入口节点的时候，首先将判断有无环，在相遇的时候break，此时假设链表中无环部分长度为a，环长度b，相遇时快指针比慢指针走了n个环长度，f=s+nb，又因为f=2s，所以s=nb，慢指针要走到环入口，需要长度为a+nb，所以慢指针需要再走a个长度，此时将fast设置为head，从head走到环入口长度正好为a，当两个指针指向相同链节点的时候，就到达了环的入口</p><h2 id="关于括号的算法"><a href="#关于括号的算法" class="headerlink" title="关于括号的算法"></a>关于括号的算法</h2><h3 id="栈"><a href="#栈" class="headerlink" title="栈"></a>栈</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">[<span class="comment">//用map存储括号的键值对</span></span><br><span class="line">unorder_map&lt;<span class="keyword">char</span>&gt; <span class="built_in">map</span>&#123;&#123;<span class="string">')'</span>,<span class="string">'('</span>&#125;,&#123;&#125;&#125;;</span><br><span class="line">]</span><br><span class="line"><span class="comment">//用栈存储读入的字符</span></span><br><span class="line"><span class="built_in">stack</span>&lt;<span class="keyword">char</span>&gt; s;</span><br><span class="line"><span class="keyword">while</span>(字符串不为空)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">if</span>(当字符是左半边括号时)</span><br><span class="line">    左括号入栈；</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">  &#123;</span><br><span class="line">    ...</span><br><span class="line">    [</span><br><span class="line">    <span class="comment">//取栈顶元素</span></span><br><span class="line">    q=s.top();</span><br><span class="line">    <span class="keyword">if</span>(栈顶元素==<span class="built_in">map</span>[q])</span><br><span class="line">      出栈</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">      未能匹配上</span><br><span class="line">    ]</span><br><span class="line">    [</span><br><span class="line">      <span class="comment">//先出栈，当出栈后栈为空，说明当前右括号前面没有匹配的左括号，此时右括号入栈作为一个位置标记，计算后面的长度，如果在else里出栈，那么会面临出栈后，s.top()不存在的现象</span></span><br><span class="line">      s.pop();</span><br><span class="line">      <span class="comment">//栈为空，存入当前括号</span></span><br><span class="line">      <span class="keyword">if</span>(s.empty())</span><br><span class="line">      s.push(i)</span><br><span class="line">      <span class="comment">//栈不为空，获得长度</span></span><br><span class="line">      <span class="keyword">else</span></span><br><span class="line">        length=<span class="built_in">max</span>(length,i-s.top())</span><br><span class="line">    ]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>应用场景：</p><p>有效的括号（给定一个只包括 <code>&#39;(&#39;</code>，<code>&#39;)&#39;</code>，<code>&#39;{&#39;</code>，<code>&#39;}&#39;</code>，<code>&#39;[&#39;</code>，<code>&#39;]&#39;</code> 的字符串，判断字符串是否有效。）：在map中存储括号的键值对，利用map[值]=键，每次读入字符串的字符时，如果是左边括号就入栈，否则就是右边括号，需要进行检测是否配对，取这时栈顶的括号，如果键值对匹配上，则出栈，检测下一个，否则是没有匹配上。</p><p>最长有效括号（给定一个只包含 <code>&#39;(&#39;</code> 和 <code>&#39;)&#39;</code> 的字符串，找出最长的包含有效括号的子串的长度。）：</p><p>（1）初始化：用栈来存储括号，栈里保存括号位置的下标，为了保证获取长度，初始化栈为-1。</p><p>（2）当扫描到左括号的时候，入栈，当扫描到右括号的时候，此时将栈顶元素出栈，分为两种情况，一种是当栈为空的时候，说明之前没有匹配的左括号，把当前括号入栈，另一种栈不为空，说明有匹配的左括号，用当前位置减去栈顶的位置，并更新长度</p><h3 id="深度遍历-剪枝"><a href="#深度遍历-剪枝" class="headerlink" title="深度遍历+剪枝"></a>深度遍历+剪枝</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//输入参数为当前由括号组成的路径，左子树括号的个数，右子树括号的个数，以及最终结果</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="built_in">string</span> str,<span class="keyword">int</span> left,<span class="keyword">int</span> right,<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&gt;&gt; res)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="comment">//结束的条件,此时出现了一条满足条件的路径:左右子树的括号个数都为0</span></span><br><span class="line">  <span class="keyword">if</span>(left==<span class="number">0</span>&amp;&amp;right==<span class="number">0</span>)</span><br><span class="line">    res.push(str)</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  <span class="comment">//剪枝：左子树的括号个数大于右子树的括号个数,出现不匹配</span></span><br><span class="line">  <span class="keyword">if</span>(left&gt;right)</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  <span class="comment">//左边括号多，先生成左括号</span></span><br><span class="line">  <span class="keyword">if</span>(left&gt;<span class="number">0</span>)</span><br><span class="line">    dfs(str+<span class="string">'('</span>,left<span class="number">-1</span>,right,res)</span><br><span class="line">  <span class="comment">//右边括号多，再生成右括号</span></span><br><span class="line">  <span class="keyword">if</span>(right&gt;<span class="number">0</span>)</span><br><span class="line">    dfs(str+<span class="string">'）'</span>,left<span class="number">-1</span>,right,res)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>应用场景：</p><p>括号的生成（给出 n 代表生成括号的对数，写出一个函数，使其能够生成所有可能的并且有效的括号组合）：实质是对括号做减法，当左括号数量大于0，就不断生成左括号，右括号数量大于0，生成右括号，但是存在不满足括号匹配的条件，当左括号数量大于右括号，此时需要剪枝（加一个判断条件，return）,当左右括号数量都为0，说明找到了一个合适的路径，将其存储下来。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;关于链表的算法&quot;&gt;&lt;a href=&quot;#关于链表的算法&quot; class=&quot;headerlink&quot; title=&quot;关于链表的算法&quot;&gt;&lt;/a&gt;关于链表的算法&lt;/h2&gt;&lt;p&gt;链表节点的定义&lt;/p&gt;&lt;figure class=&quot;highlight c++&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;ListNode&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&amp;#123;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; val;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  ListNode* next;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  ListNode(&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; x):val(x),next(&lt;span class=&quot;literal&quot;&gt;NULL&lt;/span&gt;)&amp;#123;&amp;#125;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;链表常用算法思想包括：快慢指针（对空间复杂度有要求，可以考虑使用快慢指针）&lt;/p&gt;
    
    </summary>
    
    
      <category term="算法" scheme="https://www.xiapf.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="https://www.xiapf.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="链表" scheme="https://www.xiapf.com/tags/%E9%93%BE%E8%A1%A8/"/>
    
      <category term="括号" scheme="https://www.xiapf.com/tags/%E6%8B%AC%E5%8F%B7/"/>
    
  </entry>
  
  <entry>
    <title>Leecode算法学习笔记（二）</title>
    <link href="https://www.xiapf.com/blogs/backTrackNote/"/>
    <id>https://www.xiapf.com/blogs/backTrackNote/</id>
    <published>2020-02-25T07:08:13.000Z</published>
    <updated>2020-03-04T04:56:30.496Z</updated>
    
    <content type="html"><![CDATA[<h2 id="回溯法"><a href="#回溯法" class="headerlink" title="回溯法"></a>回溯法</h2><p>回溯一般用于找到满足要求的的所有解，即相当于构造一棵树，从跟节点出发，找到满足条件的所有路径，并进行保存，当目前的路径不符合条件的时候，需要回溯，即剪枝，删除不符合的分支，当到达结束的条件时，即找到了一个满足条件的解，再继续构造下一个符合条件的路径。</p><p>常用于解决树种求得所有解的问题和排列组合中求解问题，或者问题可以转换为求树中一条路径的问题，可以画图。</p><a id="more"></a><h3 id="关于树的回溯"><a href="#关于树的回溯" class="headerlink" title="关于树的回溯"></a>关于树的回溯</h3><p>伪代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//result存储最终结果，res存储中间结果</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(TreeNode* root,<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&gt; result,<span class="built_in">vector</span> res，….)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   <span class="comment">//处理当前节点</span></span><br><span class="line">   res.push(root-&gt;val)</span><br><span class="line"></span><br><span class="line">   …</span><br><span class="line"></span><br><span class="line">  <span class="comment">//结束的条件</span></span><br><span class="line">  <span class="keyword">if</span>(满足题目要求（一般是值相等）并且到达根节点)</span><br><span class="line">  result.push(res)</span><br><span class="line">  <span class="keyword">if</span>(root-&gt;left)</span><br><span class="line">  &#123;</span><br><span class="line">    doTree(root-&gt;left,其他参数)</span><br><span class="line">    <span class="comment">//不满足条件，需要回溯</span></span><br><span class="line">    res.pop()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span>(root-&gt;right)</span><br><span class="line">  &#123;</span><br><span class="line">    doTree(root-&gt;right,其他参数)</span><br><span class="line">    <span class="comment">//不满足条件，需要回溯</span></span><br><span class="line">    res.pop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>应用场景：</p><p>找到所有满足目标节点的路径：每次把当前节点加入，并将目标值减去当前节点的值，判断当前叶子节点值（左右子树为空）与0是否相等，相等，则找到一个满足的结果，否则，继续对左右子树递归，此时若存在不满足条件的，需要回溯（pop）</p><h3 id="排列组合"><a href="#排列组合" class="headerlink" title="排列组合"></a>排列组合</h3><p>排列组合求所有符合问题的解，基本框架：</p><p>组合问题按顺序读取，不需要设置used数组，排列问题则需要设置</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="keyword">int</span> start,<span class="keyword">int</span> target)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="comment">//结束的条件：找到满足要求的一个解</span></span><br><span class="line">  <span class="keyword">if</span>(满足值为<span class="number">0</span>或者其他条件)</span><br><span class="line">  &#123;</span><br><span class="line">    result.push(res);</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//循环的条件</span></span><br><span class="line">  <span class="keyword">for</span>(i&lt;数组长度&amp;&amp;target-num[i]&gt;=<span class="number">0</span>)</span><br><span class="line">  &#123;</span><br><span class="line">    [</span><br><span class="line">      <span class="comment">//避免重复元素得出重复组合</span></span><br><span class="line">      <span class="keyword">if</span>(i&gt;start&amp;&amp;num[i]==num[i<span class="number">-1</span>])</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">    ]</span><br><span class="line">    [</span><br><span class="line">      <span class="comment">//全排列中不能使用上一层使用的元素</span></span><br><span class="line">      <span class="comment">//设置标志位或者其他判断条件</span></span><br><span class="line">      <span class="keyword">if</span>(!use[i])&#123;&#125;</span><br><span class="line">    ]</span><br><span class="line">    <span class="comment">//当时排列问题，并且有重复数字的时候，需要将上面两个条件结合</span></span><br><span class="line">    <span class="comment">//选择当前节点</span></span><br><span class="line">    res.push(num[i]);</span><br><span class="line">    [</span><br><span class="line">      <span class="comment">//选择了该节点，需要设置标志位</span></span><br><span class="line">      used[i]=<span class="literal">true</span>;</span><br><span class="line">    ]</span><br><span class="line">    <span class="comment">//[可以重复使用]</span></span><br><span class="line">    dfs(i,target-num[i]);</span><br><span class="line">    [<span class="comment">//不可以重复使用</span></span><br><span class="line">    [dfs(i+<span class="number">1</span>,target-num[i]);</span><br><span class="line">    ]</span><br><span class="line">    <span class="comment">//回溯，取消当前选择，返回上一层</span></span><br><span class="line">    res.pop();</span><br><span class="line">    [</span><br><span class="line">      <span class="comment">//撤销选择，需要还原标志位</span></span><br><span class="line">      used[i]=<span class="literal">true</span>;</span><br><span class="line">    ]</span><br><span class="line">      </span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p> 应用场景：</p><p>组合求和（数组元素内部不重复，在数组中找到和为目标值的组合）：首先对数组排序，排序能更方便去重。在dfs函数中，每次传入数组中数据的位置和目标值，当剩余值大于等于0，进入循环，将当前数组值存入路径中，再继续递归，做下一次决策，由于可重复，所以下一次可以仍然从i开始，当满足条件时，将当前路径存入结果，此时回溯到上一层，继续寻找合适的路径。</p><p>组合求和（数组元素内部可能有重复，在数组中找到和为目标值的组合，此时不能重复使用数组的元素，解集不能包含重复的组合）：思路同上，因为不能重复使用数组元素，所以在for循环中dfs函数传值只鞥从i+1开始。与此同时，由于解集不能包含重复的组合，所以在排序后数组，当前一个数组元素和当前数组元素相同，那么当前元素得出的路径和上一个元素必定相同，因此这时需要跳过当前元素，对下一个元素处理。</p><p>全排列（给定一个没有重复数字的序列，返回其所有可能的全排列）：排列题是上一次选择的节点，这次不能再选择。每一层都可以选择一个未出现的元素，需要对数组元素设置标志位，当使用过，就将标志位置为1，此时也需要在for循环中加一层判断，查看当前数组是否被使用，当使用过时，需要跳过该元素，下一层继续选择，找到一个合适的结果后存储下来，接着撤销本次选择，返回上一层查看是否有其他结果。</p><p>全排列（给定一个可包含重复数字的序列，返回所有不重复的全排列）：思路同上，此时需要额外加个判断，看是否有重复的数字，有重复的数字则跳过。具体为：本次节点与前一个相同，并且前一个已使用过，说明已有相同的排列，则本次排列跳过。</p><p>n皇后问题：转换为全排列问题，在每一个的n个位置中选一个作为当前选择，n行做n个选择，并且是做做不重复的选择，正好构成一个排列，此时的排列满足不在同一行不在同一列，只要对当前的排列需要再满足题设条件即可，即不在主对角线，不在副对角线，vector<bool> master1;//副对角线和相同，都为i+j i为行，j为列，两者相加和是当前行，vector<bool> master2;//主对角线差相同，为i-j+n-1 i为行，j为列，再设置两个标志位进行判断，最后将得到的排列用题目中格式打印出来作为结果。</bool></bool></p><p>子集（给定一组不含重复元素的整数数组 nums，返回该数组所有可能的子集（幂集））：对不同子集个数进行深度遍历，此时需要在主函数中加一个循环</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;nums.<span class="built_in">size</span>()+<span class="number">1</span>;i++)<span class="comment">//i的长度为0~nums.size(),所以需要加一</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="comment">//----通过i控制子集大小</span></span><br><span class="line">  dfs(start,i);</span><br><span class="line">  <span class="comment">//start是从数组中取元素的开始的位置，i是深度，即子集中元素的个数</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>分割字符串（给定一个字符串 <em>s</em>，将 <em>s</em> 分割成一些子串，使每个子串都是回文串）：每次判断当前的起始至末尾位置是否是回文，如果是就将结果压入res，继续递归。通过不断变换分割的位置，来构造决策树。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=start;i&lt;len;i++)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">if</span>(start~i之间不是回文)</span><br><span class="line">    继续找下一个回文的位置</span><br><span class="line">    </span><br><span class="line">   <span class="comment">//start~i之间是回文，压入结果内</span></span><br><span class="line">    res.push(s.substr(start,start+i+<span class="number">1</span>))</span><br><span class="line">    <span class="comment">//继续看下一个分割位置</span></span><br><span class="line">    dfs(i+<span class="number">1</span>,depth)</span><br><span class="line">    <span class="comment">//撤销选择，回到上一层决策</span></span><br><span class="line">    res.pop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;回溯法&quot;&gt;&lt;a href=&quot;#回溯法&quot; class=&quot;headerlink&quot; title=&quot;回溯法&quot;&gt;&lt;/a&gt;回溯法&lt;/h2&gt;&lt;p&gt;回溯一般用于找到满足要求的的所有解，即相当于构造一棵树，从跟节点出发，找到满足条件的所有路径，并进行保存，当目前的路径不符合条件的时候，需要回溯，即剪枝，删除不符合的分支，当到达结束的条件时，即找到了一个满足条件的解，再继续构造下一个符合条件的路径。&lt;/p&gt;&lt;p&gt;常用于解决树种求得所有解的问题和排列组合中求解问题，或者问题可以转换为求树中一条路径的问题，可以画图。&lt;/p&gt;
    
    </summary>
    
    
      <category term="算法" scheme="https://www.xiapf.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="https://www.xiapf.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="回溯" scheme="https://www.xiapf.com/tags/%E5%9B%9E%E6%BA%AF/"/>
    
  </entry>
  
  <entry>
    <title>Leecode算法学习笔记（一）</title>
    <link href="https://www.xiapf.com/blogs/treeNote/"/>
    <id>https://www.xiapf.com/blogs/treeNote/</id>
    <published>2020-02-24T08:56:00.000Z</published>
    <updated>2020-03-10T09:46:57.962Z</updated>
    
    <content type="html"><![CDATA[<h2 id="关于树的算法"><a href="#关于树的算法" class="headerlink" title="关于树的算法"></a>关于树的算法</h2><p>除了基本的遍历，其余关于树的处理基本都是依赖递归算法，总体思路，是对当前节点进行处理，再将剩余的交给左子树和右子树分别递归处理。</p><h3 id="层次遍历"><a href="#层次遍历" class="headerlink" title="层次遍历"></a>层次遍历</h3><p>队列：每次把同一层的节点入队，根据先进先出的特点，依次读取队首节点，再把当前节点的左右子树入队，作为下一层需要遍历的节点</p><p>伪代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//队列存储根节点</span></span><br><span class="line">p.push(root)</span><br><span class="line"><span class="keyword">while</span>(队列不为空)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="comment">//获取该层节点个数</span></span><br><span class="line">  <span class="built_in">width</span>=p.<span class="built_in">size</span>()</span><br><span class="line">  <span class="comment">//读取该层所有节点，并存储下一层节 点</span></span><br><span class="line">  <span class="keyword">for</span>(i&lt;<span class="built_in">width</span>)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="comment">//读取队首节点</span></span><br><span class="line">    r=p.front()</span><br><span class="line">    出队</span><br><span class="line">    <span class="keyword">if</span>(r的左子树存在) 左子树入队</span><br><span class="line">    <span class="keyword">if</span>(r的右子树存在) 右子树入队</span><br><span class="line">  &#125;</span><br><span class="line">  存储每层的结果</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><a id="more"></a><p>应用场景：</p><p>锯齿形层次遍历：需要把偶数行反序排列</p><p>求最大深度：每层入队时，增加一个层数</p><p>求最小深度（从根节点到最近叶子节点的最短路径上的节点数量）：每层遇到左右节点均为空的节点时，输出此时的层数</p><h3 id="前、中、后序遍历"><a href="#前、中、后序遍历" class="headerlink" title="前、中、后序遍历"></a>前、中、后序遍历</h3><p>a.前序 遍历特点：根-左-右</p><p>栈：每次先访问当前节点，并依次将右子树压入栈，再循环遍历左子树，根据先进后出的特点，循环结束后，下次将从最左边子树的右子树开始循环</p><p>伪代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//新建一个树节点</span></span><br><span class="line"></span><br><span class="line">p=root</span><br><span class="line"><span class="comment">//p不为空说明还有未遍历到的节点，s不为空说明栈里还有存储的子树</span></span><br><span class="line"><span class="keyword">while</span>(p不为空||s不为空)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">while</span>(p不为空)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="comment">//访问当前节点</span></span><br><span class="line">    res.push(p-&gt;val)</span><br><span class="line">    <span class="comment">//右子树入栈</span></span><br><span class="line">    s.push(p-&gt;right)</span><br><span class="line">    <span class="comment">//不断遍历左子树</span></span><br><span class="line">    p=p-&gt;left</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//处理下一个节点</span></span><br><span class="line">  p=s.top();</span><br><span class="line">  出栈</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>b.后序 遍历特点：左-右-根</p><p>栈：因为先对根节点访问较为简单，原理类似前序遍历，因此，可以先访问当前根节点，将左子树压入栈，再循环遍历右子树。循环结束，下一次是对最右边的左子树开始循环。形成结果是根-右-左，最后将结果逆序即可</p><p>伪代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//新建一个树节点</span></span><br><span class="line">p=root</span><br><span class="line"><span class="comment">//p不为空说明还有未遍历到的节点，s不为空说明栈里还有存储的子树</span></span><br><span class="line"><span class="keyword">while</span>(p不为空||s不为空)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">while</span>(p不为空)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="comment">//访问当前根节点</span></span><br><span class="line">    res.push(p-&gt;val)</span><br><span class="line">    <span class="comment">//左子树入栈</span></span><br><span class="line">    s.push(p-&gt;left)</span><br><span class="line">    <span class="comment">//遍历右子树</span></span><br><span class="line">    p=p-&gt;right</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//处理下一个节点</span></span><br><span class="line">  p=s.top;</span><br><span class="line">  出栈</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//将最终结果逆序</span></span><br><span class="line">reverse(res.<span class="built_in">begin</span>(),res.<span class="built_in">end</span>())</span><br></pre></td></tr></table></figure><p>c.中序 遍历特点：左-根-右</p><p>栈：中序处理方法和上两种有所不同，需要不断循环先找到最左边叶子节点，在此过程中需要不断把当前遍历到的根节点入栈，栈中存储根节点和左子树节点，当循环结束时，从栈尾中取出节点，再不断对该节点的右子树循环，</p><p>伪代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//新建一个树节点</span></span><br><span class="line">p=root</span><br><span class="line"><span class="comment">//p不为空说明还有未遍历到的节点，s不为空说明栈里还有存储的子树</span></span><br><span class="line"><span class="keyword">while</span>(p不为空||s不为空)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="comment">//循环过程中不断将根节点和左子树节点入栈</span></span><br><span class="line">  <span class="keyword">while</span>(p不为空)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="comment">//根节点入栈</span></span><br><span class="line">    s.push(p)</span><br><span class="line">    <span class="comment">//找到最左边的子树节点</span></span><br><span class="line">    p=p-&gt;left</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//访问当前栈尾节点，</span></span><br><span class="line">  p=s,top();</span><br><span class="line">  res.push(p-&gt;val)</span><br><span class="line">  <span class="comment">//从栈中取出根节点</span></span><br><span class="line">  出栈</span><br><span class="line">  <span class="comment">//循环右子树</span></span><br><span class="line">  p=p-&gt;right</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>应用场景：</p><p>验证一棵树是否是二叉搜索树（二叉搜索树的特点是左子树小于根节点，根节点小于右子树）：可以通过中序遍历比较节点值来判断</p><h3 id="递归"><a href="#递归" class="headerlink" title="递归"></a>递归</h3><p>总体框架：明确一个节点需要做的事，剩余的分别扔给左子树和右子树处理</p><p>伪代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">TreeNode* <span class="title">doTree</span><span class="params">(TreeNode* root)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="comment">//递归结束的条件：一个节点需要做的事（可能有多个结束条件）</span></span><br><span class="line">  <span class="keyword">if</span>(到达树的末尾)</span><br><span class="line">  <span class="keyword">do</span>….</span><br><span class="line">  <span class="comment">//递归循环的条件剩余的交给左右子树处理</span></span><br><span class="line">  doTree(root-&gt;left,其他参数)</span><br><span class="line">  doTree(root-&gt;right,其他参数)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​        </p><p>具体而言，可以分为以下几类：</p><p>根据前序、中序遍历或者中序、后序遍历构造二叉树：根据前序根节点在首位，可以在中序遍历确定左右子树；根据后序根节点在尾部，可以在中序遍历确定左右子树</p><p> 伪代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">TreeNode* <span class="title">getTree</span><span class="params">(<span class="built_in">vector</span> p, <span class="keyword">int</span> pl,<span class="keyword">int</span> pr, <span class="built_in">vector</span> i, <span class="keyword">int</span> il,<span class="keyword">int</span> ir)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="comment">//结束的条件</span></span><br><span class="line">  <span class="keyword">if</span>(左位置大于右位置)</span><br><span class="line">  <span class="keyword">return</span> null</span><br><span class="line">  <span class="comment">//循环的条件</span></span><br><span class="line">  <span class="comment">//根据前序或者后序遍历的特点找到根节点</span></span><br><span class="line">  pivot=p[pl]</span><br><span class="line">  <span class="comment">//根据根节点生成树节点</span></span><br><span class="line">  TreeNode root=<span class="keyword">new</span> TreeNode(pivot)</span><br><span class="line">  <span class="comment">//在中序遍历中找到根节点的位置</span></span><br><span class="line">  pivotindex=il</span><br><span class="line">  <span class="keyword">while</span>(i[pivotindex]!=pivot)</span><br><span class="line">  &#123;</span><br><span class="line">  pivotindex++</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//确定左子树</span></span><br><span class="line">  root-&gt;left= getTree(p,I,左子树的位置)</span><br><span class="line">  <span class="comment">//确定右子树</span></span><br><span class="line">  root-&gt;right= getTree(p,I,右子树的位置)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>伪代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">TreeNode* <span class="title">doTree</span><span class="params">(TreeNode* root)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">if</span>(递归到树的最后，即某一结束的条件)</span><br><span class="line">  ….</span><br><span class="line">  <span class="keyword">if</span>(满足题目要求（达到某一个值）&amp;&amp; doTree（root-&gt;left）&amp;&amp; doTree（root-&gt;right）(左右子树分别满足条件，此时递归，位置也可放到<span class="keyword">return</span>后面))</span><br><span class="line">    …</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>应用场景：</p><p>判断树的结构和值是否相同，即是否是相同的树：一个节点判断值是否相同，且左右子树的值和结构是否一致（调用函数，即递归）</p><p>对称二叉树：克隆一个数，和原来的树比较结构是否一致，思路同上</p><p>平衡二叉树（每个节点的左右子树的高度差绝对值小于1）：判断当前节点的高度，且左右子树的高度是否满足高度差小于1（调用函数，即递归）  注：里面包含求树的高度，可以找出左右子树高的那边再加1（也是使用递归）</p><p>判断一个树的路径和是否和目标值相同：每次判断当前叶子节点值（左右子树为空）与目标值是否相等，否则，将目标值减去当前节点值继续对左右子树递归</p><p>转换为二叉搜索树：需要利用中点，将原有数据进行排序,每次需要确定构造的数据的左右位置</p><p>伪代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">transBTS</span><span class="params">(<span class="built_in">vector</span> num，<span class="keyword">int</span> left,<span class="keyword">int</span> right)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="comment">//结束的条件</span></span><br><span class="line">  <span class="keyword">if</span>(left&gt;right)</span><br><span class="line">  <span class="keyword">return</span> null</span><br><span class="line">  <span class="comment">//找到中点,构造根节点</span></span><br><span class="line">  <span class="comment">//(1)数组可以直接利用索引的1/2</span></span><br><span class="line">  <span class="comment">//(2)链表需要利用快慢指针，快指针走两步，慢指针走一步，快指针到末尾的时候，慢指针正好到中点</span></span><br><span class="line">  mid=…</span><br><span class="line">  TreeNode* root=<span class="keyword">new</span> TreeNode(num(mid))</span><br><span class="line">  <span class="comment">//递归构造左子树</span></span><br><span class="line">  root-&gt;left= transBTS(root-&gt;left,left,mid<span class="number">-1</span>)</span><br><span class="line">  <span class="comment">//递归构造右子树</span></span><br><span class="line">  root-&gt;right= transBTS(root-&gt;right, mid<span class="number">-1</span>,right)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>关于二叉搜索树的框架：利用左小右大的特点（类似二分查找）</p><p>​    伪代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">TreeNode* <span class="title">BTS</span><span class="params">(TreeNode* root,<span class="keyword">int</span> num)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="comment">//结束条件</span></span><br><span class="line">  <span class="keyword">if</span>(root-&gt;val==num)</span><br><span class="line">  <span class="keyword">do</span>..(可以做增删改之类的操作)</span><br><span class="line">  <span class="comment">//循环条件</span></span><br><span class="line">  <span class="comment">//大于：对左子树</span></span><br><span class="line">  <span class="keyword">if</span>(root-&gt;val&gt;num)</span><br><span class="line">  <span class="keyword">return</span> BTS(root-&gt;left,num)</span><br><span class="line">  <span class="comment">//小于：对右子树</span></span><br><span class="line">  <span class="keyword">if</span>(root-&gt;val&lt;num)</span><br><span class="line">  <span class="keyword">return</span> BTS(root-&gt;right,num)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="求“树的高度”类型题"><a href="#求“树的高度”类型题" class="headerlink" title="求“树的高度”类型题"></a>求“树的高度”类型题</h3><p>1.求树的高度</p><p>采用递归，自底向上</p><p>找到最底层节点，每次返回当前节点的左子树高度和右子树高度的较大值，则当前节点的高度为max(left,right)+1，一直向上递归。如图所示的二叉树：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20191226110659.png" alt="二叉树"></p><p>从根节点递归，一直找到最左边的节点1，此时它的左右子树均为空，left=0,right=0,所以它的高度为1，接着返回上一层节点7的右子树4的高度，同理，节点4的高度为1，因此7的高度为max(depth(1),depth(4))+1，以此类推，一直向上找到根节点的高度即为树的高度。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">depth</span><span class="params">(TreeNode* root)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">if</span>(root==null)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//当前节点的左子树高度</span></span><br><span class="line">  <span class="keyword">int</span> l=depth(root-&gt;left);</span><br><span class="line">  <span class="comment">//当前节点的右子树高度</span></span><br><span class="line">  <span class="keyword">int</span> r=depth(root-&gt;left);</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//返回当前节点的高度为两者中大的那个再加一</span></span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">max</span>(l,r)+<span class="number">1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>2.求二叉树直径，即任意两个结点路径长度中的最大值</p><p>思路同求树的高度：任意两个节点之间的距离是当前节点左子树加上右子树的高度，因为是求最大路径和，则最大不可能出现在中间节点，只可能是任意两个叶子节点之间的距离，该距离可以通过两个叶子节点之间连线经过的根节点的左右子树的高度相加来求解，如图二叉树</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20200310165729.jpg" alt="二叉树"></p><p>节点1和节点2之间的路径为3，即为经过的根节点3的左右子树的高度和，同理，姐弟啊1和节点9之间的距离为5，即为即为经过的根节点6的左右子树的高度和.</p><p>所以问题转换为：求二叉树上所有节点的左右子树高度和的最大值，因此，可以将“求二叉树高度”的代码稍微进行改动即可。</p><p>注：因为每次要比较各个节点的高度和，所以需要传入一个maxvalue的值(传入的时候要加上取地址符号)，每次进行比较，一直更新，最终得出最大路径和。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxTrack</span><span class="params">(TreeNode* root,<span class="keyword">int</span>&amp; maxValue)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">if</span>(root==null)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//求左右子树的高度</span></span><br><span class="line">  <span class="keyword">int</span> l=maxTrack(root-&gt;left,maxValue);</span><br><span class="line">  <span class="keyword">int</span> r=maxTrack(root-&gt;right,maxValue);</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//对当前的高度和即路径和进行比较</span></span><br><span class="line">  maxValue=<span class="built_in">max</span>(maxValue,l+r);</span><br><span class="line"></span><br><span class="line">  <span class="comment">//返回左右子树高度较大的那个</span></span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">max</span>(l,r)+<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最终得出maxValue为最大路径和，maxTrack函数的最终返回值是根节点左右子树的高度和。</p><p>3.求二叉树的最大路径和，即从任意一个节点出发，到另一个节点之间的路径和的最大值。</p><p>思路同求二叉树的高度：从根节点开始递归，一直找到最左边的节点，计算此时的节点的左右子树较大的路径和，将值传递给当前父节点，父节点将自身值加上该值，继续传值给上一层节点，上一层的父节点也是在左右子树的较大值之间进行选择。保证每次都是在当前节点的左右子树中选择大的值进行回传，这样的保证路径和往大的方向走。</p><p>如图二叉树</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20191226110659.png" alt="二叉树"></p><p>节点1的路径和是1,此时maxPath=0,更新为1，节点4的路径和是4，此时maxPath=1,更新为4，当求到节点7的路径和时，在它的左子树（节点1）和右子树（节点4）的路径和之间选择大的，即选择节点4，此时路径为7+4=11，此时maxPath=4,更新为11。再对节点3处理，左子树（11），右子树（2），选择左子树，此时路径和是11+3,此时maxPath=11,更新为14，以此类推，最终得到maxPath。</p><p>注：当当前左右子树加上当前节点值是负数，说明该条路径出现了“负贡献”，此时路径归0</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxPath</span><span class="params">(TreeNode* root,<span class="keyword">int</span>&amp; maxValue)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">if</span>(root==null)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//求左右子树的路径和</span></span><br><span class="line">  <span class="keyword">int</span> l=maxPath(root-&gt;left,maxValue);</span><br><span class="line">  <span class="keyword">int</span> r=maxPath(root-&gt;right,maxValue);</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//更新当前最大路径</span></span><br><span class="line">  maxValue=<span class="built_in">max</span>(maxValue,l+r+root-&gt;value);</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//判断路径有无出现负贡献</span></span><br><span class="line">  <span class="keyword">int</span> temp=<span class="built_in">max</span>(l,r)+root-&gt;val;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//每次选择左右子树中大的和自身相加！！！！</span></span><br><span class="line">  <span class="comment">//返回当前节点的路径和，作为上一次的子树值</span></span><br><span class="line">  <span class="keyword">return</span> temp&lt;<span class="number">0</span>?<span class="number">0</span>:temp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最终得到maxValue是最大路径和，maxPath返回值是根节点的左右子树加上自身的路径和。</p><h3 id="常规思路——bfs和dfs"><a href="#常规思路——bfs和dfs" class="headerlink" title="常规思路——bfs和dfs"></a>常规思路——bfs和dfs</h3><p>二叉树的题目大多数是遍历的变形题，往两个方向考虑：使用bfs——层次遍历还是dfs——深度遍历/回溯、递归，当使用非递归，即层次遍历的时候注意叶子节点的判断。</p><p>例如求二叉树从根节点出发的路径和，可以使用层次遍历，使用两个栈，一个用来存储每层的节点，另一个用来存储每层相加的和，当出现叶子节点的时候就把相加的和累加上去。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span>(栈不为空)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">int</span> <span class="built_in">width</span>=p.<span class="built_in">size</span>();</span><br><span class="line">  <span class="comment">//存储每层累加的和</span></span><br><span class="line">  <span class="keyword">int</span> val=<span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="built_in">width</span>;i++)</span><br><span class="line">  &#123;</span><br><span class="line">    r=p.front();</span><br><span class="line">    <span class="comment">//把当前节点的值加上上一次的和的10倍</span></span><br><span class="line">    val=numQue.front()*<span class="number">10</span>+r-&gt;val;<span class="comment">//numQue中存储每层节点累加的和</span></span><br><span class="line">    p.pop();</span><br><span class="line">    numQue.pop();</span><br><span class="line">    <span class="keyword">if</span>(叶子节点)</span><br><span class="line">      res+=val;</span><br><span class="line">    <span class="keyword">if</span>(左子树存在)</span><br><span class="line">    &#123;</span><br><span class="line">      p.push(左子树)</span><br><span class="line">      <span class="comment">//之前累加的和入栈</span></span><br><span class="line">      numQue.push(val);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(右子树存在)</span><br><span class="line">    &#123;</span><br><span class="line">      p.push(右<span class="number">2</span>子树)</span><br><span class="line">      <span class="comment">//之前累加的和入栈</span></span><br><span class="line">      numQue.push(val);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;关于树的算法&quot;&gt;&lt;a href=&quot;#关于树的算法&quot; class=&quot;headerlink&quot; title=&quot;关于树的算法&quot;&gt;&lt;/a&gt;关于树的算法&lt;/h2&gt;&lt;p&gt;除了基本的遍历，其余关于树的处理基本都是依赖递归算法，总体思路，是对当前节点进行处理，再将剩余的交给左子树和右子树分别递归处理。&lt;/p&gt;&lt;h3 id=&quot;层次遍历&quot;&gt;&lt;a href=&quot;#层次遍历&quot; class=&quot;headerlink&quot; title=&quot;层次遍历&quot;&gt;&lt;/a&gt;层次遍历&lt;/h3&gt;&lt;p&gt;队列：每次把同一层的节点入队，根据先进先出的特点，依次读取队首节点，再把当前节点的左右子树入队，作为下一层需要遍历的节点&lt;/p&gt;&lt;p&gt;伪代码：&lt;/p&gt;&lt;figure class=&quot;highlight c++&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;//队列存储根节点&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;p.push(root)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;while&lt;/span&gt;(队列不为空)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;comment&quot;&gt;//获取该层节点个数&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;built_in&quot;&gt;width&lt;/span&gt;=p.&lt;span class=&quot;built_in&quot;&gt;size&lt;/span&gt;()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;comment&quot;&gt;//读取该层所有节点，并存储下一层节 点&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt;(i&amp;lt;&lt;span class=&quot;built_in&quot;&gt;width&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;//读取队首节点&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    r=p.front()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    出队&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt;(r的左子树存在) 左子树入队&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt;(r的右子树存在) 右子树入队&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  存储每层的结果&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="算法" scheme="https://www.xiapf.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="https://www.xiapf.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="树" scheme="https://www.xiapf.com/tags/%E6%A0%91/"/>
    
  </entry>
  
  <entry>
    <title>排序算法</title>
    <link href="https://www.xiapf.com/blogs/sort/"/>
    <id>https://www.xiapf.com/blogs/sort/</id>
    <published>2019-12-26T06:27:48.000Z</published>
    <updated>2020-01-20T12:17:50.705Z</updated>
    
    <content type="html"><![CDATA[<p>几种排序算法的比较，对原始序列：6，3，5，7，2，9，8，1，4  进行排序</p><h6 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h6><p>第一次：从第一个数字开始，依次相邻比较，前面大于后面则调换位置，比到最后一位，则选出了最大值放在最后</p><p>​        第一次排序后：3，5，6，2，7，8，1，4，9</p><p>​        第二次排序后：3，5，2，6，7，1，4，8，9</p><a id="more"></a><p>​        。。。。。。。</p><p>时间复杂度：O(n^2)</p><h6 id="简单选择排序"><a href="#简单选择排序" class="headerlink" title="简单选择排序"></a>简单选择排序</h6><p>第一次：首先排第一个位置，先设min=0记录最小值位置，默认第一个位置，将6与3比较，3比6小，则min=1(代表3的位置)；再将3与5比较，3比5小，不改变min值；依次将min位置的数值依次与后续数字比较，比min小则更改min为更小值的位置，直到最后一个数字比完。此时min=7(1的位置)，将6与7对换，则第一次排序完成。</p><p>​        第一次排序后顺序是：1，3，5，7，2，9，8，6，4</p><p>第二次：排第二个位置，方法同上，设min=1代表默认第二个位置，即数值3默认最小值，将3依次与后续比较，找出最小值，min=最小值的位置，交换第二个位置的值3与min位置的值2，完成第二次排序。</p><p>​        第二次排序后：1，2，5，7，3，9，8，6，4</p><p>​        第三次排序后：1，2，3，7，5，9，8，6，4</p><p>​        。。。。。。。</p><p>时间复杂度：O(n^2)</p><p>​        第五次排序后顺序为：1，2，3，4，5，9，8，6，7</p><p>​        第六次排序后顺序为：1，2，3，4，5，6，8，9，7</p><p>​        第七次排序后顺序为：1，2，3，4，5，6，7，9，8</p><p>​        第八次排序后顺序为：1，2，3，4，5，6，7，8，9</p><h6 id="直接插入排序"><a href="#直接插入排序" class="headerlink" title="直接插入排序"></a>直接插入排序</h6><p>​        第一次排序后：6</p><p>​        第二次排序后：3，6</p><p>​        第三次排序后：3，5，6</p><p>​        。。。。。。。</p><p>时间复杂度：O(n^2)</p><h6 id="折半插入排序"><a href="#折半插入排序" class="headerlink" title="折半插入排序"></a>折半插入排序</h6><p>同直接插入排序，但是在比较的时候，是折半比较，思想同折半查找</p><p>时间复杂度：O(n^2)</p><h6 id="希尔排序"><a href="#希尔排序" class="headerlink" title="希尔排序"></a>希尔排序</h6><p>第一次：设步长d=5，则6和9；3和8；5和1；7和4；直接插入排序</p><p>​        第一次排序后：6，3，1，4，2，9，8，5，7</p><p>第二次：d = d/2 = 2，则6，1，2，8，7；3，4，9，5；直接插入排序</p><p>​        第二次排序后：1，3，2，4，6，5，7，9，8</p><p>第三次：d = d/2 = 1，则所有数据直接插入排序</p><p>​        第三次排序后：1，2，3，4，5，6，7，8，9</p><p>时间复杂度：O(nlog2^n)</p><h6 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h6><p>第一次：选取第一个数6为轴，将第一个6与最后一个数字4比较（从后往前比），如果大于比较的数字，则调换，然后与原位置后面开始比（从前往后比），以此类推，第一次排完后，轴6的左边都小于6，右边都大于6。</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20191226160921.png" alt="快速排序i,j变化"></p><p>​        第一次排序后：4，3，5，1，2，6，8，9，7</p><p>第二次：则将上个比较后的轴的左右依次做快速排序，直到所有子表的表长不超过1</p><p>时间复杂度：O(nlog2^n)</p><h6 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h6><p>首先建立初始堆，按照顺序方式建立完全二叉树</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20191226110659.png" alt="二叉树"></p><p>然后建立大顶堆，分别将有子节点的7，3，5，6从底层开始，分别与子节点比较，选大的作为父节点，即可得到最大的根节点的二叉树，即大顶堆</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20191226140326.png" alt="大顶堆"></p><p>然后，将堆顶拿下来，将叶子节点放上去，再次按照大顶堆方法，得到根节点，以此类推，最终得到有序数列</p><p>时间复杂度：O(nlog2^n)</p><h6 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h6><p>对数组递归折半分割，直到分割成单个，然后递归合并比较，直到合并为整个数组</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20191226142419.png" alt="归并排序"></p><p>时间复杂度：O(nlog2^n)</p><p>总结：时间复杂度是nlog2^n的是：快些(希)归队(堆)，不稳定的是：快些(希)选队(堆)</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;几种排序算法的比较，对原始序列：6，3，5，7，2，9，8，1，4  进行排序&lt;/p&gt;&lt;h6 id=&quot;冒泡排序&quot;&gt;&lt;a href=&quot;#冒泡排序&quot; class=&quot;headerlink&quot; title=&quot;冒泡排序&quot;&gt;&lt;/a&gt;冒泡排序&lt;/h6&gt;&lt;p&gt;第一次：从第一个数字开始，依次相邻比较，前面大于后面则调换位置，比到最后一位，则选出了最大值放在最后&lt;/p&gt;&lt;p&gt;​        第一次排序后：3，5，6，2，7，8，1，4，9&lt;/p&gt;&lt;p&gt;​        第二次排序后：3，5，2，6，7，1，4，8，9&lt;/p&gt;
    
    </summary>
    
    
      <category term="算法" scheme="https://www.xiapf.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="https://www.xiapf.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="排序" scheme="https://www.xiapf.com/tags/%E6%8E%92%E5%BA%8F/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://www.xiapf.com/blogs/hello-world/"/>
    <id>https://www.xiapf.com/blogs/hello-world/</id>
    <published>2019-12-13T12:52:51.111Z</published>
    <updated>2019-12-13T12:52:51.111Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external nofollow noopener noreferrer">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external nofollow noopener noreferrer">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external nofollow noopener noreferrer">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external nofollow noopener noreferrer">GitHub</a>.</p><a id="more"></a><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external nofollow noopener noreferrer">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external nofollow noopener noreferrer">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external nofollow noopener noreferrer">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="external nofollow noopener noreferrer">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;documentation&lt;/a&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;a href=&quot;https://hexo.io/docs/troubleshooting.html&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;troubleshooting&lt;/a&gt; or you can ask me on &lt;a href=&quot;https://github.com/hexojs/hexo/issues&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>hexo部署时用户名密码设置</title>
    <link href="https://www.xiapf.com/blogs/hexo-deploy/"/>
    <id>https://www.xiapf.com/blogs/hexo-deploy/</id>
    <published>2019-12-13T06:48:37.000Z</published>
    <updated>2019-12-13T12:52:51.112Z</updated>
    
    <content type="html"><![CDATA[<p>hexo部署时用户名密码问题：</p><p>记住用户名密码，不用每次都输入用户名密码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --global credential.helper store</span><br></pre></td></tr></table></figure><p>清除用户名密码，防止用户名密码输入错误</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --system --<span class="built_in">unset</span> credential.helper</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;hexo部署时用户名密码问题：&lt;/p&gt;&lt;p&gt;记住用户名密码，不用每次都输入用户名密码&lt;/p&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/spa
      
    
    </summary>
    
    
      <category term="hexo" scheme="https://www.xiapf.com/categories/hexo/"/>
    
    
      <category term="hexo" scheme="https://www.xiapf.com/tags/hexo/"/>
    
      <category term="git" scheme="https://www.xiapf.com/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>docker简易教程</title>
    <link href="https://www.xiapf.com/blogs/docker/"/>
    <id>https://www.xiapf.com/blogs/docker/</id>
    <published>2019-12-12T05:44:29.000Z</published>
    <updated>2019-12-13T12:52:51.111Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Docker：虚拟化容器技术"><a href="#Docker：虚拟化容器技术" class="headerlink" title="Docker：虚拟化容器技术"></a>Docker：虚拟化容器技术</h2><p>三大组件：镜像，容器，仓库</p><pre><code>镜像：想象成系统iso或者ghost镜像容器：想象成一个系统环境仓库：想象成GitHub</code></pre><p>优点：隔离性   便捷性–移植和集群     轻量级   云支持</p><h2 id="安装Docker"><a href="#安装Docker" class="headerlink" title="安装Docker"></a>安装Docker</h2><p>Cenos7下安装docker</p><p>1.把yum包更新到最新</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum update</span><br></pre></td></tr></table></figure><a id="more"></a><p>2.安装需要的软件包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y yum-utils device-mapper-persistent-data lvm2</span><br></pre></td></tr></table></figure><p>3.设置yum源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo</span><br></pre></td></tr></table></figure><p>4.可以查看所有仓库中所有docker版本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum list docker-ce --showduplicates | sort -r</span><br></pre></td></tr></table></figure><p>5.安装Docker</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install docker-ce-17.12.1.ce</span><br></pre></td></tr></table></figure><p>6.启动Docker，加入开机启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl start docker</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> docker</span><br></pre></td></tr></table></figure><p>7.验证安装是否成功</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker version</span><br></pre></td></tr></table></figure><h2 id="编辑容器"><a href="#编辑容器" class="headerlink" title="编辑容器"></a>编辑容器</h2><p>从仓库中拉去一个centos7的镜像 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">doucker pull centos:7</span><br></pre></td></tr></table></figure><p>启动镜像生成容器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -i -t &lt;IMAGE ID&gt; bash</span><br></pre></td></tr></table></figure><p>进入容器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it &lt;CONTAINER ID&gt; bash</span><br></pre></td></tr></table></figure><p>此时就是进入了centos7的linux系统，可以在里安装jdk,tomcat等，然后部署war后，执行</p><p><code>curl localhost:端口号</code>    看看是否可以访问。</p><p>为了可以启动容器的时候，自动启动tomcat，可以写一个脚本，启动容器的时候，启动此脚本。<br>例如我在最根目录创建了runtomcat.sh的脚本<br>脚本内容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/soft/jdk1.5.0_22</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$PATH</span></span><br><span class="line">sh /soft/apache-tomcat-5.5.25/bin/catalina.sh run</span><br></pre></td></tr></table></figure><h2 id="制作镜像"><a href="#制作镜像" class="headerlink" title="制作镜像"></a>制作镜像</h2><p>将原来的容器制成镜像–docker commit containerid new_image:tag</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker commit centos7_base centos:7.1</span><br></pre></td></tr></table></figure><p>使用新的镜像制成容器–docker run -d -p 主机端口:容器端口/tcp –name 容器名 镜像名:tag  </p><p>并执行docker内脚本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -p 18080:8080 --name centos7_1 centos:7.1 /runtomcat.sh</span><br></pre></td></tr></table></figure><p>宿主机端口是18080映射了容器内的8080tomcat端口，现在直接访问宿主机的ip:18080/cvbs就可以访问容器的web服务了。<br>查看docker运行情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker ps -a</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20191212144010.png" alt="查看docker运行情况"></p><p>可以看到此时新的容器已经启动了</p><p>再次制成一个容器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -p 28080:8080 --name centos7_2 centos:7.1 /runtomcat.sh</span><br></pre></td></tr></table></figure><p>查看docker运行情况–<code>docker ps –a</code></p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20191212144100.png" alt="查看docker运行情况"></p><p>此时可以看到已经部署了两个服务了，分别映射在宿主机的18080和28080端口</p><p>外部浏览器访问：</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20191212144117.png" alt="浏览器双端口访问"></p><p>可以看到两个端口都可以访问了</p><p>因为我是在虚拟机里安装，所以局域网中其他机器无法访问到此ip，</p><p>所以为了省略配置虚拟机对外暴露的ip等步骤，直接在主机中通过配置nginx做代理，配置如下</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20191212144141.png" alt="nginx配置"></p><p>此时外部访问即可<code>http://10.45.12.120/cvbs</code>（我的局域网内ip）即可访问到docker中部署两个的应用。</p><p>注意：因为有些项目没有做分布式session处理，所以简单使用ip_hash策略解决session问题</p><h2 id="IDEA中配置docker"><a href="#IDEA中配置docker" class="headerlink" title="IDEA中配置docker"></a>IDEA中配置docker</h2><p>1.在根目录写Dockerfile文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#指定基础镜像，在其上进行定制FROM java:8</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#这里的 /tmp 目录就会在运行时自动挂载为匿名卷，任何向 /data 中写入的信息都不会记录进容器存储层</span></span><br><span class="line">VOLUME /tmp</span><br><span class="line"></span><br><span class="line"><span class="comment">#复制上下文目录下的target/demo-1.0.0.jar 到容器里</span></span><br><span class="line">ADD target/spring-boot-0.0.1-SNAPSHOT.jar test.jar</span><br><span class="line"></span><br><span class="line"><span class="comment">#bash方式执行，使test.jar可访问</span></span><br><span class="line"><span class="comment">#RUN新建立一层，在其上执行这些命令，执行结束后， commit 这一层的修改，构成新的镜像。</span></span><br><span class="line">RUN bash -c <span class="string">"touch /test.jar"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#声明运行时容器提供服务端口，这只是一个声明，在运行时并不会因为这个声明应用就会开启这个端口的服务</span></span><br><span class="line">EXPOSE 8088</span><br><span class="line"></span><br><span class="line"><span class="comment">#指定容器启动程序及参数   &lt;ENTRYPOINT&gt; "&lt;CMD&gt;"</span></span><br><span class="line">ENTRYPOINT [<span class="string">"java"</span>,<span class="string">"-jar"</span>,<span class="string">"test.jar"</span>]</span><br></pre></td></tr></table></figure><p>2.在启动配置中配置dockerfile,image,container,port(宿主机和docker的端口映射)</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20191212144221.png" alt="配置dockerfile"></p><p>3.Maven打包</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20191212144237.png" alt="maven打包"></p><p>4.Docker部署</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20191212144250.png" alt="docker部署"></p><p>5.启动Docker</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20191212144304.png" alt="启动docker"></p><p>6.查看docker运行情况</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20191212144319.png" alt="查看docker运行情况3"></p><p>7.浏览器访问成功</p><p><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/20191212144335.png" alt="浏览器访问"></p><h2 id="k8s"><a href="#k8s" class="headerlink" title="k8s"></a>k8s</h2><p>k8s是一个开源的容器集群管理系统，可以实现容器集群的自动化部署、自动扩缩容、维护等功能。</p><p>可以对各种虚拟化容器技术做统一的管理</p><p>实际项目中大量用到docker时，可以在研究k8s的实际使用。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Docker：虚拟化容器技术&quot;&gt;&lt;a href=&quot;#Docker：虚拟化容器技术&quot; class=&quot;headerlink&quot; title=&quot;Docker：虚拟化容器技术&quot;&gt;&lt;/a&gt;Docker：虚拟化容器技术&lt;/h2&gt;&lt;p&gt;三大组件：镜像，容器，仓库&lt;/p&gt;&lt;pre&gt;&lt;code&gt;镜像：想象成系统iso或者ghost镜像

容器：想象成一个系统环境

仓库：想象成GitHub&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;优点：隔离性   便捷性–移植和集群     轻量级   云支持&lt;/p&gt;&lt;h2 id=&quot;安装Docker&quot;&gt;&lt;a href=&quot;#安装Docker&quot; class=&quot;headerlink&quot; title=&quot;安装Docker&quot;&gt;&lt;/a&gt;安装Docker&lt;/h2&gt;&lt;p&gt;Cenos7下安装docker&lt;/p&gt;&lt;p&gt;1.把yum包更新到最新&lt;/p&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;yum update&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="docker" scheme="https://www.xiapf.com/categories/docker/"/>
    
    
      <category term="docker" scheme="https://www.xiapf.com/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>future与futuretask用法区别</title>
    <link href="https://www.xiapf.com/blogs/future-futuretask/"/>
    <id>https://www.xiapf.com/blogs/future-futuretask/</id>
    <published>2019-12-12T05:32:37.000Z</published>
    <updated>2019-12-13T12:52:51.111Z</updated>
    
    <content type="html"><![CDATA[<h6 id="Future和FutureTask方法区别，主要在于获取返回结果上。"><a href="#Future和FutureTask方法区别，主要在于获取返回结果上。" class="headerlink" title="Future和FutureTask方法区别，主要在于获取返回结果上。"></a>Future和FutureTask方法区别，主要在于获取返回结果上。</h6><p>Future方式：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.concurrent.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test7</span> <span class="keyword">implements</span> <span class="title">Callable</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"1"</span>);</span><br><span class="line">        ExecutorService executor = <span class="keyword">new</span> ThreadPoolExecutor(<span class="number">3</span>,<span class="number">3</span>,<span class="number">0</span>,TimeUnit.SECONDS,<span class="keyword">new</span> LinkedBlockingQueue&lt;Runnable&gt;());</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Future&lt;?&gt; submit = executor.submit(<span class="keyword">new</span> Test7());</span><br><span class="line">            Object o = submit.get();</span><br><span class="line">            System.out.println(o);</span><br><span class="line">        &#125;<span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            System.out.println(<span class="number">3</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        executor.shutdown();</span><br><span class="line">        System.out.println(<span class="string">"4"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">test</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span> i = <span class="number">1</span>/<span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">call</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        test();</span><br><span class="line">        System.out.println(<span class="number">5</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Integer(<span class="number">6</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><a id="more"></a><p>submit里放的是实现callable接口的类，通过返回值submit.get()，获取返回结果6，如果test抛出异常，此时，主线程会捕获，打印3，不调用submit.get()，将不会捕获异常。</p><p>FutureTask方式：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.concurrent.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test8</span> <span class="keyword">implements</span> <span class="title">Callable</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"1"</span>);</span><br><span class="line">        FutureTask ft = <span class="keyword">new</span> FutureTask(<span class="keyword">new</span> Test8());</span><br><span class="line">        ExecutorService executor = <span class="keyword">new</span> ThreadPoolExecutor(<span class="number">3</span>,<span class="number">3</span>,<span class="number">0</span>,TimeUnit.SECONDS,<span class="keyword">new</span> LinkedBlockingQueue&lt;Runnable&gt;());</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Future&lt;?&gt; submit = executor.submit(ft);</span><br><span class="line">            Object o = submit.get();</span><br><span class="line">            System.out.println(o);</span><br><span class="line">            Object o1 = ft.get();</span><br><span class="line">            System.out.println(o1);</span><br><span class="line">        &#125;<span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            System.out.println(<span class="number">3</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        executor.shutdown();</span><br><span class="line">        System.out.println(<span class="string">"4"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">test</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span> i = <span class="number">1</span>/<span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">call</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        test();</span><br><span class="line">        System.out.println(<span class="number">5</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Integer(<span class="number">6</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>executor里放的是futuretask，futuretask包装了一层callable对象，此时要获取返回结果需要使用futuretask的get()方法，不能使用submit.get(),submit.get()会返回null;只用ft.get()会捕获异常。</p>]]></content>
    
    <summary type="html">
    
      &lt;h6 id=&quot;Future和FutureTask方法区别，主要在于获取返回结果上。&quot;&gt;&lt;a href=&quot;#Future和FutureTask方法区别，主要在于获取返回结果上。&quot; class=&quot;headerlink&quot; title=&quot;Future和FutureTask方法区别，主要在于获取返回结果上。&quot;&gt;&lt;/a&gt;Future和FutureTask方法区别，主要在于获取返回结果上。&lt;/h6&gt;&lt;p&gt;Future方式：&lt;/p&gt;&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; java.util.concurrent.*;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Test7&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;implements&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Callable&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(String[] args)&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        System.out.println(&lt;span class=&quot;string&quot;&gt;&quot;1&quot;&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ExecutorService executor = &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; ThreadPoolExecutor(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;,TimeUnit.SECONDS,&lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; LinkedBlockingQueue&amp;lt;Runnable&amp;gt;());&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;try&lt;/span&gt; &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            Future&amp;lt;?&amp;gt; submit = executor.submit(&lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; Test7());&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            Object o = submit.get();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            System.out.println(o);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &amp;#125;&lt;span class=&quot;keyword&quot;&gt;catch&lt;/span&gt; (Exception e) &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            System.out.println(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        executor.shutdown();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        System.out.println(&lt;span class=&quot;string&quot;&gt;&quot;4&quot;&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt;&lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; i = &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;/&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; Object &lt;span class=&quot;title&quot;&gt;call&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;throws&lt;/span&gt; Exception &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        test();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        System.out.println(&lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; Integer(&lt;span class=&quot;number&quot;&gt;6&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="java" scheme="https://www.xiapf.com/categories/java/"/>
    
    
      <category term="java" scheme="https://www.xiapf.com/tags/java/"/>
    
      <category term="多线程" scheme="https://www.xiapf.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"/>
    
      <category term="future" scheme="https://www.xiapf.com/tags/future/"/>
    
  </entry>
  
  <entry>
    <title>用bp网络预测绿萝叶片面积</title>
    <link href="https://www.xiapf.com/blogs/bp-net/"/>
    <id>https://www.xiapf.com/blogs/bp-net/</id>
    <published>2019-12-11T02:46:04.000Z</published>
    <updated>2020-01-20T12:17:50.704Z</updated>
    
    <content type="html"><![CDATA[<h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><blockquote><p>测量绿萝叶片使用kinect相机对图像做标定，将图像映射到坐标系中，使用matlab程序计算叶片整体占图像的像素从而计算面积，对绿萝每层的叶片都需要分次进行标定求解过程较为复杂，因此针对这种情况，采用bp网络来预测绿萝叶片面积。</p></blockquote><hr><h1 id="数据采集"><a href="#数据采集" class="headerlink" title="数据采集"></a>数据采集</h1><p><em>测量9盆绿萝叶片，实测数据共557片。训练集1-7盆，共449片，测试集8-9盆，共108片。<br>数据集中共四个属性，分别是宽度为w，长边为L1,短边为L2，面积area。</em><br><strong>其中宽度，长边，短边为手动实测，面积根据叶片像素点占图片整体像素点计算得出。</strong></p><a id="more"></a><ul><li>计算面积的MATLAB程序<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">%% 分割</span><br><span class="line">k=graythresh(I);              %得到最优阈值</span><br><span class="line">j=im2bw(I,k);                  %转换成二值图，k为分割阈值</span><br><span class="line">%imshow(j); </span><br><span class="line">f = bwmorph(j,&apos;open&apos;);  %开运算</span><br><span class="line">figure, imshow(f)</span><br><span class="line">%% 像素点统计</span><br><span class="line">[m,n]=size(f);</span><br><span class="line">k=0;</span><br><span class="line">for i=1:1:m</span><br><span class="line">    for j=1:1:n</span><br><span class="line">        if f(i,j)==0</span><br><span class="line">            k=k+1;</span><br><span class="line">        end</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line">p=k/(m*n);</span><br><span class="line">s=21*29.7*p;</span><br></pre></td></tr></table></figure></li></ul><hr><h1 id="bp网络设计"><a href="#bp网络设计" class="headerlink" title="bp网络设计"></a>bp网络设计</h1><p>bp网络示意图及各变量含义<br><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/bp%E7%BD%91%E7%BB%9C%E7%A4%BA%E6%84%8F%E5%9B%BE%E5%8F%8A%E5%90%84%E5%8F%98%E9%87%8F%E5%90%AB%E4%B9%89.png" alt="bp结构图片"></p><ol><li><p>结构选取<br>a.隐含层数量选取：单隐层结构选择，当仅有一层隐含层时，测试数据正确率已达100%，为降低网络复杂度，所以选择单隐层结构。<br>b.隐含层神经元个数选取</p><table><thead><tr><th align="left">隐含层神经元个数</th><th align="left">达到最小误差时需要迭代的次数</th></tr></thead><tbody><tr><td align="left">5</td><td align="left">1873</td></tr><tr><td align="left">10</td><td align="left">2009</td></tr><tr><td align="left">15</td><td align="left">2259</td></tr></tbody></table><p>根据程序运行来看，当隐藏层神经元个数选择过小（小于5）收敛速度很慢，当到达最大迭代次数时，误差仍很大，当个数选择过大（大于15），网络出现振荡，因此，考虑隐藏层神经元个数在5-15之间，因为本实验数据集属性少，因此选择隐藏层神经元个数为5个，减少复杂度。<br> c.误差函数选择均方差公式：$[    {E_{k} } =1/2*( \hat{y}-y) ]$<br> d.结束条件：<br> 训练结束条件：当误差函数值小于1e-3或者迭代次数大于50000次时结束训练<br> 测试误差判断：当测试集输出面积和实际面积误差函数值大于1e-5时，预测错误，反之预测正确。</p></li><li><p>初始值设置<br>a.权值矩阵的初始值<br>产生0-1之间的随机数作为权值的初始值<br>代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">srand(time(NULL));//设置随机数种子，使每次产生的随机序列不同</span><br><span class="line">for (int i = 0; i &lt; n; i++)</span><br><span class="line">    w[i] = rand() % (N + 1) / (float)(N + 1);//N为设置的精度</span><br></pre></td></tr></table></figure><p>b.学习率的初始值<br>学习率控制着算法每一轮的迭代的更新步长，若太大则容易振荡，太小则收敛速度又过慢因此需要选择适合的初始值。<br><img src="https://cdn.jsdelivr.net/gh/iamxpf/pageImage/images/bp%E5%AD%A6%E4%B9%A0%E7%8E%87%E5%8F%98%E5%8C%96.png" alt="学习率变化图片"><br>由图可以看出，当学习率小于1时，网络训练次数多集中在1000以上，但当学习率大于5时，测试集数据会出现错误，因此选择训练次数少并且误差低的学习率=5.</p></li><li><p>数据归一化<br>前三列属性取值∈[2,9]，因此采用对数函数y=log10(x) 以10为底的对数函数转换。<br>最后一列属性取值∈[10,100]，因此采用反余弦函数y=atan(x)*2/PI，保证输入的数据在0-1之间，让网络更快的收敛。</p></li><li><p>前向传播和反向传播<br>a.前向传播<br>激活函数选择sigmod函数，第i个输入层神经元到第h个隐藏层神经元的权值为Vih,第h个隐藏层神经元输入为α=∑Vih<em>Xi，输出为bh;第h个输入层神经元到第j个隐藏层神经元的权值为Whj,第j个输出层神经元的输入β=∑Whj</em>bh,输出为<br>求出激活值，代入sigmod函数中，求得输出值<br>代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">for(int j=0;j&lt;hideSum;j++)</span><br><span class="line">&#123;</span><br><span class="line">   o1[j]=0.0;</span><br><span class="line">   for(int i=0;i&lt;inSum;i++)</span><br><span class="line">   //激活值</span><br><span class="line">   o1[j]=o1[j]+w[i][j]*x[i];</span><br><span class="line">   //实际输出</span><br><span class="line">   x1[j]=1.0/(1+exp(-o1[j]-b1[j])); //b1为偏置量</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>b.反向传播<br>网络在（Xk,Yk)上的均方差为，BP算法基于梯度下降的策略，以目标的负梯度方向进行调整在学习率下，采用链式法则对权值进行更新：<br>从输出层到隐藏层，有:<br>从隐藏层到输入层，有:</p></li></ol><ul><li>算法描述:<blockquote><p>在（0，1）范围内初始化权值和学习率<br>REPEAT<br>FOR all（Xk,Yk)：<br>输入正向传播公式计算输出<br>计算输出层需修改的梯度项gj<br>计算隐藏层层需修改的梯度项eh<br>根据公式更新权值wjh,vij<br>END FOR<br>UNTIL 达到停止条件</p></blockquote></li></ul><p>标准BP算法：上述算法是对每个样本更新权值，属于标准BP算法，参数更新的频繁<br>累积BP算法：当读取完所有样本之后才更新参数，参数更新的频率低<br>区别：累积BP算法在误差下降到一定阶段，下降回非常缓慢，所以往往标准BP算法能更快得到较好解。<br><em>使用标准BP算法：对每个样本更新权值</em></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">//输出层</span><br><span class="line">for(int k=0;k&lt;outSum;k++)</span><br><span class="line">&#123;</span><br><span class="line">    //与实际输出的偏差</span><br><span class="line">    qq[k]=(yd[k]-x2[k])*x2[k]*(1-x2[k]);</span><br><span class="line">    for (int j = 0; j&lt; hideSum; ++j)</span><br><span class="line">        //调整隐藏层到输出层之间的连接权</span><br><span class="line">        w1[j][k]+=rate_w1*qq[k]*x1[j];</span><br><span class="line">&#125;</span><br><span class="line">//隐藏层</span><br><span class="line">for(int j=0;j&lt;hideSum;j++)</span><br><span class="line">&#123;</span><br><span class="line">    pp[j]=0;</span><br><span class="line">    //隐藏层的偏差</span><br><span class="line">    for (int k = 0; k&lt; outSum; k++)</span><br><span class="line">        pp[j]=pp[j]+qq[k]*w1[j][k];  //隐藏层的偏差和后面所有连接的对应输出层都有关系</span><br><span class="line">    pp[j]=pp[j]*x1[j]*(1-x1[j]);</span><br><span class="line">   for (int i = 0; i &lt; inSum; ++i)</span><br><span class="line">        //调整输入层到隐藏层之间的连接权</span><br><span class="line">        w[i][j]+=rate_w*pp[j]*x[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><em>使用累积BP算法：遍历完所有样本再更新权值<br>增加权值修改矩阵，将误差进行累加</em></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">//输出层</span><br><span class="line">for(int k=0;k&lt;outSum;k++)</span><br><span class="line">&#123;</span><br><span class="line">    //与实际输出的偏差</span><br><span class="line">    qq[k]=(yd[k]-x2[k])*x2[k]*(1-x2[k]);</span><br><span class="line">    for (int j = 0; j&lt; hideSum; ++j)</span><br><span class="line">    &#123;</span><br><span class="line">        //调整隐藏层到输出层之间的连接权</span><br><span class="line">        chg_w2[j][k]+=rate_w1*qq[k]*x1[j];</span><br><span class="line">        chg_b2[k]+=rate_b2*qq[k];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">//隐藏层</span><br><span class="line">for(int j=0;j&lt;hideSum;j++)</span><br><span class="line">&#123;</span><br><span class="line">    pp[j]=0;</span><br><span class="line">    //隐藏层的偏差</span><br><span class="line">    for (int k = 0; k&lt; outSum; k++)</span><br><span class="line">    &#123;</span><br><span class="line">        pp[j]=pp[j]+qq[k]*w1[j][k];  //隐藏层的偏差和后面所有连接的对应输出层都有关系</span><br><span class="line">    &#125;</span><br><span class="line">    pp[j]=pp[j]*x1[j]*(1-x1[j]);</span><br><span class="line">   for (int i = 0; i &lt; inSum; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        //调整输入层到隐藏层之间的连接权</span><br><span class="line">        chg_w1[i][j]=chg_w1[i][j]+rate_w1*pp[j]*x[i];</span><br><span class="line">        chg_b1[j]+=rate_b1*pp[j];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h1 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h1><table><thead><tr><th align="left">对比</th><th align="left">标准BP算法</th><th align="left">累积BP算法</th></tr></thead><tbody><tr><td align="left">迭代次数</td><td align="left">548</td><td align="left">348</td></tr><tr><td align="left">训练平均误差</td><td align="left">小于1e-5</td><td align="left">小于1e-5</td></tr><tr><td align="left">识别结果</td><td align="left">100%</td><td align="left">100%</td></tr></tbody></table><p>使用标准BP算法和累积BP算法识别率都能达到100%，但是因数据量小，使用累积BP 算法训练时间更短。</p><hr><h1 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h1><ul><li>如果将长边短边合成一条边，效果还是一样吗？<br>将数据集中长边L1和短边L2相加得到叶子的长为L，输入的属性列变为长L,宽W,面积area<br>网络结构采取上述结构，训练效果如下图：<table><thead><tr><th align="left">对比</th><th align="left">标准BP算法</th><th align="left">累积BP算法</th></tr></thead><tbody><tr><td align="left">迭代次数</td><td align="left">566</td><td align="left">717</td></tr><tr><td align="left">训练平均误差</td><td align="left">小于1e-5</td><td align="left">小于1e-5</td></tr><tr><td align="left">识别结果</td><td align="left">94.5%</td><td align="left">97.3%</td></tr></tbody></table></li></ul><p>因实验所用的叶片有破损存在，直接用长边加上短边得到的叶片长度存在一定误差，因此使用BP算法时，识别结果略有下降.<br>使用标准BP算法和累积BP算法识别率都能达到100%，但是因数据量小，使用累积BP 算法训练时间更短。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;问题描述&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;测量绿萝叶片使用kinect相机对图像做标定，将图像映射到坐标系中，使用matlab程序计算叶片整体占图像的像素从而计算面积，对绿萝每层的叶片都需要分次进行标定求解过程较为复杂，因此针对这种情况，采用bp网络来预测绿萝叶片面积。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;&lt;h1 id=&quot;数据采集&quot;&gt;&lt;a href=&quot;#数据采集&quot; class=&quot;headerlink&quot; title=&quot;数据采集&quot;&gt;&lt;/a&gt;数据采集&lt;/h1&gt;&lt;p&gt;&lt;em&gt;测量9盆绿萝叶片，实测数据共557片。训练集1-7盆，共449片，测试集8-9盆，共108片。&lt;br&gt;数据集中共四个属性，分别是宽度为w，长边为L1,短边为L2，面积area。&lt;/em&gt;&lt;br&gt;&lt;strong&gt;其中宽度，长边，短边为手动实测，面积根据叶片像素点占图片整体像素点计算得出。&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://www.xiapf.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="神经网络" scheme="https://www.xiapf.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="算法" scheme="https://www.xiapf.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
</feed>
